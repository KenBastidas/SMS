TY  - JOUR
TI  - Serverless on Machine Learning: A Systematic Mapping Study
AU  - Barrak, Amine
AU  - Petrillo, Fabio
AU  - Jaafar, Fehmi
T2  - IEEE Access
AB  - Machine Learning Operations (MLOps) is an approach to managing the entire lifecycle of a machine learning model. It has evolved over the last years and has started attracting many people in research and businesses in the industry. It supports the development of machine learning (ML) pipelines typical in the phases of data collection, data pre-processing, building datasets, model training, hyper-parameters refinement, testing, and deployment to production. This complex pipeline workflow is a tedious process of iterative experimentation. Moreover, cloud computing services provide advanced features for managing ML stages and deploying them efficiently to production. Specifically, serverless computing has been applied in different stages of the machine learning pipeline. However, to the best of our knowledge, it is missing to know the serverless suitability and benefits it can provide to the ML pipeline. In this paper, we provide a systematic mapping study of machine learning systems applied on serverless architecture that include 53 relevant studies. During this study, we focused on (1) exploring the evolution trend and the main venues; (2) determining the researchers’ focus and interest in using serverless on machine learning; (3) discussing solutions that serverless computing provides to machine learning. Our results show that serverless usage is growing, and several venues are interested in the topic. In addition, we found that the most widely used serverless provider is AWS Lambda, where the primary application was used in the deployment of the ML model. Additionally, several challenges were explored, such as reducing cost, resource scalability, and reducing latency. We also discuss the potential challenges of adopting ML on serverless, such as respecting service level agreement, the cold start problem, security, and privacy. Finally, our contribution provides foundations for future research and applications that involve machine learning in serverless computing.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3206366
DP  - IEEE Xplore
VL  - 10
SP  - 99337
EP  - 99352
J2  - IEEE Access
SN  - 2169-3536
ST  - Serverless on Machine Learning
UR  - https://ieeexplore.ieee.org/document/9888122
L1  - https://ieeexplore.ieee.org/ielx7/6287639/9668973/09888122.pdf?tp=&arnumber=9888122&isnumber=9668973&ref=
L2  - https://ieeexplore.ieee.org/document/9888122
KW  - Cloud computing
KW  - Computational modeling
KW  - Serverless computing
KW  - Computer architecture
KW  - Data models
KW  - FaaS
KW  - function as a service
KW  - machine learning
KW  - Machine learning
KW  - Serverless
KW  - SLR
KW  - SM
KW  - systematic literature review
KW  - systematic mapping
ER  - 

TY  - JOUR
TI  - IoT Serverless Computing at the Edge: A Systematic Mapping Review
AU  - Kjorveziroski, Vojdan
AU  - Filiposka, Sonja
AU  - Trajkovik, Vladimir
T2  - Computers
AB  - Serverless computing is a new concept allowing developers to focus on the core functionality of their code, while abstracting away the underlying infrastructure. Even though there are existing commercial serverless cloud providers and open-source solutions, dealing with the explosive growth of new Internet of Things (IoT) devices requires more efficient bandwidth utilization, reduced latency, and data preprocessing closer to the source, thus reducing the overall data volume and meeting privacy regulations. Moving serverless computing to the edge of the network is a topic that is actively being researched with the aim of solving these issues. This study presents a systematic mapping review of current progress made to this effect, analyzing work published between 1 January 2015 and 1 September 2021. Using a document selection methodology which emphasizes the quality of the papers obtained through querying several popular databases with relevant search terms, we have included 64 entries, which we then further categorized into eight main categories. Results show that there is an increasing interest in this area with rapid progress being made to solve the remaining open issues, which have also been summarized in this paper. Special attention is paid to open-source efforts, as well as open-access contributions.
DA  - 2021/10//
PY  - 2021
DO  - 10.3390/computers10100130
DP  - www.mdpi.com
VL  - 10
IS  - 10
SP  - 130
J2  - Scopus
LA  - en
SN  - 2073-431X
ST  - IoT Serverless Computing at the Edge
UR  - https://www.mdpi.com/2073-431X/10/10/130
Y2  - 2023/06/15/04:55:37
L1  - https://www.mdpi.com/2073-431X/10/10/130/pdf?version=1634542435
KW  - Internet of Things
KW  - serverless computing
KW  - function as a service
KW  - edge computing
KW  - systematic review
ER  - 

TY  - CONF
TI  - Behavior Analysis using Serverless Machine Learning
AU  - D. Damkevala
AU  - R. Lunavara
AU  - M. Kosamkar
AU  - S. Jayachandran
T2  - 2019 6th International Conference on Computing for Sustainable Global Development (INDIACom)
AB  - This paper supplies a route for using the Watson Machine Learning API on IBM Cloud to carry out serverless data analytics using machine learning as a service. Transforming the large amount of data produced by an organization into intelligence can be done using advanced analytics methods such as using a modified Mahalanobis Distance algorithm for synthesis of correlation data under the purview of machine learning. Further refinement of correlation data is done using a Multivariate Reliability Classifier model. The consumption of this advanced analytics service can be done in a serverless manner where the developer only must be concerned with how the data is analyzed, i.e., scoring, batch or stream models with a continuous learning system without the outlay of hardware upon which to train those models. This paper examines the usage of such serverless AI systems in the scope of user behavior analysis over varied demographics.
C3  - 2019 6th International Conference on Computing for Sustainable Global Development (INDIACom)
DA  - 2019/03/13/15
PY  - 2019
DP  - IEEE Xplore
SP  - 1068
EP  - 1072
UR  - https://ieeexplore.ieee.org/document/8991407
ER  - 

TY  - CONF
TI  - The case for serverless mobile networking
AU  - M. Gramaglia
AU  - P. Serrano
AU  - A. Banchs
AU  - G. Garcia-Aviles
AU  - A. Garcia-Saavedra
AU  - R. Perez
T2  - 2020 IFIP Networking Conference (Networking)
AB  - The softwarization of communication networks provides notable benefits, such as flexibility, improved resource efficiency, and commoditization. In exchange, softwarization requires an increased management overhead and the need to re-design network operation. While the mobile networking ecosystem is currently adapting this new paradigm with other network-related aspects (e.g., network slicing), cloud computing already addressed such problems with the introduction of serverless architectures, also known as Function as a Service (FaaS). With this approach, the software is decomposed into its minimum building blocks, i.e., functions, maximizing scalability, resource efficiency, and flexibility. In this paper, we analyze the potential adoption of the FaaS paradigm by the mobile networking ecosystem, discussing the implicit advantages, the challenges to address, and some solutions to overcome them.
C3  - 2020 IFIP Networking Conference (Networking)
DA  - 2020/06/22/26
PY  - 2020
SP  - 779
EP  - 784
ER  - 

TY  - CONF
TI  - Nimbus: Improving the Developer Experience for Serverless Applications
AU  - R. Chatley
AU  - T. Allerton
T2  - 2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
AB  - We present Nimbus, a framework for writing and deploying Java applications on a Function-as-a-Service ("serverless") platform. Nimbus aims to soothe four main pain points experienced by developers working on serverless applications: that testing can be difficult, that deployment can be a slow and painful process, that it is challenging to avoid vendor lock-in, and that long cold start times can introduce unwelcome latency to function invocations. Nimbus provides a number of features that aim to overcome these challenges when working with serverless applications. It uses an annotation-based configuration to avoid having to work with large configuration files. It aims to allow the code written to be cloud-agnostic. It provides an environment for local testing where the complete application can be run locally before deployment. Lastly, Nimbus provides mechanisms for optimising the contents and size of the artifacts that are deployed to the cloud, which helps to reduce both deployment times and cold start times.
C3  - 2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
DA  - 2020/10/05/11
PY  - 2020
SP  - 85
EP  - 88
SN  - 2574-1926
ER  - 

TY  - CONF
TI  - Fast Edge-to-Edge Serverless Migration in 5G Programmable Packet-Optical Networks
AU  - I. Pelle
AU  - F. Paolucci
AU  - B. Sonkoly
AU  - F. Cugini
T2  - 2021 Optical Fiber Communications Conference and Exhibition (OFC)
AB  - Ultra-low latency serverless applications are dynamically deployed and migrated between edge computing nodes in less than 10 ms, leveraging comprehensive telemetry data retrieved from programmable packet-optical 5G x-haul.
C3  - 2021 Optical Fiber Communications Conference and Exhibition (OFC)
DA  - 2021/06/06/10
PY  - 2021
SP  - 1
EP  - 3
ER  - 

TY  - CONF
TI  - DevOps Practices in Digital Library Development
AU  - Y. Chen
T2  - 2022 ACM/IEEE Joint Conference on Digital Libraries (JCDL)
AB  - In this demonstration, we present how we apply DevOps practices to our digital library project development. We demonstrate a digital library platform, which is a cloud-native, serverless, and microservice digital libraries management system. All the services are open-source and publicly available on GitHub1. We also share our experiences and lessons learned about adopting the DevOps process.CCS CONCEPTS •Software and its engineering $\rightarrow$ Software design engineering; •Information systems $\rightarrow$ Digital libraries and archives.
C3  - 2022 ACM/IEEE Joint Conference on Digital Libraries (JCDL)
DA  - 2022/06/20/24
PY  - 2022
SP  - 1
EP  - 4
ER  - 

TY  - JOUR
TI  - QuickFaaS: Providing Portability and Interoperability between FaaS Platforms
AU  - Rodrigues, P.
AU  - Freitas, F.
AU  - Simão, J.
T2  - Future Internet
AB  - Serverless computing hides infrastructure management from developers and runs code on-demand automatically scaled and billed during the code’s execution time. One of the most popular serverless backend services is called Function-as-a-Service (FaaS), in which developers are often confronted with cloud-specific requirements. Function signature requirements, and the usage of custom libraries that are unique to cloud providers, were identified as the two main reasons for portability issues in FaaS applications, leading to various vendor lock-in problems. In this work, we define three cloud-agnostic models that compose FaaS platforms. Based on these models, we developed QuickFaaS, a multi-cloud interoperability desktop tool targeting cloud-agnostic functions and FaaS deployments. The proposed cloud-agnostic approach enables developers to reuse their serverless functions in different cloud providers with no need to change code or install extra software. We also provide an evaluation that validates the proposed solution by measuring the impact of a cloud-agnostic approach on the function’s performance, when compared to a cloud-non-agnostic one. The study shows that a cloud-agnostic approach does not significantly impact the function’s performance. © 2022 by the authors.
DA  - 2022///
PY  - 2022
DO  - 10.3390/fi14120360
VL  - 14
IS  - 12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144595365&doi=10.3390%2ffi14120360&partnerID=40&md5=d97ae3c4aa636d08cbb7c8b08b9b30e9
DB  - Scopus
L1  - https://www.mdpi.com/1999-5903/14/12/360/pdf?version=1669870772
KW  - Cloud computing
KW  - cloud computing
KW  - serverless computing
KW  - Serverless computing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - cloud interoperability
KW  - Cloud interoperability
KW  - cloud orchestration
KW  - Cloud orchestration
KW  - cloud-agnostic
KW  - Cloud-agnostic
KW  - Cloud-computing
KW  - Codes (symbols)
KW  - Computer software portability
KW  - Computer software reusability
KW  - FaaS portability
KW  - Function-as-a-service portability
KW  - Interoperability
KW  - Lock-in
KW  - Locks (fasteners)
KW  - Service platforms
KW  - vendor lock-in
KW  - Vendor lock-in
ER  - 

TY  - CONF
TI  - SeBS: A serverless benchmark suite for function-as-a-service computing
AU  - Copik, M.
AU  - Kwasniewski, G.
AU  - Besta, M.
AU  - Podstawski, M.
AU  - Hoefler, T.
T2  - Middleware 2021 - Proceedings of the 22nd International Middleware Conference
AB  - Function-as-a-Service (FaaS) is one of the most promising directions for the future of cloud services, and serverless functions have immediately become a new middleware for building scalable and cost-efficient microservices and appli cations. However, the quickly moving technology hinders reproducibility, and the lack of a standardized benchmarking suite leads to ad-hoc solutions and microbenchmarks being used in serverless research, further complicating meta-analysis and comparison of research solutions. To address this challenge, we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3464298.3476133
SP  - 64
EP  - 78
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117609008&doi=10.1145%2f3464298.3476133&partnerID=40&md5=2a8e7418f27a9e41e36fda10c0216425
DB  - Scopus
KW  - FaaS
KW  - Serverless
KW  - Function-as-a-service
KW  - serverless
KW  - benchmark
KW  - Benchmark
KW  - Benchmark suites
KW  - Benchmarking
KW  - Cloud services
KW  - Cost-efficient
KW  - Evaluation methodologies
KW  - Function evaluation
KW  - function-as-a-service
KW  - Middleware
KW  - Reproducibilities
KW  - Service computing
ER  - 

TY  - CONF
TI  - Transferring transactional business processes to FaaS
AU  - Meladakis, K.
AU  - Zeginis, C.
AU  - Magoutis, K.
AU  - Plexousakis, D.
T2  - WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022
AB  - Function-as-a-Service (FaaS) is a modern cloud service model that has gained significant attention from the research and industry communities in recent years for its many benefits such as dynamic scaling, cost efficiency, faster programming, flexibility to microservices and containers technology. However, the building and deployment of serverless applications come with many challenges that need to be tackled, like workflow design complexity and migration of other applications. When transactions between different parties are involved, the workflow becomes knotty and the communication between participants and all properties of transactions have to be properly resolved. Transactions have widely been discussed in Business processes, so same practices might be adopted by serverless workflows. In this work we provide guidelines and mapping mechanisms for transforming transactional Business Process Modeling Notation 2.0 (BPMN2) applications to a serverless platform. We shed light on the current inability of function orchestrators to express workflow definitions, and deal with various architectural dilemmas that stem from the dissimilar nature of stateful BPMN vs. stateless serverless applications. We overcome the unbalanced capabilities between well-established BPMN notations and function orchestration definitions and illustrate how to exploit and combine cloud native services that comes with FaaS to create serverless applications.  © 2022 ACM.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3565382.3565882
SP  - 25
EP  - 30
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145555314&doi=10.1145%2f3565382.3565882&partnerID=40&md5=35bdfced9355072b7473caa78f982a17
DB  - Scopus
KW  - FaaS
KW  - Function-as-a-service
KW  - transactions
KW  - BPMN2
KW  - Business Process
KW  - Business process modeling
KW  - Business process modeling notation 2.0
KW  - function orchestration
KW  - Function orchestration
KW  - Industrial research
KW  - Modeling notation
KW  - Openwhisk
KW  - OpenWhisk
KW  - serverless workflow
KW  - Serverless workflow
KW  - Transaction
KW  - Work-flows
ER  - 

TY  - CONF
TI  - Enhancing observability of serverless computing with the serverless application analytics framework
AU  - Cordingly, R.
AU  - Heydari, N.
AU  - Yu, H.
AU  - Hoang, V.
AU  - Sadeghi, Z.
AU  - Lloyd, W.
T2  - ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering
AB  - To improve the observability of workload performance, resource utilization, and infrastructure underlying serverless Function-as-a-Service (FaaS) platforms, we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can leverage to inspect performance, resource utilization, scalability, and infrastructure metrics of function deployments to commercial and open-source FaaS platforms. To automate reproducible FaaS performance experiments, we provide the FaaS Runner as a multithreaded FaaS client. FaaS Runner provides a programmable client that can orchestrate over one thousand concurrent FaaS function calls. The ReportGenerator is then used to aggregate experiment output into CSV files for consumption by popular data analytics tools. SAAF and its supporting tools combined can assess forty-eight distinct metrics to enhance observability of serverless software deployments. In this tutorial paper, we describe SAAF and its supporting tools and provide examples of observability insights that can be derived. © 2021 Association for Computing Machinery.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3447545.3451173
SP  - 161
EP  - 164
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104969464&doi=10.1145%2f3447545.3451173&partnerID=40&md5=e3b56b9331b79f7e3e75dd8fcfced694
DB  - Scopus
KW  - Computer software reusability
KW  - Analytics tools
KW  - Data Analytics
KW  - Function calls
KW  - Infrastructure as a service (IaaS)
KW  - Multithreaded
KW  - Observability
KW  - Open source software
KW  - Open sources
KW  - Performance experiment
KW  - Platform as a Service (PaaS)
KW  - Resource utilizations
KW  - Software deployment
KW  - Supporting tool
ER  - 

TY  - JOUR
TI  - Go Serverless With RADON! A Practical DevOps Experience Report
AU  - Dalla Palma, S.
AU  - Catolino, G.
AU  - Di Nucci, D.
AU  - Tamburri, D.A.
AU  - Van Den Heuvel, W.-J.
T2  - IEEE Software
AB  - We evaluate a novel DevOps methodology for serverless software delivery and evolution, called RADON. The framework harmonizes the abstraction and actuation of action-trigger rules avoiding function-as-a-service (FaaS) lock-in while optimizing decomposition and reuse through model-based FaaS-enabled development and orchestration.  © 1984-2012 IEEE.
DA  - 2023///
PY  - 2023
DO  - 10.1109/MS.2022.3170153
VL  - 40
IS  - 2
SP  - 80
EP  - 89
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129594819&doi=10.1109%2fMS.2022.3170153&partnerID=40&md5=536c120a870c73f82695cdd30b73a058
DB  - Scopus
KW  - Serverless Computing
KW  - Software
KW  - Serverless computing
KW  - DevOps
KW  - Industrial research
KW  - Code
KW  - Data structures
KW  - European union
KW  - Experience report
KW  - FAA
KW  - Horizon 2020
KW  - Industrial case study
KW  - Large-scales
KW  - Radon
KW  - Situational method engineerin
KW  - Situational Method Engineerin
KW  - Software testing
ER  - 

TY  - CONF
TI  - Faaset: A jupyter notebook to streamline every facet of serverless development
AU  - Cordingly, R.
AU  - Lloyd, W.
T2  - ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering
AB  - Function-as-a-Service platforms require developers to use many different tools and services for function development, packaging, deployment, debugging, testing, orchestration of experiments, and analysis of results. Diverse toolchains are necessary due to the differences in how each platform is designed, the technologies they support, and the APIs they provide, leading to usability challenges for developers. To combine support for all of the tasks and tools into a unified workspace, we created the FaaS Experiment Toolkit (FaaSET). At the core of FaaSET is a Jupyter notebook development environment that enables developers to write functions, deploy them across multiple platforms, invoke and test them, automate experiments, and perform data analysis all in a single environment.  © 2022 Owner/Author.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3491204.3527464
SP  - 49
EP  - 52
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135977102&doi=10.1145%2f3491204.3527464&partnerID=40&md5=4737b62f155f92ef4724200ec997e982
DB  - Scopus
KW  - Serverless
KW  - Function-as-a-service
KW  - serverless
KW  - Service platforms
KW  - function-as-a-service
KW  - development
KW  - Development
KW  - Development environment
KW  - Experiment and analysis
KW  - jupyter
KW  - Jupyter
KW  - Multiple platforms
KW  - profiling
KW  - Profiling
KW  - Program debugging
KW  - tools
KW  - Write functions
ER  - 

TY  - CONF
TI  - Scaling a Variant Calling Genomics Pipeline with FaaS
AU  - Arjona, A.
AU  - Gabriel-Atienza, A.
AU  - Lanuza-Orna, S.
AU  - Roca-Canals, X.
AU  - Bourramouss, A.
AU  - Chafin, T.K.
AU  - Marcello, L.
AU  - Ribeca, P.
AU  - García-López, P.
T2  - WoSC 2023 - Proceedings of the 2023 9th International Workshop on Serverless, Part of: Middleware 2023
AB  - With the escalating complexity and volume of genomic data, the capacity of biology institutions' HPC faces limitations. While the Cloud presents a viable solution for short-term elasticity, its intricacies pose challenges for bioinformatics users. Alternatively, serverless computing allows for workload scalability with minimal developer burden. However, porting a scientific application to serverless is not a straightforward process. In this article, we present a Variant Calling genomics pipeline migrated from single-node HPC to a serverless architecture. We describe the inherent challenges of this approach and the engineering efforts required to achieve scalability. We contribute by open-sourcing the pipeline for future systems research and as a scalable user-friendly tool for the bioinformatics community. © 2023 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3631295.3631403
SP  - 59
EP  - 64
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180123450&doi=10.1145%2f3631295.3631403&partnerID=40&md5=37a03387f7943bd1c810e64390c12d14
DB  - Scopus
L1  - https://arxiv.org/pdf/2312.07090
KW  - FaaS
KW  - Serverless
KW  - serverless
KW  - Work-flows
KW  - Bioinformatics
KW  - Faas
KW  - Genes
KW  - Genome
KW  - Genomic data
KW  - genomics
KW  - Genomics
KW  - Pipelines
KW  - Scalability
KW  - Scalings
KW  - Scientific applications
KW  - Serverless architecture
KW  - Systems research
KW  - Viable solutions
KW  - workflow
ER  - 

TY  - CONF
TI  - Serverless Isn't Server-Less: Measuring and Exploiting Resource Variability on Cloud FaaS Platforms
AU  - Ginzburg, S.
AU  - Freedman, M.J.
T2  - WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020
AB  - Serverless computing in the cloud, or functions as a service (FaaS), poses new and unique systems design challenges. Serverless offers improved programmability for customers, yet at the cost of increased design complexity for cloud providers. One such challenge is effective and consistent resource management for serverless platforms, the implications of which we explore in this paper. In this paper, we conduct one of the first detailed in situ measurement studies of performance variability in AWS Lambda. We show that the observed variations in performance are not only significant, but stable enough to exploit. We then design and evaluate an end-to-end system that takes advantage of this resource variability to exploit the FaaS consumption-based pricing model, in which functions are charged based on their fine-grain execution time rather than actual low-level resource consumption. By using both light-weight resource probing and function execution times to identify attractive servers in serverless platforms, customers of FaaS services can cause their functions to execute on better performing servers and realize a cost savings of up to 13% in the same AWS region.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429880.3430099
SP  - 43
EP  - 48
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607225&doi=10.1145%2f3429880.3430099&partnerID=40&md5=9f92f24b55a5d8c444b53772261f0413
DB  - Scopus
KW  - Costs
KW  - Performance
KW  - Resource management
KW  - Cloud providers
KW  - Design challenges
KW  - Design complexity
KW  - In-situ measurement
KW  - Lambda's
KW  - Measurement study
KW  - Performance variability
KW  - Programmability
ER  - 

TY  - CONF
TI  - RADON: rational decomposition and orchestration for serverless computing
AU  - Casale, G.
AU  - Artač, M.
AU  - van den Heuvel, W.-J.
AU  - van Hoorn, A.
AU  - Jakovits, P.
AU  - Leymann, F.
AU  - Long, M.
AU  - Papanikolaou, V.
AU  - Presenza, D.
AU  - Russo, A.
AU  - Srirama, S.N.
AU  - Tamburri, D.A.
AU  - Wurster, M.
AU  - Zhu, L.
T2  - Software-Intensive Cyber-Physical Systems
AB  - Emerging serverless computing technologies, such as function as a service (FaaS), enable developers to virtualize the internal logic of an application, simplifying the management of cloud-native services and allowing cost savings through billing and scaling at the level of individual functions. Serverless computing is therefore rapidly shifting the attention of software vendors to the challenge of developing cloud applications deployable on FaaS platforms. In this vision paper, we present the research agenda of the RADON project (http://radon-h2020.eu), which aims to develop a model-driven DevOps framework for creating and managing applications based on serverless computing. RADON applications will consist of fine-grained and independent microservices that can efficiently and optimally exploit FaaS and container technologies. Our methodology strives to tackle complexity in designing such applications, including the solution of optimal decomposition, the reuse of serverless functions as well as the abstraction and actuation of event processing chains, while avoiding cloud vendor lock-in through models. © 2019, The Author(s).
DA  - 2020///
PY  - 2020
DO  - 10.1007/s00450-019-00413-w
VL  - 35
SP  - 77
EP  - 87
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071490657&doi=10.1007%2fs00450-019-00413-w&partnerID=40&md5=05759d936c13ea2b982fd942efb8c3ad
DB  - Scopus
L1  - https://link.springer.com/content/pdf/10.1007%2Fs00450-019-00413-w.pdf
KW  - Serverless computing
KW  - DevOps
KW  - Function as a service
KW  - Radon
KW  - Cloud applications
KW  - Application programs
KW  - Computing technology
KW  - Event Processing
KW  - Optimal decomposition
KW  - Software model
KW  - Software models
KW  - Software vendors
ER  - 

TY  - CONF
TI  - Courier: Delivering serverless functions within heterogeneous FaaS deployments
AU  - Jindal, A.
AU  - Frielinghaus, J.
AU  - Chadha, M.
AU  - Gerndt, M.
T2  - ACM International Conference Proceeding Series
AB  - With the advent of serverless computing in different domains, there is a growing need for dynamic adaption to handle diverse and heterogeneous functions. However, serverless computing is currently limited to homogeneous Function-as-a-Service (FaaS) deployments or simply FaaS Deployment (FaaSD) consisting of deployments of serverless functions using a FaaS platform in a region with certain memory configurations. Extending serverless computing to support Heterogeneous FaaS Deployments (HeteroFaaSDs) consisting of multiple FaaSDs with variable configurations (FaaS platform, region, and memory) and dynamically load balancing the invocations of the functions across these FaaSDs within a HeteroFaaSD can provide an optimal way for handling such serverless functions. In this paper, we present a software system called Courier that is responsible for optimally distributing the invocations of the functions (called delivering of serverless functions) within the HeteroFaaSDs based on the execution time of the functions on the FaaSDs comprising the HeteroFaaSDs. To this end, we developed two approaches: Auto Weighted Round-Robin (AWRR) and PerFunction Auto Weighted Round-Robin (PFAWRR) that use functions execution times for delivering serverless functions within a HeteroFaaSD to reduce the overall execution time. We demonstrate and evaluate the functioning of our developed tool on three HeteroFaaSDs using three FaaS platforms: 1) on-premise Open-Whisk, 2) AWS Lambda, and 3) Google Cloud Functions (GCF). We show that Courier can improve the overall performance of the invocations of the functions within a HeteroFaaSD as compared to traditional load balancing algorithms.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3468737.3494097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119572896&doi=10.1145%2f3468737.3494097&partnerID=40&md5=8ce66e3fa02016a17262733eb1a2d8e4
DB  - Scopus
KW  - serverless computing
KW  - Serverless computing
KW  - Function-as-a-service
KW  - function-as-a-service
KW  - Configuration function
KW  - Different domains
KW  - Dynamic adaptions
KW  - Function delivery
KW  - functions delivery
KW  - Homogeneous functions
KW  - Memory configuration
KW  - Service deployment
KW  - Weighted round robins
ER  - 

TY  - CONF
TI  - When Serverless Computing Meets Different Degrees of Customization for DNN Inference
AU  - Song, M.
AU  - Hur, Y.
AU  - Lee, K.
T2  - WoSC 2023 - Proceedings of the 2023 9th International Workshop on Serverless, Part of: Middleware 2023
AB  - Serverless computing provides a method to develop application services without the burden of run-time execution environment management overhead. Since the initial offerings of serverless computing using function-as-a-service (FaaS), other variants of execution environments have been proposed, such as a special-purpose FaaS (SPF) for deep neural network (DNN) inference and a serverless container service (SCS) for general web applications. This paper qualitatively summarizes the characteristics of a general-purpose FaaS (GPF), SPF, and SCS from the perspective of customizability when setting up execution environments. To judge whether various serverless computing environments can be feasible solutions for an interactive DNN model inference application, we conduct extensive experiments and conclude that there are rooms for performance improvement serverless DNN inference, and allowing a custom environment setup can make the serverless computing platform for an interactive DNN application. © 2023 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3631295.3631400
SP  - 42
EP  - 47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180127927&doi=10.1145%2f3631295.3631400&partnerID=40&md5=c29e150e426f8aa48295096ec460d883
DB  - Scopus
KW  - serverless computing
KW  - Serverless computing
KW  - Application services
KW  - Customisation
KW  - Deep neural networks
KW  - dnn inference
KW  - Dnn inference
KW  - Environment management
KW  - Execution environments
KW  - Network inference
KW  - Run-time execution
KW  - WEB application
KW  - Web applications
ER  - 

TY  - CONF
TI  - Active-Standby for High-Availability in FaaS
AU  - Bouizem, Y.
AU  - Parlavantzas, N.
AU  - DIb, D.
AU  - Morin, C.
T2  - WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020
AB  - Serverless computing is becoming more and more attractive for cloud solution architects and developers. This new computing paradigm relies on Function-as-a-Service (FaaS) platforms that enable deploying functions without being concerned with the underlying infrastructure. An important challenge in designing FaaS platforms is ensuring the availability of deployed functions. Existing FaaS platforms address this challenge principally through retrying function executions. In this paper, we propose and implement an alternative fault-tolerance approach based on active-standby failover. Results from an experimental evaluation show that our approach increases availability and performance compared to the retry-based approach.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429880.3430097
DP  - Scopus
SP  - 31
EP  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099596478&doi=10.1145%2f3429880.3430097&partnerID=40&md5=660f0261eb6c41f814b91156c87b6507
DB  - Scopus
L1  - https://hal.inria.fr/hal-03043479/file/Active-Standby-WoSC6.pdf
KW  - FaaS
KW  - Performance
KW  - Function-as-a-service
KW  - availability
KW  - Availability
KW  - Computing paradigm
KW  - Experimental evaluation
KW  - Failover
KW  - fault tolerance
KW  - Fault tolerance
KW  - High availability
KW  - Tolerance approach
ER  - 

TY  - CONF
TI  - Serverless computing for data processing in open learning and research environments
AU  - Bezverbnyi, I.A.
AU  - Shyshkina, M.P.
T2  - CEUR Workshop Proceedings
AB  - Serverless computing is a paradigm that enables the execution of code without provisioning or managing servers. It offers benefits such as scalability, cost-efficiency, and ease of development for cloud-based applications. In this paper, we explore the potential of serverless computing for supporting data processing in open learning and research environments. We propose a concept of a hybrid serverless cloud, which combines different types of cloud services to provide access to various tools and resources for learners and researchers. We present a case study of wave files processing using a lambda function, which demonstrates the feasibility and effectiveness of our approach. We also discuss the challenges and opportunities of integrating serverless components within open systems of learning and research. Finally, we present a vision of a cloud-based open learning and research university environment that leverages serverless technologies to enhance the quality and accessibility of education and research. © 2023 Copyright for this paper by its authors.
DA  - 2023///
PY  - 2023
VL  - 3482
SP  - 229
EP  - 236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173565001&partnerID=40&md5=adb2c81777f0353992372fe8493338c6
DB  - Scopus
KW  - cloud computing
KW  - serverless computing
KW  - Serverless computing
KW  - Cloud-computing
KW  - Cloud services
KW  - Open systems
KW  - Case-studies
KW  - Cloud-based applications
KW  - Cost-efficiency
KW  - Data handling
KW  - data processing
KW  - Learning environments
KW  - Learning systems
KW  - Open Data
KW  - open learning
KW  - Open learning
KW  - open research
KW  - Open research
KW  - Research environment
ER  - 

TY  - CONF
TI  - Estimating the capacities of function-as-a-service functions
AU  - Jindal, A.
AU  - Chadha, M.
AU  - Benedict, S.
AU  - Gerndt, M.
T2  - ACM International Conference Proceeding Series
AB  - Serverless computing is a cloud computing paradigm that allows developers to focus exclusively on business logic as cloud service providers manage resource management tasks. Serverless applications follow this model, where the application is decomposed into a set of fine-grained Function-as-a-Service (FaaS) functions. However, the obscurities of the underlying system infrastructure and dependencies between FaaS functions within the application pose a challenge for estimating the performance of FaaS functions. To characterize the performance of a FaaS function that is relevant for the user, we define Function Capacity (FC) as the maximal number of concurrent invocations the function can serve in a time without violating the Service-Level Objective (SLO). The paper addresses the challenge of quantifying the FC individually for each FaaS function within a serverless application. This challenge is addressed by sandboxing a FaaS function and building its performance model. To this end, we develop FnCapacitor - an end-to-end automated Function Capacity estimation tool. We demonstrate the functioning of our tool on Google Cloud Functions (GCF) and AWS Lambda. FnCapacitor estimates the FCs on different deployment configurations (allocated memory & maximum function instances) by conducting time-framed load tests and building various models using statistical: linear, ridge, and polynomial regression, and Deep Neural Network (DNN) methods on the acquired performance data. Our evaluation of different FaaS functions shows relatively accurate predictions with an accuracy greater than 75% using DNN for both cloud providers.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3492323.3495628
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119575511&doi=10.1145%2f3492323.3495628&partnerID=40&md5=b042e769c76abd995be64aa93a116ee1
DB  - Scopus
L1  - https://arxiv.org/pdf/2201.11454
KW  - serverless computing
KW  - Serverless computing
KW  - Performance
KW  - Function-as-a-service
KW  - Resource management
KW  - Cloud-computing
KW  - function-as-a-service
KW  - Service functions
KW  - Deep neural networks
KW  - Computing paradigm
KW  - Business logic
KW  - Cloud service providers
KW  - Computation theory
KW  - function capacity
KW  - Function capacity
KW  - Load testing
ER  - 

TY  - CONF
TI  - The Serverless Application Analytics Framework: Enabling Design Trade-off Evaluation for Serverless Software
AU  - Cordingly, R.
AU  - Yu, H.
AU  - Hoang, V.
AU  - Sadeghi, Z.
AU  - Foster, D.
AU  - Perez, D.
AU  - Hatchett, R.
AU  - Lloyd, W.
T2  - WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020
AB  - To help better understand factors that impact performance on Function-as-a-Service (FaaS) platforms we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can integrate into a function's package for deployment to multiple commercial and open source FaaS platforms. SAAF improves the observability of FaaS function deployments by collecting forty-eight distinct metrics to enable developers to profile CPU and memory utilization, monitor infrastructure state, and observe platform scalability. In this paper, we describe SAAF in detail and introduce supporting tools highlighting important features and how to use them. Our client application, FaaS Runner, provides a tool to orchestrate workloads and automate the process of conducting experiments across FaaS platforms. We provide a case study demonstrating the integration of SAAF into an existing open source image processing pipeline built for AWS Lambda. Using FaaS Runner, we automate experiments and acquire metrics from SAAF to profile each function of the pipeline to evaluate performance implications. Finally, we summarize contributions using our tools to evaluate implications of different programming languages for serverless data processing, and to build performance models to predict runtime for serverless workloads.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429880.3430103
SP  - 67
EP  - 72
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099550918&doi=10.1145%2f3429880.3430103&partnerID=40&md5=ecd7e97af17c8305211d74f6b6643b41
DB  - Scopus
KW  - Serverless Computing
KW  - Serverless computing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - Computer software reusability
KW  - Function evaluation
KW  - Open source software
KW  - Pipelines
KW  - Application programs
KW  - Economic and social effects
KW  - Open-source
KW  - Data handling
KW  - Commercial sources
KW  - Design tradeoff
KW  - Framework
KW  - Frameworks
KW  - Image processing
KW  - Impact performance
KW  - Performance Evaluation
KW  - Performances evaluation
KW  - Pipeline processing systems
KW  - Programming language
KW  - Programming Languages
KW  - Source functions
ER  - 

TY  - CONF
TI  - Continuous integration of faas driven by quality attributes
AU  - Serrano-Gutierrez, P.
T2  - Ibero-American WWW / Internet Conference 2020
AB  - The expansion of the cloud has led to the development of serverless applications in which there is no need to be concerned about infrastructure, that can be considered as a service provider in the form of functions, which is known as FaaS (Function as a Service). The development of these applications consists of implementing independent functions that will communicate with each other. This work aims to explore how to combine FaaS and the continuous integration philosophy of DevOps so that applications operators can continuously re-deploy different implementations of functions in order to meet some quality of service requirements, such as latency or energy consumption. © Ibero-American WWW / Internet Conference 2020.
DA  - 2020///
PY  - 2020
DP  - Scopus
SP  - 179
EP  - 182
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099546446&partnerID=40&md5=f37982252f37dc4e48332b476fab60a7
DB  - Scopus
KW  - FaaS
KW  - Serverless
KW  - Function as a Service
KW  - DevOps
KW  - Quality of service
KW  - Infrastructure as a service (IaaS)
KW  - Quality of Service
KW  - Service provider
KW  - Continuous Integration
KW  - Continuous integrations
KW  - Energy utilization
KW  - Independent functions
KW  - Quality attributes
ER  - 

TY  - CONF
TI  - Towards Federated Learning using FaaS Fabric
AU  - Chadha, M.
AU  - Jindal, A.
AU  - Gerndt, M.
T2  - WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020
AB  - Federated learning (FL) enables resource-constrained edge devices to learn a shared Machine Learning (ML) or Deep Neural Network (DNN) model, while keeping the training data local and providing privacy, security, and economic benefits. However, building a shared model for heterogeneous devices such as resource-constrained edge and cloud makes the efficient management of FL-clients challenging. Furthermore, with the rapid growth of FL-clients, the scaling of FL training process is also difficult. In this paper, we propose a possible solution to these challenges: federated learning over a combination of connected Function-as-a-Service platforms, i.e., FaaS fabric offering a seamless way of extending FL to heterogeneous devices. Towards this, we present FedKeeper, a tool for efficiently managing FL over FaaS fabric. We demonstrate the functionality of FedKeeper by using three FaaS platforms through an image classification task with a varying number of devices/clients, different stochastic optimizers, and local computations (local epochs).  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429880.3430100
SP  - 49
EP  - 54
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099604179&doi=10.1145%2f3429880.3430100&partnerID=40&md5=7835ee3ce9f26a34046d77f90d9ad459
DB  - Scopus
KW  - FaaS
KW  - Serverless
KW  - Function-as-a-service
KW  - Faas
KW  - Deep neural networks
KW  - Faas platform
KW  - FaaS platforms
KW  - Federated learning
KW  - Heterogeneous devices
KW  - Learn+
KW  - Machine-learning
KW  - Neural network model
KW  - Neural networks
KW  - Neural-networks
KW  - Optimization
KW  - Stochastic systems
ER  - 

TY  - JOUR
TI  - Serverless Computing: A Survey of Opportunities, Challenges, and Applications
AU  - Shafiei, H.
AU  - Khonsari, A.
AU  - Mousavi, P.
T2  - ACM Computing Surveys
AB  - The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this article, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.  © 2022 Association for Computing Machinery.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3510611
DP  - Scopus
VL  - 54
IS  - 11s
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150456234&doi=10.1145%2f3510611&partnerID=40&md5=b92919c2dbfae611897b7baea5b56960
DB  - Scopus
L1  - https://arxiv.org/pdf/1911.01296
KW  - serverless computing
KW  - Serverless computing
KW  - Function-as-a-service
KW  - Cloud services
KW  - Scalings
KW  - Pricing models
KW  - Cloud applications
KW  - Web services
KW  - Computing paradigm
KW  - Pay as you go
KW  - Application developers
KW  - function-as-a-service (FaaS)
KW  - Operational complexity
ER  - 

TY  - CONF
TI  - A fault-Tolerance shim for serverless computing
AU  - Sreekanti, V.
AU  - Wu, C.
AU  - Chhatrapati, S.
AU  - Gonzalez, J.E.
AU  - Hellerstein, J.M.
AU  - Faleiro, J.M.
T2  - Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020
AB  - Serverless computing has grown in popularity in recent years, with an increasing number of applications being built on Functions-As-A-Service (FaaS) platforms. By default, FaaS platforms support retry-based fault tolerance, but this is insufficient for programs that modify shared state, as they can unwittingly persist partial sets of updates in case of failures. To address this challenge, we would like atomic visibility of the updates made by a FaaS application. In this paper, we present aft, an atomic fault tolerance shim for serverless applications. aft interposes between a commodity FaaS platform and storage engine and ensures atomic visibility of updates by enforcing the read atomic isolation guarantee. aft supports new protocols to guarantee read atomic isolation in the serverless setting. We demonstrate that aft introduces minimal overhead relative to existing storage engines and scales smoothly to thousands of requests per second, while preventing a significant number of consistency anomalies. © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3342195.3387535
DP  - Scopus
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087109549&doi=10.1145%2f3342195.3387535&partnerID=40&md5=be7f879740c5a8fd536c529c6753f207
DB  - Scopus
L1  - https://arxiv.org/pdf/2003.06007
KW  - Fault tolerance
KW  - Atoms
KW  - Engines
KW  - Fault tolerant computer systems
KW  - New protocol
KW  - Shared state
KW  - Shims
KW  - Storage engines
KW  - Virtual storage
KW  - Visibility
ER  - 

TY  - JOUR
TI  - SERVERLESS MICROSERVICES ARCHITECTURE FOR INDOOR POSITIONING SYSTEM USING BLUETOOTH LOW ENERGY
AU  - Suhendinata, H.
AU  - Kusuma, G.P.
T2  - ICIC Express Letters
AB  - The development of Indoor Positioning System (IPS) research currently focuses on improving accuracy of indoor positioning systems. The use of complex techniques to improve location accuracy has an impact on the use of resources on smartphones. So offloading techniques are used to overcome the limitations of resources on smartphones by moving computing on smartphones to cloud computing. Serverless is one of the cloud computing technologies that have scalable capabilities by performing auto provision on cloud computing resources. In this paper, we propose a design architecture of serverless microservice for indoor positioning system using Bluetooth low energy. This architecture uses serverless technology with a micro services architecture design that divides services into small parts. The test results of 360 concurrent users per second with multitenant showed availability of serverless microservices is 97.83% with an average response time of 4.63 seconds, which is better compared to availability of serverless monolith of 89.68% with an average response time of 4.82 seconds.  © 2022 ISSN.
DA  - 2022///
PY  - 2022
DO  - 10.24507/icicel.16.09.1001
VL  - 16
IS  - 9
SP  - 1001
EP  - 1009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138779069&doi=10.24507%2ficicel.16.09.1001&partnerID=40&md5=a9c9c3344221ae3b62ca61da937a01d1
DB  - Scopus
L1  - http://www.icicel.org/ell/contents/2022/9/el-16-09-11.pdf
KW  - Cloud computing
KW  - Bluetooth low energy
KW  - Computational offloading
KW  - Indoor positioning system
KW  - Serverless microservices
ER  - 

TY  - CONF
TI  - LCS : Alleviating Total Cold Start Latency in Serverless Applications with LRU Warm Container Approach
AU  - Sethi, B.
AU  - Addya, S.K.
AU  - Ghosh, S.K.
T2  - ACM International Conference Proceeding Series
AB  - Serverless computing offers "Function-as-a-Service"(FaaS), which promotes an application in the form of independent granular components called functions. FaaS goes well as a widespread standard that facilitates the development of applications in cloud-based environments. Clients can solely focus on developing applications in a serverless ecosystem, passing the overburden of resource governance to the service providers. However, FaaS platforms have to bear the degradation in performance originating from the cold starts of executables i.e. serverless functions. The cold start reflects the delay in provisioning a runtime container that processes the functions. Each serverless platform is handling the problem of cold start with its own solution. In recent times, approaches to deal with cold starts have received the attention of many researchers. This paper comes up with an extensive solution to handle the cold start problem. We propose a scheduling approach to reduce the cold start occurrences by keeping the containers alive for a longer period of time using the Least Recently Used warm Container Selection (LCS ) approach on Affinity-based scheduling. Further, we carried out an evaluation and compared the obtained results with the MRU container selection approach. The proposed LCS approach outperforms by approximately 48% compared to the MRU approach.  © 2023 ACM.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3571306.3571404
SP  - 197
EP  - 206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145875136&doi=10.1145%2f3571306.3571404&partnerID=40&md5=fc90549680a820546d1d13df60f53e1c
DB  - Scopus
KW  - Serverless Computing
KW  - Containers
KW  - Serverless computing
KW  - FaaS
KW  - Performance
KW  - Service platforms
KW  - Cold Start
KW  - Cold-start
KW  - Service provider
KW  - Cloud-based
KW  - Executables
KW  - Function-as-a-service"
KW  - Granular components
KW  - Latency
ER  - 

TY  - CONF
TI  - Software Architecture Design of a Serverless System
AU  - Hamza, M.
T2  - ACM International Conference Proceeding Series
AB  - Context: Serverless computing allows developers to create and deploy applications without the need to manage any underlying infrastructure, making it a more efficient and effective way to bring products to market. Serverless technology is gaining widespread adoption among many companies, becoming increasingly popular. However, the adoption of serverless technology brings with it several new challenges. Objective: To this end, we plan to gain a deep understanding of challenges and strategies, architectural issues and their causes, architectural patterns, antipatterns, migration towards serverless architecture, and state-of-the-art practices for vendor lock-in problems. Methodology: The research objective will be met through the use of an industrial empirical approach, including interviews, a case study, and a questionnaire survey. Possible outcomes: The expected outcomes would be (i) a multivocal literature review on design areas of serverless architecture (ii) an evidence-based framework for synthesizing serverless architectural challenges/solutions (iii) a decision-making process for migrating to serverless architecture (iv) a decision-making framework for selecting vendor platform.  © 2023 Owner/Author.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593434.3593471
SP  - 304
EP  - 306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162214035&doi=10.1145%2f3593434.3593471&partnerID=40&md5=438b90314b99274251754cf0acf234ff
DB  - Scopus
KW  - Serverless Architecture
KW  - Lock-in
KW  - Industrial research
KW  - Serverless architecture
KW  - Anti-patterns
KW  - Architectural pattern
KW  - Art practice
KW  - Decision making
KW  - Decision Model
KW  - Decision modeling
KW  - Empirical investigation
KW  - Empirical Investigation
KW  - Serverless systems
KW  - Software architecture
KW  - Software architecture design
KW  - State of the art
ER  - 

TY  - CONF
TI  - Bringing scaling transparency to Proteomics applications with serverless computing
AU  - Mirabelli, M.E.
AU  - García-López, P.
AU  - Vernik, G.
T2  - WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020
AB  - Scaling transparency means that applications can expand in scale without changes to the system structure or the application algorithms. Serverless Computing's inherent auto-scaling support and fast function launching is ideally suited to support scaling transparency in different domains. In particular, Proteomic applications could considerably benefit from scaling transparency and serverless technologies due to their high concurrency requirements. Therefore, the auto-provisioning nature of serverless platforms makes this computing model an alternative to satisfy dynamically the resources required by protein folding simulation processes. However, the transition to these architectures must face challenges: they should show comparable performance and cost to code running in Virtual Machines (VMs). In this article, we demonstrate that Proteomics applications implemented with the Replica Exchange algorithm can be moved to serverless settings guaranteeing scaling transparency. We also validate that we can reduce the total execution time by around forty percent with comparable cost to cluster technologies (Work Queue) over VMs.  © 2020 ACM.
DA  - 2020///
PY  - 2020
DO  - 10.1145/3429880.3430101
DP  - Scopus
SP  - 55
EP  - 60
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099557165&doi=10.1145%2f3429880.3430101&partnerID=40&md5=ef871c164cf4821636d078eee2a52a23
DB  - Scopus
KW  - Serverless
KW  - Scalings
KW  - Different domains
KW  - Simulation platform
KW  - Computing model
KW  - High concurrencies
KW  - Molecular biology
KW  - Protein folding simulation
KW  - Proteomics
KW  - Replica exchange
KW  - Replica Exchange
KW  - Scaling transparency
KW  - Scaling Transparency
KW  - Systems Structure
KW  - Transparency
ER  - 

TY  - CONF
TI  - Implications of Alternative Serverless Application Control Flow Methods
AU  - Quinn, S.
AU  - Cordingly, R.
AU  - Lloyd, W.
T2  - Proceedings of the 7th International Workshop on Serverless Computing, WoSC 2021
AB  - Function-as-a-Service or FaaS is a popular delivery model of serverless computing where developers upload code to be executed in the cloud as short running stateless functions. Using smaller functions to decompose processing of larger tasks or workflows introduces the question of how to instrument application control flow to orchestrate an overall task or workflow. In this paper, we examine implications of using different methods to orchestrate the control flow of a serverless data processing pipeline composed as a set of independent FaaS functions. We performed experiments on the AWS Lambda FaaS platform and compared how four different patterns of control flow impact the cost and performance of the pipeline. We investigate control flow using client orchestration, microservice controllers, event-based triggers, and state-machines. Overall, we found that asynchronous methods led to lower orchestration costs, and that event-based orchestration incurred a performance penalty.  © 2021 ACM.
DA  - 2021///
PY  - 2021
DO  - 10.1145/3493651.3493668
SP  - 17
EP  - 22
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121471495&doi=10.1145%2f3493651.3493668&partnerID=40&md5=1d1d8964c6084c9783f3dbce0ddf2bab
DB  - Scopus
KW  - Serverless Computing
KW  - Serverless computing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - Work-flows
KW  - Pipelines
KW  - Data handling
KW  - Framework
KW  - Frameworks
KW  - Performance Evaluation
KW  - Performances evaluation
KW  - Pipeline processing systems
KW  - Programming language
KW  - Programming Languages
KW  - Application-control
KW  - Control-flow
KW  - Delivery models
KW  - Event-based
ER  - 

TY  - JOUR
TI  - Integrating request replication into FaaS platforms: an experimental evaluation
AU  - Bouizem, Y.
AU  - Dib, D.
AU  - Parlavantzas, N.
AU  - Morin, C.
T2  - Journal of Cloud Computing
AB  - Function-as-a-Service (FaaS) is a popular programming model for building serverless applications, supported by all major cloud providers and many open-source software frameworks. One of the main challenges for FaaS providers is providing fault tolerance for the deployed applications, that is, providing the ability to mask failures of function invocations from clients. The basic fault tolerance approach in current FaaS platforms is automatically retrying function invocations. Although the retry approach is well suited for transient failures, it incurs delays in recovering from other types of failures, such as node crashes. This paper proposes the integration of a Request Replication mechanism in FaaS platforms and describes how this integration was implemented in Fission, a well-known, open-source platform. It provides a detailed experimental comparison of the proposed approach with the retry approach and an Active-Standby approach in terms of performance, availability, and resource consumption under different failure scenarios. © 2023, The Author(s).
DA  - 2023///
PY  - 2023
DO  - 10.1186/s13677-023-00457-z
VL  - 12
IS  - 1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162899726&doi=10.1186%2fs13677-023-00457-z&partnerID=40&md5=6a41457201d5194316802712f2dfa1dc
DB  - Scopus
L1  - https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-023-00457-z
KW  - FaaS
KW  - Serverless
KW  - Cloud
KW  - Function-as-a-service
KW  - Service platforms
KW  - Open source software
KW  - Cloud providers
KW  - Application programs
KW  - Open systems
KW  - Service provider
KW  - Experimental evaluation
KW  - Fault tolerance
KW  - High availability
KW  - Programming models
KW  - Open-source softwares
KW  - Software frameworks
ER  - 

TY  - CONF
TI  - faas-sim: A trace-driven simulation framework for serverless edge computing platforms
AU  - Raith, P.
AU  - Rausch, T.
AU  - Furutanpey, A.
AU  - Dustdar, S.
T2  - Software - Practice and Experience
AB  - This paper presents faas-sim, a simulation framework tailored to serverless edge computing platforms. In serverless computing, platform operators are tasked with efficiently managing distributed computing infrastructure completely abstracted from application developers. To that end, platform operators and researchers need tools to design, build, and evaluate resource management techniques that efficiently use of infrastructure while optimizing application performance. This challenge is exacerbated in edge computing scenarios, where, compared to cloud computing, there is a lack of reference architectures, design tools, or standardized benchmarks. faas-sim bridges this gap by providing (a) a generalized model of serverless systems that builds on the function-as-a-service abstraction, (b) a simulator that uses trace data from real-world edge computing testbeds and representative workloads, and (c) a network topology generator to model and simulate distributed and heterogeneous edge-cloud systems. We present the conceptual design, implementation, and a thorough evaluation of faas-sim. By running experiments on both real-world test beds and replicating them using faas-sim, we show that the simulator provides accurate results and reasonable simulation performance. We have profiled a wide range of edge computing infrastructure and workloads, focusing on typical edge computing scenarios such as edge AI inference or data processing. Moreover, we present several instances where we have successfully used faas-sim to either design, optimize, or evaluate serverless edge computing systems. © 2023 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.
DA  - 2023///
PY  - 2023
DO  - 10.1002/spe.3277
VL  - 53
SP  - 2327
EP  - 2361
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173703464&doi=10.1002%2fspe.3277&partnerID=40&md5=d3553627369fc425c4823f3bb3ea62c0
DB  - Scopus
KW  - Edge computing
KW  - Edge clouds
KW  - Edge-cloud continuum
KW  - Simulation platform
KW  - Data handling
KW  - Computing platform
KW  - Abstracting
KW  - co-simulation
KW  - Computing infrastructures
KW  - Conceptual design
KW  - Cosimulation
KW  - edge-cloud continuum
KW  - serverless edge computing
KW  - Serverless edge computing
KW  - simulation
KW  - Simulation
KW  - Simulation framework
KW  - Topology
KW  - Trace driven simulation
ER  - 

TY  - CONF
TI  - A Conceptual Model of Requirement Engineering in Cloud Project Delivery for Thai Government Organizations
AU  - Chaipunyathat, Ajchareeya
AU  - Porrawatpreyakorn, Nalinpat
AU  - Nuchitprasitchai, Siranee
AU  - Viriyapant, Kanchana
T2  - 2019 Research, Invention, and Innovation Congress (RI2C)
AB  - The shift to cloud computing has affected the future of software engineering in several ways i.e., multilateral software development, scalability, and new technology stack such as an open-source software, plus infrastructure as code such as container, serverless architecture and software defined network (SDN). In order to support the cloud project delivery for government organizations, requirements engineering (RE) is a crucial step in software engineering that determines whether a project will be successful or result in a failure. RE steps include requirement elicitation, requirement analysis, requirement specification and requirement validation. Data is collected by semi-structured interview from the providers and the users of cloud services in 11 Thai government organizations, and from cloud service provider for meeting enterprise requirements and user requirements. The results reveal nine key issues that affect cloud project delivery: (1) lack of trust with external cloud service provider by generation X and baby boomer (2) lack of transparency as regards the legal agreement about the cloud user's personal data protection responsibility by cloud service provider, (3) the issues of reliability, security and service level agreements (4) lack of knowledge and lack of understanding of cloud technology (5) required training and a learning by doing (6) the policy to use government cloud services instead of developing their own cloud, (7) older people at top-level tend to resist cloud technology, (8) people in general lack of knowledge and understanding of cloud technology (9) problems about pricing model, it's impact on 23 cloud requirements (functional and non-functional) as well as factors that involved in RE phases. Based on these results this paper presents a Conceptual Model of Requirement Engineering for Cloud Project Delivery in Thai Government Organizations.
C3  - 2019 Research, Invention, and Innovation Congress (RI2C)
DA  - 2019/12//
PY  - 2019
DO  - 10.1109/RI2C48728.2019.8999923
DP  - IEEE Xplore
SP  - 1
EP  - 7
UR  - https://ieeexplore.ieee.org/document/8999923
Y2  - 2023/12/29/20:50:49
L2  - https://ieeexplore.ieee.org/document/8999923
ER  - 

TY  - JOUR
TI  - A Platform for Lightweight Deployment of IoTApplications Based on a Function-as-a-ServiceModel
AU  - Sansó, Sebastià
AU  - Guerrero, Carlos
AU  - Lera, Isaac
AU  - Juiz, Carlos
T2  - IEEE Latin America Transactions
AB  - The use of Cloud computing for the development of Internet of Things (IoT) applications has emerged during the last years. But there is a lack of a platform which facilitates the deployment and the interoperability of this type of applications. This paper presents a platform to facilitate the deployment of Cloud-based applications to devices in IoT domains. The platform allows the programmers to use a Function-as-a-Service programming paradigm which is managed and configured in a Platform-as-a-Service web tool. The tool also allows establishing interoperability between the functions of the applications. The platform is validated by developing a Cloud application that orchestrates two IoT devices, one with a movement sensor and another one with a camera. Finally, a performance study is also performed. The proposed platform obtains faster and easier deployments of the applications. The resource usages of the IoT devices are also lower with regard to a deployment process based on Docker containers.
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/TLA.2019.8931204
DP  - IEEE Xplore
VL  - 17
IS  - 07
SP  - 1155
EP  - 1162
SN  - 1548-0992
UR  - https://ieeexplore.ieee.org/document/8931204
Y2  - 2023/12/29/21:05:38
L2  - https://ieeexplore.ieee.org/document/8931204
ER  - 

TY  - CONF
TI  - AI on the Move: From On-Device to On-Multi-Device
AU  - Flores, Huber
AU  - Nurmi, Petteri
AU  - Hui, Pan
T2  - 2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
AB  - On-Device AI is an emerging paradigm that aims to make devices more intelligent, autonomous and proactive by equipping them with machine and deep learning routines for robust decision making and optimal execution in devices' operations. On-Device intelligence promises the possibility of computing huge amounts of data close to its source, e.g., sensor and multimedia data. By doing so, devices can complement their counterpart cloud services with more sophisticated functionality to provide better applications and services. However, increased computational capabilities of smart devices, wearables and IoT devices along with the emergence of services at the Edge of the network are driving the trend of migrating and distributing computation between devices. Indeed, devices can reduce the burden of executing resource intensive tasks via collaborations in the wild. While several work has shown the benefits of an opportunistic collaboration of a device with others, not much is known regarding how devices can be organized as a group as they move together. In this paper, we contribute by analyzing how dynamic group organization of devices can be utilized to distribute intelligence on the moving Edge. The key insight is that instead of On-Device solutions complementing with cloud, dynamic groups can be formed to complement each other in an On-Multi-Device manner. Thus, we highlight the challenges and opportunities from extending the scope of On-Device AI from an egocentric view to a collaborative, multi-device view.
C3  - 2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
DA  - 2019/03//
PY  - 2019
DO  - 10.1109/PERCOMW.2019.8730873
DP  - IEEE Xplore
SP  - 310
EP  - 315
ST  - AI on the Move
UR  - https://ieeexplore.ieee.org/document/8730873
Y2  - 2023/12/29/21:22:26
L2  - https://ieeexplore.ieee.org/document/8730873
ER  - 

TY  - JOUR
TI  - An Edge-to-Cloud Virtualized Multimedia Service Platform for 5G Networks
AU  - Alvarez, Federico
AU  - Breitgand, David
AU  - Griffin, David
AU  - Andriani, Pasquale
AU  - Rizou, Stamatia
AU  - Zioulis, Nikolaos
AU  - Moscatelli, Francesca
AU  - Serrano, Javier
AU  - Keltsch, Madeleine
AU  - Trakadas, Panagiotis
AU  - Phan, T. Khoa
AU  - Weit, Avi
AU  - Acar, Ugur
AU  - Prieto, Oscar
AU  - Iadanza, Francesco
AU  - Carrozzo, Gino
AU  - Koumaras, Harilaos
AU  - Zarpalas, Dimitrios
AU  - Jimenez, David
T2  - IEEE Transactions on Broadcasting
AB  - The focus of research into 5G networks to date has been largely on the required advances in network architectures, technologies, and infrastructures. Less effort has been put on the applications and services that will make use of and exploit the flexibility of 5G networks built upon the concept of software-defined networking (SDN) and network function virtualization (NFV). Media-based applications are amongst the most demanding services, requiring large bandwidths for high audio-visual quality, low-latency for interactivity, and sufficient infrastructure resources to deliver the computational power for running the media applications in the networked cloud. This paper presents a novel service virtualization platform (SVP), called 5G-MEDIA SVP, which leverages the principles of NFV and SDN to facilitate the development, deployment, and operation of media services on 5G networks. The platform offers an advanced cognitive management environment for the provisioning of network services (NSs) and media-related applications, which directly link their lifecycle management with user experience as well as optimization of infrastructure resource utilization. Another innovation of 5G-MEDIA SVP is the integration of serverless computing with media intensive applications in 5G networks, increasing cost effectiveness of operation and simplifying development and deployment time. The proposed SVP is being validated against three media use cases: 1) immersive virtual reality 3-D gaming application; 2) remote production of broadcast content incorporating user generated contents; and 3) dynamically adaptive content distribution networks for the intelligent distribution of ultrahigh definition content. The preliminary results of the 5G-MEDIA SVP platform evaluation are compared against current practice and show that the proposed platform provides enhanced functionality for the operators and infrastructure owners, while ensuring better NS performance to service providers and end users.
DA  - 2019/06//
PY  - 2019
DO  - 10.1109/TBC.2019.2901400
DP  - IEEE Xplore
VL  - 65
IS  - 2
SP  - 369
EP  - 380
J2  - IEEE Transactions on Broadcasting
SN  - 1557-9611
UR  - https://ieeexplore.ieee.org/document/8667014
Y2  - 2023/12/29/21:29:04
L1  - https://ieeexplore.ieee.org/ielx7/11/8732605/08667014.pdf?tp=&arnumber=8667014&isnumber=8732605&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg2NjcwMTQ=
L2  - https://ieeexplore.ieee.org/document/8667014
ER  - 

TY  - CONF
TI  - Autonomic Computing Challenges in Fully Autonomous Precision Agriculture
AU  - Boubin, Jayson
AU  - Chumley, John
AU  - Stewart, Christopher
AU  - Khanal, Sami
T2  - 2019 IEEE International Conference on Autonomic Computing (ICAC)
AB  - Precision agriculture examines crop fields, gathers data, analyzes crop health and informs field management. This data driven approach can reduce fertilizer runoff, prevent crop disease and increase yield. Frequent data collection improves outcomes, but also increases operating costs. Fully autonomous aerial systems (FAAS) can capture detailed images of crop fields without human intervention. They can reduce operating costs significantly. However, FAAS software must embed agricultural expertise to decide where to fly, which images to capture and when to land. This paper explores fully autonomous precision agriculture where FAAS map crop fields frequently. We have designed hardware and software architecture. We use unmanned aerial systems, edge computing components and software driven by reinforcement learning and ensemble models. In early results, we have collected data from an Ohio cornfield. We use this data to simulate a FAAS modeling crop yield. Our results (1) show that our approach predicts yield well and (2) can quantify computational demand. Computational costs can be prohibitive. We discuss how research on adaptive systems can reduce costs and enable fully autonomous precision agriculture. We also provide our simulation tools and dataset as part of our open source FAAS middleware, SoftwarePilot.
C3  - 2019 IEEE International Conference on Autonomic Computing (ICAC)
DA  - 2019/06//
PY  - 2019
DO  - 10.1109/ICAC.2019.00012
DP  - IEEE Xplore
SP  - 11
EP  - 17
SN  - 2474-0756
UR  - https://ieeexplore.ieee.org/document/8831204
Y2  - 2023/12/29/22:49:58
L2  - https://ieeexplore.ieee.org/document/8831204
ER  - 

TY  - CONF
TI  - Challenges and Opportunities for Efficient Serverless Computing at the Edge
AU  - Gadepalli, Phani Kishore
AU  - Peach, Gregor
AU  - Cherkasova, Ludmila
AU  - Aitken, Rob
AU  - Parmer, Gabriel
T2  - 2019 38th Symposium on Reliable Distributed Systems (SRDS)
AB  - Serverless computing frameworks allow users to execute a small application (dedicated to a specific task) without handling operational issues such as server provisioning, resource management, and resource scaling for the increased load. Serverless computing originally emerged as a Cloud computing framework, but might be a perfect match for IoT data processing at the Edge. However, the existing serverless solutions, based on VMs and containers, are too heavy-weight (large memory footprint and high function invocation time) for operating efficiency and elastic scaling at the Edge. Moreover, many novel IoT applications require low-latency data processing and near real-time responses, which makes the current cloud-based serverless solutions unsuitable. Recently, WebAssembly (Wasm) has been proposed as an alternative method for running serverless applications at near-native speeds, while having a small memory footprint and optimized invocation time. In this paper, we discuss some existing serverless solutions, their design details, and unresolved performance challenges for an efficient serverless management at the Edge. We outline our serverless framework, called aWsm, based on the WebAssembly approach, and discuss the opportunities enabled by the aWsm design, including function profiling and SLO-driven performance management of users' functions. Finally, we present an initial assessment of aWsm performance featuring average startup time (12μs to 30μs) and an economical memory footprint (ranging from 10s to 100s of kB) for a subset of MiBench microbenchmarks used as functions.
C3  - 2019 38th Symposium on Reliable Distributed Systems (SRDS)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/SRDS47363.2019.00036
DP  - IEEE Xplore
SP  - 261
EP  - 2615
SN  - 2575-8462
UR  - https://ieeexplore.ieee.org/document/9049531
Y2  - 2023/12/30/18:57:14
L2  - https://ieeexplore.ieee.org/document/9049531
L2  - https://ieeexplore.ieee.org/document/9049531
ER  - 

TY  - CONF
TI  - Cloud Based Algorithm for Task Management
AU  - Aminov, Parvizsho
AU  - Bola, Navjot
AU  - Shiralkar, Dipti
AU  - Yoganarasimha, Meghana
T2  - 2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)
AB  - Time management is challenging because not only does one want to ensure they are able to get work done on time, they also want to utilize time in the most efficient way. This paper proposes a solution developed using Cloud Computing and a scheduling algorithm based on Pomodoro technique.
C3  - 2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)
DA  - 2019/08//
PY  - 2019
DO  - 10.1109/CSE/EUC.2019.00055
DP  - IEEE Xplore
SP  - 249
EP  - 253
UR  - https://ieeexplore.ieee.org/document/8919599
Y2  - 2023/12/30/18:57:58
L2  - https://ieeexplore.ieee.org/document/8919599
ER  - 

TY  - CONF
TI  - Comparison of Different CI/CD Tools Integrated with Cloud Platform
AU  - Singh, Charanjot
AU  - Gaba, Nikita Seth
AU  - Kaur, Manjot
AU  - Kaur, Bhavleen
T2  - 2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
AB  - A microservice architecture is a routine of enhancing cloud applications which enable large-scale enterprises to scale their application as per the demand. These microservices can be deployed using virtual machines, Docker containers or as a serverless function such as AWS lambda. However, with an increase in the number of microservices, it becomes strenuous to manage and deploy these services. Therefore, to overcome the problem, continuous integration and continuous delivery tool are used to deploy the microservices with minimum possible downtime. In this paper, we will be comparing different continuous integration and delivery tools taking into consideration different parameters like performance monitoring post-deployment, pipeline integration, cloud compatibility, and server monitoring.
C3  - 2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
DA  - 2019/01//
PY  - 2019
DO  - 10.1109/CONFLUENCE.2019.8776985
DP  - IEEE Xplore
SP  - 7
EP  - 12
UR  - https://ieeexplore.ieee.org/document/8776985
Y2  - 2023/12/30/18:58:46
L2  - https://ieeexplore.ieee.org/document/8776985
ER  - 

TY  - CONF
TI  - Data Loss Prevention and Data Protection in Cloud Environments Based on Authentication Tokens
AU  - Bucur, Vlad
AU  - Stan, Ovidiu
AU  - Miclea, Liviu C.
T2  - 2019 22nd International Conference on Control Systems and Computer Science (CSCS)
AB  - Data leek issues represents a very high or critical importance within the cloud developers. Given the tremendous growth of cloud computing in the past 3 years the authors of this paper believe that an analysis of the implementation of modern security solutions for data leaks and data protection is of the utmost importance. Authentication tokens, provided by several different cloud providers and third parties, are one of the most common and useful tools that developers have in order to secure their applications. The goal of this paper is to analyze how these tokens are currently used and to provide a solution in order to improve their implementation by reviewing their compatibility with novel security concepts or challenges such as data tagging, hybrid data security algorithms, microservices deployment using Docker or serverless applications.
C3  - 2019 22nd International Conference on Control Systems and Computer Science (CSCS)
DA  - 2019/05//
PY  - 2019
DO  - 10.1109/CSCS.2019.00128
DP  - IEEE Xplore
SP  - 720
EP  - 725
SN  - 2379-0482
UR  - https://ieeexplore.ieee.org/document/8744776
Y2  - 2023/12/30/18:59:34
L2  - https://ieeexplore.ieee.org/document/8744776
ER  - 

TY  - CONF
TI  - Designing Distributed, Scalable and Extensible System Using Reactive Architectures
AU  - Tovarnitchi, Vasile M.
T2  - 2019 22nd International Conference on Control Systems and Computer Science (CSCS)
AB  - Digital technologies are reshaping our world and are becoming an integral part of our being. It became obvious that in modern days digitalization in each industry is an irreversible process and the speed of adopting of such kind of technologies in an appropriate manner will determine the evolution and survival of any business. Digital technologies play a crucial role in any domain of our life: from simple everyday activities, to such important domains like security, safety and health. We are the witnesses of so-called "Industry 4.0 Revolution" which is dominated by Artificial Intelligence and Cloud Architectures with related technologies. When we think about big change or (r) evolution, we are looking mostly for advantages only, but don't care about challenges and what is happening under the hood. The truth is that "reality is distributed" and the things are getting more complex with each passing day. The information systems are dealing with more and more (huge) amount of data, nevertheless, a quick response (in milliseconds) is expected and no downtime is accepted. In this context, the challenges consist first of all in designing and implementing a reliable (doing well what it was designed for), performant (quick reaction time), low-overhead (non-blocking and optimal use of resources) and flexible (adding new features with minimal effect upon production environment) systems. To implement a system with such kind of features a specific programming models need to be used. In this paper is proposed an architectural approach aimed to ensure scalability and extensibility of a distributed information systems using not fresh, but still very actual concept like Actor Model and REST. Modern Event-Driven, Asynchronous and non-blocking programming models are used.
C3  - 2019 22nd International Conference on Control Systems and Computer Science (CSCS)
DA  - 2019/05//
PY  - 2019
DO  - 10.1109/CSCS.2019.00088
DP  - IEEE Xplore
SP  - 484
EP  - 488
SN  - 2379-0482
UR  - https://ieeexplore.ieee.org/document/8745176
Y2  - 2023/12/30/19:18:11
L2  - https://ieeexplore.ieee.org/document/8745176
ER  - 

TY  - CONF
TI  - Enhancement Full Open Source Hadoop Distribution Universal Big Data Up Projects (UBig) From Education To Enterprise
AU  - Cholissodin, Imam
AU  - Supianto, Ahmad Afif
T2  - 2019 International Conference on Sustainable Information Engineering and Technology (SIET)
AB  - The development of hadoop distribution has been carried out by developers from the big data field starting from Cloudera (CDH), Horton Work (HDP), and MapR, which are more widely used for enterprise version. In addition, when viewed from the field of both formal and non-formal educations, many students and laymen who learn about big data find it difficult to carry out the learning process, especially if the learning process is done locally and completely without a server (serverless), for example like using IBM cognitiveclass, AWS educate, and indeed, this local learning is very cheap without always having to connect to the internet. This is in line with the specifications of netbooks, notebooks and personal computers (PCs) that are used by most students with limited specs, for example it will run slowly when a single-node hadoop is implemented, especially a multi-note hadoop. And also in the hadoop itself requires quite a lot of supporting software that runs on hadoop, such as spark, hive, pig, flume, oozie, sqoop, hue and others. In this study, hadoop distribution with the name `UBig' or `HiDUPs' which was made using Linux Ubuntu and Windows OS so easily to be made independently by each student both the student and the general public as a great development solution to many various fields.
C3  - 2019 International Conference on Sustainable Information Engineering and Technology (SIET)
DA  - 2019/09//
PY  - 2019
DO  - 10.1109/SIET48054.2019.8986040
DP  - IEEE Xplore
SP  - 90
EP  - 93
UR  - https://ieeexplore.ieee.org/document/8986040
Y2  - 2023/12/31/00:56:55
L2  - https://ieeexplore.ieee.org/document/8986040
ER  - 

TY  - CONF
TI  - Evaluating Apache OpenWhisk - FaaS
AU  - Quevedo, Sebastián
AU  - Merchán, Freddy
AU  - Rivadeneira, Rafael
AU  - Dominguez, Federico X.
T2  - 2019 IEEE Fourth Ecuador Technical Chapters Meeting (ETCM)
AB  - Function-as-a-Service (FaaS) platforms enable users to execute user-defined functions without worrying about operational issues such as the management of infrastructure resources. In order to improve performance, different FaaS platforms are implementing optimizations and improvements, but it's not clear how good these implementations are. In this work, Apache OpenWhisk platform is evaluated from an approach that allows to determinate and characterize the performance under different configuration options; it was found that under certain premises an improvement of the performance in cold-booting latencies up to 38% is obtain.
C3  - 2019 IEEE Fourth Ecuador Technical Chapters Meeting (ETCM)
DA  - 2019/11//
PY  - 2019
DO  - 10.1109/ETCM48019.2019.9014867
DP  - IEEE Xplore
SP  - 1
EP  - 5
UR  - https://ieeexplore.ieee.org/document/9014867
Y2  - 2023/12/31/00:57:27
L2  - https://ieeexplore.ieee.org/document/9014867
ER  - 

TY  - CONF
TI  - EyeMath: Increasing Accessibility of Mathematics to Visually Impaired Readers
AU  - Dumkasem, Kanlayanee
AU  - Srisingchai, Padchaya
AU  - Rattanatamrong, Prapaporn
T2  - 2019 23rd International Computer Science and Engineering Conference (ICSEC)
AB  - Mathematics education for visually impaired students is challenging because their learning materials are generally limited to braille books, and audiobooks. In order to increase the chance of learning mathematical content for people with visual impairment, this paper presents the design and development of a cloud-based mobile application called EyeMath, using serverless microservices in Amazon AWS. Users can provide images of page snippets for the application to process and read their content to the users. EyeMath segments an input image into smaller pieces and separates pieces that have only plain text from pieces with mathematical symbols. The mathematical-related pieces are further processed into an Abstract Syntax Tree (AST) and then parsed into Thai sentences. For plain text pieces, EyeMath relies on Tesseract OCR to convert them into text. Finally, results for all pieces are combined together systematically for the device's screen reader program to read aloud. The performance evaluation of the application shows high correctness in reading math content within test images and our usability testing confirms the potential usefulness of the application to visually impaired readers.
C3  - 2019 23rd International Computer Science and Engineering Conference (ICSEC)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/ICSEC47112.2019.8974682
DP  - IEEE Xplore
SP  - 197
EP  - 202
ST  - EyeMath
UR  - https://ieeexplore.ieee.org/document/8974682
Y2  - 2023/12/31/00:57:58
ER  - 

TY  - CONF
TI  - GlobalFlow: A Cross-Region Orchestration Service for Serverless Computing Services
AU  - Zheng, Ge
AU  - Peng, Yang
T2  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
AB  - With the development of serverless computing, orchestration of multiple serverless computing services is highly desired by many cloud-based applications. In this paper, we present GlobalFlow, an orchestration service that can coordinate various geographically distributed but logically dependent serverless computing services through copy-based or connector-based strategy. Through preliminary evaluation, the proposed service has demonstrated its effectiveness in orchestrating various AWS Lambda functions in different regions without significant overhead.
C3  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/CLOUD.2019.00093
DP  - IEEE Xplore
SP  - 508
EP  - 510
SN  - 2159-6190
ST  - GlobalFlow
UR  - https://ieeexplore.ieee.org/document/8814505
Y2  - 2023/12/31/00:58:17
L2  - https://ieeexplore.ieee.org/document/8814505
ER  - 

TY  - CONF
TI  - Gradient Coding Based on Block Designs for Mitigating Adversarial Stragglers
AU  - Kadhe, Swanand
AU  - Koyluoglu, O. Ozan
AU  - Ramchandran, Kannan
T2  - 2019 IEEE International Symposium on Information Theory (ISIT)
AB  - Distributed implementations of gradient-based methods, wherein a server distributes gradient computations across worker machines, suffer from slow running machines, called stragglers. Gradient coding is a coding-theoretic framework to mitigate stragglers by enabling the server to recover the gradient sum in the presence of stragglers. Approximate gradient codes are variants of gradient codes that reduce computation and storage overhead per worker by allowing the server to approximately reconstruct the gradient sum.In this work, our goal is to construct approximate gradient codes that are resilient to stragglers selected by a computationally unbounded adversary. Our motivation for constructing codes to mitigate adversarial stragglers stems from the challenge of tackling stragglers in massive-scale elastic and serverless systems, wherein it is difficult to statistically model stragglers. Towards this end, we propose a class of approximate gradient codes based on balanced incomplete block designs (BIBDs). We show that the approximation error for these codes depends only on the number of stragglers, and thus, adversarial straggler selection has no advantage over random selection. In addition, the proposed codes admit computationally efficient decoding at the server. Next, to characterize fundamental limits of adversarial straggling, we consider the notion of adversarial threshold - the smallest number of workers that an adversary must straggle to inflict certain approximation error. We compute a lower bound on the adversarial threshold, and show that codes based on symmetric BIBDs maximize this lower bound among a wide class of codes, making them excellent candidates for mitigating adversarial stragglers.
C3  - 2019 IEEE International Symposium on Information Theory (ISIT)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/ISIT.2019.8849690
DP  - IEEE Xplore
SP  - 2813
EP  - 2817
SN  - 2157-8117
UR  - https://ieeexplore.ieee.org/document/8849690
Y2  - 2023/12/31/00:58:49
L1  - https://arxiv.org/pdf/1904.13373
L2  - https://ieeexplore.ieee.org/document/8849690
ER  - 

TY  - CONF
TI  - GTAA: A Geo-Aware Task Allocation Approach in Cloud Workflow
AU  - Niu, Meng
AU  - Cheng, Bo
AU  - Chen, Junling
T2  - 2019 IEEE International Conference on Web Services (ICWS)
AB  - The cloud computing simplifies application development into the orchestration of virtual-services workflow. However, network latency between geographically distributed hosts would slow down the workflow's makespan time. This paper proposes a geo-aware task allocation approach (GTAA). GTAA partitions the workflow for geo-distributed data centers(DCs) and reduces sub-workflows across DCs. GTAA aims to optimize overall workflow makespan time and improves the efficiency of workflow.
C3  - 2019 IEEE International Conference on Web Services (ICWS)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/ICWS.2019.00082
DP  - IEEE Xplore
SP  - 449
EP  - 451
ST  - GTAA
UR  - https://ieeexplore.ieee.org/document/8818391
Y2  - 2023/12/31/00:59:08
L2  - https://ieeexplore.ieee.org/document/8818391
ER  - 

TY  - CONF
TI  - HSV Color Space Based Robot Grasping for Personalized Manufacturing Services
AU  - Kang, Hyunchul
AU  - Han, Hyonyoung
AU  - Bae, Heechul
AU  - Lee, Eunseo
AU  - Kim, Mingi
AU  - Son, Jiyeon
AU  - Kim, Hyun
AU  - Kim, Young-Kuk
T2  - 2019 International Conference on Information and Communication Technology Convergence (ICTC)
AB  - Recently, with the development of artificial intelligence technology, robots have been utilized not only in manufacturing but also in many industrial fields. The needs of customers have been diversified and the trend of future manufacturing has been changing from the conventional mass production to the multiproduct small volume production system. In a smart factory environment for custom manufacturing, the robot must be manually programmed for each work instruction every time, especially when a production product is changed. Robot programming takes a lot of time and effort of people. In this paper, we propose a method to automatically measure the product information(color, size) automatically which are 3D printed out based on HSV (hue, saturation, value) Color Space and to control the auto grasping of robots. Also, we applied this control system to the FaaS(Factory As a Service) testbed for functional verification.
C3  - 2019 International Conference on Information and Communication Technology Convergence (ICTC)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/ICTC46691.2019.8939796
DP  - IEEE Xplore
SP  - 1010
EP  - 1012
SN  - 2162-1233
UR  - https://ieeexplore.ieee.org/document/8939796
Y2  - 2023/12/31/00:59:25
L2  - https://ieeexplore.ieee.org/document/8939796
ER  - 

TY  - CONF
TI  - Kotless: A Serverless Framework for Kotlin
AU  - Tankov, Vladislav
AU  - Golubev, Yaroslav
AU  - Bryksin, Timofey
T2  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
AB  - Recent trends in Web development demonstrate an increased interest in serverless applications, i.e. applications that utilize computational resources provided by cloud services on demand instead of requiring traditional server management. This approach enables better resource management while being scalable, reliable, and cost-effective. However, it comes with a number of organizational and technical difficulties which stem from the interaction between the application and the cloud infrastructure, for example, having to set up a recurring task of reuploading updated files. In this paper, we present Kotless - a Kotlin Serverless Framework. Kotless is a cloud-agnostic toolkit that solves these problems by interweaving the deployed application into the cloud infrastructure and automatically generating the necessary deployment code. This relieves developers from having to spend their time integrating and managing their applications instead of developing them. Kotless has proven its capabilities and has been used to develop several serverless applications already in production. Its source code is available at https://github.com/JetBrains/kotless, a tool demo can be found at https://www.youtube.com/watch?v=IMSakPNl3TY.
C3  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DA  - 2019/11//
PY  - 2019
DO  - 10.1109/ASE.2019.00114
DP  - IEEE Xplore
SP  - 1110
EP  - 1113
SN  - 2643-1572
ST  - Kotless
UR  - https://ieeexplore.ieee.org/document/8952482
Y2  - 2023/12/31/01:00:20
L1  - https://arxiv.org/pdf/2105.13866
L2  - https://ieeexplore.ieee.org/document/8952482
ER  - 

TY  - CONF
TI  - Less Energy Consumption Framework for Fog Computing With IoT
AU  - Rana, Prateek
AU  - Sharma, Monika
T2  - 2019 2nd International Conference on Power Energy, Environment and Intelligent Control (PEEIC)
AB  - IOT applications nowadays have quickly expanded and the basic standard centralized models of cloud computing have faced numerous challenging situations which has excessiveness in latency; have low capacity and the failure of network, less capacity of storing and excessive use of power. Fog computing has brought the cloud nearer to the devices of IOT, which deals with the challenges. The services being provided by the fog have quicker response moreover more better quality, in comparison to the cloud. The choices which are best, allow the IOT to offer services which are efficient and secured for most of the users of IOT, that is being measured by the fog computing. In this paper, we are focusing on the fog computing furthermore the incorporation of fog computing with the IOT by specially focusing on the implementation of the challenges is being presented. We have proposed architecture for the less consumption of energy and power.
C3  - 2019 2nd International Conference on Power Energy, Environment and Intelligent Control (PEEIC)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/PEEIC47157.2019.8976772
DP  - IEEE Xplore
SP  - 41
EP  - 46
UR  - https://ieeexplore.ieee.org/document/8976772
Y2  - 2023/12/31/04:28:36
L2  - https://ieeexplore.ieee.org/document/8976772
ER  - 

TY  - CONF
TI  - Low-Cost Serverless SIEM in the Cloud
AU  - Serckumecka, Adriano
AU  - Medeiros, Ibéria
AU  - Bessani, Alysson
T2  - 2019 38th Symposium on Reliable Distributed Systems (SRDS)
AB  - Security systems such as the Security Information and Event Management (SIEMs) have been used to monitor logs and correlate data to quickly detect and respond to incidents. Despite their advantages, SIEMs are expensive to deploy and maintain, requiring extra budget and specialized staff. Another concern is the event retention period, which events are stored for a short period of time, missing important information about how threats may have affected the company infrastructure in the past. This thesis aims to improve these issues by using low-cost cloud services to correlate and store security events. We will investigate techniques to index, compress and store events in the cloud in a cost-efficient and safe way for a long time. We will create a cloud correlation engine using a serverless platform, such as Amazon Lambda. This approach can minimize the complexity of managing SIEMs in place, charging the customer only for the time actually spent processing events. Finally, we will integrate the storage and correlation engine into a cloud SIEM, providing also a monitoring tool, building a complete and innovative low-cost cloud-based security monitoring solution.
C3  - 2019 38th Symposium on Reliable Distributed Systems (SRDS)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/SRDS47363.2019.00057
DP  - IEEE Xplore
SP  - 381
EP  - 3811
SN  - 2575-8462
UR  - https://ieeexplore.ieee.org/document/9049574
Y2  - 2023/12/31/04:29:48
L2  - https://ieeexplore.ieee.org/document/9049574
ER  - 

TY  - CONF
TI  - Model-Based Analysis of Serverless Applications
AU  - Winzinger, Stefan
AU  - Wirtz, Guido
T2  - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering (MiSE)
AB  - Serverless computing is a relatively new execution model where the cloud platform provider manages the allocation of resources for containerized functions dynamically. This evolving paradigm is called Function as a Service (FaaS). The statelessness of these functions enables the application to be scaled up elastically in the case of peak loads. They can be tested easily in isolation, but the behavior arising by integrating them to an application is both hard to predict and test. The parallel execution of the functions and the shift of its state to data storages can cause several workflows accessing the same data. These workflows are hard to detect, particularly for complex applications. Therefore, we suggest an approach for modelling an existing serverless application based on a specialized graph holding all relevant features. Our serverless-specific model can be applied during the whole life cycle of a complex application and offers a good basis for this specific class of applications. It helps to optimize an existing system by identifying hot spots, supports the generation of test cases and can be used to monitor an existing system. Furthermore, we show how the generation of the model can be automated by realizing a tool supporting Amazon's AWS Lambda.
C3  - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering (MiSE)
DA  - 2019/05//
PY  - 2019
DO  - 10.1109/MiSE.2019.00020
DP  - IEEE Xplore
SP  - 82
EP  - 88
SN  - 2575-4475
UR  - https://ieeexplore.ieee.org/document/8877078
Y2  - 2023/12/31/04:31:13
L2  - https://ieeexplore.ieee.org/document/8877078
ER  - 

TY  - CONF
TI  - Online Power Consumption Estimation for Functions in Cloud Applications
AU  - Schmitt, Norbert
AU  - Iffländer, Lukas
AU  - Bauer, André
AU  - Kounev, Samuel
T2  - 2019 IEEE International Conference on Autonomic Computing (ICAC)
AB  - The growth of cloud services leads to more and more data centers that are increasingly larger and consume considerable amounts of power. To increase energy efficiency, informed decisions on workload placement and provisioning are essential. Micro-services and the upcoming serverless platforms with more granular deployment options exacerbate this problem. For this reason, knowing the power consumption of the deployed application becomes crucial, providing the necessary information for autonomous decision making. However, the actual power draw of a server running a specific application under load is not available without specialized measurement equipment or power consumption models. Yet, granularity is often only down to machine level and not application level. In this paper, we propose a monitoring and modeling approach to estimate power consumption on an application function level. The model uses performance counters that are allocated to specific functions to assess their impact on the total power consumption. Hence our model applies to a large variety of servers and for micro-service and serverless workloads. Our model uses an additional correction to minimize falsely allocated performance counters and increase accuracy. We validate the proposed approach on real hardware with a dedicated benchmarking application. The evaluation shows that our approach can be used to monitor application power consumption down to the function level with high accuracy for reliable workload provisioning and placement decisions.
C3  - 2019 IEEE International Conference on Autonomic Computing (ICAC)
DA  - 2019/06//
PY  - 2019
DO  - 10.1109/ICAC.2019.00018
DP  - IEEE Xplore
SP  - 63
EP  - 72
SN  - 2474-0756
UR  - https://ieeexplore.ieee.org/document/8831214
Y2  - 2023/12/31/04:32:11
L2  - https://ieeexplore.ieee.org/document/8831214
ER  - 

TY  - CONF
TI  - Optimizing Latency Sensitive Applications for Amazon's Public Cloud Platform
AU  - Czentye, Janos
AU  - Pelle, Istvan
AU  - Kern, Andras
AU  - Gero, Balazs Peter
AU  - Toka, Laszlo
AU  - Sonkoly, Balazs
T2  - 2019 IEEE Global Communications Conference (GLOBECOM)
AB  - Recent cloud technologies enable a diverse set of novel applications with capabilities never seen before. Cloud native programming, microservices, serverless architectures are novel paradigms reducing the burden on both software developers and operators while enabling cloud-grade service deployments. Several types of applications fit in well with the new concepts, however, latency sensitive applications with strict delay constraints pose additional challenges on the platforms. Can we run these applications on today's public cloud platforms making use of the brand new tools and techniques? In this paper, we try to answer this question by addressing one of the most widely used and versatile public cloud platforms, namely Amazon's AWS, and we propose a novel mechanism to optimize the software "layout" based on dynamic performance measurements. Our contribution is threefold. First, we define a combined performance and cost model on CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS, based on a comprehensive performance analysis, and we also provide an application model capturing the performance requirements. Second, we formulate an optimization problem which minimizes the deployment costs on AWS while meeting the latency constraints. A polynomial algorithm finding the optimal solution is also given. Third, we evaluate the model and the algorithm for different scenarios and investigate the performance on today's system.
C3  - 2019 IEEE Global Communications Conference (GLOBECOM)
DA  - 2019/12//
PY  - 2019
DO  - 10.1109/GLOBECOM38437.2019.9013988
DP  - IEEE Xplore
SP  - 1
EP  - 7
SN  - 2576-6813
UR  - https://ieeexplore.ieee.org/document/9013988
Y2  - 2023/12/31/04:37:45
L2  - https://ieeexplore.ieee.org/document/9013988
ER  - 

TY  - CONF
TI  - Pigeon: A Dynamic and Efficient Serverless and FaaS Framework for Private Cloud
AU  - Ling, Wei
AU  - Ma, Lin
AU  - Tian, Chen
AU  - Hu, Ziang
T2  - 2019 International Conference on Computational Science and Computational Intelligence (CSCI)
AB  - Recently, voice-triggered small cloud functions such as Alexa skills, and cloud mini programs for IoT and smartphone, grow exponentially. These new developments also attract organizations to host their own cloud functions or mini programs in private cloud environment and move from traditional Microservice architecture to Serverless Function-as-a-Service (FaaS) architecture. However, current Serverless FaaS frameworks cannot meet cold start latency, resource efficiency required by short-lived cloud functions and mini programs. In this paper, we build a new Framework - Pigeon that brings Serverless and FaaS programming paradigm into private cloud to enable enterprises to host these applications. Pigeon creates function-oriented Serverless framework by introducing an independent and finer-grained function-level resource scheduler on top of Kubernetes. A new oversubscription-based static pre-warmed container solution is also proposed to effectively reduce function startup latency and increase resource recycling speed for short-lived cloud functions. Empirical results show that Pigeon framework enhances function cold trigger rate by 26% to 80% comparing to AWS Lambda Serverless platform. Comparing to Kubernetes native scheduler based serverless platforms, throughput gets 3 times improvement while handling short-lived functions.
C3  - 2019 International Conference on Computational Science and Computational Intelligence (CSCI)
DA  - 2019/12//
PY  - 2019
DO  - 10.1109/CSCI49370.2019.00265
DP  - IEEE Xplore
SP  - 1416
EP  - 1421
ST  - Pigeon
UR  - https://ieeexplore.ieee.org/document/9071414
Y2  - 2023/12/31/04:39:12
L2  - https://ieeexplore.ieee.org/document/9071414
ER  - 

TY  - CONF
TI  - Policy Management Technique Using Blockchain for Cloud VM Migration
AU  - Uchibayashi, Toshihiro
AU  - Apduhan, Bernady O.
AU  - Shiratori, Norio
AU  - Suganuma, Takuo
AU  - Hiji, Masahiro
T2  - 2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
AB  - In recent years, the use of the cloud has become one of the essential elements when constructing a system for deploying web services. By combining serverless cloud services, various web services can be performed without building servers. However, serverless environments have various limitations, and traditional methods of building servers on VM are indispensable. In addition, technology for migrating VM to physically different hosts was established, and it became possible to easily migrate VM between hosts under the same cloud management. However, depending on the server running on VM, there may be restrictions on the range that can be physically moved. We have solved the problem by focusing on the problem that migration can be performed without checking the information about the server running on the VM at the time of migration. Therefore, in this paper, we introduce a managing technique for VM migration policy in the cloud using blockchain, and show that it is more useful than the conventional policy management method.
C3  - 2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
DA  - 2019/08//
PY  - 2019
DO  - 10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00073
DP  - IEEE Xplore
SP  - 360
EP  - 362
UR  - https://ieeexplore.ieee.org/document/8890425
Y2  - 2023/12/31/04:39:50
L2  - https://ieeexplore.ieee.org/document/8890425
ER  - 

TY  - CONF
TI  - Sapparchi: An Architecture for Smart City Applications from Edge, Fog and Cloud Computing
AU  - Souza, Arthur
AU  - Izidio, Larysse
AU  - Rocha, Aluízio
AU  - Cacho, Nélio
AU  - Batista, Thais
T2  - 2019 IEEE International Smart Cities Conference (ISC2)
AB  - In the Smart Cities context, a plethora of Middleware Platforms has been proposed to support applications execution and data processing. However, just a few of them have explored the overall Smart Cities computing environment. The vast majority focuses on specific domains, typically presenting a sensor-acquisition architecture for processing in Cloud Computing. Most recent initiatives try to include Cloud Computing and Edge Computing, while few of them use the three computing levels (Cloud, Fog, and Edge). Besides, many of these platforms do not define the services that should be deployed at each level, nor how the developer can better use each feature. This work fulfills this gap presenting an architecture for applications classifying services implemented by a typical Computing Environment of Smart Cities. Our architecture uses all the computational levels (Cloud, Fog, Edge) of a city infrastructure, and it defines how to deploy each type of service at each level. We also present an example of the proposed architecture that we are implementing in the city of Natal, where some evaluative tests have been carried out.
C3  - 2019 IEEE International Smart Cities Conference (ISC2)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/ISC246665.2019.9071686
DP  - IEEE Xplore
SP  - 262
EP  - 267
SN  - 2687-8860
ST  - Sapparchi
UR  - https://ieeexplore.ieee.org/document/9071686
Y2  - 2023/12/31/04:40:29
L2  - https://ieeexplore.ieee.org/document/9071686
ER  - 

TY  - CONF
TI  - Scalability Analysis of Blockchain on a Serverless Cloud
AU  - Kaplunovich, Alex
AU  - Joshi, Karuna P.
AU  - Yesha, Yelena
T2  - 2019 IEEE International Conference on Big Data (Big Data)
AB  - While adopting Blockchain technologies to automate their enterprise functionality, organizations are recognizing the challenges of scalability and manual configuration that the state of art present. Scalability of Hyperledger Fabric is an open challenge recognized by the research community. We have automated many of the configuration steps of installing Hyperledger Fabric Blockchain on AWS infrastructure and have benchmarked the scalability of that system. We have used the UCR (University of California Riverside) Time Series Archive with 128 timeseries datasets containing over 191,177 rows of data totaling 76,453,742 numbers. Using an automated Serverless approach, we have loaded this dataset, by chunks, into different AWS instances, triggering the load by SQS messaging. In this paper, we present the results of this benchmarking study and describe the approach we took to automate the Hyperledger Fabric processes using serverless Lambda functions and SQS triggering. We will also discuss what is needed to make the Blockchain technology more robust and scalable.
C3  - 2019 IEEE International Conference on Big Data (Big Data)
DA  - 2019/12//
PY  - 2019
DO  - 10.1109/BigData47090.2019.9005529
DP  - IEEE Xplore
SP  - 4214
EP  - 4222
UR  - https://ieeexplore.ieee.org/document/9005529
Y2  - 2023/12/31/04:40:48
L1  - https://mdsoar.org/bitstream/11603/17207/1/949.pdf
L2  - https://ieeexplore.ieee.org/document/9005529
ER  - 

TY  - CONF
TI  - Serverless Architecture for Big Data Analytics
AU  - Rahman, Md Mijanur
AU  - Hasibul Hasan, Md
T2  - 2019 Global Conference for Advancement in Technology (GCAT)
AB  - Big data is a phrase that describes the large quantity of data, it would be structure, semi structure and unstructured. In the present industry data is indispensable for the business organization. The Big Data initiatives and technologies are used to analyze this massive amount of data for gaining insights which may help in making strategic decisions. For example, data size is increasing day by day like petabyte, Exabyte, zettabyte, yottabyte and more. That is why it is going to tough and complex to manage this large scale of data. In practical, there are many challenges to process and compute this data like server management, storage, clustering, algorithm deployment, etc. As everything is done by manually, so it is hard to design the exact architecture for data analysis in the Cloud. Serverless computing is a mechanism to provide pay-per-use backend services to clients. A serverless provider lets users create and deploy code without worrying about operating and managing servers. In this paper, we present serverless architecture for big data analytics, also we show how to implement, maintain, and governance of a serverless big data application on AWS (Amazon Web Service). In addition to it, we will demonstrate the difference between traditional data analytics and big data analytics in a serverless system.
C3  - 2019 Global Conference for Advancement in Technology (GCAT)
DA  - 2019/10//
PY  - 2019
DO  - 10.1109/GCAT47503.2019.8978443
DP  - IEEE Xplore
SP  - 1
EP  - 5
UR  - https://ieeexplore.ieee.org/document/8978443
Y2  - 2023/12/31/04:43:45
L2  - https://ieeexplore.ieee.org/document/8978443
ER  - 

TY  - CONF
TI  - Serverless Edge Computing for Green Oil and Gas Industry
AU  - Hussain, Razin Farhan
AU  - Salehi, Mohsen Amini
AU  - Semiari, Omid
T2  - 2019 IEEE Green Technologies Conference(GreenTech)
AB  - Escalating demand of petroleum led the Oil and Gas (O&G) industry to extend oil extraction operation in the remote reservoirs. Oil extraction is a fault intolerant process where the maximum penalty is disaster impacting the environment seriously. Therefore, efficient and nature-friendly green oil extraction is a challenging operation, especially with location constrained in accessing the sites. To overcome these challenges and protect the environment from pollution, smart oil fields with numerous sensors (e.g., for pipeline pressure, gas leakage, air pollution) are established to achieve clean O&G extraction. Conventionally, cloud datacenters are utilized to process the generated data. High-latency satellite communication are used for data transfer, which is not suitable for time-sensitive operations/tasks. To process such latency-sensitive tasks, edge computing can be a suitable candidate, however, their computational power goes downhill at disaster time due to surge demand of many coordinated activities. Therefore, we propose green smart oil fields that operate based on edge computing. To overcome shortage of resources and rapid deployment of the edge computing systems, we propose to use lightweight serverless computing on a federation of edge computing resources from nearby oil rigs. Our solution coordinates urgent coordinated operations/tasks to prevent disasters in oil fields and enable the idea of green smart oil fields. Evaluation results demonstrate the efficacy of our proposed solution in compare to conventional solutions for smart oil fields.
C3  - 2019 IEEE Green Technologies Conference(GreenTech)
DA  - 2019/04//
PY  - 2019
DO  - 10.1109/GreenTech.2019.8767119
DP  - IEEE Xplore
SP  - 1
EP  - 4
SN  - 2166-5478
UR  - https://ieeexplore.ieee.org/document/8767119
Y2  - 2023/12/31/04:44:55
L1  - https://arxiv.org/pdf/1905.04460
L2  - https://ieeexplore.ieee.org/document/8767119
ER  - 

TY  - CONF
TI  - Short Paper: TigerAware Assistant: A New Serverless Implementation of Conversational Agents for Customizable Surveys on Smart Devices
AU  - Handrianto, Yohanes
AU  - Huang, Rui
AU  - Shang, Yi
T2  - 2019 First International Conference on ​Transdisciplinary AI (TransAI)
AB  - Applications of conversational agents, also called chatbots, have accelerated in recent years. However, it is difficult for people with limited programming skills to implement chatbots and analyze the data collected by chatbots. In this paper, we present a new serverless implementation of conversational agents for delivering customizable surveys on smart devices, including smartphones and smart speakers, and a web-based system for applying advanced analytics techniques to survey data using cloud services. The system is called TigerAware Assistant, as part of the TigerAware platform for customizable mobile survey and sensor data collection. TigerAware Assistant enables non-technical people to easily create and deploy chatbots on various mobile devices to conduct surveys through conversations in natural languages via auditory and textual methods. It also provides a web-based system for survey data visualization, statistical analysis, and advanced machine learning-based data analysis using cloud services on survey responses collected by chatbots.
C3  - 2019 First International Conference on ​Transdisciplinary AI (TransAI)
DA  - 2019/09//
PY  - 2019
DO  - 10.1109/TransAI46475.2019.00023
DP  - IEEE Xplore
SP  - 88
EP  - 91
ST  - Short Paper
UR  - https://ieeexplore.ieee.org/document/8940418
Y2  - 2023/12/31/04:45:18
L2  - https://ieeexplore.ieee.org/document/8940418
ER  - 

TY  - CONF
TI  - Simulation-as-a-Service with Serverless Computing
AU  - Kritikos, Kyriakos
AU  - Skrzypek, Pawel
T2  - 2019 IEEE World Congress on Services (SERVICES)
AB  - Simulations are the greatest means for evaluating systems and producing knowledge related to their optimal configuration for production. Simulation systems support the execution of simulations. These can be installed and executed internally to an organisation or can be offered as a service in the cloud. Current simulation-as-a-service (SimaaS) offerings rely on VM or container-based deployments which lead to additional costs due to the charging in an hourly basis. Further, such offerings cannot be easily adapted at runtime to still be able to sustain their promised service level. To resolve these issues, this paper proposes a novel SimaaS architecture and solution which exploits the serverless computing paradigm for reducing the simulation cost based on the actual usage of resources as well as accelerating the simulation time through the limitless, parallelised invocation of functions. Further, this solution relies on the MELODIC/Functionizer multi-cloud platform which enables adapting the simulation execution at runtime in order to sustain the right service level according to the user requirements and preferences. For the validation of our solution, a real business application provided by AI Investments has been used. It aims to optimise investment portfolio using the most advanced AI-based methods and requires heavy computational power to accomplish the respective tasks.
C3  - 2019 IEEE World Congress on Services (SERVICES)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/SERVICES.2019.00056
DP  - IEEE Xplore
VL  - 2642-939X
SP  - 200
EP  - 205
SN  - 2642-939X
UR  - https://ieeexplore.ieee.org/document/8817117
Y2  - 2023/12/31/04:45:48
ER  - 

TY  - CONF
TI  - Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud
AU  - Gunasekaran, Jashwant Raj
AU  - Thinakaran, Prashanth
AU  - Kandemir, Mahmut Taylan
AU  - Urgaonkar, Bhuvan
AU  - Kesidis, George
AU  - Das, Chita
T2  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
AB  - We are witnessing the emergence of elastic web services which are hosted in public cloud infrastructures. For reasons of cost-effectiveness, it is crucial for the elasticity of these web services to match the dynamically-evolving user demand. Traditional approaches employ clusters of virtual machines (VMs) to dynamically scale resources based on application demand. However, they still face challenges such as higher cost due to over-provisioning or incur service level objective (SLO) violations due to under-provisioning. Motivated by this observation, we propose Spock, a new scalable and elastic control system that exploits both VMs and serverless functions to reduce cost and ensure SLO for elastic web services. We show that under two different scaling policies, Spock reduces SLO violations of queries by up to 74% when compared to VM-based resource procurement schemes. Further, Spock yields significant cost savings, by up to 33% compared to traditional approaches which use only VMs.
C3  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/CLOUD.2019.00043
DP  - IEEE Xplore
SP  - 199
EP  - 208
SN  - 2159-6190
ST  - Spock
UR  - https://ieeexplore.ieee.org/document/8814535
Y2  - 2023/12/31/04:46:13
L2  - https://ieeexplore.ieee.org/document/8814535
ER  - 

TY  - CONF
TI  - Straggler Resilient Serverless Computing Based on Polar Codes
AU  - Bartan, Burak
AU  - Pilanci, Mert
T2  - 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)
AB  - We propose a serverless computing mechanism for distributed computation based on polar codes. Serverless computing is an emerging cloud based computation model that lets users run their functions on the cloud without provisioning or managing servers. Our proposed approach is a hybrid computing framework that carries out computationally expensive tasks such as linear algebraic operations involving large-scale data using serverless computing and does the rest of the processing locally. We address the limitations and reliability issues of serverless platforms such as straggling workers using coding theory, drawing ideas from recent literature on coded computation. The proposed mechanism uses polar codes to ensure straggler-resilience in a computationally effective manner. We provide extensive evidence showing polar codes outperform other coding methods. We have designed a sequential decoder specifically for polar codes in erasure channels with full-precision input and outputs. In addition, we have extended the proposed method to the matrix multiplication case where both matrices being multiplied are coded. The proposed coded computation scheme is implemented for AWS Lambda. Experiment results are presented where the performance of the proposed coded computation technique is tested in optimization via gradient descent. Finally, we introduce the idea of partial polarization which reduces the computational burden of encoding and decoding at the expense of straggler-resilience.
C3  - 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)
DA  - 2019/09//
PY  - 2019
DO  - 10.1109/ALLERTON.2019.8919767
DP  - IEEE Xplore
SP  - 276
EP  - 283
UR  - https://ieeexplore.ieee.org/document/8919767
Y2  - 2023/12/31/04:47:44
L1  - https://arxiv.org/pdf/1901.06811
L2  - https://ieeexplore.ieee.org/document/8919767
ER  - 

TY  - CONF
TI  - ToLambda–Automatic Path to Serverless Architectures
AU  - Kaplunovich, Alex
T2  - 2019 IEEE/ACM 3rd International Workshop on Refactoring (IWoR)
AB  - Serverless architectures are becoming computing standard and best practice. It is inevitable that more and more software systems will embrace the trend. Our tool toLambda provides automatic conversion of Java monolith application code into AWS Lambda Node.js microservices. During the refactoring, we provide assorted useful transformations of the original code and generate all the necessary artifacts to deploy the generated functions to the Cloud. In this paper we will describe the challenges we have faced including parsing, transformation, performance and testing. We will also underline the advantages of serverless compared to other architectures. Our approach will help to migrate systems to serverless microservices easier and faster.
C3  - 2019 IEEE/ACM 3rd International Workshop on Refactoring (IWoR)
DA  - 2019/05//
PY  - 2019
DO  - 10.1109/IWoR.2019.00008
DP  - IEEE Xplore
SP  - 1
EP  - 8
UR  - https://ieeexplore.ieee.org/document/8844428
Y2  - 2023/12/31/04:48:24
L2  - https://ieeexplore.ieee.org/document/8844428
ER  - 

TY  - CONF
TI  - Towards Latency Sensitive Cloud Native Applications: A Performance Study on AWS
AU  - Pelle, István
AU  - Czentye, János
AU  - Dóka, János
AU  - Sonkoly, Balázs
T2  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
AB  - Microservices, serverless architectures, cloud native programming are novel paradigms and techniques which could significantly reduce the burden on both developers and operators of future services. Several types of applications fit in well with the new concepts easing the life of different stakeholders while enabling cloud-grade service deployments. However, latency sensitive applications with strict delay constraints between different components pose additional challenges on the platforms. In order to gain benefit from recent cloud technologies for latency sensitive applications as well, a comprehensive performance analysis of available platforms and relevant components is a crucial first step. In this paper, we address one of the most widely used and versatile cloud platforms, namely Amazon Web Services (AWS), and reveal the delay characteristics of key components and services which impact the overall performance of latency sensitive applications. Our contribution is threefold. First, we define a detailed measurement methodology for CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS. Second, we provide a comprehensive analysis of AWS components focusing on delay characteristics. Third, we attempt to adjust a drone control application to the platform and investigate the performance on today's system.
C3  - 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/CLOUD.2019.00054
DP  - IEEE Xplore
SP  - 272
EP  - 280
SN  - 2159-6190
ST  - Towards Latency Sensitive Cloud Native Applications
UR  - https://ieeexplore.ieee.org/document/8814512
Y2  - 2023/12/31/04:49:32
L2  - https://ieeexplore.ieee.org/document/8814512
ER  - 

TY  - CONF
TI  - Transpiling Applications into Optimized Serverless Orchestrations
AU  - Scheuner, Joel
AU  - Leitner, Philipp
T2  - 2019 IEEE 4th International Workshops on Foundations and Applications of Self* Systems (FAS*W)
AB  - The serverless computing paradigm promises increased development productivity by abstracting the underlying hardware infrastructure and software runtime when building distributed cloud applications. However, composing a serverless application consisting of many tiny functions is still a cumbersome and inflexible process due to the lack of a unified source code view and strong coupling to non-standardized function-level interfaces for code and configuration. In our vision, developers can focus on writing readable source code in a logical structure, which then gets transformed into an optimized multi-function serverless orchestration. Our idea involves transpilation (i.e., source-to-source transformation) based on an optimization model (e.g., cost optimization) by dynamically deciding which set of methods will be grouped into individual deployment units. A successful implementation of our vision would enable a broader range of serverless applications and allow for dynamic deployment optimization based on monitoring runtime metrics. Further, we would expect increased developer productivity by using more familiar abstractions and facilitating clean coding practices and code reuse.
C3  - 2019 IEEE 4th International Workshops on Foundations and Applications of Self* Systems (FAS*W)
DA  - 2019/06//
PY  - 2019
DO  - 10.1109/FAS-W.2019.00031
DP  - IEEE Xplore
SP  - 72
EP  - 73
UR  - https://ieeexplore.ieee.org/document/8791968
Y2  - 2023/12/31/04:49:54
L2  - https://ieeexplore.ieee.org/document/8791968
ER  - 

TY  - CONF
TI  - Visage – A Visualization and Exploration Framework for Environmental Data
AU  - Conover, Helen
AU  - Berendes, Todd
AU  - Gatlin, Patrick
AU  - Maskey, Manil
AU  - Naeger, Aaron
AU  - Wingo, Stephanie
AU  - Kulkarni, Ajinkya
AU  - Marouane, Abdelhak
AU  - Wang, Lihua
AU  - Ellingson, Brian
AU  - Dahal, Bibek
AU  - Singhirunnusorn, Khomsun
T2  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
AB  - Diverse airborne and ground-based environmental observations are important technologies for disaster assessment and response, as well as for the validation of environmental satellite observations and atmospheric models which can improve forecasts. The VISAGE (Visualization for Integrated Satellite, Airborne and Ground-based data Exploration) project is working to provide three-dimensional visualization and basic analytics capabilities for such datasets in an interactive user interface. The use of cloud-native, serverless technologies and analysis optimized data storage will position VISAGE for integration with other technologies into a Data Analytic Center Framework.
C3  - IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
DA  - 2019/07//
PY  - 2019
DO  - 10.1109/IGARSS.2019.8897954
DP  - IEEE Xplore
SP  - 5405
EP  - 5408
SN  - 2153-7003
UR  - https://ieeexplore.ieee.org/document/8897954
Y2  - 2023/12/31/04:50:26
L2  - https://ieeexplore.ieee.org/document/8897954
ER  - 

TY  - JOUR
TI  - Will Serverless Computing Revolutionize NFV?
AU  - Aditya, Paarijaat
AU  - Akkus, Istemi Ekin
AU  - Beck, Andre
AU  - Chen, Ruichuan
AU  - Hilt, Volker
AU  - Rimac, Ivica
AU  - Satzke, Klaus
AU  - Stein, Manuel
T2  - Proceedings of the IEEE
AB  - Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.
DA  - 2019/04//
PY  - 2019
DO  - 10.1109/JPROC.2019.2898101
DP  - IEEE Xplore
VL  - 107
IS  - 4
SP  - 667
EP  - 678
J2  - Proceedings of the IEEE
SN  - 1558-2256
UR  - https://ieeexplore.ieee.org/document/8653379
Y2  - 2023/12/31/04:51:38
L2  - https://ieeexplore.ieee.org/document/8653379
ER  - 

TY  - JOUR
TI  - A case study on the stability of performance tests for serverless applications
AU  - Eismann, Simon
AU  - Costa, Diego Elias
AU  - Liao, Lizhi
AU  - Bezemer, Cor-Paul
AU  - Shang, Weiyi
AU  - van Hoorn, André
AU  - Kounev, Samuel
T2  - Journal of Systems and Software
AB  - Context:
While in serverless computing, application resource management and operational concerns are generally delegated to the cloud provider, ensuring that serverless applications meet their performance requirements is still a responsibility of the developers. Performance testing is a commonly used performance assessment practice; however, it traditionally requires visibility of the resource environment.
Objective:
In this study, we investigate whether performance tests of serverless applications are stable, that is, if their results are reproducible, and what implications the serverless paradigm has for performance tests.
Method:
We conduct a case study where we collect two datasets of performance test results: (a) repetitions of performance tests for varying memory size and load intensities and (b) three repetitions of the same performance test every day for ten months.
Results:
We find that performance tests of serverless applications are comparatively stable if conducted on the same day. However, we also observe short-term performance variations and frequent long-term performance changes.
Conclusion:
Performance tests for serverless applications can be stable; however, the serverless model impacts the planning, execution, and analysis of performance tests.
DA  - 2022/07/01/
PY  - 2022
DO  - 10.1016/j.jss.2022.111294
DP  - ScienceDirect
VL  - 189
SP  - 111294
J2  - Journal of Systems and Software
SN  - 0164-1212
UR  - https://www.sciencedirect.com/science/article/pii/S0164121222000498
Y2  - 2023/12/31/05:23:47
L1  - https://arxiv.org/pdf/2107.13320
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0164121222000498
KW  - FaaS
KW  - Serverless
KW  - Function-as-a-Service
KW  - Performance
KW  - Reproducibility
KW  - Stability
ER  - 

TY  - JOUR
TI  - A mixed-method empirical study of Function-as-a-Service software development in industrial practice
AU  - Leitner, Philipp
AU  - Wittern, Erik
AU  - Spillner, Josef
AU  - Hummer, Waldemar
T2  - Journal of Systems and Software
AB  - Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.
DA  - 2019/03/01/
PY  - 2019
DO  - 10.1016/j.jss.2018.12.013
DP  - ScienceDirect
VL  - 149
SP  - 340
EP  - 359
J2  - Journal of Systems and Software
SN  - 0164-1212
UR  - https://www.sciencedirect.com/science/article/pii/S0164121218302735
Y2  - 2023/12/31/05:24:03
L1  - https://digitalcollection.zhaw.ch/bitstream/11475/14313/2/2019_Spillner_A_mixed-method_empirical_study_of_Function-as-a-Service_software.pdf
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0164121218302735
KW  - Cloud computing
KW  - Serverless
KW  - Function-as-a-Service
KW  - Empirical research
ER  - 

TY  - JOUR
TI  - A package-aware scheduling strategy for edge serverless functions based on multi-stage optimization
AU  - Zheng, Senjiong
AU  - Liu, Bo
AU  - Lin, Weiwei
AU  - Ye, Xiaoying
AU  - Li, Keqin
T2  - Future Generation Computer Systems
AB  - Serverless computing offers a promising deployment model for edge IoT applications. However, serverless functions that rely on large libraries suffer from severe library loading latency when containerized, which is unfriendly to edge latency-sensitive applications. Most function offload strategies in edge environments ignore the impact of this latency. We also found that the measures taken by serverless platforms to reduce loading latency may not work in edge environments. To remedy that, this paper proposes a function offloading strategy to minimize loading latency, a new way to deeply integrate placement optimization with cache optimization. In this way, we first design a package caching policy suitable for edge environments based on the consistency of execution topology. Then a Double Layers Dynamic Programming algorithm (DLDP) is proposed to solve the problem of function offloading considering the dependent packages using a multi-stage progressive optimization approach. The caching policy is embedded in the scheduling algorithm through a phased optimization approach to achieve joint optimization. Extensive experiments on the cluster trace from Alibaba show that DLDP reduces the loading latency of packages by more than 97.84% and significantly outperforms four baselines in the application completion time by more than 55.67%.
DA  - 2023/07/01/
PY  - 2023
DO  - 10.1016/j.future.2023.02.013
DP  - ScienceDirect
VL  - 144
SP  - 105
EP  - 116
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23000547
Y2  - 2023/12/31/05:24:25
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X23000547
KW  - Dependency package awareness
KW  - Package caching strategy
KW  - Serverless function offloading
ER  - 

TY  - JOUR
TI  - A Resource-optimized and Accelerated Sentiment Analysis Method using Serverless Computing
AU  - Jefferson, Steve
AU  - Chelliah, Pethuru
AU  - Surianarayanan, Chellammal
T2  - Procedia Computer Science
T3  - 4th International Conference on Innovative Data Communication Technology and Application
AB  - Serverless computing is being touted as one of the promising and potential computing paradigms for the future. It clearly represents a deeper automation in information technology (IT) operations. Application developers can get server resources easily and quickly to run their applications and services without bothering about setting up server infrastructure and its sustenance. Application scalability is being met through the infrastructure elasticity. The faster maturity and stability of the containerization aspect has led to the success of the serverless paradigm. With serverless flourishing, the adoption of function as a service (FaaS) is catching up fast. A user request or any noteworthy event can trigger a request to this function to be astutely serviced through the serverless phenomenon. In this paper, we have explained how we are able to optimally use the power of serverless computing to bring forth a new resource-constrained and accelerated sentiment analysis method.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.procs.2022.12.004
DP  - ScienceDirect
VL  - 215
SP  - 33
EP  - 43
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S1877050922020750
Y2  - 2023/12/31/05:25:44
L2  - https://www.sciencedirect.com/science/article/pii/S1877050922020750
KW  - serverless computing
KW  - machine learning techniques
KW  - sentiment analysis
ER  - 

TY  - JOUR
TI  - A scalable modified deep reinforcement learning algorithm for serverless IoT microservice composition infrastructure in fog layer
AU  - Emami Khansari, Mina
AU  - Sharifian, Saeed
T2  - Future Generation Computer Systems
AB  - Nowadays many modern and Artificial Intelligence (AI) enabled Internet of Things (IoT) applications consist of chains connecting microservices distributed across the fog and cloud layers to achieve a certain functionality. These microservices should be assigned to the available processing instances placed on the things, fog and cloud layers while considering the resource utilization, Quality of Service (QoS) parameters and requirements of the user. Despite several existing optimization approaches, IoT microservice composition remains a challenge in serverless computing architecture due to the large-scale environment, its dynamic nature and the heterogeneous nodes with limited capacity. To address these challenges, a novel IoT microservice composition based on serverless architecture is proposed in the fog layer. Serverless computing is a new paradigm which offers the ability to build scalable applications without worrying about back-end management making this paradigm particularly effective in IoT environment. We specifically propose a modified Deep Reinforcement Learning (DRL)-based Microservice Chaining at Fog Layer (DRLMCF) algorithm to improve the distributed chained microservice placement in order to minimize resource utilization and delay in a fog based serverless architecture. Unlike existing methods, the proposed DRLMCF is built on realistic assumptions and does not require expert tuning or a large amount of labeled data to make optimal decisions. Furthermore, our method is not entirely dependent on a central controller and fog nodes take part in the decision making thus making the algorithm scalable in IoT environment. Compared to state-of-the art methods including load balance, shortest path and random schedule algorithms, evaluation by simulating several real-world scenarios demonstrates the proposed DRLMCF improves end to end delay up to 57.3% and the number of successfully chained microservices by up to 84%.
DA  - 2024/04/01/
PY  - 2024
DO  - 10.1016/j.future.2023.11.022
DP  - ScienceDirect
VL  - 153
SP  - 206
EP  - 221
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23004284
Y2  - 2023/12/31/05:25:57
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X23004284
KW  - Serverless computing
KW  - IoT
KW  - Function as a service
KW  - Fog computing
KW  - Microservice composition
KW  - Modified deep reinforcement learning
ER  - 

TY  - JOUR
TI  - A Seer knows best: Auto-tuned object storage shuffling for serverless analytics
AU  - Eizaguirre, Germán T.
AU  - Sánchez-Artigas, Marc
T2  - Journal of Parallel and Distributed Computing
AB  - Serverless platforms offer high resource elasticity and pay-as-you-go billing, making them a compelling choice for data analytics. To craft a “pure” serverless solution, the common practice is to transfer intermediate data between serverless functions via serverless object storage (IBM COS; AWS S3). However, prior works have led to inconclusive results about the performance of object storage systems, since they have left large margin for optimization. To verify that object storage has been underrated, we devise a novel shuffle manager for serverless data analytics called Seer. Specifically, Seer dynamically chooses between two shuffle algorithms to maximize performance. The algorithm choice is made online based on some predictive models, and very importantly, without end users having to specify intermediate shuffle data sizes at the time of the job submission. We integrate Seer with PyWren-IBM [31], a well-known serverless analytics framework, and evaluate it against both serverful (e.g., Spark) and serverless systems (e.g., Google BigQuery, Caerus [46] and SONIC [22]). Our results certify that our new shuffle manager can deliver performance improvements over them.
DA  - 2024/01/01/
PY  - 2024
DO  - 10.1016/j.jpdc.2023.104763
DP  - ScienceDirect
VL  - 183
SP  - 104763
J2  - Journal of Parallel and Distributed Computing
SN  - 0743-7315
ST  - A Seer knows best
UR  - https://www.sciencedirect.com/science/article/pii/S0743731523001338
Y2  - 2023/12/31/05:26:15
L2  - https://www.sciencedirect.com/science/article/pii/S0743731523001338
KW  - Serverless computing
KW  - I/O optimization
KW  - Object storage
KW  - Shuffle
ER  - 

TY  - JOUR
TI  - AFCL: An Abstract Function Choreography Language for serverless workflow specification
AU  - Ristov, Sasko
AU  - Pedratscher, Stefan
AU  - Fahringer, Thomas
T2  - Future Generation Computer Systems
AB  - Serverless workflow applications or function choreographies (FCs), which connect serverless functions by data- and control-flow, have gained considerable momentum recently to create more sophisticated applications as part of Function-as-a-Service (FaaS) platforms. Initial experimental analysis of the current support for FCs uncovered important weaknesses, including provider lock-in, and limited support for important data-flow and control-flow constructs. To overcome some of these weaknesses, we introduce the Abstract Function Choreography Language (AFCL) for describing FCs at a high-level of abstraction, which abstracts the function implementations from the developer. AFCL is a YAML-based language that supports a rich set of constructs to express advanced control-flow (e.g. parallelFor loops, parallel sections, dynamic loop iterations counts) and data-flow (e.g multiple input and output parameters of functions, DAG-based data-flow). We introduce data collections which can be distributed to loop iterations and parallel sections that may substantially reduce the delays for function invocations due to reduced data transfers between functions. We also support asynchronous functions to avoid delays due to blocking functions. AFCL supports properties (e.g. expected size of function input data) and constraints (e.g. minimize execution time) for the user to optionally provide hints about the behavior of functions and FCs and to control the optimization by the underlying execution environment. We implemented a prototype AFCL environment that supports AFCL as input language with multiple backends (AWS Lambda and IBM Cloud Functions) thus avoiding provider lock-in which is a common problem in serverless computing. We created two realistic FCs from two different domains and encoded them with AWS Step Functions, IBM Composer and AFCL. Experimental results demonstrate that our current implementation of the AFCL environment substantially outperforms AWS Step Functions and IBM Composer in terms of development effort, economic costs, and makespan.
DA  - 2021/01/01/
PY  - 2021
DO  - 10.1016/j.future.2020.08.012
DP  - ScienceDirect
VL  - 114
SP  - 368
EP  - 382
J2  - Future Generation Computer Systems
SN  - 0167-739X
ST  - AFCL
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X20302648
Y2  - 2023/12/31/05:28:54
L1  - https://diglib.uibk.ac.at/ulbtirolfodok/content/titleinfo/6683272/full.pdf
L2  - https://www.sciencedirect.com/science/article/pii/S0167739X20302648
KW  - FaaS
KW  - Performance
KW  - AWS Step Functions
KW  - Cost
KW  - IBM Composer
ER  - 

TY  - JOUR
TI  - AI augmented Edge and Fog computing: Trends and challenges
AU  - Tuli, Shreshth
AU  - Mirhakimi, Fatemeh
AU  - Pallewatta, Samodha
AU  - Zawad, Syed
AU  - Casale, Giuliano
AU  - Javadi, Bahman
AU  - Yan, Feng
AU  - Buyya, Rajkumar
AU  - Jennings, Nicholas R.
T2  - Journal of Network and Computer Applications
AB  - In recent years, the landscape of computing paradigms has witnessed a gradual yet remarkable shift from monolithic computing to distributed and decentralized paradigms such as Internet of Things (IoT), Edge, Fog, Cloud, and Serverless. The frontiers of these computing technologies have been boosted by shift from manually encoded algorithms to Artificial Intelligence (AI)-driven autonomous systems for optimum and reliable management of distributed computing resources. Prior work focuses on improving existing systems using AI across a wide range of domains, such as efficient resource provisioning, application deployment, task placement, and service management. This survey reviews the evolution of data-driven AI-augmented technologies and their impact on computing systems. We demystify new techniques and draw key insights in Edge, Fog and Cloud resource management-related uses of AI methods and also look at how AI can innovate traditional applications for enhanced Quality of Service (QoS) in the presence of a continuum of resources. We present the latest trends and impact areas such as optimizing AI models that are deployed on or for computing systems. We layout a roadmap for future research directions in areas such as resource management for QoS optimization and service reliability. Finally, we discuss blue-sky ideas and envision this work as an anchor point for future research on AI-driven computing systems.
DA  - 2023/07/01/
PY  - 2023
DO  - 10.1016/j.jnca.2023.103648
DP  - ScienceDirect
VL  - 216
SP  - 103648
J2  - Journal of Network and Computer Applications
SN  - 1084-8045
ST  - AI augmented Edge and Fog computing
UR  - https://www.sciencedirect.com/science/article/pii/S108480452300067X
Y2  - 2023/12/31/05:29:17
L1  - https://spiral.imperial.ac.uk/bitstream/10044/1/104461/7/1-s2.0-S108480452300067X-main.pdf
L2  - https://www.sciencedirect.com/science/article/pii/S108480452300067X
KW  - Cloud computing
KW  - AI
KW  - Fog computing
KW  - Edge computing
KW  - Deployment
KW  - Fault-tolerance
KW  - Scheduling
ER  - 

TY  - JOUR
TI  - Applications of blockchain in unmanned aerial vehicles: A review
AU  - Alladi, Tejasvi
AU  - Chamola, Vinay
AU  - Sahu, Nishad
AU  - Guizani, Mohsen
T2  - Vehicular Communications
AB  - The recent advancement in Unmanned Aerial Vehicles (UAVs) in terms of manufacturing processes, and communication and networking technology has led to a rise in their usage in civilian and commercial applications. The regulations of the Federal Aviation Administration (FAA) in the US had earlier limited the usage of UAVs to military applications. However more recently, the FAA has outlined new enforcement that will also expand the usage of UAVs in civilian and commercial applications. Due to being deployed in open atmosphere, UAVs are vulnerable to being lost, destroyed or physically hijacked. With the UAV technology becoming ubiquitous, various issues in UAV networks such as intra-UAV communication, UAV security, air data security, data storage and management, etc. need to be addressed. Blockchain being a distributed ledger protects the shared data using cryptography techniques such as hash functions and public key encryption. It can also be used for assuring the truthfulness of the information stored and for improving the security and transparency of the UAVs. In this paper, we review various applications of blockchain in UAV networks such as network security, decentralized storage, inventory management, surveillance, etc., and discuss some broader perspectives in this regard. We also discuss various challenges to be addressed in the integration of blockchain and UAVs and suggest some future research directions.
DA  - 2020/06/01/
PY  - 2020
DO  - 10.1016/j.vehcom.2020.100249
DP  - ScienceDirect
VL  - 23
SP  - 100249
J2  - Vehicular Communications
SN  - 2214-2096
ST  - Applications of blockchain in unmanned aerial vehicles
UR  - https://www.sciencedirect.com/science/article/pii/S2214209620300206
Y2  - 2023/12/31/05:30:35
L2  - https://www.sciencedirect.com/science/article/pii/S2214209620300206
KW  - Blockchain technology
KW  - Internet of Things (IoT)
KW  - Security and privacy
KW  - Unmanned Aerial Vehicle (UAV) network
ER  - 

TY  - JOUR
TI  - Automatic abdominal segmentation using novel 3D self-adjustable organ aware deep network in CT images
AU  - Li, Laquan
AU  - Zhao, Haiguo
AU  - Wang, Hong
AU  - Li, Weisheng
AU  - Zheng, Shenhai
T2  - Biomedical Signal Processing and Control
AB  - CT scan is an important reference means of disease diagnosis in practice. Automatic segmentation of organ regions can save a lot of time and labor costs, and allow doctors to produce more intuitive observations of the organization of the human body. However, automatic multi-organ segmentation in CT images remains challenging due to the complicated anatomical structures and low tissue contrast in CT images. Traditional segmentation methods are relatively inefficient for organ segmentation with large abdominal deformation, small volume, and blurry tissue boundaries, and the traditional network architectures are rarely designed to meet the requirements of lightweight and efficient clinical practice. In this paper, we propose a novel segmentation network named Self-Adjustable Organ Attention U-Net (SOA-Net) to overcome these limitations. To be a pragmatic solution for effective segmentation method, the SOA-Net includes multi-branches feature attention (MBFA) module and the feature attention aggregation (FAA) module. These two modules have multiple branches with different kernel sizes to capture different scales feature information based on multiple scales of the target organs. An adjustable attention is used on these branches to generate different sizes of the receptive fields in the fusion layer. On the whole, SOA-Net is a 3D self-adjustable organ aware deep network which can adaptively adjust their attention and receptive field sizes based on multiple scales of the target organs to realize the efficient segmentation of multiple abdominal organs. We evaluate our method on AbdomenCT-1K and AMOS2022 datasets and the final experiments proved that our model achieves the best segmentation performance compared with the state-of-the-art segmentation networks. (Our code will be publicly available soon).
DA  - 2023/07/01/
PY  - 2023
DO  - 10.1016/j.bspc.2023.104691
DP  - ScienceDirect
VL  - 84
SP  - 104691
J2  - Biomedical Signal Processing and Control
SN  - 1746-8094
UR  - https://www.sciencedirect.com/science/article/pii/S1746809423001246
Y2  - 2023/12/31/05:31:08
L2  - https://www.sciencedirect.com/science/article/abs/pii/S1746809423001246
KW  - Deep learning
KW  - Abdominal image
KW  - Attention mechanism
KW  - Multi-organ segmentation
ER  - 

TY  - JOUR
TI  - Challenges In Optimizing Migration Costs From On-Premises To Microsoft Azure
AU  - Olariu, Florin
AU  - Alboaie, Lenuța
T2  - Procedia Computer Science
T3  - 27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)
AB  - The current paper analyzes the feasibility of a modular web application's migration procedure from on-premises to the cloud. Our focus is on identifying cost savings and options for hosting the application in the cloud. The research specifically examines the impact of architectural decisions records (ADR) on a modular monolith use case, utilizing .NET Core for the backend and Angular for the front end, with Clean Architecture as the design pattern. We investigate different cloud models (IaaS, PaaS, Serverless) considering project management's triple constraints (time, cost, performance). The study demonstrates that a modular monolith can be migrated with varying effort and costs depending on the chosen cloud model and technology stack. We explore optimizations such as resource utilization, licensing fees, and cost reduction through infrastructure reservations. Our findings show that the migration cost can range from a 20% increase with IaaS to approximately 70% cost reduction with the Serverless strategy compared to the on-premises environment, using equivalent resources. We also explore methods to lower expenses for each model, including resource modifications, Linux operating systems, and longer resource reservations. Considering limitations, we propose a two-stage migration strategy: initially lifting and shifting the application to IaaS with cost optimization, and subsequently migrating to PaaS for scalability and simplified resource management in the long term.
DA  - 2023/01/01/
PY  - 2023
DO  - 10.1016/j.procs.2023.10.360
DP  - ScienceDirect
VL  - 225
SP  - 3649
EP  - 3659
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S1877050923015181
Y2  - 2023/12/31/05:34:11
L2  - https://www.sciencedirect.com/science/article/pii/S1877050923015181
KW  - Azure Pricing
KW  - Clean Architecture
KW  - Cloud Migration
KW  - Lift and Shift
KW  - Modular monolith
KW  - Optimizing costs
KW  - Optimizing resources
ER  - 

TY  - JOUR
TI  - Composition of heterogeneous web services: A systematic review
AU  - Huf, Alexis
AU  - Siqueira, Frank
T2  - Journal of Network and Computer Applications
AB  - Initial developments in Service-Oriented Computing (SOC) led to the development of Web Services using the SOAP protocol and an extensive set of tools and methods for composing new services from those existing. Subsequently, other types of services also emerged, such as event-oriented services and RESTful services. Nevertheless, all mentioned service types expose data and functionality, and users can benefit from their composition, regardless of the service type chosen for their implementation. In the Internet of Things, it is relevant to employ event-oriented services for sensing and SOAP, RESTful or lightweight web APIs for control. In the emerging field of microservices, heterogeneity is embraced as a design principle and services that are part of a single system may be implemented using heterogeneous technologies and paradigms. The research question of this review is: How heterogeneous services can be composed? There are several surveys that cover service composition with each of the existing service types, but the composition of heterogeneous services is only marginally addressed. This systematic literature review focuses explicitly on the heterogeneity of the aforementioned service types. A total of 66 documents, published from 2005 to 2018, have been surveyed, targeting all possible combinations of the three service types. In addition to summarizing existing works, the specific methods employed for supporting service type heterogeneity are grouped into archetypes and have their limitations and capabilities analyzed. Despite the large number of documents found, there are several open issues on heterogeneous service composition. The results of this review are confronted with emerging fields in service computing, namely microservices, serverless and IoT, yielding additional research directions.
DA  - 2019/10/01/
PY  - 2019
DO  - 10.1016/j.jnca.2019.06.008
DP  - ScienceDirect
VL  - 143
SP  - 89
EP  - 110
J2  - Journal of Network and Computer Applications
SN  - 1084-8045
ST  - Composition of heterogeneous web services
UR  - https://www.sciencedirect.com/science/article/pii/S108480451930205X
Y2  - 2023/12/31/05:42:08
L2  - https://www.sciencedirect.com/science/article/abs/pii/S108480451930205X
KW  - Microservices
KW  - Event-oriented services
KW  - Heterogeneous services
KW  - RESTful services
KW  - SOAP services
KW  - Web service composition
ER  - 

TY  - JOUR
TI  - Container security: Precaution levels, mitigation strategies, and research perspectives
AU  - V s, Devi Priya
AU  - Chakkaravarthy Sethuraman, Sibi
AU  - Khan, Muhammad Khurram
T2  - Computers & Security
AB  - The enterprise technique for application deployment has undergone a major transformation during the past two decades. Using conventional techniques, software developers write code in a particular computing environment, frequently leading to mistakes and defects when moving it to a new computing environment. However, during the past few years, enterprises have begun to use containers & microservices to segregate infrastructure in a particular perspective and develop new models of the technology stack. Software developers could construct and deploy apps more quickly and effectively now, thanks to containerization. Despite the fact that containers have their own namespace, it is still feasible for a containerized image to attack the host system by inserting malicious software into it. This necessitates threat modeling of the container life span. During the investigation, we were able to create the elemental systematic modelling that identifies threats pertaining to container application workflow and its preliminary mitigation techniques, where attack trees are defined alongside the model, which helps academics and enthusiasts better comprehend the significance of container security. We utilize the well-known threat modeling framework, DREAD, to further advance threat modeling across the infrastructure of containers that aids in prioritizing the risks. Additionally, tools for assessing container vulnerabilities and discrete real-world exploits were researched, and approaches for security analysis in container technology were compared to the existing literature. Finally, this study brings to a conclusion by outlining the state-of-the-art survey for future research and identifying potential research topics in server-based and serverless containers.
DA  - 2023/12/01/
PY  - 2023
DO  - 10.1016/j.cose.2023.103490
DP  - ScienceDirect
VL  - 135
SP  - 103490
J2  - Computers & Security
SN  - 0167-4048
ST  - Container security
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823004005
Y2  - 2023/12/31/05:42:52
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167404823004005
KW  - Microservices
KW  - Container security- root-based and rootless
KW  - DREAD
KW  - Software development
KW  - Threat modeling-attack trees
ER  - 

TY  - JOUR
TI  - Cost estimates for modern e-business systems
AU  - Aoshima, Tomohisa
AU  - Yoshida, Kenichi
T2  - Procedia Computer Science
T3  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022
AB  - Recently, the adoption of cloud computing for e-business has become standardized and began evolving into further execution models, such as serverless computing. One of the features of cloud computing is to provide computational resources on a pay-as-you-go model, where users pay only for what they use. These managed and serviced resources have characteristics that make them suitable for cost estimation in the early stages of business. This paper proposes a rapid cost estimation method for serverless computing using directed acyclic graph- (DAG) based formalization and matrix operations. Using a case study, we demonstrate that cost estimation under serverless computing can be effective in the early stages of business. Furthermore, we show that the cost estimation of serverless computing is more proportional to the required computational resources than the virtual machine- (VM) based cost estimation method.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.procs.2022.09.121
DP  - ScienceDirect
VL  - 207
SP  - 664
EP  - 673
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S187705092201002X
Y2  - 2023/12/31/05:43:13
L2  - https://www.sciencedirect.com/science/article/pii/S187705092201002X
KW  - cloud computing
KW  - serverless computing
KW  - application topology
KW  - cost estimation
KW  - e-business
ER  - 

TY  - JOUR
TI  - DCML: Deep contrastive mutual learning for COVID-19 recognition
AU  - Zhang, Hongbin
AU  - Liang, Weinan
AU  - Li, Chuanxiu
AU  - Xiong, Qipeng
AU  - Shi, Haowei
AU  - Hu, Lang
AU  - Li, Guangli
T2  - Biomedical Signal Processing and Control
AB  - COVID-19 is a form of disease triggered by a new strain of coronavirus. Automatic COVID-19 recognition using computer-aided methods is beneficial for speeding up diagnosis efficiency. Current researches usually focus on a deeper or wider neural network for COVID-19 recognition. And the implicit contrastive relationship between different samples has not been fully explored. To address these problems, we propose a novel model, called deep contrastive mutual learning (DCML), to diagnose COVID-19 more effectively. A multi-way data augmentation strategy based on Fast AutoAugment (FAA) was employed to enrich the original training dataset, which helps reduce the risk of overfitting. Then, we incorporated the popular contrastive learning idea into the conventional deep mutual learning (DML) framework to mine the relationship between diverse samples and created more discriminative image features through a new adaptive model fusion method. Experimental results on three public datasets demonstrate that the DCML model outperforms other state-of-the-art baselines. More importantly, DCML is easier to reproduce and relatively efficient, strengthening its high practicality.
DA  - 2022/08/01/
PY  - 2022
DO  - 10.1016/j.bspc.2022.103770
DP  - ScienceDirect
VL  - 77
SP  - 103770
J2  - Biomedical Signal Processing and Control
SN  - 1746-8094
ST  - DCML
UR  - https://www.sciencedirect.com/science/article/pii/S1746809422002920
Y2  - 2023/12/31/05:43:34
L1  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9058053/pdf/main.pdf
KW  - Adaptive model fusion
KW  - Contrastive learning
KW  - COVID-19 recognition
KW  - Deep mutual learning
KW  - Fast AutoAugment
ER  - 

TY  - JOUR
TI  - Deep learning architectures in emerging cloud computing architectures: Recent development, challenges and next research trend
AU  - Jauro, Fatsuma
AU  - Chiroma, Haruna
AU  - Gital, Abdulsalam Y.
AU  - Almutairi, Mubarak
AU  - Abdulhamid, Shafi’i M.
AU  - Abawajy, Jemal H.
T2  - Applied Soft Computing
AB  - The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures.
DA  - 2020/11/01/
PY  - 2020
DO  - 10.1016/j.asoc.2020.106582
DP  - ScienceDirect
VL  - 96
SP  - 106582
J2  - Applied Soft Computing
SN  - 1568-4946
ST  - Deep learning architectures in emerging cloud computing architectures
UR  - https://www.sciencedirect.com/science/article/pii/S1568494620305202
Y2  - 2023/12/31/05:43:45
L2  - https://www.sciencedirect.com/science/article/abs/pii/S1568494620305202
KW  - Serverless computing
KW  - Fog computing
KW  - Convolutional neural network
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Edge computing
KW  - Emerging cloud computing
ER  - 

TY  - JOUR
TI  - Deep reinforcement learning for application scheduling in resource-constrained, multi-tenant serverless computing environments
AU  - Mampage, Anupama
AU  - Karunasekera, Shanika
AU  - Buyya, Rajkumar
T2  - Future Generation Computer Systems
AB  - Serverless computing has sparked a massive interest in both the cloud service providers and their clientele in recent years. This model entails the shift of the entire matter of resource management of user applications to the service provider. In serverless systems, the provider is highly motivated to attain cost efficient usage of their infrastructure, given the granular billing modules involved. However, due to the dynamic and multi-tenant nature of the serverless workloads and systems, achieving efficient resource management while maintaining function performance is a challenging task. Rapid changes in demand levels for applications cause variations in actual resource usage patterns of function instances. This leads to performance variations in co-located functions which compete for similar resources, due to resource contentions. Most existing serverless scheduling works offer heuristic techniques for function scheduling, which are unable to capture the true dynamism in these systems caused by multi-tenancy and varying user request patterns. Further, they rarely consider the often contradicting dual objectives of achieving provider resource efficiency along with application performance. In this article, we propose a novel technique incorporating Deep Reinforcement Learning (DRL) to overcome the aforementioned challenges for function scheduling in a highly dynamic serverless computing environment with heterogeneous computing resources. We train and evaluate our model in a practical setting incorporating Kubeless, an open-source serverless framework, deployed on a 23-node Kubernetes cluster setup. Extensive experiments done on this testbed environment show promising results with improvements of up to 24% and 34% in terms of application response time and resource usage cost respectively, compared to baseline techniques.
DA  - 2023/06/01/
PY  - 2023
DO  - 10.1016/j.future.2023.02.006
DP  - ScienceDirect
VL  - 143
SP  - 277
EP  - 292
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X2300047X
Y2  - 2023/12/31/05:46:01
KW  - Serverless computing
KW  - Function scheduling
KW  - Practical experiments
KW  - Reinforcement learning
KW  - Resource contention
KW  - Resource cost efficiency
ER  - 

TY  - JOUR
TI  - Deep Reinforcement Learning-based scheduling for optimizing system load and response time in edge and fog computing environments
AU  - Wang, Zhiyu
AU  - Goudarzi, Mohammad
AU  - Gong, Mingming
AU  - Buyya, Rajkumar
T2  - Future Generation Computer Systems
AB  - Edge/fog computing, as a distributed computing paradigm, satisfies the low-latency requirements of ever-increasing number of IoT applications and has become the mainstream computing paradigm behind IoT applications. However, because large number of IoT applications require execution on the edge/fog resources, the servers may be overloaded. Hence, it may disrupt the edge/fog servers and also negatively affect IoT applications’ response time. Moreover, many IoT applications are composed of dependent components incurring extra constraints for their execution. Besides, edge/fog computing environments and IoT applications are inherently dynamic and stochastic. Thus, efficient and adaptive scheduling of IoT applications in heterogeneous edge/fog computing environments is of paramount importance. However, limited computational resources on edge/fog servers imposes an extra burden for applying optimal but computationally demanding techniques. To overcome these challenges, we propose a Deep Reinforcement Learning-based IoT application Scheduling algorithm, called DRLIS to adaptively and efficiently optimize the response time of heterogeneous IoT applications and balance the load of the edge/fog servers. We implemented DRLIS as a practical scheduler in the FogBus2 function-as-a-service framework for creating an edge–fog–cloud integrated serverless computing environment. Results obtained from extensive experiments show that DRLIS significantly reduces the execution cost of IoT applications by up to 55%, 37%, and 50% in terms of load balancing, response time, and weighted cost, respectively, compared with metaheuristic algorithms and other reinforcement learning techniques.
DA  - 2024/03/01/
PY  - 2024
DO  - 10.1016/j.future.2023.10.012
DP  - ScienceDirect
VL  - 152
SP  - 55
EP  - 69
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23003862
Y2  - 2023/12/31/05:46:16
L1  - https://arxiv.org/pdf/2309.07407
KW  - Internet of Things
KW  - Machine learning
KW  - Fog computing
KW  - Deep reinforcement learning
KW  - Edge computing
ER  - 

TY  - JOUR
TI  - Denial of wallet—Defining a looming threat to serverless computing
AU  - Kelly, Daniel
AU  - Glavin, Frank G.
AU  - Barrett, Enda
T2  - Journal of Information Security and Applications
AB  - Serverless computing is the latest paradigm in cloud computing, offering a framework for the development of event driven, pay-as-you-go functions in a highly scalable environment. While these traits offer a powerful new development paradigm, they have also given rise to a new form of cyber-attack known as Denial of Wallet (forced financial exhaustion). In this work, we define and identify the threat of Denial of Wallet and its potential attack patterns. Also, we demonstrate how this new form of attack can potentially circumvent existing mitigation systems developed for a similar style of attack, Denial of Service. Our goal is twofold. Firstly, we will provide a concise and informative overview of this emerging attack paradigm. Secondly, we propose this paper as a starting point to enable researchers and service providers to create effective mitigation strategies. We include some simulated experiments to highlight the potential financial damage that such attacks can cause and the creation of an isolated test bed for continued safe research on these attacks.
DA  - 2021/08/01/
PY  - 2021
DO  - 10.1016/j.jisa.2021.102843
DP  - ScienceDirect
VL  - 60
SP  - 102843
J2  - Journal of Information Security and Applications
SN  - 2214-2126
UR  - https://www.sciencedirect.com/science/article/pii/S221421262100079X
Y2  - 2023/12/31/05:47:11
L1  - https://arxiv.org/pdf/2104.08031
KW  - Cloud computing
KW  - Serverless computing
KW  - Cloud security
KW  - Denial-of-wallet
KW  - Function-as-a-service
ER  - 

TY  - JOUR
TI  - Design and Implementation of Medical Searching System Based on Microservices and Serverless Architectures
AU  - Sadek, Jawad
AU  - Craig, Dawn
AU  - Trenell, Michael
T2  - Procedia Computer Science
T3  - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021
AB  - Microservices and Serverless computing are promising cloud-based architectural models in the software development industry that have many advantages over previous technology models. The benefit of adopting these more novel models, however, is dependent on the volume of the application workload and the execution behavior. ScanMedicine is a new searching system dedicated to providing health care professionals, patients and researchers with easy access to intelligence underpinning health technology innovations. We present the design and implementation of ScanMedicine’s framework using AWS lambda functions and microservices. We incorporated a layered architectural style where each layer run on separate hardware and adopt different architectural technique.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.procs.2021.12.056
DP  - ScienceDirect
VL  - 196
SP  - 615
EP  - 622
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921022791
Y2  - 2023/12/31/05:47:20
KW  - AWS
KW  - Cloud Computing
KW  - Serverless
KW  - Clinical Trials
KW  - Medical Devices
KW  - Microservices
ER  - 

TY  - JOUR
TI  - Dissecting lightning strike hazard impact patterns to National Airspace System facilities in the contiguous United States
AU  - He, Yiyi
AU  - Yue, Xiangyu
AU  - Lindbergh, Sarah
AU  - Gao, Jianxi
AU  - Graves, Chuck
AU  - Rakas, Jasenka
T2  - Computers, Environment and Urban Systems
AB  - Lightning strikes pose a severe threat to the United States (US) National Airspace System (NAS). Although the US Federal Aviation Administration (FAA) implements lightning protection practices and procedures to protect personnel, electronic equipment, and structures within the NAS, many lightning-induced outages still occur. To date we found that most research on lightning-induced facility outages has focused on understanding the physical processes of lightning strike effects on aircraft and airport ramp operations. Very little research has been done on examining the overall patterns and characteristics of such hazards to aviation from a geo-spatial standpoint. To bridge this gap, we analyze nationwide lightning strike spatiotemporal data and FAA airport facility outage records from 2009 through 2020 and apply innovative pattern recognition methods to identify key characteristics of lightning strike hazards. Our results uncover the complexities of lightning strike hazard impact patterns to NAS facilities, identifying five distinct typologies with climatological signatures critical to creating better hazard mitigation strategies.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.compenvurbsys.2021.101735
DP  - ScienceDirect
VL  - 91
SP  - 101735
J2  - Computers, Environment and Urban Systems
SN  - 0198-9715
UR  - https://www.sciencedirect.com/science/article/pii/S0198971521001423
Y2  - 2023/12/31/05:47:43
KW  - Facility management
KW  - Lightning strike hazard
KW  - National Airspace System
KW  - Pattern recognition
ER  - 

TY  - JOUR
TI  - Fast asymptotic algorithm for real-time causal connectivity analysis of multivariate systems and signals
AU  - Rezaei, Farnaz
AU  - Alamoudi, Omar Ali
AU  - Davani, Shayan
AU  - Hou, Songming
T2  - Signal Processing
AB  - The purpose of this work is to optimize the current state-of-the-art asymptotic algorithm of connectivity Granger-causality measures in the frequency-domain, such as the Directed Transfer Function (DTF), Partial Directed Coherence (PDC), and their variants. These measures stem from the modeling of multidimensional time series by multivariate autoregressive model. Surrogate and asymptotic analysis are the most frequently used methods to quantify the statistical significance of such derived interactions, a critical step for validation of the results. The current asymptotic algorithms run fairly fast on low-dimensional datasets but become impractical for high-dimensional datasets due to the involved computational time and memory demand. This is a huge limitation in the application of these connectivity measures to the fields dealing with numerous concurrently acquired signals from probing of complex systems such as the human brain. Here, we optimized the current algorithms for the fast asymptotic analysis of these connectivity measures and achieved a reduction of their computation time by at least three orders of magnitude. The optimizations were accomplished by decreasing the dimension of the involved matrices, eliminating the complicated functions (e.g., eigenvalue estimation and Cholesky factorization), and variable separation. The superior performance of the proposed optimized algorithm is shown with simulation examples.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.1016/j.sigpro.2022.108822
DP  - ScienceDirect
VL  - 204
SP  - 108822
J2  - Signal Processing
SN  - 0165-1684
UR  - https://www.sciencedirect.com/science/article/pii/S0165168422003619
Y2  - 2023/12/31/05:47:56
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0165168422003619
KW  - Causality measures
KW  - Directed transfer function
KW  - Fast asymptotic analysis
KW  - Multivariate autoregressive model
KW  - Partial directed coherence
ER  - 

TY  - JOUR
TI  - iAssistMe - Adaptable Assistant for Persons with Eye Disabilities
AU  - Calancea, Cristina Georgiana
AU  - Miluţ, Camelia-Maria
AU  - Alboaie, Lenuţa
AU  - Iftene, Adrian
T2  - Procedia Computer Science
T3  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019
AB  - Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.
DA  - 2019/01/01/
PY  - 2019
DO  - 10.1016/j.procs.2019.09.169
DP  - ScienceDirect
VL  - 159
SP  - 145
EP  - 154
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S187705091931347X
Y2  - 2023/12/31/05:48:47
L2  - https://www.sciencedirect.com/science/article/pii/S187705091931347X
KW  - Cloud Architectures
KW  - Personal Assistants
KW  - Visually Challenged
ER  - 

TY  - JOUR
TI  - M2FaaS: Transparent and fault tolerant FaaSification of Node.js monolith code blocks
AU  - Pedratscher, Stefan
AU  - Ristov, Sasko
AU  - Fahringer, Thomas
T2  - Future Generation Computer Systems
AB  - Porting existing monoliths to the Function-as-a-Service (FaaS) (FaaSification) can be very challenging for software developers due to different architectural styles. For a successful porting, developers need to resolve various dependencies, such as method invocations of external packages or user-defined codes, as well as global and local variables used in and after the code block that should be faasified. To bridge the gap and automatize FaaSification, this paper introduces M2FaaS, a FaaSifier that automatically converts a Node.js monolith into a hybrid by faasifying annotated code blocks as serverless functions on multiple FaaS providers. M2FaaS is a novel FaaSifier that resolves many challenges for the resulting monolith to work properly after the FaaSification. Developers may annotate all dependencies that need to be resolved for the generated functions to run properly and specify variables that should be returned by the function to the monolith because they are used later in the monolith. Moreover, M2FaaS is the first FaaSifier that faasifies arbitrary code blocks. The current M2FaaS prototype supports FaaSification of individual functions on two FaaS providers, AWS Lambda and IBM Cloud Functions. Finally, M2FaaS introduces an optional annotation for alternative functions to be invoked in case the primary faasified function fails. The resulting hybrid application invokes the automatically deployed serverless functions, while the original code remains executable. Experiments with four complementary monoliths demonstrate that M2FaaS outperforms state-of-the-art FaaSifiers in terms of development effort by up to 73.3%. Moreover, with the fault tolerance support, M2FaaS finishes all submitted functions, thereby achieving by 18.5% higher throughput than the other FaaSifiers.
DA  - 2022/10/01/
PY  - 2022
DO  - 10.1016/j.future.2022.04.021
DP  - ScienceDirect
VL  - 135
SP  - 57
EP  - 71
J2  - Future Generation Computer Systems
SN  - 0167-739X
ST  - M2FaaS
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X22001509
Y2  - 2023/12/31/05:49:02
L1  - https://diglib.uibk.ac.at/ulbtirolfodok/content/titleinfo/8865023/full.pdf
KW  - FaaS
KW  - Serverless
KW  - Cloud
KW  - FaaSification
KW  - Functions
KW  - Portability
ER  - 

TY  - JOUR
TI  - MLLess: Achieving cost efficiency in serverless machine learning training
AU  - Gimeno Sarroca, Pablo
AU  - Sánchez-Artigas, Marc
T2  - Journal of Parallel and Distributed Computing
AB  - Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this endeavor, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100 ms). Our results certify that MLLess can be 15X faster than serverful ML systems [27] at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization. Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers.
DA  - 2024/01/01/
PY  - 2024
DO  - 10.1016/j.jpdc.2023.104764
DP  - ScienceDirect
VL  - 183
SP  - 104764
J2  - Journal of Parallel and Distributed Computing
SN  - 0743-7315
ST  - MLLess
UR  - https://www.sciencedirect.com/science/article/pii/S074373152300134X
Y2  - 2023/12/31/05:49:23
L2  - https://www.sciencedirect.com/science/article/pii/S074373152300134X
KW  - Serverless computing
KW  - Machine learning
KW  - Function-as-a-Service
ER  - 

TY  - JOUR
TI  - Modified deep residual network architecture deployed on serverless framework of IoT platform based on human activity recognition application
AU  - Keshavarzian, Alireza
AU  - Sharifian, Saeed
AU  - Seyedin, Sanaz
T2  - Future Generation Computer Systems
AB  - In the last few years, human activity recognition (HAR) is a subject undergoing intense study in various contexts such as pattern recognition and human-device interaction. HAR applications come to an aid of Telecare system which is paving the way for doctors and nurses to measure the health status of their patients. Due to the ubiquitous influence of smartphones in an individual’s life, we take embedded smartphone sensors into account as our case study. The proposed method, Modified Deep Residual Network, outperforms the accuracy of Human activity recognition compared with state-of-the-art machine learning techniques which are using Raw signals as their input. we defined new pooling layer called smooth-pooling to leverage the model performance. The accuracy of proposed architecture is evaluated on three common dataset that comprises accelerometer and gyroscope raw data. The results demonstrated the proposed method outperforms accuracy of classification while requiring just raw data with lower parameters compared to other works. Furthermore, The proposed HAR method is deployed in our IoT cloud platform which enables users to create scenarios based on what they are doing at home. Using Function as a Service (FaaS) architecture in this platform solves the scalability issues by running each function in a separate container. The IoT platform prepares an infrastructure for developers who want to integrate their application into the platform and use its functionality along with other IoT platform options.
DA  - 2019/12/01/
PY  - 2019
DO  - 10.1016/j.future.2019.06.009
DP  - ScienceDirect
VL  - 101
SP  - 14
EP  - 28
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X19304376
Y2  - 2023/12/31/05:49:58
KW  - Cloud computing
KW  - Deep residual network
KW  - Function as a service
KW  - Human action recognition
ER  - 

TY  - JOUR
TI  - Nonlinear eigenvalue topology optimization for structures with frequency-dependent material properties
AU  - Li, Quhao
AU  - Wu, Qiangbo
AU  - Dou, Suguang
AU  - Wang, Jilai
AU  - Liu, Shutian
AU  - Chen, Wenjiong
T2  - Mechanical Systems and Signal Processing
AB  - Eigenvalue topology optimization problem has been a hot topic in recent years for its wide applications in many engineering areas. In the previous studies, the applied materials are usually assumed as elastic, and the resulting structural eigenfrequencies are obtained by solving a linear eigenvalue problem. However, many engineering materials, such as viscoelastic materials, have frequency-dependent modulus, which results in a more complicated nonlinear eigenvalue problem. This paper presents a systematic study on the nonlinear eigenvalue topology optimization problem with frequency-dependent material properties. The nonlinear eigenvalue problem is solved by a continuous asymptotic numerical method based on the homotopy algorithm and perturbation expansion technique, which involves higher-order differentiation of the nonlinear term and shows a fast convergence. Several schemes are proposed to improve the computational accuracy, applicability, and robustness of the method for the application in topology optimization, including Faà di Bruno's theorem, bisection method, and inverse iteration based eigenvector modification method. Three optimization problems are solved to demonstrate the effectiveness of the developed methods, including the maximization of the fundamental frequency, the eigenfrequency separation interval between two adjacent eigenfrequencies of given orders, and the eigenfrequency separation interval at a given frequency. Numerical examples show the large influence of the frequency-dependent material properties on the optimized results and validate the effectiveness of the developed method.
DA  - 2022/05/01/
PY  - 2022
DO  - 10.1016/j.ymssp.2022.108835
DP  - ScienceDirect
VL  - 170
SP  - 108835
J2  - Mechanical Systems and Signal Processing
SN  - 0888-3270
UR  - https://www.sciencedirect.com/science/article/pii/S0888327022000322
Y2  - 2023/12/31/05:50:07
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0888327022000322
KW  - Asymptotic numerical method
KW  - Eigenvector modification
KW  - Frequency-dependent material properties
KW  - Nonlinear eigenvalue problem
KW  - Topology optimization
ER  - 

TY  - JOUR
TI  - NoOps – A Multivocal literature review
AU  - Stefanac, Tommy
AU  - Colomo-Palacios, Ricardo
T2  - Procedia Computer Science
T3  - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021
AB  - Traditionally, an organization had to have in-house servers and hardware to build a web application. This evolved into Cloud computing where the possibility for cost reduction and scalable data storage became a reality. With the introduction of cloud computing came a concept known as NoOps, or No Operations. This paper aims to take a closer look into what NoOps is and the benefits and challenges of NoOps. The authors identified three RQs that could help to give more insight into NoOps. Further we discussed the findings and RQs and lay out the way forward for future studies into NoOps. We also looked at artificial intelligence (AI) and how AI seems to be heavily linked with a true NoOps environment. With the lack of scientific studies into NoOps, a Multivocal literature review was selected as the method used to investigate the concept and its implications. We try to show voices both for and against NoOps. Further, we try to look at a misconception of what NoOps really is, what true NoOps could be. Finally we look at what requirements there are for companies wanting to go NoOps, and discuss the possibility that many companies unknowingly are moving towards a NoOps environment.
DA  - 2022/01/01/
PY  - 2022
DO  - 10.1016/j.procs.2021.12.002
DP  - ScienceDirect
VL  - 196
SP  - 167
EP  - 174
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921022250
Y2  - 2023/12/31/05:50:19
KW  - Cloud computing
KW  - Serverless
KW  - AI
KW  - Artificial intelligence
KW  - Evolution IT
KW  - NoOps
ER  - 

TY  - JOUR
TI  - Optimal Fitness Aware Cloud Service Composition using an Adaptive Genotypes Evolution based Genetic Algorithm
AU  - Jatoth, Chandrashekar
AU  - Gangadharan, G. R.
AU  - Buyya, Rajkumar
T2  - Future Generation Computer Systems
AB  - With the seamless proliferation of cloud services, it becomes challenging to select and compose cloud services that satisfy the requirements of users. A service may be connected with another service(s) for satisfying a workflow/function in a service composition. Further, the service assessment based on one or two QoS parameters is not accurate enough to achieve the desired optimality in a cloud service composition. Most of the existing methods in the literature consider either a single QoS parameter or two QoS parameters for QoS-aware composition and do not consider the balancing of QoS parameters and/or the connectivity constraints between two compositions. In this paper, we present an Optimal Fitness Aware Cloud Service Composition (OFASC) using an Adaptive Genotype Evolution based Genetic Algorithm (AGEGA) dealing with multiple QoS parameters and providing the solutions that satisfy the balancing QoS parameters and connectivity constraints of service composition. Experimental results show that our approach enhances the efficiency of cloud service composition by converging quickly and obtains better composition when compared to other approaches.
DA  - 2019/05/01/
PY  - 2019
DO  - 10.1016/j.future.2018.11.022
DP  - ScienceDirect
VL  - 94
SP  - 185
EP  - 198
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18308501
Y2  - 2023/12/31/05:50:29
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X18308501
KW  - Adaptive Genotypes Evolution
KW  - Cloud service composition
KW  - Genetic algorithm
KW  - Quality of Service (QoS)
ER  - 

TY  - JOUR
TI  - Optimized container scheduling for data-intensive serverless edge computing
AU  - Rausch, Thomas
AU  - Rashed, Alexander
AU  - Dustdar, Schahram
T2  - Future Generation Computer Systems
AB  - Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals.
DA  - 2021/01/01/
PY  - 2021
DO  - 10.1016/j.future.2020.07.017
DP  - ScienceDirect
VL  - 114
SP  - 259
EP  - 271
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X2030399X
Y2  - 2023/12/31/05:52:01
L2  - https://www.sciencedirect.com/science/article/pii/S0167739X2030399X
KW  - Machine learning
KW  - Serverless
KW  - Edge computing
KW  - Container scheduling
ER  - 

TY  - JOUR
TI  - Optimized resource usage with hybrid auto-scaling system for knative serverless edge computing
AU  - Tran, Minh-Ngoc
AU  - Kim, YoungHan
T2  - Future Generation Computer Systems
AB  - In the most popular serverless platform - Knative, dynamic resource allocation is implemented using horizontal auto-scaling algorithms to create or delete service instances based on different monitored metrics. However, the assigned resources for each instance are fixed. Vertical scaling up or down assigned resources per instance is required to avoid over-provisioning resources which are limited at the edge. Hybrid (horizontal and vertical) auto-scaling solutions proposed by existing works have several limitations. These solutions are optimized for separated services and get degraded performance when applied in a normal environment with multiple concurrent services. Further, most methods make significant changes to the original Knative platform, and have not been considered to be adopted since then. In this article, instead of Knative modification, we developed separated Kubernetes operators and custom resources (CRs) that can assist the Knative auto-scaler with optimal hybrid auto-scaling configurations based on traffic prediction. First, we characterize each service with a profile of different assigned resource levels pairing with their optimal target Knative’s horizontal scaling request concurrency. Then, based on these profiles, we calculate the best-assigned resources level, target concurrency level, and the number of required instances corresponding to each future time step’s predicted traffic. Finally, these configurations are applied to Knative’s default auto-scaler and services’ CR. Experiments done on our testbed compared our solution with a Knative hybrid auto-scaler solution that does not consider the service’s target request concurrency, and the default Knative horizontal auto-scaler. The results show our solution improvements up to 14% and 20% in terms of resource usage, respectively.
DA  - 2024/03/01/
PY  - 2024
DO  - 10.1016/j.future.2023.11.010
DP  - ScienceDirect
VL  - 152
SP  - 304
EP  - 316
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23004156
Y2  - 2023/12/31/05:52:11
KW  - Serverless computing
KW  - Edge computing
KW  - Horizontal scaling
KW  - Quality of service
KW  - Resource management
KW  - Vertical scaling
ER  - 

TY  - JOUR
TI  - P4-assisted seamless migration of serverless applications towards the edge continuum
AU  - Pelle, István
AU  - Paolucci, Francesco
AU  - Sonkoly, Balázs
AU  - Cugini, Filippo
T2  - Future Generation Computer Systems
AB  - Serverless computing has recently been presented as an effective technology for handling short-lived compute tasks in the cloud. It has the potential of becoming an attractive option also in the context of edge computing where resource-aware deployment, constrained by both limited edge computing resources and experienced latency, plays a vital role. In this paper, we present and experimentally validate a framework that oversees serverless applications in an edge computing scenario. It completely automates serverless application deployment and provides hitless dynamic migration of application compute tasks between a pair of edge nodes, paving the way for handling significantly more complex cases. The framework relies on an integrated deployment, monitoring and offloading infrastructure that enhances AWS IoT Greengrass features and performance. Our implementation provides two separate options for relocating compute tasks by steering application traffic towards the most suitable node. One builds on an on-the-fly application component reconfiguration, while the other selects the suitable node through P4 in-network processing of resource metrics emitted by the nodes. Our experimental demonstration evaluates the migration performance using a latency-sensitive application decomposed to serverless functions. Results reveal extremely fast dynamic reconfiguration and traffic rerouting operations. The used methods avoid congestion peaks at the edge and show no end-to-end latency increase upon migration between the nodes.
DA  - 2023/09/01/
PY  - 2023
DO  - 10.1016/j.future.2023.04.010
DP  - ScienceDirect
VL  - 146
SP  - 122
EP  - 138
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23001450
Y2  - 2023/12/31/05:52:21
L1  - https://zenodo.org/record/7896612/files/revised-2-manuscript-unmarked.pdf
KW  - FaaS
KW  - Serverless
KW  - Edge
KW  - AWS Greengrass
KW  - AWS Lambda
KW  - P4
ER  - 

TY  - JOUR
TI  - Performance optimization of serverless edge computing function offloading based on deep reinforcement learning
AU  - Yao, Xuyi
AU  - Chen, Ningjiang
AU  - Yuan, Xuemei
AU  - Ou, Pingjie
T2  - Future Generation Computer Systems
AB  - It is difficult for resource-constrained edge servers to simultaneously meet the performance requirements of all the latency-sensitive Internet of Things (IoT) applications in edge computing. Therefore, it is a significant challenge to efficiently generate a task offloading strategy. Recently, deep reinforcement learning (DRL)-based task offloading methods have been studied to ensure long-term performance optimization. However, there are challenges in existing DRL-based task offloading methods, such as insufficient sample diversity and high exploration cost. To optimize the performance of edge computing and facilitate the development and deployment of event-driven IoT applications, the serverless edge computing model has emerged. It combines serverless computing, also known as Function as a Service (FaaS), with edge computing and has been adopted in edge AI inference and prediction, stream processing, face recognition, etc. In this paper, an experience-sharing deep reinforcement learning-based distributed function offloading method called ES-DRL is proposed in the setting of a combined stateful and stateless execution model for serverless edge computing. ES-DRL adopts a distributed learning architecture, where each edge FaaS (EFaaS) obtains the current state of the local environment and inputs them to the local DRL agent, which outputs the function offloading strategy. Then, each EFaaS uploads the experience data interacting with the environment to a global shared replay buffer located in the cloud and randomly draws a batch of data from it to optimize the parameters of the local network. A population-guided policy search method is introduced to speed up the convergence of the DRL agent and avoid falling into the local optimum. The experimental results demonstrate that ES-DRL can reduce the average latency by up to approximately 17 percent compared to the existing DRL-based task offloading method.
DA  - 2023/02/01/
PY  - 2023
DO  - 10.1016/j.future.2022.09.009
DP  - ScienceDirect
VL  - 139
SP  - 74
EP  - 86
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X2200293X
Y2  - 2023/12/31/05:52:32
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X2200293X
KW  - Serverless computing
KW  - Deep reinforcement learning
KW  - Edge computing
KW  - Function offloading
KW  - Task offloading
ER  - 

TY  - JOUR
TI  - PSAC-PDB: Analysis and classification of protein structures
AU  - Nawaz, M. Saqib
AU  - Fournier-Viger, Philippe
AU  - He, Yulin
AU  - Zhang, Qin
T2  - Computers in Biology and Medicine
AB  - This paper presents a novel framework, called PSAC-PDB, for analyzing and classifying protein structures from the Protein Data Bank (PDB). PSAC-PDB first finds, analyze and identifies protein structures in PDB that are similar to a protein structure of interest using a protein structure comparison tool. Second, the amino acids (AA) sequences of identified protein structures (obtained from PDB), their aligned amino acids (AAA) and aligned secondary structure elements (ASSE) (obtained by structural alignment), and frequent AA (FAA) patterns (discovered by sequential pattern mining), are used for the reliable detection/classification of protein structures. Eleven classifiers are used and their performance is compared using six evaluation metrics. Results show that three classifiers perform well on overall, and that FAA patterns can be used to efficiently classify protein structures in place of providing the whole AA sequences, AAA or ASSE. Furthermore, better classification results are obtained using AAA of protein structures rather than AA sequences. PSAC-PDB also performed better than state-of-the-art approaches for SARS-CoV-2 genome sequences classification.
DA  - 2023/05/01/
PY  - 2023
DO  - 10.1016/j.compbiomed.2023.106814
DP  - ScienceDirect
VL  - 158
SP  - 106814
J2  - Computers in Biology and Medicine
SN  - 0010-4825
ST  - PSAC-PDB
UR  - https://www.sciencedirect.com/science/article/pii/S0010482523002792
Y2  - 2023/12/31/05:52:57
KW  - Classification
KW  - DALI
KW  - PDB
KW  - Protein structures
KW  - SARS-CoV-2
KW  - Spike
KW  - SPM
ER  - 

TY  - JOUR
TI  - Research challenges in nextgen service orchestration
AU  - Vaquero, Luis M.
AU  - Cuadrado, Felix
AU  - Elkhatib, Yehia
AU  - Bernal-Bernabe, Jorge
AU  - Srirama, Satish N.
AU  - Zhani, Mohamed Faten
T2  - Future Generation Computer Systems
AB  - Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.
DA  - 2019/01/01/
PY  - 2019
DO  - 10.1016/j.future.2018.07.039
DP  - ScienceDirect
VL  - 90
SP  - 20
EP  - 38
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X18303157
Y2  - 2023/12/31/05:53:07
L1  - https://eprints.lancs.ac.uk/id/eprint/126964/1/collab_orch_paper_1_.pdf
L1  - https://arxiv.org/pdf/1806.00764
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X18303157
KW  - FaaS
KW  - Serverless
KW  - Edge
KW  - Fog
KW  - Churn
KW  - Large scale
KW  - NFV
KW  - NVM
KW  - Orchestration
KW  - SDN
ER  - 

TY  - JOUR
TI  - Scalable Remote Cloud Data Center for Vessel Equipment Predictive Maintenance Service-as-a-Product (SaaP)
AU  - Chit, Tan Wei
AU  - Toro, Carlos
AU  - Lim, Ho Choon
AU  - Muthu, Raguram
T2  - Procedia Computer Science
T3  - 27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)
AB  - Predictive maintenance is gradually replacing conventional preventive maintenance, through an informed decision-making process for a fleet operator to proactively monitor the health status of the sea-based equipment and machinery. As a result, the fleet operator will only need to ask for on-time repair and maintenance services instead of periodic maintenance, which saves both time and cost significantly. While moving into this new maritime business model, which is also known as Service-as-a-Product (SaaP), both fleet operators and maintenance, repair, and overhaul (MRO) companies could also build a stronger collaboration in the maritime industries. With the advent of digital transformation towards Industry 4.0 (I4.0), the Industrial Internet-of-Things (IIoT) renders the massive collection of operational and machinery process data from the vessel equipment through sensorization, where these collected real-time information are useful for advanced analytics to predict equipment failure and to avoid unplanned downtime. Unlike other industries, there are a few challenges in the maritime industries when developing a centralized smart vessel equipment monitoring platform. One of the key challenges, is the lack of a feasible data management system, requires the centralized host must be able to handle and manage the telemetry IoT data that is transmitted over the satellite communication, by complying with all cybersecurity considerations and regulations in maritime sector. In this paper, we present a scalable, hybrid cloud-based data management framework that can connect to multiple edge systems where each system is deployed on a physical vessel, to establish a scalable SaaP business model. Hence, the ship-to-shore sensorized data can be processed and monitored via a real-time visualization dashboard through the centralized cloud-based platform. We argue that our approach, would be a turn-key solution that can be implemented for mostly all types of marine equipment and machinery, and thus to further improve the prediction tools to support advanced decision-making techniques, such as optimal time repair of vessel equipment.
DA  - 2023/01/01/
PY  - 2023
DO  - 10.1016/j.procs.2023.10.275
DP  - ScienceDirect
VL  - 225
SP  - 2826
EP  - 2834
J2  - Procedia Computer Science
SN  - 1877-0509
UR  - https://www.sciencedirect.com/science/article/pii/S1877050923014333
Y2  - 2023/12/31/05:53:23
L2  - https://www.sciencedirect.com/science/article/pii/S1877050923014333
KW  - Serverless Computing
KW  - Data Management System
KW  - Industry 4.0 applications
KW  - Service-as-a-Product (SaaP)
ER  - 

TY  - JOUR
TI  - SD-SRF: An Intelligent Service Deployment Scheme for Serverless-operated Cloud-Edge Computing in 6G Networks
AU  - Wang, Luying
AU  - Liu, Anfeng
AU  - Xiong, Neal N.
AU  - Zhang, Shaobo
AU  - Wang, Tian
AU  - Dong, Mianxiong
T2  - Future Generation Computer Systems
AB  - With the development of serverless computing, developers can implement and deploy business applications as a combination of stateless functions. Although originally proposed for cloud computing, serverless computing is gradually applied to cloud-edge systems for service deployment to provide users with high-quality, low-latency services. However, optimized service deployment in 6G networks is a very challenging issue because of the vast number of deployable devices in the network, and its permutations are highly exponential. In this paper, we propose optimized service deployment schemes for online and offline, respectively, to minimize the overall latency at a lower cost. (1) First, a SD-SRF algorithm based on the greedy algorithm is proposed to optimize the service deployment for a multi-layer edge network, which consists of two phases: SR and SF. (a) Services are deployed in the nearest ancestor devices in the routing tree of all such service requests with the least cost of deployment. (b) When the deployment cost of moving some service replicas to devices in the lower layer is less than the benefit, the service will fall. (2) However, the offline algorithm relies on the availability of prior information heavily, such as request arrival pattern and number, which is difficult to obtain. Therefore, this paper proposes a PPO-MSD algorithm for optimal deployment online, where a Markov decision process (MDP) is modeled. Extensive simulation results show that PPO-MSD outperforms existing algorithms in terms of overall delay and utility, and its performance is close to the optimal ones obtained by SD-SRF, with the SD-SRF and PPO-MSD algorithms reducing the delay on average by 32.40% and 9.91%.
DA  - 2024/02/01/
PY  - 2024
DO  - 10.1016/j.future.2023.09.027
DP  - ScienceDirect
VL  - 151
SP  - 242
EP  - 259
J2  - Future Generation Computer Systems
SN  - 0167-739X
ST  - SD-SRF
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X2300359X
Y2  - 2023/12/31/05:53:58
KW  - Serverless computing
KW  - Reinforcement learning
KW  - 6G networks
KW  - Cloud-Edge computing
KW  - Intelligent service deployment
ER  - 

TY  - JOUR
TI  - Security computing resource allocation based on deep reinforcement learning in serverless multi-cloud edge computing
AU  - Zhang, Hang
AU  - Wang, Jinsong
AU  - Zhang, Hongwei
AU  - Bu, Chao
T2  - Future Generation Computer Systems
AB  - Handling computationally intensive tasks is challenging for user devices (UDs) with limited computing resources. Serverless cloud edge computing solves this problem and reduces maintenance and management. Its crucial function is to allocate computing resources reasonably. However, linking multiple computing resource nodes to perform computing resource allocation and ensure data security is a significant challenge. This study proposes an approach based on action-constrained deep reinforcement learning (DRL) to allocate computing resources securely. First, we consider a model of a serverless multi-cloud edge computing network with multiple computing resource nodes that possess various attribute characteristics. Then, we design a security mechanism to guarantee data security. Afterward, we formalize the network model and objectives and further transform them into a modeling process known as the Markov decision process. Finally, we propose DRL based on action constraints to provide an optimal resource allocation scheduling policy. Simulation results demonstrate that our approach can reduce system costs and improve working performance compared with the comparison schemes.
DA  - 2024/02/01/
PY  - 2024
DO  - 10.1016/j.future.2023.09.016
DP  - ScienceDirect
VL  - 151
SP  - 152
EP  - 161
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23003461
Y2  - 2023/12/31/05:54:08
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X23003461
KW  - Deep reinforcement learning
KW  - Resource allocation
KW  - Serverless cloud edge computing
ER  - 

TY  - JOUR
TI  - Serverless application composition leveraging function fusion: Theory and algorithms
AU  - Czentye, János
AU  - Sonkoly, Balázs
T2  - Future Generation Computer Systems
AB  - Serverless computing is a novel cloud computing paradigm enabling flexible, cost-efficient and fine-granular development of cloud-native applications, although without any guarantees on scheduling or execution times. Thus, various high-level solutions have been proposed in recent years to find proper configurations of individual FaaS functions, while considering user-given QoS requirements. However, the ever-increasing complexity of invocation patterns among stateless functions, externalized management of intermediate states, and diverse public cloud resources pose new challenges to the composition of highly data-intensive serverless applications. In this paper, we fill this gap by proposing novel algorithms based on the emerging function fusion technique, along with the related cost/performance models of composite functions supporting implicit instance parallelization and internal state propagation. We prove the NP-completeness of the underlying latency-constrained tree partitioning problem, and design a bicriteria approximation scheme and a greedy heuristic to derive cost-efficient deployment configurations in polynomial time. With the help of extensive simulations using synthetic call graphs generated from public cloud traces, we demonstrate the applicability and superior runtime performance of our proposed methods compared to state-of-the-art solutions. In addition, we showcase that further cost-reduction of up to 3–6 % can be achieved compared to the optimal partitioning with the allowance of tolerable latency violations.
DA  - 2024/04/01/
PY  - 2024
DO  - 10.1016/j.future.2023.12.010
DP  - ScienceDirect
VL  - 153
SP  - 403
EP  - 418
J2  - Future Generation Computer Systems
SN  - 0167-739X
ST  - Serverless application composition leveraging function fusion
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X23004648
Y2  - 2023/12/31/05:54:21
KW  - FaaS
KW  - Serverless
KW  - Cloud native
KW  - Function fusion
KW  - Tree partitioning
ER  - 

TY  - JOUR
TI  - Serverless computing for Internet of Things: A systematic literature review
AU  - Cassel, Gustavo André Setti
AU  - Rodrigues, Vinicius Facco
AU  - da Rosa Righi, Rodrigo
AU  - Bez, Marta Rosecler
AU  - Nepomuceno, Andressa Cruz
AU  - André da Costa, Cristiano
T2  - Future Generation Computer Systems
AB  - Serverless computing, or Function as a Service (FaaS), represents a research trend where applications are built and deployed as a group of stateless functions. Although initially proposed for the cloud, serverless computing has also found its place on Internet of Things (IoT) while bringing functions closer to the devices, in order to reduce latency and avoid unnecessary energy and resource consumption. It is interesting that solutions can work in an integrated manner on edge, fog, and cloud layers. Mission-critical functions can be executed on edge and fog in order to benefit from low-latency responses, while heavy functions can be executed on the cloud to process huge amount of data produced by IoT sensors, as long as Internet connection is available. Existing surveys focus on serverless computing for specific layers and do not address a broad, integrated, and systematic vision regarding how IoT benefits from serverless on edge, fog, and cloud. With this in mind, this paper provides a comprehensive Systematic Literature Review that, after the selection process, covers 60 papers on the field of serverless computing for IoT on the three layers. This gives us insights about how functions are offloaded to different devices and how they interact with each other. We bring main components employed to incubate and execute functions, as well as the main challenges and open questions for this subject. Protocols, programming languages, and storage services related to the solutions are also presented. Finally, we show a rich taxonomy summarizing all characteristics in a single figure, along with a discussion about the overall architecture of serverless applications for IoT. We conclude that serverless computing is a promising technology for IoT applications, but several improvements still need to be made to popularize this concept and make it easier to use.
DA  - 2022/03/01/
PY  - 2022
DO  - 10.1016/j.future.2021.10.020
DP  - ScienceDirect
VL  - 128
SP  - 299
EP  - 316
J2  - Future Generation Computer Systems
LA  - en
SN  - 0167-739X
ST  - Serverless computing for Internet of Things
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X21004167
Y2  - 2023/12/31/05:54:39
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X21004167
KW  - Internet of Things
KW  - Serverless
KW  - Cloud
KW  - Edge
KW  - Fog
KW  - Function as a Service
ER  - 

TY  - JOUR
TI  - Serverless data pipeline approaches for IoT data in fog and cloud computing
AU  - Poojara, Shivananda R.
AU  - Dehury, Chinmaya Kumar
AU  - Jakovits, Pelle
AU  - Srirama, Satish Narayana
T2  - Future Generation Computer Systems
AB  - With the increasing number of Internet of Things (IoT) devices, massive amounts of raw data is being generated. The latency, cost, and other challenges in cloud-based IoT data processing have driven the adoption of Edge and Fog computing models, where some data processing tasks are moved closer to data sources. Properly dealing with the flow of such data requires building data pipelines, to control the complete life cycle of data streams from data acquisition at the data source, edge and fog processing, to Cloud side storage and analytics. Data analytics tasks need to be executed dynamically at different distances from the data sources and often on very heterogeneous hardware devices. This can be streamlined by the use of a Serverless (or FaaS) cloud computing model, where tasks are defined as virtual functions, which can be migrated from edge to cloud (and vice versa) and executed in an event-driven manner on data streams. In this work, we investigate the benefits of building Serverless data pipelines (SDP) for IoT data analytics and evaluate three different approaches for designing SDPs: (1) Off-the-shelf data flow tool (DFT) based, (2) Object storage service (OSS) based and (3) MQTT based. Further, we applied these strategies on three fog applications (Aeneas, PocketSphinx, and custom Video processing application) and evaluated the performance by comparing their processing time (computation time, network communication and disk access time), and resource utilization. Results show that DFT is unsuitable for compute-intensive applications such as video or image processing, whereas OSS is best suitable for this task. However, DFT is nicely fit for bandwidth-intensive applications due to the minimum use of network resources. On the other hand, MQTT-based SDP is observed with increase in CPU and Memory usage as the number of users rose, and experienced a drop in data units in the pipeline for PocketSphinx and custom video processing applications, however it performed well for Aeneas which had low size data units.
DA  - 2022/05/01/
PY  - 2022
DO  - 10.1016/j.future.2021.12.012
DP  - ScienceDirect
VL  - 130
SP  - 91
EP  - 105
J2  - Future Generation Computer Systems
SN  - 0167-739X
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X21004933
Y2  - 2023/12/31/05:54:57
L1  - https://arxiv.org/pdf/2112.09974
L2  - https://www.sciencedirect.com/science/article/abs/pii/S0167739X21004933
KW  - Cloud computing
KW  - Serverless computing
KW  - Fog computing
KW  - Edge computing
KW  - Data pipelines
KW  - Internet of things
ER  - 

TY  - JOUR
TI  - The uphill journey of FaaS in the open-source community
AU  - Eskandani, Nafise
AU  - Salvaneschi, Guido
T2  - Journal of Systems and Software
AB  - Since its introduction in 2014 by Amazon, the Function as a Service (FaaS) model of serverless computing has set the expectation to fulfill the promise of on-demand, pay-as-you-go, infrastructure-independent processing, originally formulated by cloud computing. Yet, serverless applications are fundamentally different than traditional service-oriented software in that they pose specific performance (e.g., cold start), design (e.g., stateless), and development challenges (e.g., debugging). A growing number of cloud solutions have been continuously attempting to address each of these challenges as a result of the increasing popularity of FaaS. Yet, the characteristics of this model have been poorly understood; therefore, the challenges are poorly tackled. In this paper, we assess the state of FaaS in open-source community with a study on almost 2K real-world serverless applications. Our results show a jeopardized ecosystem, where, despite the hype of serverless solutions in the last years, a number of challenges remain untackled, especially concerning component reuse, support for software development, and flexibility among different platforms — resulting in arguably slow adoption of the FaaS model. We believe that addressing the issues discussed in this paper may help researchers shaping the next generation of cloud computing models.
DA  - 2023/04/01/
PY  - 2023
DO  - 10.1016/j.jss.2022.111589
DP  - ScienceDirect
VL  - 198
SP  - 111589
J2  - Journal of Systems and Software
SN  - 0164-1212
UR  - https://www.sciencedirect.com/science/article/pii/S0164121222002655
Y2  - 2023/12/31/05:55:53
KW  - Cloud computing
KW  - FaaS
KW  - Serverless
KW  - Function as a service
KW  - Performance
KW  - Cloud-computing
KW  - Computer software reusability
KW  - Open source software
KW  - Program debugging
KW  - Application programs
KW  - Open systems
KW  - On demands
KW  - Open source communities
KW  - Pay as you go
KW  - Service modeling
KW  - Service-oriented softwares
KW  - Software design
KW  - Traditional services
ER  - 

TY  - JOUR
TI  - TOSCAdata: Modeling data pipeline applications in TOSCA
AU  - Dehury, Chinmaya Kumar
AU  - Jakovits, Pelle
AU  - Srirama, Satish Narayana
AU  - Giotis, Giorgos
AU  - Garg, Gaurav
T2  - Journal of Systems and Software
AB  - The serverless platform allows a customer to effectively use cloud resources and pay for the exact amount of used resources. A number of dedicated open source and commercial cloud data management tools are available to handle the massive amount of data. Such modern cloud data management tools are not enough matured to integrate the generic cloud application with the serverless platform due to the lack of mature and stable standards. One of the most popular and mature standards, TOSCA (Topology and Orchestration Specification for Cloud Applications), mainly focuses on application and service portability and automated management of the generic cloud application components. This paper proposes the extension of the TOSCA standard, TOSCAdata, that focuses on the modeling of data pipeline-based cloud applications. Keeping the requirements of modern data pipeline cloud applications, TOSCAdata provides a number of TOSCA models that are independently deployable, schedulable, scalable, and re-usable, while effectively handling the flow and transformation of data in a pipeline manner. We also demonstrate the applicability of proposed TOSCAdata models by taking a web-based cloud application in the context of tourism promotion as a use case scenario.
DA  - 2022/04/01/
PY  - 2022
DO  - 10.1016/j.jss.2021.111164
DP  - ScienceDirect
VL  - 186
SP  - 111164
J2  - Journal of Systems and Software
SN  - 0164-1212
ST  - TOSCAdata
UR  - https://www.sciencedirect.com/science/article/pii/S0164121221002508
Y2  - 2023/12/31/05:56:00
L1  - https://arxiv.org/pdf/2111.02524
KW  - Serverless computing
KW  - Data flow management
KW  - Data migration
KW  - Data pipeline
KW  - DevOps
KW  - TOSCA
ER  - 

TY  - JOUR
TI  - Uncoordinated access to serverless computing in MEC systems for IoT
AU  - Cicconetti, Claudio
AU  - Conti, Marco
AU  - Passarella, Andrea
T2  - Computer Networks
AB  - Edge computing is a promising solution to enable low-latency Internet of Things (IoT) applications, by shifting computation from remote data centers to local devices, less powerful but closer to the end user devices. However, this creates the challenge on how to best assign clients to edge nodes offering compute capabilities. So far, two antithetical architectures are proposed: centralized resource orchestration or distributed overlay. In this work we explore a third way, called uncoordinated access, which consists in letting every device exploring multiple opportunities, to opportunistically embrace the heterogeneity of network and load conditions towards diverse edge nodes. In particular, our contribution is intended for emerging serverless IoT applications, which do not have a state on the edge nodes executing tasks. We model the proposed system as a set of M/M/1 queues and show that it achieves a smaller delay than single edge node allocation. Furthermore, we compare uncoordinated access with state-of-the-art centralized and distributed alternatives in testbed experiments under more realistic conditions. Based on the results, our proposed approach, which requires a tiny fraction of the complexity of the alternatives in both the device and network components, is very effective in using the network resources, while incurring only a small penalty in terms of increased compute load and high percentiles of delay.
DA  - 2020/05/08/
PY  - 2020
DO  - 10.1016/j.comnet.2020.107184
DP  - ScienceDirect
VL  - 172
SP  - 107184
J2  - Computer Networks
SN  - 1389-1286
UR  - https://www.sciencedirect.com/science/article/pii/S1389128619313684
Y2  - 2023/12/31/05:56:09
L2  - https://www.sciencedirect.com/science/article/abs/pii/S1389128619313684
KW  - Internet of Things
KW  - Serverless computing
KW  - Computation offloading
KW  - Distributed cloud
KW  - Mobile Edge Computing
KW  - Online job dispatching
KW  - Performance evaluation
ER  - 

TY  - JOUR
TI  - A Cloud-Based Framework for Machine Learning Workloads and Applications
AU  - López García, Álvaro
AU  - De Lucas, Jesús Marco
AU  - Antonacci, Marica
AU  - Zu Castell, Wolfgang
AU  - David, Mario
AU  - Hardt, Marcus
AU  - Lloret Iglesias, Lara
AU  - Moltó, Germán
AU  - Plociennik, Marcin
AU  - Tran, Viet
AU  - Alic, Andy S.
AU  - Caballer, Miguel
AU  - Plasencia, Isabel Campos
AU  - Costantini, Alessandro
AU  - Dlugolinsky, Stefan
AU  - Duma, Doina Cristina
AU  - Donvito, Giacinto
AU  - Gomes, Jorge
AU  - Heredia Cacha, Ignacio
AU  - Ito, Keiichi
AU  - Kozlov, Valentin Y.
AU  - Nguyen, Giang
AU  - Orviz Fernández, Pablo
AU  - Šustr, Zděnek
AU  - Wolniewicz, Pawel
T2  - IEEE Access
AB  - In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.2964386
DP  - IEEE Xplore
VL  - 8
SP  - 18681
EP  - 18692
J2  - IEEE Access
SN  - 2169-3536
UR  - https://ieeexplore.ieee.org/document/8950411
Y2  - 2023/12/31/19:45:41
L1  - https://ieeexplore.ieee.org/ielx7/6287639/8948470/08950411.pdf
L2  - https://ieeexplore.ieee.org/document/8950411
ER  - 

TY  - CONF
TI  - A Control Loop-based Algorithm for Operational Transformation
AU  - Gadea, Cristian
AU  - Ionescu, Bogdan
AU  - Ionescu, Dan
T2  - 2020 IEEE 14th International Symposium on Applied Computational Intelligence and Informatics (SACI)
AB  - Operational Transformation (OT) has emerged as a viable theoretical principle for the implementation of real-time collaboration applications. In such systems, the collaboration consists of operations generated by members of a group who are performing concurrent actions on the same document or content. This powerful multi-user co-editing has been researched ever since the seminal works of the late 1980s. As the web evolved into a dominant platform for content consumption and creation, classes of algorithms like OT and Conflict-free Replicated Data Types (CRDT) have enabled flexible content synchronization for applications such as online word processors. Despite their long history in academia, OT and CRDT continue to have unsolved issues due to the centralized approach required for scalable and reliable web-based document editing. This paper proposes a Control Loop-based OT approach based on a serverless architecture and on Finite State Automata (FSA). A control loop principle is used to design a series of algorithms for distributed conflict resolution. The proposed architecture consists of a series of blocks which internally contain a number of multi-level Finite State Machines. The architecture of the new serverless approach for OT is introduced and the basic FSAs that model the co-editing processes are described. Cases encountered in the dynamics of the co-editing processes were modeled to prove that the essential OT properties of causality preservation, convergence, and intention preservation are all satisfied. Simulation results are given at the end of the paper.
C3  - 2020 IEEE 14th International Symposium on Applied Computational Intelligence and Informatics (SACI)
DA  - 2020/05//
PY  - 2020
DO  - 10.1109/SACI49304.2020.9118822
DP  - IEEE Xplore
SP  - 000247
EP  - 000254
UR  - https://ieeexplore.ieee.org/document/9118822
Y2  - 2023/12/31/19:46:03
ER  - 

TY  - CONF
TI  - A NodeRED-based dashboard to deploy pipelines on top of IoT infrastructure
AU  - Tricomi, Giuseppe
AU  - Benomar, Zakaria
AU  - Aragona, Francesco
AU  - Merlino, Giovanni
AU  - Longo, Francesco
AU  - Puliafito, Antonio
T2  - 2020 IEEE International Conference on Smart Computing (SMARTCOMP)
AB  - With the widespread emergence of the Internet of Things (IoT), our environment and locations are turning progressively into smart environments ranging from individual houses/offices to schools, factories, and hospitals. Even more interesting, with the rise of Fog/Edge paradigms, the IoT application scope has been extended to provide critical services. By pushing resources such as compute and storage to the network edge, IoT-based services are taking benefits from their proximity to provide better performances. However, albeit an exciting development in and by itself, Edge/Fog computing platforms currently do not provide a convenient level of flexibility and efficiency to support the dynamic composition of services with a data-oriented approach. In this context, the Function-as-a-Service computing paradigm rises as a convenient/suitable paradigm to be adopted in the IoT landscape. For the sake of providing flexible IoT Edge/Fog deployments, this paper introduces a system providing FaaS services based on a distributed IoT infrastructure. Besides, we provide a dashboard based on Node-RED that exploits, in the backend, the FaaS system to make the users able to conceive customized applications using the resources (i.e., sensors and actuators) that the IoT devices can host.
C3  - 2020 IEEE International Conference on Smart Computing (SMARTCOMP)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/SMARTCOMP50058.2020.00036
DP  - IEEE Xplore
SP  - 122
EP  - 129
UR  - https://ieeexplore.ieee.org/document/9239699
Y2  - 2023/12/31/19:46:17
ER  - 

TY  - CONF
TI  - A Survey on Serverless Computing and Its Implications for JointCloud Computing
AU  - Wu, Mingyu
AU  - Mi, Zeyu
AU  - Xia, Yubin
T2  - 2020 IEEE International Conference on Joint Cloud Computing
AB  - Serverless computing is known as an appealing alternative cloud computing paradigm with its auto-scaling nature and pay-as-you-go charging model. Mainstream cloud vendors have proposed their own serverless platforms, while various kinds of applications have been refactored in a serverless manner for execution. However, the serverless computing model still entails refinement as it introduces performance and security issues. In this paper, we conduct a comprehensive survey on the serverless computing, mainly in three aspects: the type of applications suitable for serverless, the performance issues, and the security issues. We specifically elaborate previous efforts on resolving issues in serverless and shed light on the unresolved issues. We also discuss the opportunities and challenges in integrating serverless computing in the jointcloud infrastructure.
C3  - 2020 IEEE International Conference on Joint Cloud Computing
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/JCC49151.2020.00023
DP  - IEEE Xplore
SP  - 94
EP  - 101
UR  - https://ieeexplore.ieee.org/document/9183650
Y2  - 2023/12/31/21:18:41
L2  - https://ieeexplore.ieee.org/document/9183650
ER  - 

TY  - JOUR
TI  - A Survey on the Internet of Things (IoT) Forensics: Challenges, Approaches, and Open Issues
AU  - Stoyanova, Maria
AU  - Nikoloudakis, Yannis
AU  - Panagiotakis, Spyridon
AU  - Pallis, Evangelos
AU  - Markakis, Evangelos K.
T2  - IEEE Communications Surveys & Tutorials
AB  - Today is the era of the Internet of Things (IoT). The recent advances in hardware and information technology have accelerated the deployment of billions of interconnected, smart and adaptive devices in critical infrastructures like health, transportation, environmental control, and home automation. Transferring data over a network without requiring any kind of human-to-computer or human-to-human interaction, brings reliability and convenience to consumers, but also opens a new world of opportunity for intruders, and introduces a whole set of unique and complicated questions to the field of Digital Forensics. Although IoT data could be a rich source of evidence, forensics professionals cope with diverse problems, starting from the huge variety of IoT devices and non-standard formats, to the multi-tenant cloud infrastructure and the resulting multi-jurisdictional litigations. A further challenge is the end-to-end encryption which represents a trade-off between users' right to privacy and the success of the forensics investigation. Due to its volatile nature, digital evidence has to be acquired and analyzed using validated tools and techniques that ensure the maintenance of the Chain of Custody. Therefore, the purpose of this paper is to identify and discuss the main issues involved in the complex process of IoT-based investigations, particularly all legal, privacy and cloud security challenges. Furthermore, this work provides an overview of the past and current theoretical models in the digital forensics science. Special attention is paid to frameworks that aim to extract data in a privacy-preserving manner or secure the evidence integrity using decentralized blockchain-based solutions. In addition, the present paper addresses the ongoing Forensics-as-a-Service (FaaS) paradigm, as well as some promising cross-cutting data reduction and forensics intelligence techniques. Finally, several other research trends and open issues are presented, with emphasis on the need for proactive Forensics Readiness strategies and generally agreed-upon standards.
DA  - 2020///
PY  - 2020
DO  - 10.1109/COMST.2019.2962586
DP  - IEEE Xplore
VL  - 22
IS  - 2
SP  - 1191
EP  - 1221
J2  - IEEE Communications Surveys & Tutorials
SN  - 1553-877X
ST  - A Survey on the Internet of Things (IoT) Forensics
UR  - https://ieeexplore.ieee.org/document/8950109
Y2  - 2023/12/31/21:19:11
L1  - https://ieeexplore.ieee.org/ielx7/9739/9102343/08950109.pdf?tp=&arnumber=8950109&isnumber=9102343&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5NTAxMDk=
L2  - https://ieeexplore.ieee.org/document/8950109
ER  - 

TY  - CONF
TI  - An autonomous Mobile Robot System based on Serverless Computing and Edge Computing
AU  - Thong Tran, Tri
AU  - Zhang, Yu-Chen
AU  - Liao, Wei-Tung
AU  - Lin, Yu-Jen
AU  - Li, Ming-Chia
AU  - Huang, Huai-Sheng
T2  - 2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)
AB  - Strengthen by Artificial Intelligence (AI) and complexly integrated sensors, an autonomous mobile robot (AMR) is extensively applied in coping with various human resources tasks in indoor office environments. However, implementing an AMR system from scratch needs a strong Electric Engineer background due to the complexity of robot-controlling. Besides, Communication between robot-server, sensor, and client-service also increases the difficulty and time-cost in AMR developing. In this paper, the AMR system we proposed aims to be implemented by people without a EE background, and we will achieve this by employing Robot Operating System. Besides, the AMR system should work independently but still capable of responding to human requests. We will demonstrate a serverless could structure that includes the client-side service and integrate the edge-computing, which in charge of the immediacy-demanding job.
C3  - 2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)
DA  - 2020/09//
PY  - 2020
DO  - 10.23919/APNOMS50412.2020.9236976
DP  - IEEE Xplore
SP  - 334
EP  - 337
SN  - 2576-8565
UR  - https://ieeexplore.ieee.org/document/9236976
Y2  - 2023/12/31/21:19:34
L2  - https://ieeexplore.ieee.org/document/9236976
ER  - 

TY  - CONF
TI  - Automatic Tuning of Hyperparameters for Neural Networks in Serverless Cloud
AU  - Kaplunovich, Alex
AU  - Yesha, Yelena
T2  - 2020 IEEE International Conference on Big Data (Big Data)
AB  - Deep Neural Networks are used to solve the most challenging world problems. In spite of the numerous advancements in the field, most of the models are being tuned manually. Experienced Data Scientists have to manually optimize hyperparameters, such as dropout rate, learning rate or number of neurons for Big Data applications. We have implemented a flexible automatic real-time hyperparameter tuning methodology. It works for arbitrary models written in Python and Keras. We also utilized state of the art Cloud services such as trigger based serverless computing (Lambda), and advanced GPU instances to implement automation, reliability and scalability.The existing tuning libraries, such as hyperopt, Scikit-Optimize or SageMaker, require developers to provide a list of hyperparameters and the range of their values manually. Our novel approach detects potential hyperparameters automatically from the source code, updates the original model to tune the parameters, runs the evaluation in the Cloud on spot instances, finds the optimal hyperparameters, and saves the results in the No-SQL database. The methodology can be applied to numerous Big Data Machine Learning systems.
C3  - 2020 IEEE International Conference on Big Data (Big Data)
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/BigData50022.2020.9378280
DP  - IEEE Xplore
SP  - 2751
EP  - 2756
UR  - https://ieeexplore.ieee.org/document/9378280
Y2  - 2023/12/31/21:19:52
L2  - https://ieeexplore.ieee.org/document/9378280
ER  - 

TY  - CONF
TI  - BATCH: Machine Learning Inference Serving on Serverless Platforms with Adaptive Batching
AU  - Ali, Ahsan
AU  - Pinciroli, Riccardo
AU  - Yan, Feng
AU  - Smirni, Evgenia
T2  - SC20: International Conference for High Performance Computing, Networking, Storage and Analysis
AB  - Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker.
C3  - SC20: International Conference for High Performance Computing, Networking, Storage and Analysis
DA  - 2020/11//
PY  - 2020
DO  - 10.1109/SC41405.2020.00073
DP  - IEEE Xplore
SP  - 1
EP  - 15
ST  - BATCH
UR  - https://ieeexplore.ieee.org/document/9355312
Y2  - 2023/12/31/21:20:14
ER  - 

TY  - CONF
TI  - Caching Techniques to Improve Latency in Serverless Architectures
AU  - Ghosh, Bishakh Chandra
AU  - Addya, Sourav Kanti
AU  - Somy, Nishant Baranwal
AU  - Nath, Shubha Brata
AU  - Chakraborty, Sandip
AU  - Ghosh, Soumya K
T2  - 2020 International Conference on COMmunication Systems & NETworkS (COMSNETS)
AB  - Serverless computing has gained a significant traction in recent times because of its simplicity of development, deployment and fine-grained billing. However, while implementing complex services comprising databases, file stores, or more than one serverless function, the performance in terms of latency of serving requests often degrades severely. In this work, we analyze different serverless architectures with AWS Lambda services and compare their performance in terms of latency with a traditional virtual machine (VM) based approach. We observe that database access latency in serverless architecture is almost 14 times than that in VM based setup. Further, we introduce some caching strategies which can improve the response time significantly, and compare their performance.
C3  - 2020 International Conference on COMmunication Systems & NETworkS (COMSNETS)
DA  - 2020/01//
PY  - 2020
DO  - 10.1109/COMSNETS48256.2020.9027427
DP  - IEEE Xplore
SP  - 666
EP  - 669
SN  - 2155-2509
UR  - https://ieeexplore.ieee.org/abstract/document/9027427
Y2  - 2023/12/31/21:20:28
L1  - https://arxiv.org/pdf/1911.07351
L2  - https://ieeexplore.ieee.org/abstract/document/9027427
ER  - 

TY  - CONF
TI  - Cold Start in Serverless Computing: Current Trends and Mitigation Strategies
AU  - Vahidinia, Parichehr
AU  - Farahani, Bahar
AU  - Aliee, Fereidoon Shams
T2  - 2020 International Conference on Omni-layer Intelligent Systems (COINS)
AB  - Serverless Computing is the latest cloud computing model, which facilitates application development. By adopting and leveraging the modern paradigm of Serverless Computing, developers do not need to manage the servers. In this computational model, the executables are independent functions that are individually deployed on a Serverless platform offering instant per-request elasticity. Such elasticity typically comes at the cost of the “Cold Starts” problem. This phenomenon is associated with a delay occurring due to provision a runtime container to execute the functions. Shortly after Amazon introduced this computing model with the AWS Lambda platform in 2014, several open source and commercial platforms also started embracing and offering this technology. Each platform has its own solution to deal with Cold Starts. The evaluation of the performance of each platform under the load and factors influencing the cold start problem has received much attention over the past few years. This paper provides a comprehensive overview on the recent advancements and state-of-the-art works in mitigating the cold start delay. Moreover, several sets of experiments have been performed to study the behavior of the AWS Lambda as the base platform with respect to the cold start delay.
C3  - 2020 International Conference on Omni-layer Intelligent Systems (COINS)
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/COINS49042.2020.9191377
DP  - IEEE Xplore
SP  - 1
EP  - 7
ST  - Cold Start in Serverless Computing
UR  - https://ieeexplore.ieee.org/document/9191377
Y2  - 2024/01/01/07:15:28
ER  - 

TY  - CONF
TI  - Computational Edge for Multiple Autonomous Objects
AU  - McClenaghan, Karoline
AU  - Moholth, Ole Christian
AU  - Bjerknes, Jan Dyre
T2  - 2020 IEEE International Conference on Human-Machine Systems (ICHMS)
AB  - This research explores possibilities of creating software architectures for managing multiple autonomous objects in computational environments, which move away from clouds and use computational power of objects, at the edge of computing and communication networks. The emphasis is on shaping the behaviour of autonomous objects through human involvement in order to manipulate functions and change the purpose and levels of autonomy of these objects. The proposed computational model, generated from the software architectures, which gives rise to serverless and edge computing, should work across problem domain. By collecting relevant data and allowing a variable level of human input, the solution will enable us to choose, merge and combine multiple objects for a variety of tasks and according to environments in which autonomous objects reside.
C3  - 2020 IEEE International Conference on Human-Machine Systems (ICHMS)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/ICHMS49158.2020.9209547
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9209547
Y2  - 2024/01/01/07:22:47
ER  - 

TY  - CONF
TI  - Controlling Garbage Collection and Request Admission to Improve Performance of FaaS Applications
AU  - Quaresma, David
AU  - Fireman, Daniel
AU  - Pereira, Thiago Emmanuel
T2  - 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)
AB  - Runtime environments like Java's JRE, .NET's CLR, and Ruby's MRI, are popular choices for cloud-based applications and particularly in the Function as a Service (FaaS) serverless computing context. A critical component of runtime environments of these languages is the garbage collector (GC). The GC frees developers from manual memory management, which could potentially ease development and avoid bugs. The benefits of using the GC come with a negative impact on performance; that impact happens because either the GC needs to pause the runtime execution or competes with the running program for computational resources. In this work, we evaluated the usage of a technique - Garbage Collector Control Interceptor (GCI) - that eliminates the negative impact of GC on performance by controlling GC executions and transparently shedding requests while the collections are happening. We executed experiments simulating AWS Lambda's behavior and found that GCI is a viable solution. It benefited the user by improving the response time up to 10.86% at 99.9th percentile and reducing cost by 7.22%, but it also helped the platform provider by improving resource utilization by 14.52%.
C3  - 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/SBAC-PAD49847.2020.00033
DP  - IEEE Xplore
SP  - 175
EP  - 182
SN  - 2643-3001
UR  - https://ieeexplore.ieee.org/document/9235063
Y2  - 2024/01/01/07:23:48
L2  - https://ieeexplore.ieee.org/document/9235063
ER  - 

TY  - CONF
TI  - Cost efficiency under mixed serverless and serverful deployments
AU  - Reuter, Anja
AU  - Back, Timon
AU  - Andrikopoulos, Vasilios
T2  - 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
AB  - Function as a Service (FaaS) is an integral part of the serverless computing paradigm. It offers a true pay-per-use billing model and releases developers from the burden of managing the application stack. A discussion on whether and when this model is more appropriate for cloud computing users in terms of accruing costs compared to the more "traditional" delivery models has already been started by existing works. However, by treating this subject as a regular service selection problem, these approaches fail to exploit the space created by distributing the load between simultaneous FaaS and non-FaaS deployments of an application in a hybrid deployment model. This work aims to provide the means for application owners to decide which deployment scenario is cost optimal for their needs. In case this scenario is a hybrid deployment, the proposed approach also determines the optimal number of virtual machines that will need to be provisioned. An extensible and configurable FaasSimulator open source tool is presented for these purposes.
C3  - 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/SEAA51224.2020.00049
DP  - IEEE Xplore
SP  - 242
EP  - 245
UR  - https://ieeexplore.ieee.org/document/9226321
Y2  - 2024/01/01/07:24:20
L1  - https://pure.rug.nl/ws/files/129332312/74886887_5689684_main.pdf
ER  - 

TY  - CONF
TI  - Cost-Effective Malware Detection as a Service Over Serverless Cloud Using Deep Reinforcement Learning
AU  - Birman, Yoni
AU  - Hindi, Shaked
AU  - Katz, Gilad
AU  - Shabtai, Asaf
T2  - 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)
AB  - The current trends of cloud computing in general, and serverless computing in particular, affect multiple aspects of organizational activity. Organizations of all sizes are transitioning parts of their operations off-premise in order to reduce costs and scale their operations more efficiently. The field of network security is no exception, with many organizations taking advantage of the distributed and scalable cloud environment. Since the charging model for serverless computing is "pay as you go" (i.e., payment per action), a reduction in the number of required computations translates into significant cost savings. This understanding is also relevant to the field of malware detection, where organizations often deploy multiple types of detectors to increase detection accuracy. In this study, we utilize deep reinforcement learning to reduce computational costs in the cloud by selectively querying only a subset of available detectors. We demonstrate that our approach is not only effective both for on-premise and cloud-based computing architectures, but that applying it to serverless computing can reduce costs by an order of magnitude while maintaining near-optimal performance.
C3  - 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)
DA  - 2020/05//
PY  - 2020
DO  - 10.1109/CCGrid49817.2020.00-51
DP  - IEEE Xplore
SP  - 420
EP  - 429
UR  - https://ieeexplore.ieee.org/document/9139646
Y2  - 2024/01/01/07:24:34
L2  - https://ieeexplore.ieee.org/document/9139646
ER  - 

TY  - CONF
TI  - Demonstrating the Practicality of Unikernels to Build a Serverless Platform at the Edge
AU  - Mistry, Chetankumar
AU  - Stelea, Bogdan
AU  - Kumar, Vijay
AU  - Pasquier, Thomas
T2  - 2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)
AB  - The rise of IoT has led to large volumes of personal data being produced at the network's edge. Most IoT applications process data in the cloud raising concerns over privacy and security. As many IoT applications are event-based and are implemented on cloud-based, serverless platforms, we've seen a number of proposals to deploy serverless solutions at the edge to address concerns over data transfer. However, conventional serverless platforms use container technology to run user-defined functions. Containers introduce their own issues regarding security - due to a large trusted computing base -, and performance issues including long initialisation times. Additionally, OpenWhisk a popular and widely used container-based serverless platform available for edge devices perform relatively poorly as we demonstrate in our evaluation. In this paper, we propose to investigate unikernel as a solution to build serverless platform at the edge, addressing in particular performance and security concerns. We present UniFaaS, a prototype edge-serverless platform which leverages unikernels - tiny library single-address-space operating systems that only contain the parts of the OS needed to run a given application - to execute functions. The result is a serverless platform with extremely low memory and CPU footprints, and excellent performance. UniFaaS has been designed to be deployed on low-powered single-board computer devices, such as Raspberry Pi or Arduino, without compromising on performance.
C3  - 2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/CloudCom49646.2020.00001
DP  - IEEE Xplore
SP  - 25
EP  - 32
SN  - 2330-2186
UR  - https://ieeexplore.ieee.org/document/9407311
Y2  - 2024/01/01/07:24:55
L2  - https://ieeexplore.ieee.org/document/9407311
ER  - 

TY  - CONF
TI  - DetectSignal: A Cloud-Based Traffic Signal Notification System for the Blind and Visually Impaired
AU  - Ihejimba, Chikadibia
AU  - Wenkstern, Rym Z.
T2  - 2020 IEEE International Smart Cities Conference (ISC2)
AB  - The ability to know when to cross the road at an intersection has always been a problem for the Visually Impaired or Blind person (VIB). In this paper, we present DetectSignal, a cloud-based Internet of Things (IoT) edge computing solution that provides a highly available, highly scalable, high-speed, and low latency traffic signal notification for the VIB. DetectSignal provides an interconnected traffic signal by reusing existing infrastructure and attaching a low-cost IoT device that connects with a traffic light and the public cloud. Our experimental results involve testing a notification system using a regular compute virtual machine and IoT edge-based serverless compute. The experimental results show that DetectSignal provides a more reliable solution that alleviates the current issues facing the VIB.
C3  - 2020 IEEE International Smart Cities Conference (ISC2)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/ISC251055.2020.9239004
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2687-8860
ST  - DetectSignal
UR  - https://ieeexplore.ieee.org/document/9239004
Y2  - 2024/01/01/07:25:11
ER  - 

TY  - CONF
TI  - FaaS2F: A Framework for Defining Execution-SLA in Serverless Computing
AU  - Elsakhawy, Mohamed
AU  - Bauer, Michael
T2  - 2020 IEEE Cloud Summit
AB  - The emergence of serverless computing has brought significant advancements to the delivery of computing resources to cloud users. With the abstraction of infrastructure, platform, and execution environments, the administration overhead of these layers is shifted to the cloud provider. Thus, cloud users can focus on the application layer while relying on the cloud provider to provision and operate the underlying layers. Furthermore, desirable features such as autoscaling and high-availability are implemented by the cloud provider and adopted by the user's code at no extra overhead. Despite such advancements, as applications transition from monolithic stand-alone deployments to the ephemeral and stateless microservice model of serverless computing, significant challenges must be overcome. These challenges pertain to the uniqueness of the conceptual and implementation models of serverless computing. In this paper, we investigate the modeling of the Service Level Agreement (SLA) of serverless functions' executions. We highlight how serverless SLA fundamentally differs from earlier cloud delivery models. We then propose an approach to define SLA for serverless functions in terms of resource utilization fingerprints for functions' executions, and a method to assess if executions adhere to that SLA. We conclude by presenting the empirical validation of our proposed approach, which was able to detect Execution-SLA violations with accuracy exceeding 76% and up to 100%.
C3  - 2020 IEEE Cloud Summit
DA  - 2020/10//
PY  - 2020
DO  - 10.1109/IEEECloudSummit48914.2020.00015
DP  - IEEE Xplore
SP  - 58
EP  - 65
ST  - FaaS2F
UR  - https://ieeexplore.ieee.org/document/9283723
Y2  - 2024/01/01/07:25:27
ER  - 

TY  - JOUR
TI  - Fabrication-as-a-Service: A Web-Based Solution for STEM Education Using Internet of Things
AU  - Cornetta, Gianluca
AU  - Touhafi, Abdellah
AU  - Togou, Mohammed Amine
AU  - Muntean, Gabriel-Miro
T2  - IEEE Internet of Things Journal
AB  - Recently, fabrication laboratories (Fab Labs) have been shown to have a great impact on learners' academic and personal progress. As a result, an increasing effort is being put to integrate Fab Labs into schools' curricula. Yet, owing to the high cost of setting up and maintaining Fab Labs as well as the lack of sufficient funding for most schools and universities, only a limited number of institutions can afford them. In this article, we propose a new concept called Fabrication-as-a-Service (FaaS) that uses Internet of Things to democratize access to Fab Labs via enabling a wide learning community to remotely access these computer-controlled tools and equipment over the Internet. It employs a two-tier architecture consisting of a hub, deployed in the cloud, and a network of distributed Fab Labs. Each Fab Lab interacts with the hub and other digital labs via a Fab Lab Gateway. This is to support scalability and high availability of fabrication services as well as ensure the system's security. FaaS also adopts an innovative master-slave approach that uses inexpensive external hardware to monitor and control the activity of expensive fabrication equipment. This article also describes the FaaS deployment in the context of the European Union Horizon 2020 NEWTON project. Multiple scenarios have been deployed to fully illustrate the benefits of the FaaS architecture and to assess the performance of its communication protocol stack.
DA  - 2020/02//
PY  - 2020
DO  - 10.1109/JIOT.2019.2956401
DP  - IEEE Xplore
VL  - 7
IS  - 2
SP  - 1519
EP  - 1530
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - Fabrication-as-a-Service
UR  - https://ieeexplore.ieee.org/document/8915690
Y2  - 2024/01/01/07:25:41
ER  - 

TY  - JOUR
TI  - Federated Learning With Cooperating Devices: A Consensus Approach for Massive IoT Networks
AU  - Savazzi, Stefano
AU  - Nicoli, Monica
AU  - Rampa, Vittorio
T2  - IEEE Internet of Things Journal
AB  - Federated learning (FL) is emerging as a new paradigm to train machine learning (ML) models in distributed systems. Rather than sharing and disclosing the training data set with the server, the model parameters (e.g., neural networks' weights and biases) are optimized collectively by large populations of interconnected devices, acting as local learners. FL can be applied to power-constrained Internet of Things (IoT) devices with slow and sporadic connections. In addition, it does not need data to be exported to third parties, preserving privacy. Despite these benefits, a main limit of existing approaches is the centralized optimization which relies on a server for aggregation and fusion of local parameters; this has the drawback of a single point of failure and scaling issues for increasing network size. This article proposes a fully distributed (or serverless) learning approach: the proposed FL algorithms leverage the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. The approach lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing, with intelligence distributed over the end devices. The proposed methodology is verified by the experimental data sets collected inside an Industrial IoT (IIoT) environment.
DA  - 2020/05//
PY  - 2020
DO  - 10.1109/JIOT.2020.2964162
DP  - IEEE Xplore
VL  - 7
IS  - 5
SP  - 4641
EP  - 4654
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - Federated Learning With Cooperating Devices
UR  - https://ieeexplore.ieee.org/document/8950073
Y2  - 2024/01/01/07:25:58
L1  - https://arxiv.org/pdf/1912.13163
ER  - 

TY  - CONF
TI  - Federated Learning with Mutually Cooperating Devices: A Consensus Approach Towards Server-Less Model Optimization
AU  - Savazzi, Stefano
AU  - Nicoli, Monica
AU  - Rampa, Vittorio
AU  - Kianoush, Sanaz
T2  - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
AB  - Federated learning (FL) is emerging as a new paradigm for training a machine learning model in cooperative networks. The model parameters are optimized collectively by large populations of interconnected devices, acting as cooperative learners that exchange local model updates with the server, rather than user data. The FL framework is however centralized, as it relies on the server for fusion of the model updates and as such it is limited by a single point of failure. In this paper we propose a distributed FL approach that performs a decentralized fusion of local model parameters by leveraging mutual cooperation between the devices and local (in-network) data operations via consensus-based methods. Communication with the server can be partially, or fully, replaced by in-network operations, scaling down the traffic load on the server as well as paving the way towards a fully serverless FL approach. This proposal also lays the groundwork for integration of FL methods within future (beyond 5G) wireless networks characterized by distributed and decentralized connectivity. The proposed algorithms are implemented and published as open source. They are also designed and verified by experimental data.
C3  - ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
DA  - 2020/05//
PY  - 2020
DO  - 10.1109/ICASSP40776.2020.9054055
DP  - IEEE Xplore
SP  - 3937
EP  - 3941
SN  - 2379-190X
ST  - Federated Learning with Mutually Cooperating Devices
UR  - https://ieeexplore.ieee.org/document/9054055
Y2  - 2024/01/01/07:26:18
ER  - 

TY  - CONF
TI  - Implications of Programming Language Selection for Serverless Data Processing Pipelines
AU  - Cordingly, Robert
AU  - Yu, Hanfei
AU  - Hoang, Varik
AU  - Perez, David
AU  - Foster, David
AU  - Sadeghi, Zohreh
AU  - Hatchett, Rashad
AU  - Lloyd, Wes J.
T2  - 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
AB  - Serverless computing platforms have emerged offering software engineers an option for application hosting without the need to configure servers or manage scaling while guaranteeing high availability and fault tolerance. In the ideal scenario, a developer should be able to create a microservice, deploy it to a serverless platform, and never have to manage or configure anything; a truly serverless platform. The current implementation of serverless computing platforms is known as Function-as-a-Service or FaaS. Adoption of FaaS platforms, however, requires developers to address a major question- what programming language should functions be written in? To investigate this question, we implemented identical multi-function data processing pipelines in Java, Python, Go, and Node.js. Using these pipelines as a case study, we ran experiments tailored to investigate FaaS data processing performance. Specifically, we investigate data processing performance implications: for data payloads of varying size, with cold and warm serverless infrastructure, over scaling workloads, and when varying the available function memory. We found that Node.js had up to 94% slower runtime vs. Java for the same workload. In other scenarios, Java had 20% slower runtime than Go resulting from differences in how the cloud provider orchestrates infrastructure for each language with respect to the serverless freeze/thaw lifecycle. We found that no single language provided the best performance for every stage of a data processing pipeline and the fastest pipeline could be derived by combining a hybrid mix of languages to optimize performance.
C3  - 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00120
DP  - IEEE Xplore
SP  - 704
EP  - 711
UR  - https://ieeexplore.ieee.org/document/9251194
Y2  - 2024/01/01/07:26:31
L2  - https://ieeexplore.ieee.org/document/9251194
ER  - 

TY  - CONF
TI  - Knative Autoscaler Optimize Based on Double Exponential Smoothing
AU  - Fan, Dayong
AU  - He, Dongzhi
T2  - 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)
AB  - Knative is a popular Kubernetes-based platform for managing serverless workloads with two main components Eventing and Serving. The Serving primitive helps to deploy serverless apps as Knative services and automatically scale them, even down to zero. However, the serving module uses a moving average method to calculate the number of pods, that calculated based on past data and not good at accounting for future changes. In this paper, we use double exponential smoothing to optimize the calculation of the number of pods. Preliminary experiments show that the results are better than the moving average.
C3  - 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)
DA  - 2020/06//
PY  - 2020
DO  - 10.1109/ITOEC49072.2020.9141858
DP  - IEEE Xplore
SP  - 614
EP  - 617
UR  - https://ieeexplore.ieee.org/document/9141858
Y2  - 2024/01/01/07:26:52
ER  - 

TY  - CONF
TI  - Migrating Large Deep Learning Models to Serverless Architecture
AU  - Chahal, Dheeraj
AU  - Ojha, Ravi
AU  - Ramesh, Manju
AU  - Singhal, Rekha
T2  - 2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)
AB  - Serverless computing platform is emerging as a solution for event-driven artificial intelligence applications. Function-as-a-Service (FaaS) using serverless computing paradigm provides high performance and low cost solutions for deploying such applications on cloud while minimizing the operational logic. Using FaaS for efficient deployment of complex applications, such as natural language processing (NLP) and image processing, containing large deep learning models will be an advantage. However, constrained resources and stateless nature of FaaS offers numerous challenges while deploying such applications. In this work, we discuss the methodological suggestions and their implementation for deploying pre-trained large size machine learning and deep learning models on FaaS. We also evaluate the performance and deployment cost of an enterprise application, consisting of suite of deep vision preprocessing algorithms and models, on VM and FaaS platform. Our evaluation shows that migration from monolithic to FaaS platform significantly improves the performance of the application at a reduced cost.
C3  - 2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)
DA  - 2020/10//
PY  - 2020
DO  - 10.1109/ISSREW51248.2020.00047
DP  - IEEE Xplore
SP  - 111
EP  - 116
UR  - https://ieeexplore.ieee.org/document/9307673
Y2  - 2024/01/01/07:27:09
L2  - https://ieeexplore.ieee.org/document/9307673
KW  - FaaS
KW  - Serverless
KW  - AI
KW  - Function-as-a-service
KW  - Deep learning
KW  - serverless
KW  - Serverless architecture
KW  - Computing paradigm
KW  - Computation theory
KW  - Image processing
KW  - Software as a service (SaaS)
KW  - cloud
KW  - Computing platform
KW  - Event-driven
KW  - High-low
KW  - Learning algorithms
KW  - Learning models
KW  - Low-cost solution
KW  - Natural language processing systems
KW  - Performance costs
ER  - 

TY  - CONF
TI  - NanoLambda: Implementing Functions as a Service at All Resource Scales for the Internet of Things
AU  - George, Gareth
AU  - Bakir, Fatih
AU  - Wolski, Rich
AU  - Krintz, Chandra
T2  - 2020 IEEE/ACM Symposium on Edge Computing (SEC)
AB  - Internet of Things (IoT) devices are becoming increasingly prevalent in our environment, yet the process of programming these devices and processing the data they produce remains difficult. Typically, data is processed on device, involving arduous work in low level languages, or data is moved to the cloud, where abundant resources are available for Functions as a Service (FaaS) or other handlers. FaaS is an emerging category of flexible computing services, where developers deploy self-contained functions to be run in portable and secure containerized environments; however, at the moment, these functions are limited to running in the cloud or in some cases at the “edge” of the network using resource rich, Linux-based systems.In this paper, we present NanoLambda, a portable platform that brings FaaS, high-level language programming, and familiar cloud service APIs to non-Linux and microcontroller-based IoT devices. To enable this, NanoLambda couples a new, minimal Python runtime system that we have designed for the least capable end of the IoT device spectrum, with API compatibility for AWS Lambda and S3. NanoLambda transfers functions between IoT devices (sensors, edge, cloud), providing power and latency savings while retaining the programmer productivity benefits of high-level languages and FaaS. A key feature of NanoLambda is a scheduler that intelligently places function executions across multi-scale IoT deployments according to resource availability and power constraints. We evaluate a range of applications that use NanoLambda to run on devices as small as the ESP8266 with 64KB of ram and 512KB flash storage.
C3  - 2020 IEEE/ACM Symposium on Edge Computing (SEC)
DA  - 2020/11//
PY  - 2020
DO  - 10.1109/SEC50012.2020.00035
DP  - IEEE Xplore
SP  - 220
EP  - 231
ST  - NanoLambda
UR  - https://ieeexplore.ieee.org/document/9355717
Y2  - 2024/01/01/07:28:32
ER  - 

TY  - CONF
TI  - Nanoservice Infrastructure Notation (NINo) and the ASPIRE Interns
AU  - Pascale, Chancellor T.
AU  - Rice, Maria
AU  - Sharma, Shiva
T2  - 2020 IEEE Integrated STEM Education Conference (ISEC)
AB  - NINo is a future DevOps / Data Science pipeline tool that is being developed by JHU APL and two ASPIRE interns. The goal of this capability is to expose function-level capabilities, via either a simple application or configuration file, for use in Docker [1], Serverless Architectures [2], or data science/analytic pipelines. The goal is similar to efforts such as Metaparticle [3] and Source-to-Image[4] that aim to lower the barrier to horizontal scaling of data processing and analysis capabilities. In previous years ASPIRE interns have developed tools to ease the acceptance of DevOps principles in JHU APL. They have created a web application, Harmonia, that asked users a few simple questions and supplied the scaffolding for a software project with artifacts to support sound software engineering processes. The lack of user interest has driven us to a more focused objective. NINo will focus on easing deployment to cloud environments. Ideally, any person could develop cloud-based data science services. The team and its work has been organized in an asynchronous and agile manner. There have been three members working on three subsystems: configuration, framework/integration, and artifact generation. An incremental and prototype-driven approach has allowed for creation of increasingly more functional software as internship has proceeded. Interns have been given extensive control over their development processes and have investigated the programming frameworks used. While the initial stages have not resulted in a complete system, the interns have improved their programming skills and complete common coding challenges. The team is close to integration testing and initial demonstration. As the academic year closes, team members will work on design improvement, refactoring, and generation of future feature requests from prospective users. One summer intern will focus on developing a user interface for configuring and observing results.
C3  - 2020 IEEE Integrated STEM Education Conference (ISEC)
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/ISEC49744.2020.9397808
DP  - IEEE Xplore
SP  - 1
EP  - 1
SN  - 2330-331X
UR  - https://ieeexplore.ieee.org/document/9397808
Y2  - 2024/01/01/07:28:46
L2  - https://ieeexplore.ieee.org/document/9397808
ER  - 

TY  - CONF
TI  - Natural Language Processing on Diverse Data Layers Through Microservice Architecture
AU  - Bhagat, Vandana
AU  - J, Bastin Robins
T2  - 2020 IEEE International Conference for Innovation in Technology (INOCON)
AB  - With the rapid growth in Natural Language Processing (NLP), all types of industries find a need for analyzing a massive amount of data. Sentiment analysis is becoming a more exciting area for the businessmen and researchers in Text mining & NLP. This process includes the calculation of various sentiments with the help of text mining. Supplementary to this, the world is connected through Information Technology and, businesses are moving toward the next step of the development to make their system more intelligent. Microservices have fulfilled the need for development platforms which help the developers to use various development tools (Languages and applications) efficiently. With the consideration of data analysis for business growth, data security becomes a major concern in front of developers. This paper gives a solution to keep the data secured by providing required access to data scientists without disturbing the base system software. This paper has discussed data storage and exchange policies of microservices through common JavaScript Object Notation (JSON) response which performs the sentiment analysis of customer's data fetched from various microservices through secured APIs.
C3  - 2020 IEEE International Conference for Innovation in Technology (INOCON)
DA  - 2020/11//
PY  - 2020
DO  - 10.1109/INOCON50539.2020.9298027
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9298027
Y2  - 2024/01/01/07:29:07
L2  - https://ieeexplore.ieee.org/document/9298027
ER  - 

TY  - CONF
TI  - Operator as a Service: Stateful Serverless Complex Event Processing
AU  - Luthra, Manisha
AU  - Hennig, Sebastian
AU  - Razavi, Kamran
AU  - Wang, Lin
AU  - Koldehofe, Boris
T2  - 2020 IEEE International Conference on Big Data (Big Data)
AB  - Complex Event Processing (CEP) is a powerful paradigm for scalable data management that is employed in many real-world scenarios such as detecting credit card fraud in banks. The so-called complex events are expressed using a specification language that is typically implemented and executed on a specific runtime system. While the tight coupling of these two components has been regarded as the key for supporting CEP at high performance, such dependencies pose several inherent challenges as follows. (1) Application development atop a CEP system requires extensive knowledge of how the runtime system operates, which is typically highly complex in nature. (2) The specification language dependence requires the need of domain experts and further restricts and steepens the learning curve for application developers.In this paper, we propose CEPless, a scalable data management system that decouples the specification from the runtime system by building on the principles of serverless computing. CEPless provides "operator as a service" and offers flexibility by enabling the development of CEP application in any specification language while abstracting away the complexity of the CEP runtime system. As part of CEPless, we designed and evaluated novel mechanisms for in-memory processing and batching that enable the stateful processing of CEP operators even under high rates of ingested events. Our evaluation demonstrates that CEPless can be easily integrated into existing CEP systems like Apache Flink while attaining similar throughput under high scale of events (up to 100K events per second) and dynamic operator update in 238 ms.
C3  - 2020 IEEE International Conference on Big Data (Big Data)
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/BigData50022.2020.9378142
DP  - IEEE Xplore
SP  - 1964
EP  - 1973
ST  - Operator as a Service
UR  - https://ieeexplore.ieee.org/document/9378142
Y2  - 2024/01/01/07:29:58
L1  - https://arxiv.org/pdf/2012.04982
ER  - 

TY  - CONF
TI  - OverSketched Newton: Fast Convex Optimization for Serverless Systems
AU  - Gupta, Vipul
AU  - Kadhe, Swanand
AU  - Courtade, Thomas
AU  - Mahoney, Michael W.
AU  - Ramchandran, Kannan
T2  - 2020 IEEE International Conference on Big Data (Big Data)
AB  - Motivated by recent developments in serverless systems for large-scale computation as well as improvements in scalable randomized matrix algorithms, we develop OverSketched Newton, a randomized Hessian-based optimization algorithm to solve large-scale convex optimization problems in serverless systems. OverSketched Newton leverages matrix sketching ideas from Randomized Numerical Linear Algebra to compute the Hessian approximately. These sketching methods lead to inbuilt resiliency against stragglers that are a characteristic of serverless architectures. Depending on whether or not the problem is strongly convex, we propose different iteration updates using the approximate Hessian. For both cases, we establish convergence guarantees for OverSketched Newton, and we empirically validate our results by solving large-scale supervised learning problems on real-world datasets. Experiments demonstrate a reduction of 50% in total running time on AWS Lambda, compared to state-of-the-art distributed optimization schemes.
C3  - 2020 IEEE International Conference on Big Data (Big Data)
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/BigData50022.2020.9378289
DP  - IEEE Xplore
SP  - 288
EP  - 297
ST  - OverSketched Newton
UR  - https://ieeexplore.ieee.org/document/9378289
Y2  - 2024/01/01/07:31:18
L1  - https://arxiv.org/pdf/1903.08857
ER  - 

TY  - CONF
TI  - PanOpticon: A Comprehensive Benchmarking Tool for Serverless Applications
AU  - Somu, Nikhila
AU  - Daw, Nilanjan
AU  - Bellur, Umesh
AU  - Kulkarni, Purushottam
T2  - 2020 International Conference on COMmunication Systems & NETworkS (COMSNETS)
AB  - Serverless computing manifests as the FaaS offering where clients submit code to be managed and run by the service provider in the place of hiring and managing VMs for this purpose. Multiple Cloud Service providers have come up with their own implementations of FaaS infrastructure providing end-users with a multitude of choices. Each such platform provides a non-overlapping set of features which satisfies a subset of users. Further the design of the platform dictates the performance overheads of triggering the function. A tool that automates capturing how a function behaves under different configurations of a platform and across platforms will, therefore, be useful for end-users intending to deploy applications as a collection of FaaS units. In spite of the presence of a few benchmarking tools for FaaS offerings, they lack the comprehensive breadth required to understand the performance aspects of the design choices made by the end-users. Most tools focus on tuning resource parameters like memory, CPU requirements and measure metrics like execution time. They lack the option to measure the effects of features such as function chaining and choice of function triggers. We present PanOpticon - a tool that automates the deployment of FaaS applications on different platforms under a set of tunable configuration choices and presents the users with performance measurements for each configuration and platform.
C3  - 2020 International Conference on COMmunication Systems & NETworkS (COMSNETS)
DA  - 2020/01//
PY  - 2020
DO  - 10.1109/COMSNETS48256.2020.9027346
DP  - IEEE Xplore
SP  - 144
EP  - 151
SN  - 2155-2509
ST  - PanOpticon
UR  - https://ieeexplore.ieee.org/document/9027346
Y2  - 2024/01/01/07:31:32
ER  - 

TY  - CONF
TI  - Potential Bottleneck and Measuring Performance of Serverless Computing: A Literature Study
AU  - Khatri, Deepak
AU  - Khatri, Sunil Kumar
AU  - Mishra, Deepti
T2  - 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
AB  - Trending form of cloud computing is Serverless computing, where developer just needs to focus on his code rather than worrying about server management. In serverless computing, application is nothing but collection of one or more functions, written for specific business functionality, which triggers on an event. There are various cloud service providers, i.e. Amazon, Microsoft, Google, IBM, etc. who provide serverless services, on pay as you use and auto scalable solution to execute the application code as a function. The developer just needs to upload the code for execution. The performance of the serverless computing may vary due to dynamic configuration of the solution, technologies and different technology used by the service provider.This paper reviews various past and recent work in the serverless computing to identify possible bottlenecks and the scope of measuring performance of serverless computing. It will also put some light to leverage machine learning in various possible ways to do performance engineering for future research.
C3  - 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
DA  - 2020/06//
PY  - 2020
DO  - 10.1109/ICRITO48877.2020.9197837
DP  - IEEE Xplore
SP  - 161
EP  - 164
ST  - Potential Bottleneck and Measuring Performance of Serverless Computing
UR  - https://ieeexplore.ieee.org/document/9197837
Y2  - 2024/01/01/07:31:48
ER  - 

TY  - CONF
TI  - Predicting Performance and Cost of Serverless Computing Functions with SAAF
AU  - Cordingly, Robert
AU  - Shu, Wen
AU  - Lloyd, Wes J.
T2  - 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
AB  - Next generation software built for the cloud recently has embraced serverless computing platforms that use temporary infrastructure to host microservices offering building blocks for resilient, loosely coupled systems that are scalable, easy to manage, and extend. Serverless architectures enable decomposing software into independent components packaged and run using isolated containers or microVMs. This decomposition approach enables application hosting using very fine-grained cloud infrastructure enabling cost savings as deployments are billed granularly for resource use. Adoption of serverless platforms promise reduced hosting costs while achieving high availability, fault tolerance, and dynamic elasticity. These benefits are offset by pricing obfuscation, as performance variance from CPU heterogeneity, multitenancy, and provisioning variation obscure the true cost of hosting applications with serverless platforms. Where determining hosting costs for traditional VM-based application deployments simply involves accounting for the number of VMs and their uptime, predicting hosting costs for serverless applications can be far more complex. To address these challenges, we introduce the Serverless Application Analytics Framework (SAAF), a tool that allows profiling FaaS workload performance, resource utilization, and infrastructure to enable accurate performance predictions. We apply Linux CPU time accounting principles and multiple regression to estimate FaaS function runtime. We predict runtime using a series of increasingly variant compute bound workloads that execute across heterogeneous CPUs, different memory settings, and to alternate FaaS platforms evaluating our approach for 77 different scenarios. We found that the mean absolute percentage error of our runtime predictions for these scenarios was just 3.49% resulting in an average cost error of 6.46 for 1-million FaaS function workloads averaging 150.45 in price.
C3  - 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
DA  - 2020/08//
PY  - 2020
DO  - 10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00111
DP  - IEEE Xplore
SP  - 640
EP  - 649
UR  - https://ieeexplore.ieee.org/document/9251165
Y2  - 2024/01/01/07:32:02
ER  - 

TY  - JOUR
TI  - QoS-Aware Workload Distribution in Hierarchical Edge Clouds: A Reinforcement Learning Approach
AU  - Cho, Chunglae
AU  - Shin, Seungjae
AU  - Jeon, Hongseok
AU  - Yoon, Seunghyun
T2  - IEEE Access
AB  - Recently, edge computing is getting attention as a new computing paradigm that is expected to achieve short-delay and high-throughput task offloading for large scale Internet-of-Things (IoT) applications. In edge computing, workload distribution is one of the most critical issues that largely influences the delay and throughput performance of edge clouds, especially in distributed Function-as-a-Service (FaaS) over networked edge nodes. In this paper, we propose the Resource Allocation Control Engine with Reinforcement learning (RACER), which provides an efficient workload distribution strategy to reduce the task response slowdown with per-task response time Quality-of-Service (QoS). First, we present a novel problem formulation with the per-task QoS constraint derived from the well-known token bucket mechanism. Second, we employ a problem relaxation to reduce the overall computation complexity by compromising just a bit of optimality. Lastly, we take the deep reinforcement learning approach as an alternative solution to the workload distribution problem to cope with the uncertainty and dynamicity of underlying environments. Evaluation results show that RACER achieves a significant improvement in terms of per-task QoS violation ratio, average slowdown, and control efficiency, compared to AREA, a state-of-the-art workload distribution method.
DA  - 2020///
PY  - 2020
DO  - 10.1109/ACCESS.2020.3033421
DP  - IEEE Xplore
VL  - 8
SP  - 193297
EP  - 193313
J2  - IEEE Access
SN  - 2169-3536
ST  - QoS-Aware Workload Distribution in Hierarchical Edge Clouds
UR  - https://ieeexplore.ieee.org/document/9237967
Y2  - 2024/01/01/07:32:36
L1  - https://ieeexplore.ieee.org/ielx7/6287639/8948470/09237967.pdf?tp=&arnumber=9237967&isnumber=8948470&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkyMzc5Njc=
L2  - https://ieeexplore.ieee.org/document/9237967
ER  - 

TY  - CONF
TI  - Rule-Based Resource Matchmaking for Composite Application Deployments across IoT-Fog-Cloud Continuums
AU  - Spillner, Josef
AU  - Gkikopoulos, Panagiotis
AU  - Buzachis, Alina
AU  - Villari, Massimo
T2  - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)
AB  - Where shall my new shiny application run? Hundreds of such questions are asked by software engineers who have many cloud services at their disposition, but increasingly also many other hosting options around managed edge devices and fog spectrums, including for functions and container hosting (FaaS/CaaS). Especially for composite applications prevalent in this field, the combinatorial deployment space is exploding. We claim that a systematic and automated approach is unavoidable in order to scale functional decomposition applications further so that each hosting facility is fully exploited. To support engineers while they transition from cloud-native to continuum-native, we provide a rule-based matchmaker called RBMM that combines several decision factors typically present in software description formats and applies rules to them. Using the MaestroNG orchestrator and OsmoticToolkit, we also contribute an integration of the matchmaker into an actual deployment environment.
C3  - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/UCC48980.2020.00053
DP  - IEEE Xplore
SP  - 336
EP  - 341
UR  - https://ieeexplore.ieee.org/document/9302804
Y2  - 2024/01/01/10:16:04
L1  - https://digitalcollection.zhaw.ch/bitstream/11475/22691/3/2020_Spillner-etal_RBMM.pdf
ER  - 

TY  - CONF
TI  - Scheduling Methods to Reduce Response Latency of Function as a Service
AU  - Zuk, Pawel
AU  - Rzadca, Krzysztof
T2  - 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)
AB  - Function as a Service (FaaS) permits cloud customers to deploy to cloud individual functions, in contrast to complete virtual machines or Linux containers. All major cloud providers offer FaaS products (Amazon Lambda, Google Cloud Functions, Azure Serverless); there are also popular open-source implementations (Apache OpenWhisk) with commercial offerings (Adobe I/O Runtime, IBM Cloud Functions). A new feature of FaaS is function composition: a function may (sequentially) call another function, which, in turn, may call yet another function - forming a chain of invocations. From the perspective of the infrastructure, a composed FaaS is less opaque than a virtual machine or a container. We show that this additional information enables the infrastructure to reduce the response latency. In particular, knowing the sequence of future invocations, the infrastructure can schedule these invocations along with environment preparation. We model resource management in FaaS as a scheduling problem combining (1) sequencing of invocations, (2) deploying execution environments on machines, and (3) allocating invocations to deployed environments. For each aspect, we propose heuristics. We explore their performance by simulation on a range of synthetic workloads. Our results show that if the setup times are long compared to invocation times, algorithms that use information about the composition of functions consistently outperform greedy, myopic algorithms, leading to significant decrease in response latency.
C3  - 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/SBAC-PAD49847.2020.00028
DP  - IEEE Xplore
SP  - 132
EP  - 140
SN  - 2643-3001
UR  - https://ieeexplore.ieee.org/document/9235070
Y2  - 2024/01/01/10:16:19
L1  - https://arxiv.org/pdf/2008.04830
ER  - 

TY  - CONF
TI  - Scalable IoT Solution using Cloud Services – An Automobile Industry Use Case
AU  - Shaw, Ankit Kumar
AU  - Chakraborty, Amit
AU  - Mohapatra, Debaniranjan
AU  - Dutta, Subrata
T2  - 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
AB  - The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.
C3  - 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
DA  - 2020/10//
PY  - 2020
DO  - 10.1109/I-SMAC49090.2020.9243544
DP  - IEEE Xplore
SP  - 326
EP  - 331
UR  - https://ieeexplore.ieee.org/document/9243544
Y2  - 2024/01/01/17:28:15
ER  - 

TY  - JOUR
TI  - SDCF: A Software-Defined Cyber Foraging Framework for Cloudlet Environment
AU  - Nithya, S.
AU  - Sangeetha, M.
AU  - Prethi, K. N. Apinaya
AU  - Sahoo, Kshira Sagar
AU  - Panda, Sanjaya Kumar
AU  - Gandomi, Amir H.
T2  - IEEE Transactions on Network and Service Management
AB  - The cloudlets can be deployed over mobile devices or even fixed state powerful servers that can provide services to its users in physical proximity. Executing workloads on cloudlets involves challenges centering on limited computing resources. Executing Virtual Machine (VM) based workloads for cloudlets does not scale due to the high computational demands of a VM. Another approach is to execute container-based workloads on cloudlets. However, container-based methods suffer from the cold-start problem, making it unfit for mobile edge computing scenarios. In this work, we introduce executing serverless functions on Web-assembly as workloads for both mobile and fixed state cloudlets. To execute the serverless workload on mobile cloudlets, we built a lightweight Web-assembly runtime. The orchestration of workloads and management of cloudlets or serverless runtime is done by introducing software-defined Cyber Foraging (SDCF) framework, which is a hybrid controller including a control plane for local networks and cloudlets. The SDCF framework integrates the management of cloudlets by utilizing the control plane traffic of the underlying network and thus avoids the extra overhead of cloudlet control plane traffic management. We evaluate SDCF using three use cases: (1) Price aware resource allocation (2) Energy aware resource scheduling for mobile cloudlets (3) Mobility pattern aware resource scheduling in mobile cloudlets. Through the virtualization of cloudlet resources, SDCF preserves minimal maintenance property by providing a centralized approach for configuring and management of cloudlets.
DA  - 2020/12//
PY  - 2020
DO  - 10.1109/TNSM.2020.3015657
DP  - IEEE Xplore
VL  - 17
IS  - 4
SP  - 2423
EP  - 2435
J2  - IEEE Transactions on Network and Service Management
SN  - 1932-4537
ST  - SDCF
UR  - https://ieeexplore.ieee.org/document/9163409
Y2  - 2024/01/01/17:29:05
ER  - 

TY  - CONF
TI  - Serverless based control and monitoring for search and rescue robots
AU  - Mejía, Alexander
AU  - Marcillo, Diego
AU  - Guaño, Miguel
AU  - Gualotuña, Tatiana
T2  - 2020 15th Iberian Conference on Information Systems and Technologies (CISTI)
AB  - The number and scale of disasters, being natural or human made, has influenced the creation of search and rescue missions and the use of modern technologies in them. Robots have become an attractive and more common tool in these operations. Even though there still are challenges to overcome with them. These challenges arise predominantly in the way they are controlled and monitored. Challenges like complex and costly infrastructure are the main obstacle to have a widespread use of these robots. This article presents an architecture for the control and monitoring of search and rescue robots based in serverless technologies to reduce infrastructure complexity, cost and setup. A custom methodology was used based on investigation, design, implementation and validation to develop this architecture. Design considerations, usefulness, limitations are analyzed and discussed. Overall effectiveness of the architecture in search and rescue missions was found a future work for improvement has been established.
C3  - 2020 15th Iberian Conference on Information Systems and Technologies (CISTI)
DA  - 2020/06//
PY  - 2020
DO  - 10.23919/CISTI49556.2020.9140444
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2166-0727
UR  - https://ieeexplore.ieee.org/document/9140444
Y2  - 2024/01/01/17:29:24
L2  - https://ieeexplore.ieee.org/document/9140444
KW  - cloud services
KW  - Control and monitoring
KW  - Design considerations
KW  - Firebase
KW  - Information systems
KW  - Information use
KW  - Modern technologies
KW  - Overall effectiveness
KW  - robots
KW  - Robots
KW  - search and rescue
KW  - Search and rescue
KW  - Search and rescue robot
KW  - serverless technologies
ER  - 

TY  - CONF
TI  - Serverless Computing: Behind the Scenes of Major Platforms
AU  - Kelly, Daniel
AU  - Glavin, Frank
AU  - Barrett, Enda
T2  - 2020 IEEE 13th International Conference on Cloud Computing (CLOUD)
AB  - Serverless computing offers an event driven pay-as-you-go framework for application development. A key selling point is the concept of no back-end server management, allowing developers to focus on application functionality. This is achieved through severe abstraction of the underlying architecture the functions run on. We examine the underlying architecture and report on the performance of serverless functions and how they are effected by certain factors such as memory allocation and interference caused by load induced by other users on the platform. Specifically, we focus on the serverless offerings of the four largest platforms; AWS Lambda, Google Cloud Functions, Microsoft Azure Functions and IBM Cloud Functions. In this paper, we observe and contrast between these platforms in their approach to the common issue of “cold starts”, we devise a means to unveil the underlying architecture serverless functions execute on and we investigate the effects of interference from load on the platform over the time span of one month.
C3  - 2020 IEEE 13th International Conference on Cloud Computing (CLOUD)
DA  - 2020/10//
PY  - 2020
DO  - 10.1109/CLOUD49709.2020.00050
DP  - IEEE Xplore
SP  - 304
EP  - 312
SN  - 2159-6190
ST  - Serverless Computing
UR  - https://ieeexplore.ieee.org/abstract/document/9284261
Y2  - 2024/01/01/17:29:42
L1  - https://arxiv.org/pdf/2012.05600
L2  - https://ieeexplore.ieee.org/abstract/document/9284261
ER  - 

TY  - CONF
TI  - Serverless Straggler Mitigation using Error-Correcting Codes
AU  - Gupta, Vipul
AU  - Carrano, Dominic
AU  - Yang, Yaoqing
AU  - Shankar, Vaishaal
AU  - Courtade, Thomas
AU  - Ramchandran, Kannan
T2  - 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)
AB  - Inexpensive cloud services, such as serverless computing, are often vulnerable to straggling nodes that increase the end-to-end latency for distributed computation. We propose and implement simple yet principled approaches for straggler mitigation in serverless systems for matrix multiplication and evaluate them on several common applications from machine learning and high-performance computing. The proposed schemes are inspired by error-correcting codes and employ parallel encoding and decoding over the data stored in the cloud using serverless workers. This creates a fully distributed computing framework without using a master node to conduct encoding or decoding, which removes the computation, communication and storage bottleneck at the master. On the theory side, we establish that our proposed scheme is asymptotically optimal in terms of decoding time and provide a lower bound on the number of stragglers it can tolerate with high probability. Through extensive experiments, we show that our scheme outperforms existing schemes such as speculative execution and other coding theoretic methods by at least 25%.
C3  - 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)
DA  - 2020/11//
PY  - 2020
DO  - 10.1109/ICDCS47774.2020.00019
DP  - IEEE Xplore
SP  - 135
EP  - 145
SN  - 2575-8411
UR  - https://ieeexplore.ieee.org/document/9355638
Y2  - 2024/01/01/17:30:14
ER  - 

TY  - CONF
TI  - Serverless: What it Is, What to Do and What Not to Do
AU  - Nupponen, Jussi
AU  - Taibi, Davide
T2  - 2020 IEEE International Conference on Software Architecture Companion (ICSA-C)
AB  - Serverless, the new buzzword, has been gaining a lot of attention from the developers and industry. Cloud vendors such as AWS and Microsoft have hyped the architecture almost everywhere, from practitioners' conferences to local events, to blog posts. In this work, we introduce serverless functions (also known as Function-as-a-Service or FaaS), together with on bad practices experienced by practitioners, members of the Tampere Serverless Meetup group.
C3  - 2020 IEEE International Conference on Software Architecture Companion (ICSA-C)
DA  - 2020/03//
PY  - 2020
DO  - 10.1109/ICSA-C50368.2020.00016
DP  - IEEE Xplore
SP  - 49
EP  - 50
ST  - Serverless
UR  - https://ieeexplore.ieee.org/document/9095731
Y2  - 2024/01/01/17:30:27
L2  - https://ieeexplore.ieee.org/document/9095731
ER  - 

TY  - CONF
TI  - STOIC: Serverless Teleoperable Hybrid Cloud for Machine Learning Applications on Edge Device
AU  - Zhang, Michael
AU  - Krintz, Chandra
AU  - Wolski, Rich
T2  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
AB  - Serverless computing is a promising new event-driven programming model that was designed by cloud vendors to expedite the development and deployment of scalable web services on cloud computing systems. Using the model, developers write applications that consist of simple, independent, stateless functions that the cloud invokes on-demand (i.e. elastically), in response to system-wide events (data arrival, messages, web requests, etc.). In this work, we present STOIC (Serverless TeleOperable HybrId Cloud), an application scheduling and deployment system that extends the serverless model in two ways. First, it uses the model in a distributed setting and schedules application functions across multiple cloud systems. Second, STOIC supports serverless function execution using hardware acceleration (e.g. GPU resources) when available from the underlying cloud system. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multi-tier (e.g. edge-cloud) deployments. We find that STOIC's combined use of edge and cloud resources is able to outperform using either cloud in isolation for the applications and datasets that we consider.
C3  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
DA  - 2020/03//
PY  - 2020
DO  - 10.1109/PerComWorkshops48775.2020.9156239
DP  - IEEE Xplore
SP  - 1
EP  - 6
ST  - STOIC
UR  - https://ieeexplore.ieee.org/document/9156239
Y2  - 2024/01/01/17:30:50
L2  - https://ieeexplore.ieee.org/document/9156239
ER  - 

TY  - CONF
TI  - The Ifs and Buts of Less is More: A Serverless Computing Reality Check
AU  - Kuhlenkamp, Jörn
AU  - Werner, Sebastian
AU  - Tai, Stefan
T2  - 2020 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Serverless computing defines a pay-as-you-go cloud execution model, where the unit of computation is a function that a cloud provider executes and auto-scales on behalf of a cloud consumer. Serverless suggests not (or less) caring about servers but focusing (more) on business logic expressed in functions. Server'less' may be `more' when getting developer expectations and platform propositions right and when engineering solutions that take specific behavior and constraints of (current) Function-as-a-Service platforms into account. To this end, in this invited paper, we present a summary of findings and lessons learned from a series of research experiments conducted over the past two years. We argue that careful attention must be placed on the promises associated with the serverless model, provide a reality-check for five common assumptions, and suggest ways to mitigate unwanted effects. Our findings focus on application workload distribution and computational processing complexity, the specific auto-scaling mechanisms in place, the behavior and strategies implemented with operational tasks, the constraints and limitations existing when composing functions, and the costs of executing functions.
C3  - 2020 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2020/04//
PY  - 2020
DO  - 10.1109/IC2E48712.2020.00023
DP  - IEEE Xplore
SP  - 154
EP  - 161
ST  - The Ifs and Buts of Less is More
UR  - https://ieeexplore.ieee.org/document/9096486
Y2  - 2024/01/01/17:32:04
ER  - 

TY  - CONF
TI  - tinyFaaS: A Lightweight FaaS Platform for Edge Environments
AU  - Pfandzelter, Tobias
AU  - Bermbach, David
T2  - 2020 IEEE International Conference on Fog Computing (ICFC)
AB  - The Function-as-a-Service (FaaS) model is a great fit for data and event processing in the Internet of Things (IoT). Sending all data to a cloud-based FaaS platform, however, may cause performance and privacy issues. While these issues could be mitigated using edge computing, existing FaaS approaches, designed for the cloud, are too heavyweight to run on small, constrained edge nodes. In this paper, we propose tinyFaaS, a new FaaS system that is specifically designed for edge environments and their unique challenges. Our platform is lightweight enough to run on low-performance single machine edge nodes, provides a CoAP endpoint to support communication with low-power devices, and uses Docker containers to isolate tenants. We evaluate tinyFaaS through a proof-of-concept implementation that we benchmark and compare to state-of-the-art FaaS platforms. For IoT processing scenarios, we find that tinyFaaS outperforms existing systems by at least an order of magnitude.
C3  - 2020 IEEE International Conference on Fog Computing (ICFC)
DA  - 2020/04//
PY  - 2020
DO  - 10.1109/ICFC49376.2020.00011
DP  - IEEE Xplore
SP  - 17
EP  - 24
ST  - tinyFaaS
UR  - https://ieeexplore.ieee.org/document/9103476
Y2  - 2024/01/01/17:32:15
ER  - 

TY  - CONF
TI  - Toward a Function-as-a-Service Framework for Genomic Analysis
AU  - Tricomi, Giuseppe
AU  - Giosa, Domenico
AU  - Merlino, Giovanni
AU  - Romeo, Orazio
AU  - Longo, Francesco
T2  - 2020 IEEE International Conference on Smart Computing (SMARTCOMP)
AB  - Nowadays, the study of nucleic acids (DNA/RNA) has become a digital science thanks to the advent of modern massive parallel sequencing technologies, better known with the acronym NGS standing for next-generation sequencing, and to the availability of a vast amount of genetic data easily accessible from publicly available databases. Due to the quantity and complexity of such data, its processing requires strong computer science knowledge and skills. This background includes topics such as programming and scripting languages, command-line interfaces, low-level data management tools, which are not always part of the toolbox of molecular biologists and geneticists. The need to adapt to entirely new IT tools and workflows slow down even the more experienced researchers, thus dedicated and customizable GUIs would be much more preferable and conducive. In this paper, we tackle this issue by proposing a preliminary architecture for a framework providing the following benefits: i) it supports the post-NGS analysis process definition phase (commonly called pipeline definition) via a graphical dashboard designed with NodeRED; ii) it automatically deploys the workflows on top of a cluster of computational resources, according to the Function-as-a-Service paradigm, i.e., treating each step of the pipeline as a function to be executed within Linux-based containers, pre-configured with all the necessary dependencies; iii) it runs such containers taking care automatically of resource load balancing. Finally, the framework is thought to include human feedback in the loop, thanks to the availability of a smart notification system, allowing the researcher to monitor the workflows and make any decision needed for its continuation.
C3  - 2020 IEEE International Conference on Smart Computing (SMARTCOMP)
DA  - 2020/09//
PY  - 2020
DO  - 10.1109/SMARTCOMP50058.2020.00070
DP  - IEEE Xplore
SP  - 314
EP  - 319
UR  - https://ieeexplore.ieee.org/document/9239679
Y2  - 2024/01/01/17:32:39
ER  - 

TY  - CONF
TI  - Towards Auction-Based Function Placement in Serverless Fog Platforms
AU  - Bermbach, David
AU  - Maghsudi, Setareh
AU  - Hasenburg, Jonathan
AU  - Pfandzelter, Tobias
T2  - 2020 IEEE International Conference on Fog Computing (ICFC)
AB  - The Function-as-a-Service (FaaS) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud.In this position paper, we propose an auction-based approach in which application developers bid on resources. This allows fog nodes to make a local decision about which functions to offload while maximizing revenue. For a first evaluation of our approach, we use simulation.
C3  - 2020 IEEE International Conference on Fog Computing (ICFC)
DA  - 2020/04//
PY  - 2020
DO  - 10.1109/ICFC49376.2020.00012
DP  - IEEE Xplore
SP  - 25
EP  - 31
UR  - https://ieeexplore.ieee.org/document/9103477
Y2  - 2024/01/01/17:32:51
L1  - https://arxiv.org/pdf/1912.06096
ER  - 

TY  - CONF
TI  - λ-NIC: Interactive Serverless Compute on Programmable SmartNICs
AU  - Choi, Sean
AU  - Shahbaz, Muhammad
AU  - Prabhakar, Balaji
AU  - Rosenblum, Mendel
T2  - 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)
AB  - There is a growing interest in serverless compute, a cloud computing model that automates infrastructure resource- allocation and management while billing customers only for the resources they use. Workloads like stream processing benefit from high elasticity and fine-grain pricing of these serverless frameworks. However, so far, limited concurrency and high latency of server CPUs prohibit many interactive workloads (e.g., web servers and database clients) from taking advantage of serverless compute to achieve high performance.In this paper, we argue that server CPUs are ill-suited to run serverless workloads (i.e., lambdas) and present λ-NIC, an open- source framework, that runs interactive workloads directly on a SmartNIC; more specifically an ASIC-based NIC that consists of a dense grid of Network Processing Unit (NPU) cores. λ- NIC leverages SmartNIC's proximity to the network and a vast array of NPU cores to simultaneously run thousands of lambdas on a single NIC with strict tail-latency guarantees. To ease the development and deployment of lambdas, λ-NIC exposes an event-based programming abstraction, Match+Lambda, and a machine model that allows developers to compose and execute lambdas on SmartNICs easily. Our evaluation shows that λ- NIC achieves up to 880x and 736x improvements in workloads' response latency and throughput, respectively, while significantly reducing host CPU and memory usage.
C3  - 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)
DA  - 2020/11//
PY  - 2020
DO  - 10.1109/ICDCS47774.2020.00029
DP  - IEEE Xplore
SP  - 67
EP  - 77
SN  - 2575-8411
ST  - λ-NIC
UR  - https://ieeexplore.ieee.org/document/9355790
Y2  - 2024/01/01/17:33:21
L2  - https://ieeexplore.ieee.org/document/9355790
ER  - 

TY  - CONF
TI  - A Design of Serverless Computing Service for Edge Clouds
AU  - Cho, Jaeeun
AU  - Kim, Younghan
T2  - 2021 International Conference on Information and Communication Technology Convergence (ICTC)
AB  - The method of Serverless Computing at the Edge is drawing attention because of the efficiency of using resources. This paper presents a design of Serverless Computing for Edge Clouds, based on an open-source project. One of the methods of Serverless at the Edge is that centralized architecture with a Serverless Platform in the control plane cluster. However, in a centralized architecture, if the resources or the status of a particular Edge is not good, the control plane cluster must detect anomalies and redeploy the Functions considering the environment of other Edges. It will take time to select and deploy to the appropriate Edge. The proposed architecture introduces cross-monitoring which imports monitoring metrics directly from a nearby Edge. Grouping Edge sites as a Zone and cross-monitoring each other in the Zone. So that, Serverless computing-based Functions can operate quickly when adjacent Edges are in poor condition. As a result, the neighboring Edges of the Zone can back up with each other.
C3  - 2021 International Conference on Information and Communication Technology Convergence (ICTC)
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/ICTC52510.2021.9621162
DP  - IEEE Xplore
SP  - 1889
EP  - 1891
SN  - 2162-1233
UR  - https://ieeexplore.ieee.org/document/9621162
Y2  - 2024/01/01/19:39:58
L2  - https://ieeexplore.ieee.org/document/9621162
ER  - 

TY  - CONF
TI  - A Measurement Study on Serverless Workflow Services
AU  - Wen, Jinfeng
AU  - Liu, Yi
T2  - 2021 IEEE International Conference on Web Services (ICWS)
AB  - Major cloud providers increasingly roll out their serverless workflow services to orchestrate serverless functions, making it possible to construct complex applications effectively. A comprehensive study is necessary to help developers understand the pros and cons, and make better choices among these serverless workflow services. However, the characteristics of these serverless workflow services have not been systematically analyzed. To fill the knowledge gap, we conduct a comprehensive measurement study on four mainstream serverless workflow services, focusing on both features and the performance. First, we review their official documentation and extract their features from six dimensions, including programming model, state management, etc. Then, we compare their performance (i.e., the execution time of functions, execution time of workflows, orchestration overhead time of workflows) under various settings considering activity complexity and data-flow complexity of workflows, as well as function complexity of serverless functions. Our findings and implications could help developers and cloud providers improve their development efficiency and user experience.
C3  - 2021 IEEE International Conference on Web Services (ICWS)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/ICWS53863.2021.00102
DP  - IEEE Xplore
SP  - 741
EP  - 750
UR  - https://ieeexplore.ieee.org/document/9590280
Y2  - 2024/01/01/19:42:07
ER  - 

TY  - CONF
TI  - A Preliminary Evaluation of QUIC for Mobile Serverless Edge Applications
AU  - Cicconetti, Claudio
AU  - Lossi, Leonardo
AU  - Mingozzi, Enzo
AU  - Passarella, Andrea
T2  - 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)
AB  - Deployment of computing infrastructures at the edge of the network will drive a revolution in integrated solutions for smart mobility in the cities of the future, thanks to the promises of reduced latency and outbound traffic. The adoption of serverless computing will help realising this vision since it simplifies management while at the same time providing the application developers with a neat and clean Function-as-a-Service (FaaS) programming model. Today FaaS relies on HTTP over TCP, but QUIC is emerging fast as a replacement because it is more robust to packet losses and it allows connection roaming: both these advantages are especially important for mobile scenarios. In this paper we report the results of a preliminary evaluation of QUIC+HTTP/3 when used instead of TCP+HTTP within a framework for decentralized dispatching of FaaS function invocations, which shows that this direction is promising and deserves to be delved further in the future.
C3  - 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/WoWMoM51794.2021.00050
DP  - IEEE Xplore
SP  - 268
EP  - 273
UR  - https://ieeexplore.ieee.org/document/9469496
Y2  - 2024/01/01/19:42:29
ER  - 

TY  - CONF
TI  - A Randomized Greedy Method for AI Applications Component Placement and Resource Selection in Computing Continua
AU  - Sedghani, Hamta
AU  - Filippini, Federica
AU  - Ardagna, Danilo
T2  - 2021 IEEE International Conference on Joint Cloud Computing (JCC)
AB  - Artificial Intelligence (AI) and Deep Learning (DL) are pervasive today, with applications spanning from personal assistants to healthcare. Nowadays, the accelerated migration towards mobile computing and Internet of Things, where a huge amount of data is generated by widespread end devices, is determining the rise of the edge computing paradigm, where computing resources are distributed among devices with highly heterogeneous capacities. In this fragmented scenario, efficient component placement and resource allocation algorithms are crucial to orchestrate at best the computing continuum resources. In this paper, we propose a tool to effectively address the component placement problem for AI applications at design time. Through a randomized greedy algorithm, it identifies the placement of minimum cost providing performance guarantees across heterogeneous resources including edge devices, cloud GPU-based Virtual Machines and Function as a Service solutions.
C3  - 2021 IEEE International Conference on Joint Cloud Computing (JCC)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/JCC53141.2021.00022
DP  - IEEE Xplore
SP  - 65
EP  - 70
UR  - https://ieeexplore.ieee.org/document/9566164
Y2  - 2024/01/01/19:47:32
L1  - https://re.public.polimi.it/bitstream/11311/1190351/1/SPACE4_AI_JCC_.pdf
ER  - 

TY  - CONF
TI  - A Random Greedy based Design Time Tool for AI Applications Component Placement and Resource Selection in Computing Continua
AU  - Sedghani, Hamta
AU  - Filippini, Federica
AU  - Ardagna, Danilo
T2  - 2021 IEEE International Conference on Edge Computing (EDGE)
AB  - Artificial Intelligence (AI) and Deep Learning (DL) are pervasive today, with applications spanning from personal assistants to healthcare. Nowadays, the accelerated migration towards mobile computing and Internet of Things, where a huge amount of data is generated by widespread end devices, is determining the rise of the edge computing paradigm, where computing resources are distributed among devices with highly heterogeneous capacities. In this fragmented scenario, efficient component placement and resource allocation algorithms are crucial to orchestrate at best the computing continuum resources. In this paper, we propose a tool to effectively address the component placement problem for AI applications at design time. Through a randomized greedy algorithm, our approach identifies the placement of minimum cost providing performance guar-antees across heterogeneous resources including edge devices, cloud GPU-based Virtual Machines and Function as a Service solutions. Finally, we compare the random greedy method with the HyperOpt framework and demonstrate that our proposed approach converges to a near-optimal solution much faster, especially in large scale systems.
C3  - 2021 IEEE International Conference on Edge Computing (EDGE)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/EDGE53862.2021.00014
DP  - IEEE Xplore
SP  - 32
EP  - 40
SN  - 2767-9918
UR  - https://ieeexplore.ieee.org/document/9712094
Y2  - 2024/01/01/19:47:46
L2  - https://ieeexplore.ieee.org/document/9712094
ER  - 

TY  - CONF
TI  - A Reinforcement Learning Approach to Reduce Serverless Function Cold Start Frequency
AU  - Agarwal, Siddharth
AU  - Rodriguez, Maria A.
AU  - Buyya, Rajkumar
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Serverless computing is an event-driven cloud computing architecture for processing requests on-demand, using light weight function containers and a micro-services model. A variety of applications like Internet of Things (IoT) services, edge computing, and stream processing have been introduced to the serverless paradigm. These applications are characterized by their stringent response time requirements, therefore expecting a quick and fault tolerant feedback from the application. The serverless, or Function-as-a-Service (FaaS), paradigm suffers from function ‘cold start’ challenges, where the serverless platform takes time to set up the dependencies, prepare the runtime environment and code for execution before serving the incoming workload. Most of the current works address the problem of cold start by (1) reducing the start-up or preparation time of function containers, or (2) reducing the frequency of function cold starts on the platform. Recent industrial research has identified that factors such as runtime environment, CPU and memory settings, invocation concurrency, and networking requirements, affect the cold start of a function. Therefore, we propose a Reinforcement Learning (Q-Learning) agent setting, to analyze the identified factors such as function CPU utilization, to ascertain the function-invocation patterns and reduce the function cold start frequency by preparing the function instances in advance. The proposed Q-Learning agent interacts with the Kubeless serverless platform by discretizing the environment states, actions and rewards with the use of per-instance CPU utilization, available function instances and success or failure rate of response, respectively. The workload is replicated using the Apache JMeter non-GUI toolkit and our agent is evaluated against the baseline default auto-scale feature of Kubeless. The agent demonstrates the capability of learning the invocation pattern, make informed decisions by preparing the optimal number of function instances over the period of learning, under controlled environment settings.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00097
DP  - IEEE Xplore
SP  - 797
EP  - 803
UR  - https://ieeexplore.ieee.org/document/9499423
Y2  - 2024/01/01/19:48:48
ER  - 

TY  - JOUR
TI  - A TinyML Platform for On-Device Continual Learning With Quantized Latent Replays
AU  - Ravaglia, Leonardo
AU  - Rusci, Manuele
AU  - Nadalini, Davide
AU  - Capotondi, Alessandro
AU  - Conti, Francesco
AU  - Benini, Luca
T2  - IEEE Journal on Emerging and Selected Topics in Circuits and Systems
AB  - In the last few years, research and development on Deep Learning models & techniques for ultra-low-power devices– in a word, TinyML – has mainly focused on a train-then-deploy assumption, with static models that cannot be adapted to newly collected data without cloud-based data collection and fine-tuning. Latent Replay-based Continual Learning (CL) techniques (Pellegrini et al., 2020) enable online, serverless adaptation in principle, but so far they have still been too computation- and memory-hungry for ultra-low-power TinyML devices, which are typically based on microcontrollers. In this work, we introduce a HW/SW platform for end-to-end CL based on a 10-core FP32 -enabled parallel ultra-low-power (PULP) processor. We rethink the baseline Latent Replay CL algorithm, leveraging quantization of the frozen stage of the model and Latent Replays (LRs) to reduce their memory cost with minimal impact on accuracy. In particular, 8-bit compression of the LR memory proves to be almost lossless (−0.26% with 3000LR) compared to the full-precision baseline implementation, but requires 4\times less memory, while 7-bit can also be used with an additional minimal accuracy degradation (up to 5%). We also introduce optimized primitives for forward and backward propagation on the PULP processor, together with data tiling strategies to fully exploit its memory hierarchy, while maximizing efficiency. Our results show that by combining these techniques, continual learning can be achieved in practice using less than 64MB of memory – an amount compatible with embedding in TinyML devices. On an advanced 22nm prototype of our platform, called VEGA, the proposed solution performs on average 65 \times faster than a low-power STM32 L4 microcontroller, being 37\times more energy efficient – enough for a lifetime of 535h when learning a new mini-batch of data once every minute.
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/JETCAS.2021.3121554
DP  - IEEE Xplore
VL  - 11
IS  - 4
SP  - 789
EP  - 802
J2  - IEEE Journal on Emerging and Selected Topics in Circuits and Systems
SN  - 2156-3365
UR  - https://ieeexplore.ieee.org/document/9580920
Y2  - 2024/01/01/19:49:15
L1  - https://arxiv.org/pdf/2110.10486
ER  - 

TY  - CONF
TI  - AI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments
AU  - Schuler, Lucia
AU  - Jamil, Somaya
AU  - Kühl, Niklas
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach to request-based auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00098
DP  - IEEE Xplore
SP  - 804
EP  - 811
ST  - AI-based Resource Allocation
UR  - https://ieeexplore.ieee.org/document/9499535
Y2  - 2024/01/01/19:49:29
L1  - https://arxiv.org/pdf/2005.14410
L2  - https://ieeexplore.ieee.org/document/9499535
ER  - 

TY  - CONF
TI  - AI@EDGE: A Secure and Reusable Artificial Intelligence Platform for Edge Computing
AU  - Riggio, Roberto
AU  - Coronado, Estefanía
AU  - Linder, Neiva
AU  - Jovanka, Adzic
AU  - Mastinu, Gianpiero
AU  - Goratti, Leonardo
AU  - Rosa, Miguel
AU  - Schotten, Hans
AU  - Pistore, Marco
T2  - 2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit)
AB  - Artificial Intelligence (AI) has become a major innovative force and a major pillar in the fourth industrial revolution. This trend has been acknowledged by the European Commission, who has pointed out how high-performance, intelligent, and secure networks are fundamental for the evolution of the multiservice Next Generation Internet (NGI). While great progress has been done in the accuracy and performance of AI-enabled platforms, their integration in autonomous decision-making and critical systems requires end-to-end quality assurance. AI@EDGE addresses these challenges harnessing the concept of “reusable, secure, and trustworthy AI for network automation”. To this end, AI@EDGE targets significant breakthroughs in two fields: (i) general-purpose frameworks for closed-loop network automation capable of supporting flexible and programmable pipelines for the creation, utilization, and adaptation of the secure, reusable, and trustworthy AI/ML models; and (ii) converged connect-compute platform for creating and managing resilient, elastic, and secure end-to-end slices supporting a diverse range of AI-enabled network applications. Cooperative perception for vehicular networks, secure, multi-stakeholder AI for Industrial Internet of Things, aerial infrastructure inspections, and in-flight entertainment are the uses cases targeted by AI@EDGE to maximise its commercial, societal, and environmental impact.
C3  - 2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit)
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/EuCNC/6GSummit51104.2021.9482440
DP  - IEEE Xplore
SP  - 610
EP  - 615
SN  - 2575-4912
ST  - AI@EDGE
UR  - https://ieeexplore.ieee.org/document/9482440
Y2  - 2024/01/01/19:51:38
L1  - https://re.public.polimi.it/bitstream/11311/1207732/1/eucnc2021.pdf
ER  - 

TY  - CONF
TI  - Algorithms for scheduling scientific workflows on serverless architecture
AU  - Majewski, Marcin
AU  - Pawlik, Maciej
AU  - Malawski, Maciej
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Serverless computing is a novel cloud computing paradigm where the cloud provider manages the underlying infrastructure, while users are only required to upload the code of the application. Function as a Service (FaaS) is a serverless computing model where short-lived methods are executed in the cloud. One of the promising use cases for FaaS is running scientific workflow applications, which represent a scientific process composed of related tasks. Due to the distinctive features of FaaS, which include rapid resource provisioning, indirect infrastructure management, and fine-grained billing model a need arises to create dedicated scheduling methods to effectively use the novel infrastructures as an environment for workflow applications. In this paper we propose two novel scheduling algorithms SMOHEFT and SML, which are designed to create a schedule for executing scientific workflows on serverless infrastructures concerning time and cost constraints. We evaluated proposed algorithms by performing experiments, where we planned the execution of three applications: Ellipsoids, Vina and Montage. SDBWS and SDBCS algorithms were used as a baseline. SML achieved the best results when executing Ellipsoids workflow, with a success rate above 80%, while other algorithms were below 60%. In the case of Vina, all the algorithms, except SDBWS, had a success rate above 87.5% and in the case of Montage, the success rate of all algorithms was similar, over 87.5%. The proposed algorithms' success rate is comparable or better than offered by other studied solutions.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00095
DP  - IEEE Xplore
SP  - 782
EP  - 789
UR  - https://ieeexplore.ieee.org/document/9499676
Y2  - 2024/01/01/19:51:53
L2  - https://ieeexplore.ieee.org/document/9499676
ER  - 

TY  - CONF
TI  - An FPGA Implementation of 4×4 Arbiter PUF
AU  - Aknesil, Can
AU  - Dubrova, Elena
T2  - 2021 IEEE 51st International Symposium on Multiple-Valued Logic (ISMVL)
AB  - The need of protecting data and bitstreams increases in computation environments such as FPGA as a Service (FaaS). Physically Unclonable Functions (PUFs) have been proposed as a solution to this problem. In this paper, we present an implementation of Arbiter PUF with 4 × 4 switch blocks in Xilinx Series 7 FPGA, perform its statistical analysis, and compare it to other Arbiter PUF variants. We show that the presented implementation utilizes five times less area than 2 × 2 Arbiter PUF-based implementations. It is suitable for many real-world applications, including identification, authentication, key provisioning, and random number generation.
C3  - 2021 IEEE 51st International Symposium on Multiple-Valued Logic (ISMVL)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/ISMVL51352.2021.00035
DP  - IEEE Xplore
SP  - 160
EP  - 165
SN  - 2378-2226
UR  - https://ieeexplore.ieee.org/document/9459657
Y2  - 2024/01/01/19:52:49
L2  - https://ieeexplore.ieee.org/document/9459657
ER  - 

TY  - CONF
TI  - Architectural Vision of Cloud Computing in the Indian Government
AU  - Choudhari, Nitin Vishnu
AU  - Sasankar, Ashish B.
T2  - 2021 International Conference on Innovative Trends in Information Technology (ICITIIT)
AB  - The GI (Govt. of India) cloud started in 2014 is built on the state of art technologies and rich architecture with the nationwide network infrastructure and Data Centres located across the country on National and State data centres. This paper investigates, study and analyze the cloud architecture of Govt. of India and suggests modifications that need to be adapted for sustainable development as per the global changing scenario and fulfill the future needs with improved service delivery, increased throughput, and increased efficiency to provide secured cloud services and to minimize the gap between the cloud service providers and end-users. The cloud services are designed for centralized storage and processing. The cloud data centers are generally located thousands of miles away from the end-users where the data is actually generated. The physical distance between the cloud infrastructure and the data source at edge level end-users produces latency for the real-time processing of the huge amount of data generated at the source level. In recent years the automation scenario is changing globally with various emerging technologies such as the Internet of Things (IoT), Wireless Fidelity 6 (Wi-Fi 6), Fifth Generation Mobile Network connectivity (5G), Artificial Intelligence (AI), and Machine Learning, etc. Emerging technologies like IoT, Wi-Fi 6, 5G gives large scope for boundary level computing and generates a very huge amount of data at the data source level produced by the end-users. These technologies require agile real-time processing and analysis of the data at the source level. Edge computing and Fog computing are the distributed architectures that work together, for reduced latency and speedy real-time processing where the data is actually generated by the end-user. According to the new implementation demands, various emerging cloud technologies such as Mobile Cloud Apps, Containers, Serverless, Microservices, Development and Information Technology Operations (DevOps), BlockChain, Fog computing, Edge Computing, and Software-Defined Infrastructure (SDI), etc are proposed for implementation.
C3  - 2021 International Conference on Innovative Trends in Information Technology (ICITIIT)
DA  - 2021/02//
PY  - 2021
DO  - 10.1109/ICITIIT51526.2021.9399598
DP  - IEEE Xplore
SP  - 1
EP  - 7
UR  - https://ieeexplore.ieee.org/document/9399598
Y2  - 2024/01/01/19:54:20
L2  - https://ieeexplore.ieee.org/document/9399598
ER  - 

TY  - CONF
TI  - Asset Modeling using Serverless Computing
AU  - Jayaraman, Srideepika
AU  - Reddy, Chandra
AU  - Khabiri, Elham
AU  - Patel, Dhaval
AU  - Bhamidipaty, Anuradha
AU  - Kalagnanam, Jayant
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - Assets in the domain of Internet of Things (IoT) generate time-series data such as sensor readings and alerts. In addition, the assets have associated static data such as the make, model and other manufacturing information. The sensors in the asset components may have implicit relationships with each other, which are not interpretable without domain knowledge. Many problems exist which involve computation of relationships between sensors or subsystems in the asset components. Typically, the number of sensors in a real world asset may range anywhere from tens to thousands of sensors - and in this case, finding relationships between them becomes a highly computationally intensive task. In this paper, we study one such problem of anomaly detection in industrial data based on the functioning of the sensors and their interrelationships in both normal and abnormal conditions. We further demonstrate the issue of run-time and performance complexity in this problem, and present a speed-up strategy using Serverless Computing for parallelization, and demonstrate the usefulness of this method by comparing the speed-up achieved.
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9671711
DP  - IEEE Xplore
SP  - 4084
EP  - 4090
UR  - https://ieeexplore.ieee.org/document/9671711
Y2  - 2024/01/01/19:54:44
ER  - 

TY  - CONF
TI  - Astra: Autonomous Serverless Analytics with Cost-Efficiency and QoS-Awareness
AU  - Jarachanthan, Jananie
AU  - Chen, Li
AU  - Xu, Fei
AU  - Li, Bo
T2  - 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter with the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astra, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astra relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy Astra in the AWS Lambda platform and conduct real-world experiments over three representative benchmarks with different scales. Results demonstrate that Astra can achieve the optimal execution decision for serverless analytics, by improving the performance of 21% to 60% under a given budget constraint, and resulting in a cost reduction of 20% to 80% without violating performance requirement, when compared with three baseline configuration algorithms.
C3  - 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/IPDPS49936.2021.00085
DP  - IEEE Xplore
SP  - 756
EP  - 765
SN  - 1530-2075
ST  - Astra
UR  - https://ieeexplore.ieee.org/document/9460548
Y2  - 2024/01/01/19:54:54
ER  - 

TY  - CONF
TI  - BeFaaS: An Application-Centric Benchmarking Framework for FaaS Platforms
AU  - Grambow, Martin
AU  - Pfandzelter, Tobias
AU  - Burchard, Luk
AU  - Schubert, Carsten
AU  - Zhao, Max
AU  - Bermbach, David
T2  - 2021 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Following the increasing interest and adoption of FaaS systems, benchmarking frameworks for determining nonfunctional properties have also emerged. While existing (microbenchmark) frameworks only evaluate single aspects of FaaS platforms, a more holistic, application-driven approach is still missing. In this paper, we design and present BeFaaS, an extensible application-centric benchmarking framework for FaaS environments that focuses on the evaluation of FaaS platforms through realistic and typical examples of FaaS applications. BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload profiles and new platforms, supports federated benchmark runs in which the benchmark application is distributed over multiple providers, and supports a fine-grained result analysis. Our evaluation compares three major FaaS providers in single cloud provider setups and shows that BeFaaS is capable of running each benchmark automatically with minimal configuration effort and providing detailed insights for each interaction.
C3  - 2021 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/IC2E52221.2021.00014
DP  - IEEE Xplore
SP  - 1
EP  - 8
ST  - BeFaaS
UR  - https://ieeexplore.ieee.org/document/9610428
Y2  - 2024/01/01/19:55:14
L1  - https://arxiv.org/pdf/2102.12770
L2  - https://ieeexplore.ieee.org/document/9610428
ER  - 

TY  - CONF
TI  - Characterizing and Mitigating the I/O Scalability Challenges for Serverless Applications
AU  - Roy, Rohan Basu
AU  - Patel, Tirthak
AU  - Tiwari, Devesh
T2  - 2021 IEEE International Symposium on Workload Characterization (IISWC)
AB  - As serverless computing paradigm becomes widespread, it is important to understand the I/O performance characteristics on serverless computing platforms. To the best of our knowledge, we provide the first study that analyzes the observed I/O performance characteristics - some expected and some unexpected findings that reveal the hidden, complex interactions between the application I/O characteristics, the serverless computing platform, and the storage engines. The goal of this analysis is to provide data-driven guidelines to serverless programmers and system designers about the performance trade-offs and pitfalls of serverless I/O.
C3  - 2021 IEEE International Symposium on Workload Characterization (IISWC)
DA  - 2021/11//
PY  - 2021
DO  - 10.1109/IISWC53511.2021.00018
DP  - IEEE Xplore
SP  - 74
EP  - 86
UR  - https://ieeexplore.ieee.org/document/9668299
Y2  - 2024/01/01/20:07:51
L2  - https://ieeexplore.ieee.org/document/9668299
ER  - 

TY  - CONF
TI  - Cloud Computing: Serverless
AU  - Koschel, Arne
AU  - Klassen, Samuel
AU  - Jdiya, Kerim
AU  - Schaaf, Marc
AU  - Astrova, Irina
T2  - 2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)
AB  - A serverless architecture is a new approach to offering services over the Internet. It combines BaaS (Backend-as-a-service) and FaaS (Function-as-a-service). With the serverless architecture no own or rented infrastructures are needed anymore. In addition, the company does not have to worry about scaling any longer, as this happens automatically and immediately. Furthermore, there is no need any longer for maintenance work on the servers, as this is completely taken over by the provider. Administrators are also no longer needed for the same reason. Finally, many ready-made functions are offered, with which the development effort can be reduced. As a result, the serverless architecture is very well suited to many application scenarios, and it can save considerable costs (server costs, maintenance costs, personnel costs, electricity costs, etc.). The company only must subdivide the source code of the application and upload it to the provider’s server. The rest is done by the provider.
C3  - 2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/IISA52424.2021.9555534
DP  - IEEE Xplore
SP  - 1
EP  - 7
ST  - Cloud Computing
UR  - https://ieeexplore.ieee.org/document/9555534
Y2  - 2024/01/01/20:12:47
L1  - https://serwiss.bib.hs-hannover.de/files/2558/cam-ready_1_svrless-FaaS.pdf
KW  - Cloud computing
KW  - cloud computing
KW  - Costs
KW  - Computer architecture
KW  - Function-as-a-service
KW  - Cloud-computing
KW  - Scalings
KW  - Serverless architecture
KW  - Serverless function
KW  - Architecture
KW  - Service modeling
KW  - Application scenario
KW  - BaaS (Backend-as-a-service)
KW  - Backend-as-a-service
KW  - FaaS (Function-as-a-service)
KW  - Maintenance work
KW  - New approaches
KW  - scaling
KW  - serverless architecture
KW  - serverless functions
KW  - service models
ER  - 

TY  - CONF
TI  - Coding the Computing Continuum: Fluid Function Execution in Heterogeneous Computing Environments
AU  - Kumar, Rohan
AU  - Baughman, Matt
AU  - Chard, Ryan
AU  - Li, Zhuozhao
AU  - Babuji, Yadu
AU  - Foster, Ian
AU  - Chard, Kyle
T2  - 2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
AB  - Advances in network technologies have greatly decreased barriers to accessing physically distributed computers. This newfound accessibility coincides with increasing hardware specialization, creating exciting new opportunities to dispatch workloads to the best resource for a specific purpose, rather than those that are closest or most easily accessible. We present Delta, a service designed to intelligently schedule function-based workloads across a distributed set of heterogeneous computing resources. Delta implements an extensible architecture in which different predictors and scheduling algorithms can be integrated to provide dynamically evolving estimates of function execution times on different resources-estimates that can be used to determine the most appropriate location for execution. We describe predictors for function runtime, data transfer time, and cold-start resource provisioning and configuration delay; dynamic learning methods that update predictor models over time; and scheduling strategies that take into account both function and endpoint information. We show that these methods can halve workload makespan when compared with a strategy that selects the fastest resource, and decrease makespan by a factor of five when compared to a round robin strategy, when deployed on a heterogeneous testbed with resources ranging from a Raspberry Pi to a GPU node in an academic cloud.
C3  - 2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/IPDPSW52791.2021.00018
DP  - IEEE Xplore
SP  - 66
EP  - 75
ST  - Coding the Computing Continuum
UR  - https://ieeexplore.ieee.org/document/9460607
Y2  - 2024/01/01/20:12:57
ER  - 

TY  - CONF
TI  - CoFunc: A unified development framework for heterogeneous FaaS computing platforms
AU  - Li, Bao
AU  - Li, Zhe
AU  - Tan, Yusong
AU  - Yu, Jie
T2  - 2021 International Conference on Communications, Information System and Computer Engineering (CISCE)
AB  - Function as a Service (FaaS) is a new popular cloud computing model in recent years, which has the characteristics of automatic scaling, on-demand billing and easy maintenance. However, the SDKs provided by different public FaaS platforms are inconsistent, increasing the cost of learning and application migration for developers. This paper proposes to build a unified development framework by encapsulating the SDKs of each FaaS platform, which uses the factory pattern of object-oriented design mode to define a set of unified abstract classes. This framework can help developers to operate across different FaaS platforms with a unified set of SDK, and has good scalability.
C3  - 2021 International Conference on Communications, Information System and Computer Engineering (CISCE)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CISCE52179.2021.9445991
DP  - IEEE Xplore
SP  - 726
EP  - 730
ST  - CoFunc
UR  - https://ieeexplore.ieee.org/document/9445991
Y2  - 2024/01/01/20:13:26
L2  - https://ieeexplore.ieee.org/document/9445991
ER  - 

TY  - CONF
TI  - Confidential Serverless Made Efficient with Plug-In Enclaves
AU  - Li, Mingyu
AU  - Xia, Yubin
AU  - Chen, Haibo
T2  - 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)
AB  - Serverless computing has become a fact of life on modern clouds. A serverless function may process sensitive data from clients. Protecting such a function against untrusted clouds using hardware enclave is attractive for user privacy. In this work, we run existing serverless applications in SGX enclave, and observe that the performance degradation can be as high as 5.6× to even 422.6×. Our investigation identifies these slowdowns are related to architectural features, mainly from page-wise enclave initialization. Leveraging insights from our overhead analysis, we revisit SGX hardware design and make minimal modification to its enclave model. We extend SGX with a new primitive—region-wise plugin enclaves that can be mapped into existing enclaves to reuse attested common states amongst functions. By remapping plugin enclaves, an enclave allows in-situ processing to avoid expensive data movement in a function chain. Experiments show that our design reduces the enclave function latency by 94.74-99.57%, and boosts the autoscaling throughput by 19-179×.
C3  - 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/ISCA52012.2021.00032
DP  - IEEE Xplore
SP  - 306
EP  - 318
SN  - 2575-713X
UR  - https://ieeexplore.ieee.org/document/9499793
Y2  - 2024/01/01/20:13:46
L2  - https://ieeexplore.ieee.org/document/9499793
ER  - 

TY  - CONF
TI  - Cross-Platform Performance Evaluation of Stateful Serverless Workflows
AU  - Shahidi, Narges
AU  - Gunasekaran, Jashwant Raj
AU  - Kandemir, Mahmut Taylan
T2  - 2021 IEEE International Symposium on Workload Characterization (IISWC)
AB  - Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and AWS Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that AWS is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, AWS is 89% more expensive than Azure for machine learning training application while Azure is 2× faster than AWS for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms,
C3  - 2021 IEEE International Symposium on Workload Characterization (IISWC)
DA  - 2021/11//
PY  - 2021
DO  - 10.1109/IISWC53511.2021.00017
DP  - IEEE Xplore
SP  - 63
EP  - 73
UR  - https://ieeexplore.ieee.org/document/9668304
Y2  - 2024/01/01/20:15:09
L2  - https://ieeexplore.ieee.org/document/9668304
ER  - 

TY  - CONF
TI  - Customized IoT devices for the architectural education future in connectivity for smart cities
AU  - Reátegui, José L.
AU  - Herrera, Pablo C.
T2  - 2021 2nd Sustainable Cities Latin America Conference (SCLA)
AB  - Architecture, and urbanism, in synergy with electronic engineering, computing, and mechatronics, promote the development of applications and services for smart cities in the context of the Fourth Industrial revolution. We link this knowledge from the implementation of IoT in undergraduate education in the field of architecture, in a model to analyze behavior patterns of users and environment variables, monitoring thousands of daily data, to optimize the efficiency and performance of the problems of buildings, promoting people-centered design. Our academic ecosystem is inspired by the traditional use of the PBL of design studios but powered by emerging technologies selected for this research (IoT, Cloud Computing, Microservices, and Parametric Design), to customize GPRS/GSM devices, according to students needs and design problems. Our validation included four knowledge integration mechanisms and the evaluation of the effectiveness of student work to build a low-cost implementation ecosystem: Socialization (Microcontroller Software and Circuit Design), exchange (Telecommunication software), direction (Backend Serverless software), and internalization (Parametric Design). As a whole, it promotes teamwork, collaboration, interaction, constant feedback, and student adaptability, providing an academic background for the construction of sustainable cities from a Latin American perspective.
C3  - 2021 2nd Sustainable Cities Latin America Conference (SCLA)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/SCLA53004.2021.9540124
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9540124
Y2  - 2024/01/01/20:15:47
L2  - https://ieeexplore.ieee.org/document/9540124
ER  - 

TY  - CONF
TI  - Data Transfer between Cloud Document Management Systems in Serverless Paradigm
AU  - Bhole, Ritika
AU  - Dalvi, Sanket
AU  - Acharya, Manas
AU  - Nirkhe, Sahil
AU  - Rane, Pradnya
AU  - Jain, Sheetal
T2  - 2021 4th International Conference on Computing and Communications Technologies (ICCCT)
AB  - Data and communication are at the heart of business and interactions. With the onset of economic and reliable Document Management System (DMS) providers, there has been a large-scale migration to these cloud-based services. When it comes to Data Transfer we find ourselves in the realm of mailing files, link sharing, FTP and the list goes on, however, all of these methods involve some level of local computing and storage resource consumption and do not support transfers across different DMS providers. This paper proposes a system that aims to facilitate cloud-agnostic data transfers built over a cloud system with Serverless computing paradigm.
C3  - 2021 4th International Conference on Computing and Communications Technologies (ICCCT)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/ICCCT53315.2021.9711817
DP  - IEEE Xplore
SP  - 280
EP  - 284
UR  - https://ieeexplore.ieee.org/document/9711817
Y2  - 2024/01/01/20:16:03
L2  - https://ieeexplore.ieee.org/document/9711817
ER  - 

TY  - CONF
TI  - Data-Intensive Workload Consolidation in Serverless (Lambda/FaaS) Platforms
AU  - HoseinyFarahabady, M.Reza
AU  - Taheri, Javid
AU  - Zomaya, Albert Y.
AU  - Tari, Zahir
T2  - 2021 IEEE 20th International Symposium on Network Computing and Applications (NCA)
AB  - A significant amount of research studies in the past years has been devoted on developing efficient mechanisms to control the level of degradation among consolidate workloads in a shared platform. Workload consolidation is a promising feature that is employed by most service providers to reduce the total operating costs in traditional computing systems [1]–[3]. Serverless paradigm - also known as Function as a Service, FaaS, and Lambda - recently emerged as a new virtualization run-time model that disentangles the traditional state of applications' users from the burden of provisioning physical computing resources, leaving the difficulty of providing the adequate resource capacity on the service provider's side. This paper focuses on a number of challenges associated with workload consolidation when a serverless platform is expected to execute several data-intensive functional units. Each functional unit is considered to be the atomic component that reacts to a stream of input data. A serverless application in the proposed model is composed of a series of functional units. Through a systematic approach, we highlight the main challenges for devising an efficient workload consolidation process in a data-intensive serverless platform. To this end, we first study the performance interference among multiple workloads to obtain the capacity of last level cache (LLC). We show how such contention among workloads can lead to a significant throughput degradation on a single physical server. We expand our investigation into a general case with the aim to prevent the total throughput never falling below a predefined utilization level. Based on the empirical results, we develop a consolidation model and then design a computationally efficient controller to optimize the throughput degradation among a platform consists fs multiple machines. The performance evaluation is conducted using modern workloads inspired by data management services, and data analytic benchmark tools in our in-house four node platform showing the efficiency of the proposed solution to mitigate the QoS violation rate for high priority applications by 90% while can enhance the normalized throughput usage of disk devices by 39 %.
C3  - 2021 IEEE 20th International Symposium on Network Computing and Applications (NCA)
DA  - 2021/11//
PY  - 2021
DO  - 10.1109/NCA53618.2021.9685244
DP  - IEEE Xplore
SP  - 1
EP  - 8
SN  - 2643-7929
UR  - https://ieeexplore.ieee.org/document/9685244
Y2  - 2024/01/01/20:16:22
ER  - 

TY  - CONF
TI  - Deadline-aware Dynamic Resource Management in Serverless Computing Environments
AU  - Mampage, Anupama
AU  - Karunasekera, Shanika
AU  - Buyya, Rajkumar
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Serverless computing enables rapid application development and deployment by composing loosely coupled microservices at a scale. This emerging paradigm greatly unburdens the users of cloud environments, from the need to provision and manage the underlying cloud resources. With this shift in responsibility, the cloud provider faces the challenge of providing acceptable performance to the user without compromising on reliability, while having minimal knowledge of the application requirements. Sub-optimal resource allocations, specifically the CPU resources, could result in the violation of performance requirements of applications. Further, the fine-grained serverless billing model only charges for resource usage in terms of function execution time. At the same time, the provider has to maintain the underlying infrastructure in always-on mode to facilitate asynchronous function calls. Thus, achieving optimum utilization of cloud resources without compromising on application requirements is of high importance to the provider. Most of the current works only focus on minimizing function execution times caused by delays in infrastructure set up and reducing resource costs for the end-user. However, in this paper, we focus on both the provider and user’s perspective and propose a function placement policy and a dynamic resource management policy for applications deployed in serverless computing environments. The policies minimize the resource consumption cost for the service provider while meeting the user’s application requirement, i.e., deadline. The proposed solutions are sensitive to deadline and efficiently increase the resource utilization for the provider, while dynamically managing resources to improve function response times. We implement and evaluate our approach through simulation using ContainerCloudSim toolkit. The proposed function placement policy when compared with baseline scheduling techniques can reduce resource consumption by up to three times. The dynamic resource allocation policy when evaluated with a fixed resource allocation policy and a proportional CPU-shares policy shows improvements of up to 25% in meeting the required function deadlines.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00058
DP  - IEEE Xplore
SP  - 483
EP  - 492
UR  - https://ieeexplore.ieee.org/document/9499407
Y2  - 2024/01/01/20:16:40
L2  - https://ieeexplore.ieee.org/document/9499407
ER  - 

TY  - CONF
TI  - Defuse: A Dependency-Guided Function Scheduler to Mitigate Cold Starts on FaaS Platforms
AU  - Shen, Jiacheng
AU  - Yang, Tianyi
AU  - Su, Yuxin
AU  - Zhou, Yangfan
AU  - Lyu, Michael R.
T2  - 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)
AB  - Function-as-a-Service (FaaS) is becoming a prevalent paradigm in developing cloud applications. With FaaS, clients can develop applications as serverless functions, leaving the burden of resource management to cloud providers. However, FaaS platforms suffer from the performance degradation caused by the cold starts of serverless functions. Cold starts happen when serverless functions are invoked before they have been loaded into the memory. The problem is unavoidable because the memory in datacenters is typically too limited to hold all serverless functions simultaneously. The latency of cold function invocations will greatly degenerate the performance of FaaS platforms. Currently, FaaS platforms employ various scheduling methods to reduce the occurrences of cold starts. However, they do not consider the ubiquitous dependencies between serverless functions. Observing the potential of using dependencies to mitigate cold starts, we propose Defuse, a Dependency-guided Function Scheduler on FaaS platforms. Specifically, Defuse identifies two types of dependencies between serverless functions, i.e., strong dependencies and weak ones. It uses frequent pattern mining and positive point-wise mutual information to mine such dependencies respectively from function invocation histories. In this way, Defuse constructs a function dependency graph. The connected components (i.e., dependent functions) on the graph can be scheduled to diminish the occurrences of cold starts. We evaluate the effectiveness of Defuse by applying it to an industrial serverless dataset. The experimental results show that Defuse can reduce 22% of memory usage while having a 35% decrease in function cold-start rates compared with the state-of-the-art method.
C3  - 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/ICDCS51616.2021.00027
DP  - IEEE Xplore
VL  - 2021-July
SP  - 194
EP  - 204
SN  - 2575-8411
ST  - Defuse
UR  - https://ieeexplore.ieee.org/document/9546470
Y2  - 2024/01/01/20:17:00
L2  - https://ieeexplore.ieee.org/document/9546470
KW  - Cloud Computing
KW  - FaaS
KW  - Serverless
KW  - Function-as-a-service
KW  - Resource management
KW  - Scheduling
KW  - Cloud-computing
KW  - Cloud providers
KW  - Cloud applications
KW  - Cold Start
KW  - Cold-start
KW  - Datacenter
KW  - Performance degradation
KW  - Service dependency
KW  - Service Dependency
ER  - 

TY  - CONF
TI  - Deployment and Management of Time Series Forecasts in Ocean Industry
AU  - O’Donncha, Fearghal
AU  - Akhriev, Albert
AU  - Eck, Bradley
AU  - Burke, Meredith
AU  - Filgueira, Ramon
AU  - Grant, Jon
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - Machine learning has not achieved the same degree of success in environmental applications as in other industries. Challenges around data sparsity, quality, and consistency have limited the impact of deep neural network approaches and restricted the focus to research applications. An alternative approach – that is more amenable to the characteristics of data coming from disparate IoT devices deployed at different times and locations in the ocean – is to develop many lightweight models that can be readily scaled up or down based on the number of devices available at any time. This paper presents a serverless framework that naturally marries a single IoT sensor device with a forecasting model. Aspects related to data ingestion, data processing, model training and deployment are described. The framework is applied to a fish farm site in Atlantic Canada.
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9671877
DP  - IEEE Xplore
SP  - 4091
EP  - 4096
UR  - https://ieeexplore.ieee.org/document/9671877
Y2  - 2024/01/01/21:05:02
L1  - https://zenodo.org/records/5716082/files/2021252329.pdf
ER  - 

TY  - CONF
TI  - Design and Implementation of a New Serverless Conversational Survey System
AU  - Guo, Wenbin
AU  - Zong, Shaoyi
AU  - Chen, Songxi
AU  - Zhao, Fengxiang
AU  - Shang, Yi
T2  - 2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)
AB  - Conversational agents or chatbots have great potentials in improving survey responses and accessibility. However, it is still a challenge for researchers without programming skills to create voice-enabled chatbots to conduct customizable surveys. This paper presents a new easy-to-use serverless survey chatbot system based on the existing TigerAware mobile survey platform, called TigerAware chatbot. This chatbot system enables non-technical persons to build and deploy customized surveys on mobile devices as text or voice-based conversations. The system is based on Dialogflow and Firebase Realtime database, supports voice input and social dialog, and provides visual responses for users to select answers among several options. The chatbots can be deployed on any iOS and Android mobile device and platform that support Google Assistant. Survey question types that have been implemented include yes/no, multiple-choice, free response, numeric entry, date/time picker, and scale. This system is more efficient and effective than existing survey chatbot systems.
C3  - 2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/ICDSCA53499.2021.9650203
DP  - IEEE Xplore
SP  - 358
EP  - 363
UR  - https://ieeexplore.ieee.org/document/9650203
Y2  - 2024/01/01/22:39:36
L2  - https://ieeexplore.ieee.org/document/9650203
ER  - 

TY  - CONF
TI  - Design and Implementation of Device Monitoring SaaS for DIY-IoT Systems
AU  - Nagano, Motoki
AU  - Arai, Yusuke
AU  - Fujihashi, Takuya
AU  - Watanabe, Takashi
AU  - Saruwatari, Shunsuke
T2  - 2021 IEEE International Conference on Consumer Electronics (ICCE)
AB  - We propose a novel SaaS platform, namely, motch to support the operation of the IoT systems developed by the end-users. The proposed motch consists of three modules to monitor the availability of various IoT devices to enable the end-users to easily operate their own IoT system: 1) survival report realization in various IoT devices, 2) Web front-end, and 3) serverless back-end. Performance evaluation shows that the proposed motch SDK and motch daemon can realize availability monitoring for various IoT devices. In addition, the serverless back-end achieves adequate running costs according to the required computational resource for the motch.
C3  - 2021 IEEE International Conference on Consumer Electronics (ICCE)
DA  - 2021/01//
PY  - 2021
DO  - 10.1109/ICCE50685.2021.9427662
DP  - IEEE Xplore
SP  - 1
EP  - 4
SN  - 2158-4001
UR  - https://ieeexplore.ieee.org/document/9427662
Y2  - 2024/01/01/22:40:45
L2  - https://ieeexplore.ieee.org/document/9427662
ER  - 

TY  - CONF
TI  - Design of a Reference Architecture for Serverless IoT Systems
AU  - Shah, Neel Pradip
T2  - 2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)
AB  - The Internet of Things (IoT) proposes the concept of a global and pervasive network of heterogeneous devices that interact with the users and their surroundings improving work efficiency and quality of life. From the conventional software systems development perspective, a good system design and architecture are critical. Its importance increases many-fold when the domain’s objective is to connect an infinite set of heterogeneous devices, i.e., smaller systems should integrate and interoperate with each other with no or low modifications. Therefore, a consensus from a design perspective must be achieved in the community and the corporations working in the IoT domain to achieve this integrability and interoperability. One way to document this consensus is a reference architecture. This paper proposes a reference architecture that will act as a blueprint in developing Serverless IoT systems that integrate and interoperate with each other easily. The motivation behind selecting the serverless computing paradigm for IoT systems is that the system engineers can incorporate more decentralization. Thus, a significant amount of tasks will be handled at the Edge and the Fog layer. Only the heavy functions will be delegated to the Cloud, resulting in more efficient and inexpensive systems. Also, the event-driven programming model at the core of serverless computing, its advantages in scalability, focus on the core business logic, and high cohesion becomes necessary and valuable tools in the toolbox for efficiently developing IoT systems.
C3  - 2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/COINS51742.2021.9524180
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9524180
Y2  - 2024/01/01/22:41:44
L2  - https://ieeexplore.ieee.org/document/9524180
ER  - 

TY  - CONF
TI  - Deviceless: A Serverless Approach for the Internet of Things
AU  - Benomar, Zakaria
AU  - Longo, Francesco
AU  - Merlino, Giovanni
AU  - Puliafito, Antonio
T2  - 2021 ITU Kaleidoscope: Connecting Physical and Virtual Worlds (ITU K)
AB  - Developers of cloud-native applications have rapidly adopted the Serverless/Function-as-a-Service (FaaS) paradigm as it exempts them from provisioning and operating the infrastructure. Within this context, an interesting approach that can foster IoT applications’ development is extending the serverless paradigm towards the network edge to cover IoT environments: a paradigm that we refer to as ‘deviceless’. In this approach, an IoT infrastructure composed of devices deployed at the network edge can seamlessly be integrated as an application execution infrastructure to enable interactions with hosted sensors/actuators. This paper discusses several perspectives available in the literature on the (IoT) edge-based serverless paradigm and related use cases in an IoT/edge computing context. Besides, we present our preliminary prototype for implementing the deviceless approach to show its viability. We exploit the deviceless paradigm to conceive data pipelines under a flow-based development environment leveraging a geographically distributed IoT infrastructure.
C3  - 2021 ITU Kaleidoscope: Connecting Physical and Virtual Worlds (ITU K)
DA  - 2021/12//
PY  - 2021
DO  - 10.23919/ITUK53220.2021.9662096
DP  - IEEE Xplore
SP  - 1
EP  - 8
ST  - Deviceless
UR  - https://ieeexplore.ieee.org/document/9662096
Y2  - 2024/01/01/22:42:13
L2  - https://ieeexplore.ieee.org/document/9662096
KW  - Cloud computing
KW  - Virtualization
KW  - FaaS
KW  - Serverless
KW  - edge computing
KW  - IoT
KW  - Function-as-a-service
KW  - Edge computing
KW  - Internet of things
KW  - virtualization
KW  - serverless
KW  - Application development
KW  - Cloud-computing
KW  - Service paradigm
KW  - Application execution
KW  - deviceless
KW  - Deviceless
KW  - Network edges
KW  - Virtualizations
ER  - 

TY  - JOUR
TI  - EBI-PAI: Toward an Efficient Edge-Based IoT Platform for Artificial Intelligence
AU  - Yang, Shu
AU  - Xu, Kunkun
AU  - Cui, Laizhong
AU  - Ming, Zhongxing
AU  - Chen, Ziteng
AU  - Ming, Zhong
T2  - IEEE Internet of Things Journal
AB  - Edge computing, especially multiaccess edge computing, is seen as a promising technology to improve the Quality of user Experience (QoE) of many artificial intelligence (AI) applications in the evolution toward Internet-of-Things (IoT) infrastructure. However, the management and deployment of massive edge data centers bring new challenges for the current network. In this article, we propose a new edge-based IoT platform for AI (EBI-PAI), based on software-defined network (SDN) and serverless technology. EBI-PAI provides a unified service calling interface and schedules the resources automatically to satisfy the QoE requirements of users. To optimize performances during incremental deployment, we formulate the deployment problem, prove its complexity, and design heuristic algorithms to solve it. We implement EBI-PAI based on an opensource serverless project and deploy it in real networks. To evaluate EBI-PAI, we conduct comprehensive simulations based on the generated and real-world network topology, and real-world base station data set. The simulation results show that EBI-PAI can greatly improve QoE with the same budget and save the budget to achieve similar QoE. We finally carry out a case study with real user demands, and it further validates the simulation results.
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/JIOT.2020.3019008
DP  - IEEE Xplore
VL  - 8
IS  - 12
SP  - 9580
EP  - 9593
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - EBI-PAI
UR  - https://ieeexplore.ieee.org/document/9174943
Y2  - 2024/01/01/22:44:18
L2  - https://ieeexplore.ieee.org/document/9174943
ER  - 

TY  - CONF
TI  - Edge-to-cloud Virtualized Cyberinfrastructure for Near Real-time Water Quality Forecasting in Lakes and Reservoirs
AU  - Daneshmand, Vahid
AU  - Breef-Pilz, Adrienne
AU  - Carey, Cayelan C.
AU  - Jin, Yuqi
AU  - Ku, Yun-Jung
AU  - Subratie, Kensworth C.
AU  - Thomas, R. Quinn
AU  - Figueiredo, Renato J.
T2  - 2021 IEEE 17th International Conference on eScience (eScience)
AB  - The management of drinking water quality is critical to public health and can benefit from techniques and technologies that support near real-time forecasting of lake and reservoir conditions. The cyberinfrastructure (CI) needed to support forecasting has to overcome multiple challenges, which include: 1) deploying sensors at the reservoir requires the CI to extend to the network’s edge and accommodate devices with constrained network and power; 2) different lakes need different sensor modalities, deployments, and calibrations; hence, the CI needs to be flexible and customizable to accommodate various deployments; and 3) the CI requires to be accessible and usable to various stakeholders (water managers, reservoir operators, and researchers) without barriers to entry. This paper describes the CI underlying FLARE (Forecasting Lake And Reservoir Ecosystems), a novel system co-designed in an interdisciplinary manner between CI and domain scientists to address the above challenges. FLARE integrates R packages that implement the core numerical forecasting (including lake process modeling and data assimilation) with containers, overlay virtual networks, object storage, versioned storage, and event-driven Function-as-a-Service (FaaS) serverless execution. It is a flexible forecasting system that can be deployed in different modalities, including the Manual Mode suitable for end-users’ personal computers and the Workflow Mode ideal for cloud deployment. The paper reports on experimental data and lessons learned from the operational deployment of FLARE in a drinking water supply (Falling Creek Reservoir in Vinton, Virginia, USA). Experiments with a FLARE deployment quantify its edge-to-cloud virtual network performance and serverless execution in OpenWhisk deployments on both XSEDE-Jetstream and the IBM Cloud Functions FaaS system.
C3  - 2021 IEEE 17th International Conference on eScience (eScience)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/eScience51609.2021.00024
DP  - IEEE Xplore
SP  - 138
EP  - 148
UR  - https://ieeexplore.ieee.org/document/9582387
Y2  - 2024/01/01/22:45:32
L2  - https://ieeexplore.ieee.org/document/9582387
ER  - 

TY  - JOUR
TI  - Enclavisor: A Hardware-Software Co-Design for Enclaves on Untrusted Cloud
AU  - Gu, Jinyu
AU  - Wu, Xinyue
AU  - Zhu, Bojun
AU  - Xia, Yubin
AU  - Zang, Binyu
AU  - Guan, Haibing
AU  - Chen, Haibo
T2  - IEEE Transactions on Computers
AB  - The releases of Intel SGX and AMD SEV mark the transition of hardware-based enclaves from research prototypes to mainstream products. These two paradigms of secure enclaves are attractive to both the cloud providers and tenants, since security is one of the key pillars of cloud computing. However, it is found that current hardware-defined enclaves are not flexible and efficient enough for the cloud. For example, although SGX can provide strong memory protection with both confidentiality and integrity, the size of secure memory is tightly restricted. On the contrary, SEV enables enclaves to use more memory but has critical security flaws due to no memory integrity protection. Meanwhile, both types of enclaves have relatively long booting latency, which makes them not suitable for short-term tasks like serverless workloads. After an in-depth analysis, we find that there are some intrinsic tradeoffs between security and performance due to the limitation of architectural designs. In this article, we investigate a novel hardware-software co-design of enclaves to meet the requirements of cloud by placing a part of the logic of the enclave mechanism into a lightweight software layer, named Enclavisor, to achieve a balance between security, performance, and flexibility. Specifically, our implementation is based on AMD's SEV and, Enclavisor is placed in the guest kernel mode of SEV's secure virtual machines. Enclavisor inherently supports memory encryption with no memory limitation and also achieves efficient booting, multiple enclave granularities, and post-launch remote attestation. Meanwhile, we also propose hardware/software solutions to mitigate the security flaws caused by the lack of memory integrity. We implement a prototype of Enclavisor on an AMD SEV server. The experiments on both micro-benchmarks and application benchmarks show that enclaves on Enclavisor can have close-to-native performance.
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/TC.2020.3019704
DP  - IEEE Xplore
VL  - 70
IS  - 10
SP  - 1598
EP  - 1611
J2  - IEEE Transactions on Computers
SN  - 1557-9956
ST  - Enclavisor
UR  - https://ieeexplore.ieee.org/document/9178442
Y2  - 2024/01/01/22:46:14
L2  - https://ieeexplore.ieee.org/document/9178442
ER  - 

TY  - CONF
TI  - Evolving the Edge and the Cloud: A hybrid computing paradigm
AU  - Wang, Jin
T2  - 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)
AB  - The edge-to-cloud ecosystem includes cloud management service providers in addition to cloud service providers. The system is also an isolated disorder state, and there are both edge computation and fog computation structures. To realize the effective flow of resources, a hybrid computing paradigm HEC is analyzed with respect to its typical characteristics and applications in various industry scenarios. This paper analyzes the challenges in edge serverless computing, edge big data processing and edge AI, and expounds our views on the technologies adopted, such as offloading, resource allocation, resource provisioning, and deep learning. In the end, it looks forward to the flexible work of HEC integrating resources, which will lead to dynamic, flexible and intelligent changes in computing from edge to cloud.
C3  - 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/QRS-C55045.2021.00108
DP  - IEEE Xplore
SP  - 718
EP  - 721
SN  - 2693-9371
ST  - Evolving the Edge and the Cloud
UR  - https://ieeexplore.ieee.org/document/9741997
Y2  - 2024/01/01/22:46:56
L2  - https://ieeexplore.ieee.org/document/9741997
ER  - 

TY  - CONF
TI  - FaaSter Troubleshooting - Evaluating Distributed Tracing Approaches for Serverless Applications
AU  - Borges, Maria C.
AU  - Werner, Sebastian
AU  - Kilic, Ahmet
T2  - 2021 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Serverless applications can be particularly difficult to troubleshoot, as these applications are often composed of various managed and partly managed services. Faults are often unpredictable and can occur at multiple points, even in simple compositions. Each additional function or service in a serverless composition introduces a new possible fault source and a new layer to obfuscate faults. Currently, serverless platforms offer only limited support for identifying runtime faults. Developers looking to observe their serverless compositions often have to rely on scattered logs and ambiguous error messages to pinpoint root causes. In this paper, we investigate the use of distributed tracing for improving the observability of faults in serverless applications. To this end, we first introduce a model for characterizing fault observability, then provide a prototypical tracing implementation-specifically, a developer-driven and a platform-supported tracing approach. We compare both approaches with our model, measure associated trade-offs (execution latency, resource utilization), and contribute new insights for troubleshooting serverless compositions.
C3  - 2021 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/IC2E52221.2021.00022
DP  - IEEE Xplore
SP  - 83
EP  - 90
UR  - https://ieeexplore.ieee.org/document/9610265
Y2  - 2024/01/01/22:47:05
L1  - https://arxiv.org/pdf/2110.03471
ER  - 

TY  - CONF
TI  - Feature and Performance Based Comparative Study on Serverless Frameworks
AU  - Nasrin, Sabiha
AU  - Sahryer, T.I.M. Fahim
AU  - Al Islam, A. B. M. Alim
AU  - Noor, Jannatun
T2  - 2021 24th International Conference on Computer and Information Technology (ICCIT)
AB  - We conduct experiments on the different public clouds provided in this paper to bring out a comparative study, to help the developers understand the diversity of the platforms. This study will help them choose a suitable platform for their desired application. The contemporary and future usage of this diverse nature of cloud computing is hugely demanding. Thus, we elaborate our study on serverless cloud computing to suffice the demand. Serverless prevents a great deal of unessential consumption of power and is a pay-as-you-go service. This technology has added a great impact on software and application development. Although the major obstacle to this development field is that there is not enough documentation on how the big companies provide this facility and how their architecture is built. The comparative study on this diverse platform is missing in the literature. Therefore, our research is based on the on-demand serverless use cases and comparative study with necessary measures. This can be effective and efficient to use for further serverless implementation. Hence, we and others can follow our research for understanding the technical complexity.
C3  - 2021 24th International Conference on Computer and Information Technology (ICCIT)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/ICCIT54785.2021.9689779
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9689779
Y2  - 2024/01/01/22:47:31
ER  - 

TY  - CONF
TI  - FederatedTree: A Secure Serverless Algorithm for Federated Learning to Reduce Data Leakage
AU  - Gharibi, Mohamed
AU  - Bhagavan, Srini
AU  - Rao, Praveen
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - In Federated Learning there have been many op-timization methods that allow flexible local updating such as FedAvg that has become the de facto mechanism for averaging local stochastic gradient descent without sharing the data. Classic FL methods such as FedAvg struggle with trust and data leakage issues. In FedAvg and similar techniques, clients assume the aggregator server is a trusted but curious server. However, even if the server is trusted, the models still leak a lot of data through the weights. Several techniques have been proposed to reduce data leakage. One mechanism involves sharing pieces of the data with the server, but it violates the key privacy assumption of federated learning. Other solutions such as Federated Learning with Differential Privacy aim to reduce data leakage by adding noise to the weights/gradients. However, there is a trade-off between accuracy and the amount of noise added.In this paper, we propose a practical Federated Learning algorithm of deep neural networks on iterative model averaging we called FederatedTree. While FedAvg with differential privacy adds noise to the weights to provide a level of privacy, our algorithm applies a secure sequential averaging without adding noise to the models. FederatedTree solves the trust issue between client-to-client, client-to-server (if exists) and reduces the amount of data leakage without adding noise that lowers the model accuracy. The results show that the FederatedTree algorithm provides a high privacy rate with higher accuracy on popular datasets: MNIST, Fashion MNIST, CIFAR-10. Furthermore, FederatedTree utilizes a binary tree structure to reduce the sequential averaging time and remove the overhead of the excessive communication between the server and the clients.
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9672039
DP  - IEEE Xplore
SP  - 4078
EP  - 4083
ST  - FederatedTree
UR  - https://ieeexplore.ieee.org/document/9672039
Y2  - 2024/01/01/22:47:43
ER  - 

TY  - CONF
TI  - FedLess: Secure and Scalable Federated Learning Using Serverless Computing
AU  - Grafberger, Andreas
AU  - Chadha, Mohak
AU  - Jindal, Anshul
AU  - Gu, Jianfeng
AU  - Gerndt, Michael
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables clients to learn a shared ML model while keeping the data local. However, conventional FL systems face challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient.
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9672067
DP  - IEEE Xplore
SP  - 164
EP  - 173
ST  - FedLess
UR  - https://ieeexplore.ieee.org/document/9672067
Y2  - 2024/01/01/22:47:52
L1  - https://arxiv.org/pdf/2111.03396
KW  - serverless computing
KW  - Serverless computing
KW  - Function-as-a-service
KW  - Deep learning
KW  - Scalability
KW  - Service provider
KW  - Federated learning
KW  - Central servers
KW  - Data privacy
KW  - deep learning
KW  - federated learning
KW  - Federated learning system
KW  - Function-as-a-service (FaaS)
KW  - Infrastructure managements
KW  - Learning paradigms
KW  - Training data
ER  - 

TY  - CONF
TI  - FedSmarteum: Secure Federated Matrix Factorization Using Smart Contracts for Multi-Cloud Supply Chain
AU  - Bhagavan, Srini
AU  - Gharibi, Mohamed
AU  - Rao, Praveen
T2  - 2021 IEEE International Conference on Big Data (Big Data)
AB  - With increased awareness comes unprecedented expectations. We live in a digital, cloud era wherein the underlying information architectures are expected to be elastic, secure, resilient, and handle petabyte scaling. The expectation of epic proportions from the next generation of the data frameworks is to not only do all of the above but also build it on a foundation of trust and explainability across multi-organization business networks. From cloud providers to automobile industries or even vaccine manufacturers, components are often sourced by a complex, not full digitized thread of disjoint suppliers. Building Machine Learning and AI-based order fulfillment and predictive models, remediating issues, is a challenge for multi-organization supply chain automation. We posit that Federated Learning in conjunction with blockchain and smart contracts are technologies primed to tackle data privacy and centralization challenges. In this paper, motivated by challenges in the industry, we propose a decentralized distributed system in conjunction with a recommendation system model (Matrix Factorization) that is trained using Federated Learning on an Ethereum blockchain network. We leverage smart contracts that allow decentralized serverless aggregation to update local-ized items vectors. Furthermore, we utilize Homomorphic Encryption (HE) to allow sharing the encrypted gradients over the network while maintaining their privacy. Based on our results, we argue that training a model over a serverless Blockchain network using smart contracts will provide the same accuracy as in a centralized model while maintaining our serverless model privacy and reducing the overhead communication to a central server. Finally, we assert such a system that provides transparency, audit-ready and deep insights into supply chain operations for enterprise cloud customers resulting in cost savings and higher Quality of Service (QoS).
C3  - 2021 IEEE International Conference on Big Data (Big Data)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/BigData52589.2021.9671789
DP  - IEEE Xplore
SP  - 4054
EP  - 4063
ST  - FedSmarteum
UR  - https://ieeexplore.ieee.org/document/9671789
Y2  - 2024/01/01/22:48:02
L1  - https://mospace.umsystem.edu/xmlui/bitstream/10355/89572/1/Bhagavan_umkc_0134D_11808.pdf
ER  - 

TY  - CONF
TI  - Gaffer: Cloud Computing based Serverless Orchestration Framework for Unprecedented Workflow
AU  - Roy, Saurav
AU  - Kolanu, Shreekar
AU  - S, Krishnaveni
T2  - 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)
AB  - Modern serverless orchestrations are based on either simple step chaining or function invocations. This raises the challenges of implementing high-performance systems. In the context of this, the paper explores how a system can be structured to scale across cross-platform infrastructures by design. This research work has proposed a framework that can be used under circumstances where heavy computation with data consistent results are demanded from the serverless networks, in which the instances depend upon multiple nested servers and the flow of the data is unprecedented without worrying about concurrency and function invocation limitation resulting in zero throttling of requests on the service. This means that the function that needs to be triggered next will be decided by the present function which can demand multiple arguments to run that are to be provided by some other functions. The algorithm is based on the Gaffer Framework which is designed to take care of resource allocation, maintaining the function(servers) level order dependency also sub-level dependency, and resource integration using event-driven semaphore signals. The framework proposed in this paper demonstrates high coordination-free consistency along with the performance that exceeds the current orchestrations. Gaffer Framework run-time system also takes care of distributed data partitioning and efficient handling of machine failures. This makes it simple for programmers with little or no expertise with serverless and distributed systems to create and maintain large interconnected serverless containers.
C3  - 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/ICIRCA51532.2021.9544528
DP  - IEEE Xplore
SP  - 1054
EP  - 1060
ST  - Gaffer
UR  - https://ieeexplore.ieee.org/document/9544528
Y2  - 2024/01/01/22:48:18
ER  - 

TY  - CONF
TI  - GITCBot: A Novel Approach for the Next Generation of C&C Malware
AU  - Ghasemshirazi, Saeid
AU  - Shirvani, Ghazaleh
T2  - 2021 26th International Computer Conference, Computer Society of Iran (CSICC)
AB  - Online Social Networks (OSNs) attracted millions of users in the world. OSNs made adversaries more passionate to create malware variants to subvert the cyber defence of OSNs. Through various threat vectors, adversaries persuasively lure OSN users into installing malware on their devices at an enormous scale. One of the most horrendous forms of named malware is OSNs' botnets that conceal C&C information using OSNs' accounts of unaware users. In this paper, we present GITC (Ghost In The Cloud), which uses Telegram as a C&C server to communicate with threat actors and access targets' information in an undetectable way. Furthermore, we present our implementation of GITC. We show how GITC uses the encrypted telegram Application Programming Interface (API) to cover up records of the adversary connections to the target, and we discuss why current intrusion detection systems cannot detect GITC. In the end, we run some sets of experiments that confirm the feasibility of GITC.
C3  - 2021 26th International Computer Conference, Computer Society of Iran (CSICC)
DA  - 2021/03//
PY  - 2021
DO  - 10.1109/CSICC52343.2021.9420590
DP  - IEEE Xplore
SP  - 1
EP  - 6
ST  - GITCBot
UR  - https://ieeexplore.ieee.org/document/9420590
Y2  - 2024/01/01/22:48:32
ER  - 

TY  - CONF
TI  - High Performance Serverless Architecture for Deep Learning Workflows
AU  - Chahal, Dheeraj
AU  - Ramesh, Manju
AU  - Ojha, Ravi
AU  - Singhal, Rekha
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Serverless architecture is a rapidly growing paradigm for deploying deep learning applications performing ephemeral computing and serving bursty workloads. Serverless architecture promises automatic scaling and cost efficiency for inferencing deep learning models while minimizing the operational logic. However, serverless computing is stateless with constraints on local resources. Hence, deploying complex deep learning applications containing large size models, frameworks, and libraries is a challenge.In this work, we discuss a methodology and architecture for migrating deep vision algorithms and model based applications to a serverless computing platform. We have tested our methodology using AWS infrastructure (AWS Lambda, Provisioned Concurrency, VPC endpoint, S3 and EFS) to mitigate the challenges in deploying composition of APIs containing large deep learning models and frameworks. We evaluate the performance and cost of our architecture for a real-life enterprise application used for document processing.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00096
DP  - IEEE Xplore
SP  - 790
EP  - 796
UR  - https://ieeexplore.ieee.org/document/9499397
Y2  - 2024/01/01/22:48:43
ER  - 

TY  - JOUR
TI  - Hysteretic Optimality of Container Warming Control in Serverless Computing Systems
AU  - Chiang, Yi-Han
AU  - Zhu, Chao
AU  - Lin, Hai
AU  - Ji, Yusheng
T2  - IEEE Networking Letters
AB  - Keeping containers warm for prompt service responses and reducing warm containers for light-weight system management exhibit a fundamental tradeoff in serverless computing systems. In this letter, we investigate the problem of container warming control for serverless computing, and we formulate it as a Markov decision process (MDP). By observing that the value functions corresponding to the MDP are partially submodular, we show that the derived optimal policy is hysteretic and partially non-decreasing. Our numerical results show that the derived optimal policy exhibits a hysteretic structure, which can be realized via switching-up/-down thresholds in practice.
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/LNET.2021.3086489
DP  - IEEE Xplore
VL  - 3
IS  - 3
SP  - 138
EP  - 141
J2  - IEEE Networking Letters
SN  - 2576-3156
UR  - https://ieeexplore.ieee.org/document/9447030
Y2  - 2024/01/01/22:48:57
L2  - https://ieeexplore.ieee.org/document/9447030
ER  - 

TY  - CONF
TI  - In the Fog: Application Deployment for the Cloud Continuum
AU  - Apostolou, Dimitris
AU  - Verginadis, Yiannis
AU  - Mentzas, Gregoris
T2  - 2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)
AB  - Serverless and the Function-as-a-Service (FaaS) paradigms are seen as two enabling technologies for next-generation computing on the cloud continuum. This article discusses prominent frameworks to deploy and monitor applications that span the cloud continuum. It discusses associated challenges and proposes a novel architecture for a framework that manages intelligently multi-cloud, fog and edge resources in order to cope with the requirements of FaaS-enabled applications and services.
C3  - 2021 12th International Conference on Information, Intelligence, Systems & Applications (IISA)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/IISA52424.2021.9555532
DP  - IEEE Xplore
SP  - 1
EP  - 7
ST  - In the Fog
UR  - https://ieeexplore.ieee.org/document/9555532
Y2  - 2024/01/01/22:49:29
KW  - Serverless
KW  - Fog
KW  - Fog computing
KW  - serverless
KW  - Service paradigm
KW  - Multi-clouds
KW  - Application deployment
KW  - cloud continuum
KW  - Cloud continuum
KW  - Edge resources
KW  - Enabling technologies
KW  - fog computing
KW  - Novel architecture
ER  - 

TY  - CONF
TI  - In-Browser Attendance System using Face Recognition and Serverless Edge Computing
AU  - Yadav, Deepak
AU  - Maniar, Sarthak
AU  - Sukhani, Krish
AU  - Devadkar, Kailas
T2  - 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)
AB  - Because of Covid-19, schools, colleges, and institutions have moved to online learning. The education system has encountered and continues to encounter various challenges in this online format in managing the attendance of the students. The teacher used to call out the students' roll numbers or names when they were in the physical education mode. Nowadays as the world is developing towards a digital era, numerous techniques of collecting attendance such as attendance via biometric technologies like eye recognition, face scanning, voice recognition, fingerprint analysis have earned a lot of fame. Face recognition is the most efficient of these approaches as the face can be captured using a camera and compared using a trained model, but the others are more complex to implement at the user end, and some even need hardware. A lot of research work has been already done related to face recognition using models such as YOLO, MTCNN, FaceNet, HOG, LBPH, C2D-CNN. Models are usually loaded in the backend which causes latency issues and makes the system inefficient to use. Our proposed system aims to perform face recognition within the browser itself with the help of serverless edge computing. For the students, a simple web portal is developed, from which they can navigate to our plugin extension, where the model will capture attendance and dynamically update it in a Google Sheet. Face detection was done with Tiny Face Detector, while face recognition was done with Face Recognition Net. A few more models operate in conjunction with these two, recognizing the student from his or her livestream, checking the student's authenticity using logged in credentials, and updating the attendance in real-time across the browser.
C3  - 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/ICCCNT51525.2021.9580042
DP  - IEEE Xplore
SP  - 01
EP  - 06
UR  - https://ieeexplore.ieee.org/document/9580042
Y2  - 2024/01/01/22:49:39
ER  - 

TY  - CONF
TI  - Infrastructure in Code: Towards Developer-Friendly Cloud Applications
AU  - Tankov, Vladislav
AU  - Valchuk, Dmitriy
AU  - Golubev, Yaroslav
AU  - Bryksin, Timofey
T2  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
AB  - The popularity of cloud technologies has led to the development of a new type of applications that specifically target cloud environments. Such applications require a lot of cloud infrastructure to run, which brought about the Infrastructure as Code approach, where the infrastructure is also coded using a separate language in parallel to the main application. In this paper, we propose a new concept of Infrastructure in Code, where the infrastructure is deduced from the application code itself, without the need for separate specifications. We describe this concept, discuss existing solutions that can be classified as Infrastructure in Code and their limitations, and then present our own framework called Kotless — an extendable cloud-agnostic serverless framework for Kotlin that supports two cloud providers, three DSLs, and two runtimes. Finally, we showcase the usefulness of Kotless by demonstrating its efficiency in migrating an existing application to a serverless environment.
C3  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DA  - 2021/11//
PY  - 2021
DO  - 10.1109/ASE51524.2021.9678943
DP  - IEEE Xplore
SP  - 1166
EP  - 1170
SN  - 2643-1572
ST  - Infrastructure in Code
UR  - https://ieeexplore.ieee.org/document/9678943
Y2  - 2024/01/01/22:49:50
L1  - https://arxiv.org/pdf/2108.07842
L2  - https://ieeexplore.ieee.org/document/9678943
ER  - 

TY  - CONF
TI  - Introduction to Digital Libraries
AU  - Fox, Edward A.
AU  - Chen, Yinlin
T2  - 2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)
AB  - This tutorial is a thorough and deep introduction to the Digital Libraries (DL) field, providing a firm foundation: covering key concepts and terminology, as well as services, systems, technologies, methods, standards, projects, issues, and practices. It introduces and builds upon a firm theoretical foundation (starting with the ‘5S’ set of intuitive aspects: Streams, Structures, Spaces, Scenarios, Societies), giving careful definitions and explanations of all the key parts of a ‘minimal digital library’, and expanding from that basis to cover key DL issues. Illustrations come from a set of case studies, including from multiple current projects, including with the application of natural language processing and machine learning to webpages, tweets, and long documents. Attendees will be exposed to four Morgan and Claypool books that elaborate on 5S. Further, new material will be added on building digital libraries using container and cloud services, on developing a digital library for electronic theses and dissertations, and methods to integrate UX and DL design approaches.
C3  - 2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/JCDL52503.2021.00074
DP  - IEEE Xplore
SP  - 354
EP  - 355
UR  - https://ieeexplore.ieee.org/document/9651898
Y2  - 2024/01/01/22:59:32
ER  - 

TY  - CONF
TI  - Low-latency Serverless Computing: Characterization, Optimization and Outlooking: JCC 2021 Invited Keynote
AU  - Chen, Haibo
T2  - 2021 IEEE International Conference on Joint Cloud Computing (JCC)
AB  - Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Serverless computing promises cost-efficiency and elasticity for high-productive software development. To achieve this, the serverless computing platform must address two challenges: strong isolation between function instances, and low startup latency to ensure user experience. In this talk, I will first present a characterization of state-of-the-art serverless platform and derive several key metrics, which collectly forms a systematic methodology and a benchmark called severlessbench. Then, I will show how severless platform can be optimized for (sub-)millisecond startup latency for both normal and condential severless computing. Finally, I will present an outlook on challenges and opportunities of serverless comptuting, including how to make it secure and efficient for joint cloud computing.
C3  - 2021 IEEE International Conference on Joint Cloud Computing (JCC)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/JCC53141.2021.00008
DP  - IEEE Xplore
SP  - xii
EP  - xii
ST  - Low-latency Serverless Computing
UR  - https://ieeexplore.ieee.org/document/9566166
Y2  - 2024/01/01/22:59:44
ER  - 

TY  - CONF
TI  - OpenDC 2.0: Convenient Modeling and Simulation of Emerging Technologies in Cloud Datacenters
AU  - Mastenbroek, Fabian
AU  - Andreadis, Georgios
AU  - Jounaid, Soufiane
AU  - Lai, Wenchen
AU  - Burley, Jacob
AU  - Bosch, Jaro
AU  - van Eyk, Erwin
AU  - Versluis, Laurens
AU  - van Beek, Vincent
AU  - Iosup, Alexandru
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Cloud datacenters are important for the digital society, serving stakeholders across industry, government, and academia. Simulation is a critical part of exploring datacenter technologies, enabling scalable experimentation with millions of jobs and hundreds of thousands of machines, and what-if analysis in a matter of minutes to hours. Although the community has already developed powerful simulators, emerging technologies and applications in modern datacenters require new approaches. Addressing this requirement, in this work we propose OpenDC, a new platform for datacenter simulation. OpenDC includes novel models for emerging cloud-datacenter technologies and applications, such as serverless computing with FaaS deployment and TensorFlow-based machine learning. Our design also focuses on convenience, with a web-based interface for interactive experimentation, support for experiment automation, a library of prefabs for constructing and sharing datacenter designs, and support for diverse input formats and output metrics. We implement, validate, and open-source OpenDC 2.0, a significant redesign and release after a multi-year research and development process. We demonstrate the benefits of OpenDC for the field through a set of representative use-cases: serverless, machine learning, procurement of HPC-as-a-Service infrastructure, educational practices, and reproducibility studies. Overall, OpenDC helps understand how datacenters work, design datacenter infrastructure, and train the next generation of experts.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00055
DP  - IEEE Xplore
SP  - 455
EP  - 464
ST  - OpenDC 2.0
UR  - https://ieeexplore.ieee.org/abstract/document/9499454
Y2  - 2024/01/01/23:00:02
L1  - https://research.vu.nl/files/239054370/OpenDC_2.0.pdf
L2  - https://ieeexplore.ieee.org/abstract/document/9499454
ER  - 

TY  - JOUR
TI  - Operating Latency Sensitive Applications on Public Serverless Edge Cloud Platforms
AU  - Pelle, István
AU  - Czentye, János
AU  - Dóka, János
AU  - Kern, András
AU  - Gerő, Balázs P.
AU  - Sonkoly, Balázs
T2  - IEEE Internet of Things Journal
AB  - Cloud native programming and serverless architectures provide a novel way of software development and operation. A new generation of applications can be realized with features never seen before while the burden on developers and operators will be reduced significantly. However, latency sensitive applications, such as various distributed IoT services, generally do not fit in well with the new concepts and today's platforms. In this article, we adapt the cloud native approach and related operating techniques for latency sensitive IoT applications operated on public serverless platforms. We argue that solely adding cloud resources to the edge is not enough and other mechanisms and operation layers are required to achieve the desired level of quality. Our contribution is threefold. First, we propose a novel system on top of a public serverless edge cloud platform, which can dynamically optimize and deploy the microservice-based software layout based on live performance measurements. We add two control loops and the corresponding mechanisms which are responsible for the online reoptimization at different timescales. The first one addresses the steady-state operation, while the second one provides fast latency control by directly reconfiguring the serverless runtime environments. Second, we apply our general concepts to one of today's most widely used and versatile public cloud platforms, namely, Amazon's AWS, and its edge extension for IoT applications, called Greengrass. Third, we characterize the main operation phases and evaluate the overall performance of the system. We analyze the performance characteristics of the two control loops and investigate different implementation options.
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/JIOT.2020.3042428
DP  - IEEE Xplore
VL  - 8
IS  - 10
SP  - 7954
EP  - 7972
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
UR  - https://ieeexplore.ieee.org/document/9279315
Y2  - 2024/01/01/23:00:54
L1  - https://ieeexplore.ieee.org/ielx7/6488907/9425408/09279315.pdf?tp=&arnumber=9279315&isnumber=9425408&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkyNzkzMTU=
L2  - https://ieeexplore.ieee.org/document/9279315
ER  - 

TY  - CONF
TI  - Optimizing and Extending Serverless Platforms: A Survey
AU  - Nazari, Maziyar
AU  - Goodarzy, Sepideh
AU  - Keller, Eric
AU  - Rozner, Eric
AU  - Mishra, Shivakant
T2  - 2021 Eighth International Conference on Software Defined Systems (SDS)
AB  - Serverless Computing is a new cloud computing paradigm wherein people in academia and industry are actively proposing either interesting improvements or building excellent applications on top of it. AWS, Google Cloud, Microsoft Azure, and IBM are popular samples of public clouds that offer Function-as-a-Service on top of their Serverless Computing platforms. Although this paradigm has had numerous advantages for software developers and programmers, it has introduced new challenges to cloud providers. Factors like fine-grained pricing and pay-as-you-go manner, eliminating the responsibility of resource management on the developer side, promises of elasticity and highly-available service, fault tolerance, auto-scaling, and being able to run embarrassingly parallel jobs make it a suitable platform for developers. On the other hand, efficient resource management, offering low-latency service, and providing proper security/isolation have been partly the main challenges introduced on the cloud provider side. This paper presents a literature review on today's Serverless platform optimizations and extensions that people have proposed and implemented to further capitalize the Serverless infras-tructure. In the end, we will provide the current Serverless paradigm's limitations and a few future directions and research opportunities regarding Serverless Computing.
C3  - 2021 Eighth International Conference on Software Defined Systems (SDS)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/SDS54264.2021.9732138
DP  - IEEE Xplore
SP  - 1
EP  - 8
ST  - Optimizing and Extending Serverless Platforms
UR  - https://ieeexplore.ieee.org/document/9732138
Y2  - 2024/01/01/23:02:02
L2  - https://ieeexplore.ieee.org/document/9732138
ER  - 

TY  - CONF
TI  - Optimizing Cloud Function Configuration via Local Simulations
AU  - Manner, Johannes
AU  - Endreβ, Martin
AU  - Böhm, Sebastian
AU  - Wirtz, Guido
T2  - 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)
AB  - Function as a Service (FaaS) - the reason why so many practitioners and researchers talk about Serverless Computing - claims to hide all operational concerns. The promise when using FaaS is that users only have to focus on the core business functionality in form of cloud functions. However, a few configuration options remain within the developer's responsibility. Most of the currently available cloud function offerings force the user to choose a memory or other resource setting and a timeout value. CPU is scaled based on the chosen options. At a first glance, this seems like an easy task, but the tradeoff between performance and cost has implications on the quality of service of a cloud function. Therefore, in this paper we present a local simulation approach for cloud functions and support developers in choosing a suitable configuration. The methodology we propose simulates the execution behavior of cloud functions locally, makes the cloud and local environment comparable and maps the local profiling data to a cloud platform. This reduces time during the development and enables developers to work with their familiar tools. This is especially helpful when implementing multi-threaded cloud functions.
C3  - 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/CLOUD53861.2021.00030
DP  - IEEE Xplore
SP  - 168
EP  - 178
SN  - 2159-6190
UR  - https://ieeexplore.ieee.org/document/9582177
Y2  - 2024/01/01/23:02:26
ER  - 

TY  - CONF
TI  - Performance Analysis of Serverless Execution Environments
AU  - Elsakhawy, Mohamed
AU  - Bauer, Michael
T2  - 2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)
AB  - Serverless computing has introduced a novel Cloud delivery model that abstracts the infrastructure, platform, and execution layers, allowing users to focus on code development. With such abstraction, users lose insight into the factors that affect their serverless functions' execution performance. In this paper, we investigate the factors affecting serverless functions' execution performance. We examine the impact of container choices, language interpreter versions, and compilation options on a function's execution performance and execution consistency. The results of our investigation illustrate that seemingly trivial decisions can result in significant performance degradations for serverless functions' executions
C3  - 2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)
DA  - 2021/06//
PY  - 2021
DO  - 10.1109/ICECCE52056.2021.9514081
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9514081
Y2  - 2024/01/01/23:17:48
ER  - 

TY  - CONF
TI  - Poster: Function Delivery Network: Extending Serverless to Heterogeneous Computing
AU  - Jindal, Anshul
AU  - Chadha, Mohak
AU  - Gerndt, Michael
AU  - Frielinghaus, Julian
AU  - Podolskiy, Vladimir
AU  - Chen, Pengfei
T2  - 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)
AB  - Several of today's cloud applications are spread over heterogeneous connected computing resources and are highly dynamic in their structure and resource requirements. However, serverless computing and Function-as-a-Service (FaaS) platforms are limited to homogeneous clusters and homogeneous functions. We introduce an extension of FaaS to heterogeneous computing and to support heterogeneous functions through a network of distributed heterogeneous target platforms called Function Delivery Network (FDN). A target platform is a combination of a cluster of a homogeneous computing system and a FaaS platform on top of it. FDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function invocations to the right target platform. We showcase the opportunities such as collaborative execution between multiple target platforms and varied target platform's characteristics that the FDN offers in fulfilling two objectives: Service Level Objective (SLO) requirements and energy efficiency when scheduling functions invocations by evaluating over five distributed target platforms.
C3  - 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/ICDCS51616.2021.00120
DP  - IEEE Xplore
SP  - 1128
EP  - 1129
SN  - 2575-8411
ST  - Poster
UR  - https://ieeexplore.ieee.org/document/9546403
Y2  - 2024/01/01/23:17:58
ER  - 

TY  - CONF
TI  - Quality Assurance of Micro-Services - When to trust your micro-service test results?
AU  - Vassiliou-Gioles, Theofanis
T2  - 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)
AB  - Micro-service architecture has become a standard software architecture style, with loosely coupled, specified, and implemented services owned by small teams and independently deployable. In particular, with the emergence of managed services, deployment aspects have to be addressed explicitly. While tools and frameworks support micro-service developers in developing and unit-testing their services, less attention has been given to higher testing levels, particularly to the integration testing phase. This paper identifies aspects that limit the expressiveness and therefore the trust of integration testing and test results in the context of managed micros-services and function as a service. We propose the introduction of instance identification to overcome these limitations and illustrate how instance identification can be used to enhance integration testing's expressiveness and trust into integration test results.
C3  - 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)
DA  - 2021/12//
PY  - 2021
DO  - 10.1109/QRS-C55045.2021.00024
DP  - IEEE Xplore
SP  - 01
EP  - 06
SN  - 2693-9371
UR  - https://ieeexplore.ieee.org/document/9742182
Y2  - 2024/01/01/23:18:10
ER  - 

TY  - CONF
TI  - Real-time task scheduling in a FaaS cloud
AU  - Szalay, Márk
AU  - Mátray, Péter
AU  - Toka, László
T2  - 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)
AB  - Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgrad-ability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multinode systems make real-time scheduling significantly complex since guaranteeing real-time task execution is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm for a partitioned scheduling system suitable for real-time FaaS platforms of multi-node clusters. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.
C3  - 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/CLOUD53861.2021.00065
DP  - IEEE Xplore
SP  - 497
EP  - 507
SN  - 2159-6190
UR  - https://ieeexplore.ieee.org/document/9582186
Y2  - 2024/01/01/23:18:22
L2  - https://ieeexplore.ieee.org/document/9582186
ER  - 

TY  - CONF
TI  - SAT Solving in the Serverless Cloud
AU  - Ozdemir, Alex
AU  - Wu, Haoze
AU  - Barrett, Clark
T2  - 2021 Formal Methods in Computer Aided Design (FMCAD)
AB  - In recent years, cloud service providers have sold computation in increasingly granular units. Most recently, “serverless” executors run a single executable with restricted network access and for a limited time. The benefit of these restrictions is scale: thousand-way parallelism can be allocated in seconds, and CPU time is billed with sub-second granularity. To exploit these executors, we introduce gg-SAT: an implementation of divide-and-conquer SAT solving. Infrastructurally, gg-SAT departs substantially from previous implementations: rather than handling process or server management itself, gg-SAT builds on the gg framework, allowing computations to be executed on a configurable backend, including serverless offerings such as AWS Lambda. Our experiments suggest that when run on the same hardware, gg-SAT performs competitively with other D&C solvers, and that the 1000-way parallelism it offers (through AWS Lambda) is useful for some challenging SAT instances.
C3  - 2021 Formal Methods in Computer Aided Design (FMCAD)
DA  - 2021/10//
PY  - 2021
DO  - 10.34727/2021/isbn.978-3-85448-046-4_33
DP  - IEEE Xplore
SP  - 241
EP  - 245
SN  - 2708-7824
UR  - https://ieeexplore.ieee.org/document/9617671
Y2  - 2024/01/01/23:18:41
L1  - https://repositum.tuwien.at/bitstream/20.500.12708/18627/1/Ozdemir-2021-SAT%20Solving%20in%20the%20Serverless%20Cloud-vor.pdf
L2  - https://ieeexplore.ieee.org/document/9617671
ER  - 

TY  - CONF
TI  - Serverless and Deviceless Dew Computing: Founding an Infrastructureless Computing
AU  - Gusev, Marjan
T2  - 2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)
AB  - Cloud computing provides computing resources on a subscription basis, targeting infrastructure (hardware), platform (hardware plus system software), and software. Although post-cloud computing models bring the computing closer to the user, they still use the same principles to provide computing resources to the requestor. While building applications, designers face challenges to specify optimal computing resources. Serverless computing is the answer from cloud providers to take care about the availability of computing resources to relieve programmers suggesting to concentrate on programming functions to be executed as a service. Deviceless approach goes even further allowing functions to be executed on nearby devices instead of servers. In this sense, we define infrastructureless computing as the architectural approach where the programming is isolated from specifying the infrastructure requirements, as a general platform of serverless and deviceless computing. Moreover, the generalization of this approach initiates a new computing model where the functions are executed on a lower architectural layer instead of the higher one. For example, an edge server or device, can activate smart devices on the dew computing level and distribute computing to devices (embedded systems) on a lower architectural level. An example may be activating smartphones to perform computing tasks while being charged overnight, or using smart devices installed in cars, while parked in a parking lot. This concept enhances the dew computing architectural model making it a sophisticated platform for future architectural models.
C3  - 2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)
DA  - 2021/07//
PY  - 2021
DO  - 10.1109/COMPSAC51774.2021.00273
DP  - IEEE Xplore
SP  - 1814
EP  - 1818
SN  - 0730-3157
ST  - Serverless and Deviceless Dew Computing
UR  - https://ieeexplore.ieee.org/document/9529887
Y2  - 2024/01/01/23:19:04
ER  - 

TY  - CONF
TI  - Serverless Architecture for Service Robot Management System
AU  - Nishimiya, Kenji
AU  - Imai, Yuta
T2  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
AB  - We have developed service robot management system to facilitate effective collaboration between multiple units and types of robots in operation. This system is implemented by serverless architecture on cloud and using cellular based IoT communication. So it has not only usual cloud system advantage that it is not necessary to prepare dedicated server and network equipment, but it reduces management efforts of servers. We have tested the system with robots in a public facility, and successfully confirmed its performance and functionality.
C3  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/ICRA48506.2021.9561824
DP  - IEEE Xplore
SP  - 11379
EP  - 11385
SN  - 2577-087X
UR  - https://ieeexplore.ieee.org/document/9561824
Y2  - 2024/01/01/23:19:14
L1  - https://arxiv.org/pdf/2108.04531
L2  - https://ieeexplore.ieee.org/document/9561824
ER  - 

TY  - CONF
TI  - Serverless Computing and the Emergence of Function-as-a-Service
AU  - Patil, Rishabh
AU  - Chaudhery, Tanveesh Singh
AU  - Qureshi, Muhammad Ali
AU  - Sawant, Vinaya
AU  - Dalvi, Harshal
T2  - 2021 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)
AB  - Serverless computing is one of the most recent additions to the long list of services provided by cloud computing. Improving upon the attributes of scalability, affordability, and granularity present in earlier services offered by cloud computing, serverless computing is the next major development. Function as a Service has further modularized applications and enabled the individual execution of functions using triggers which has led to reduced costs, improved scaling of applications and almost no configuration expenses. Several cloud computing providers have developed their serverless computing services, each having its advantages and disadvantages. This paper aims to describe the need for serverless computing, its working, its economics, applications, its pros and cons in the current state, and the developments over existing technologies. Various serverless computing providers and their services have also been compared and studied.
C3  - 2021 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/RTEICT52294.2021.9573962
DP  - IEEE Xplore
SP  - 764
EP  - 769
UR  - https://ieeexplore.ieee.org/document/9573962
Y2  - 2024/01/01/23:19:54
L2  - https://ieeexplore.ieee.org/document/9573962
ER  - 

TY  - CONF
TI  - Serverless Multi-Query Motion Planning for Fog Robotics
AU  - Anand, Raghav
AU  - Ichnowski, Jeffrey
AU  - Wu, Chenggang
AU  - Hellerstein, Joseph M.
AU  - Gonzalez, Joseph E.
AU  - Goldberg, Ken
T2  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
AB  - Robots in semi-structured environments such as homes and warehouses sporadically require computation of high-dimensional motion plans. Cloud and fog-based parallelization of motion planning can speed up planning. This can be further made efficient by the use of "serverless" on-demand computing as opposed to always-on high end computers. This paper explores parallelizing the computation of a sampling-based multi-query motion planner based on asymptotically-optimal Probabilistic Road Maps (PRM*) using the simultaneous execution of 100s of cloud-based serverless functions. We propose an algorithm to overcome the communication and bandwidth limitations of serverless computing and use different work-sharing techniques to further optimize the cost and run time. Additionally, we provide proofs of probabilistic completeness and asymptotic optimality. In experiments on synthetic benchmarks and on a physical Fetch robot performing a sequence of decluttering motions, we observe up to a 50x speedup relative to a 4 core edge computer with only a marginally higher cost.
C3  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/ICRA48506.2021.9561571
DP  - IEEE Xplore
SP  - 7457
EP  - 7463
SN  - 2577-087X
UR  - https://ieeexplore.ieee.org/document/9561571
Y2  - 2024/01/01/23:20:29
L2  - https://ieeexplore.ieee.org/document/9561571
ER  - 

TY  - CONF
TI  - Tackling Cold Start of Serverless Applications by Efficient and Adaptive Container Runtime Reusing
AU  - Suo, Kun
AU  - Son, Junggab
AU  - Cheng, Dazhao
AU  - Chen, Wei
AU  - Baidya, Sabur
T2  - 2021 IEEE International Conference on Cluster Computing (CLUSTER)
AB  - During the past few years, serverless computing has changed the paradigm of application development and deployment in the cloud and edge due to its unique advantages, including easy administration, automatic scaling, built-in fault tolerance, etc. Nevertheless, serverless computing is also facing challenges such as long latency due to the cold start. In this paper, we present an in-depth performance analysis of cold start in the serverless framework and propose HotC, a container-based runtime management framework that leverages the lightweight containers to mitigate the cold start and improve the network performance of serverless applications. HotC maintains a live container runtime pool, analyzes the user input or configuration file, and provides available runtime for immediate reuse. To precisely predict the request and efficiently manage the hot containers, we design an adaptive live container control algorithm combining the exponential smoothing model and Markov chain method. Our evaluation results show that HotC introduces negligible overhead and can efficiently improve the performance of various applications with different network traffic patterns in both cloud servers and edge devices.
C3  - 2021 IEEE International Conference on Cluster Computing (CLUSTER)
DA  - 2021/09//
PY  - 2021
DO  - 10.1109/Cluster48925.2021.00018
DP  - IEEE Xplore
SP  - 433
EP  - 443
SN  - 2168-9253
UR  - https://ieeexplore.ieee.org/document/9556063
Y2  - 2024/01/01/23:20:59
ER  - 

TY  - JOUR
TI  - TaScaaS: A Multi-Tenant Serverless Task Scheduler and Load Balancer as a Service
AU  - Giménez-Alventosa, Vicent
AU  - Moltó, Germán
AU  - Segrelles, J. Damian
T2  - IEEE Access
AB  - A combination of distributed multi-tenant infrastructures, such as public Clouds and on-premises installations belonging to different organisations, are frequently used for scientific research because of the high computational requirements involved. Although resource sharing maximises their usage, it typically causes undesirable effects such as the noisy neighbour, producing unpredictable variations of the infrastructure computing capabilities. These fluctuations affect execution efficiency, even of loosely coupled applications, such as many Monte Carlo based simulation programs. This highlights the need of a service capable to handle workload distribution across multiple infrastructures to mitigate these unpredictable performance fluctuations. With this aim, this work introduces TaScaaS, a highly scalable and completely serverless service deployed on AWS to distribute loosely coupled jobs among several computing infrastructures, and load balance them using a completely asynchronous approach to cope with the performance fluctuations with minimum impact in the execution time. We demonstrate how TaScaaS is not only capable of handling these fluctuations efficiently, achieving reduction in execution times up to 45% in our experiments, but also split the jobs to be computed to meet the user-defined execution time.
DA  - 2021///
PY  - 2021
DO  - 10.1109/ACCESS.2021.3109972
DP  - IEEE Xplore
VL  - 9
SP  - 125215
EP  - 125228
J2  - IEEE Access
SN  - 2169-3536
ST  - TaScaaS
UR  - https://ieeexplore.ieee.org/document/9528423
Y2  - 2024/01/01/23:21:16
L1  - https://ieeexplore.ieee.org/ielx7/6287639/9312710/09528423.pdf?tp=&arnumber=9528423&isnumber=9312710&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzk1Mjg0MjM=
L2  - https://ieeexplore.ieee.org/document/9528423
ER  - 

TY  - CONF
TI  - Towards a Proactive Lightweight Serverless Edge Cloud for Internet-of-Things Applications
AU  - Wang, Ian-Chin
AU  - Qi, Shixiong
AU  - Liri, Elizabeth
AU  - Ramakrishnan, K. K.
T2  - 2021 IEEE International Conference on Networking, Architecture and Storage (NAS)
AB  - Edge cloud solutions that bring the cloud closer to the sensors can be very useful to meet the low latency requirements of many Internet-of-Things (IoT) applications. However, IoT traffic can also be intermittent, so running applications constantly can be wasteful. Therefore, having a serverless edge cloud that is responsive and provides low-latency features is a very attractive option for a resource and cost-efficient IoT application environment.In this paper, we discuss the key components needed to support IoT traffic in the serverless edge cloud and identify the critical challenges that make it difficult to directly use existing serverless solutions such as Knative, for IoT applications. These include overhead from heavyweight components for managing the overall system and software adaptors for communication protocol translation used in off-the-shelf serverless platforms that are designed for large-scale centralized clouds. The latency imposed by ‘cold start’ is a further deterrent.To address these challenges we redesign several components of the Knative serverless framework. We use a streamlined protocol adaptor to leverage the MQTT IoT protocol in our serverless framework for IoT event processing. We also create a novel, event-driven proxy based on the extended Berkeley Packet Filter (eBPF), to replace the regular heavyweight Knative queue proxy. Our preliminary experimental results show that the event-driven proxy is a suitable replacement for the queue proxy in an IoT serverless environment and results in lower CPU usage and a higher request throughput.
C3  - 2021 IEEE International Conference on Networking, Architecture and Storage (NAS)
DA  - 2021/10//
PY  - 2021
DO  - 10.1109/NAS51552.2021.9605384
DP  - IEEE Xplore
SP  - 1
EP  - 4
UR  - https://ieeexplore.ieee.org/document/9605384
Y2  - 2024/01/01/23:21:39
ER  - 

TY  - CONF
TI  - Towards a Serverless Java Runtime
AU  - Zhang, Yifei
AU  - Gu, Tianxiao
AU  - Zheng, Xiaolin
AU  - Yu, Lei
AU  - Kuai, Wei
AU  - Li, Sanhong
T2  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
AB  - Java virtual machine (JVM) has the well-known slow startup and warmup issues. This is because the JVM needs to dynamically create many runtime data before reaching peak performance, including class metadata, method profile data, and just-in-time (JIT) compiled native code, for each run of even the same application. Many techniques are then proposed to reuse and share these runtime data across different runs. For example, Class Data Sharing (CDS) and Ahead-of-time (AOT) compilation aim to save and share class metadata and compiled native code, respectively. Unfortunately, these techniques are developed independently and cannot leverage the ability of each other well. This paper presents an approach that systematically reuses JVM runtime data to accelerate application startup and warmup. We first propose and implement JWarmup, a technique that can record and reuse JIT compilation data (e.g., compiled methods and their profile data). Then, we feed JIT compilation data to the AOT compiler to perform profile-guided optimization (PGO). We also integrate existing CDS and AOT techniques to further optimize application startup. Evaluation on real-world applications shows that our approach can bring a 41.35% improvement to the application startup. Moreover, our approach can trigger JIT compilation in advance and reduce CPU load at peak time.
C3  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DA  - 2021/11//
PY  - 2021
DO  - 10.1109/ASE51524.2021.9678709
DP  - IEEE Xplore
SP  - 1156
EP  - 1160
SN  - 2643-1572
UR  - https://ieeexplore.ieee.org/document/9678709
Y2  - 2024/01/01/23:21:53
L2  - https://ieeexplore.ieee.org/document/9678709
ER  - 

TY  - CONF
TI  - Understanding, Predicting and Scheduling Serverless Workloads under Partial Interference
AU  - Zhao, Laiping
AU  - Yang, Yanan
AU  - Li, Yiming
AU  - Zhou, Xian
AU  - Li, Keqiu
T2  - SC21: International Conference for High Performance Computing, Networking, Storage and Analysis
AB  - Interference among distributed cloud applications can be classified into three types: full, partial and zero. While prior research merely focused on full interference, the partial interference that occurs at parts of applications is far more common yet still lacks in-depth study. Serverless computing that structures applications into small-sized, short-lived functions further exacerbate partial interference. We characterize the features of partial interference in serverless as exhibiting high volatility, spatial-temporal variation, and propagation. Given these observations, we propose an incremental learning predictor, named Gsight, which can achieve high precision by harnessing the spatial-temporal overlap codes and profiles of functions via an end-to-end call path. Experimental results show that Gsight can achieve an average error of 1.71%. Its convergence speed is at least 3x faster than that in a serverful system. A scheduling case study shows that the proposed method can improve function density by ≥18.79% while guaranteeing the quality of service (QoS).
C3  - SC21: International Conference for High Performance Computing, Networking, Storage and Analysis
DA  - 2021/11//
PY  - 2021
DO  - 10.1145/3458817.3476215
DP  - IEEE Xplore
SP  - 1
EP  - 14
SN  - 2167-4337
UR  - https://ieeexplore.ieee.org/document/9910093
Y2  - 2024/01/01/23:22:53
ER  - 

TY  - CONF
TI  - Unmanned Aerial Vehicle (UAV) in Precision Agriculture: Business Information Technology Towards Farming as a Service
AU  - Yaqot, Mohammed
AU  - Menezes, Brenno C.
T2  - 2021 1st International Conference on Emerging Smart Technologies and Applications (eSmarTA)
AB  - Humanity has facing emerging global issues as new virus diseases, extremes in weather conditions, increasing climatic changes, depletion of the environment and natural resources, sharply rising demand for food, to just name a few. Therefore, the agriculture industry has been challenged in its processes and products, resulting in a surge of application of novel technologies and practices to maintain itself sustainable. Despite that, this industry is still responsible for 37% of the worldwide workforce, consumes 34% of the global arable land, utilizes 70% of the total water, and emits up to 30% of greenhouse gases (GHG). Progressively widespread in the sector, smart farming is a high-tech, efficient, and sustainable approach achieved by applying integrated technologies within the agricultural value chain processes. It includes increasing feed and food production and decreasing of their waste, prediction of diseases, better estimation of product yields ahead of time, determination of the best harvest time, monitoring of plants-growth cycles, etc. The results are going to yield a sustainable use of soil and water resources while maintaining the green landscape and biodiversity of nature. Emerging remote-sensing technologies and artificial intelligence applications have become essential tools to address these challenges. Drones, also known as Unmanned aerial vehicles (UAVs) are among the most promising industry 4.0 (I4) applications for the next generation of agriculture. This paper is a forehead into applications of drones from the innovation economy's standpoint as a viable tool and an effective manpower replacement in the agro-industry. In such a field, artificial intelligence (AI) has the potential to be the engine for automation of processes to be integrated into cyber-physical systems and enhanced modeling towards improved agriculture, more efficiently than the previous stages of the applications of technologies in this sector. Agricultural communities and businesses must take a strategic approach for continuous improvement production processes by implementing quicker, safer, and cheaper plans through data analytics and farming as a service (FaaS).
C3  - 2021 1st International Conference on Emerging Smart Technologies and Applications (eSmarTA)
DA  - 2021/08//
PY  - 2021
DO  - 10.1109/eSmarTA52612.2021.9515736
DP  - IEEE Xplore
SP  - 1
EP  - 7
ST  - Unmanned Aerial Vehicle (UAV) in Precision Agriculture
UR  - https://ieeexplore.ieee.org/document/9515736
Y2  - 2024/01/01/23:23:08
L2  - https://ieeexplore.ieee.org/document/9515736
ER  - 

TY  - CONF
TI  - Virtual Device Model extending NGSI-LD for FaaS at the Edge
AU  - Martella, Francesco
AU  - Parrino, Giovanni
AU  - Ciulla, Giuseppe
AU  - Bernardo, Roberto Di
AU  - Celesti, Antonio
AU  - Fazio, Maria
AU  - Villari, Massimo
T2  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Smart environments are composed of an ever-increasing number of heterogeneous resources and devices for collecting and processing of a large amount of context data. These activities can be performed at the edge of the smart area, over a distributed and heterogeneous infrastructure, so to be close to the end-user and optimize response time. However, it is hard to define a data model able to support data exchange among different systems and between systems and users. This paper presents the key features of a smart environment and introduces the concept of virtual device, that is an abstracted component characterized by specific high-level functionalities. Then, the paper proposes a data model useful to represent and optimize the adoption of virtual device in smart environments. To better explain the data model features and benefits, we refer to a video surveillance use case, where a smart camera is able to provide the solid angle detection as a service.
C3  - 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2021/05//
PY  - 2021
DO  - 10.1109/CCGrid51090.2021.00079
DP  - IEEE Xplore
SP  - 660
EP  - 667
UR  - https://ieeexplore.ieee.org/abstract/document/9499631
Y2  - 2024/01/01/23:23:34
L1  - https://zenodo.org/record/8406863/files/Virtual%20Device%20Model%20extending%20NGSI-LD%20for%20FaaS%20at%20the%20Edge%20%28002%29.pdf
L2  - https://ieeexplore.ieee.org/abstract/document/9499631
ER  - 

TY  - CONF
TI  - A Microservice Store for Efficient Edge Offloading
AU  - Gedeon, Julien
AU  - Wagner, Martin
AU  - Heuschkel, Jens
AU  - Wang, Lin
AU  - Muhlhauser, Max
T2  - 2019 IEEE Global Communications Conference (GLOBECOM)
AB  - Current edge computing frameworks require tight coupling between mobile clients and surrogates, i.e., the offloaded code has been preconfigured with its required execution environment. In many cases, this includes prior transfers of code blocks or execution environments from mobile devices to the offloading infrastructure. This approach incurs additional latency and is detrimental for the energy consumption of the mobile devices. In this paper, we propose the concept of a microservice store. Using the microservice abstraction common in software development and following the serverless paradigm, we envision a repository through which said services are made accessible to developers and can be re-used across applications. We implement a proof-of-concept edge computing system based on a microservice repository and demonstrate its benefits with real-world applications on mobile devices. Our results show that we were able to reduce latencies by up to 14x and save up to 94% of battery life.
C3  - 2019 IEEE Global Communications Conference (GLOBECOM)
DA  - 2019/12//
PY  - 2019
DO  - 10.1109/GLOBECOM38437.2019.9014114
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9014114
Y2  - 2024/01/01/23:30:06
L2  - https://ieeexplore.ieee.org/document/9014114
ER  - 

TY  - JOUR
TI  - A Lightweight and Secure Data Collection Serverless Protocol Demonstrated in an Active RFIDs Scenario
AU  - Cherif, Amina
AU  - Belkadi, Malika
AU  - Sauveron, Damien
T2  - ACM Transactions on Embedded ComputingComputing Systems
AB  - In the growinggrowing Internet of ThingsThings context, thousands of computingcomputing devices with variousvarious functionalities areare producingproducing data (from environmental sensors or other sourcessources). However, they areare also collectingcollecting, storingstoring, processingprocessing and transmittingtransmitting data to eventually communicatecommunicate them securelysecurely to third partiesparties (e.g., owners of devices or cloud data storagestorage). The deployeddeployed devices areare oftenoften battery-poweredbattery-powered mobilemobile or static nodesnodes equippedequipped with sensors and/or actuators, and they communicatecommunicate usingusing wirelesswireless technologiestechnologies. ExamplesExamples includeinclude unmannedunmanned aerialaerial vehiclesvehicles, wirelesswireless sensor nodesnodes, smart beacons, and wearablewearable healthhealth objects. Such resource-constrainedresource-constrained devices includeinclude ActiveActive Radio Frequency IDentification (RFID) nodesnodes, and thesethese areare usedused to illustrateillustrate our proposal. In most scenariosscenarios, thesethese nodesnodes areare unattended in an adverseadverse environment, so data confidentiality must be ensuredensured from the sensingsensing phasephase throughthrough to delivery to authorizedauthorized entitiesentities: in other words, data must be securelysecurely storedstored and transmitted to prevent attack by activeactive adversariesadversaries even if the nodesnodes areare capturedcaptured. However, duedue to the scarcescarce resourcesresources availableavailable to nodesnodes in terms of energy, storagestorage, and/or computation, the proposedproposed security solution has to be lightweightlightweight. In this articlearticle, we proposepropose a serverless protocol to enableenable MobileMobile Data Collectors (MDCs), such as dronesdrones, to securelysecurely collect data from mobilemobile and static ActiveActive RFID nodesnodes and then deliver them later to an authorizedauthorized third party. The wholewhole solution ensuresensures data confidentiality at each step (from the sensingsensing phasephase, beforebefore data collection by the MDC, onceonce data havehave been collected by MDC, and duringduring final delivery), whilewhile fulfillingfulfilling the lightweightlightweight requirementsrequirements for the resource-limitedresource-limited entitiesentities involvedinvolved. To assess the suitabilitysuitability of the protocol againstagainst the performanceperformance requirementsrequirements, it was implemented on the most resource-constrainedresource-constrained devices to get the worst possiblepossible results. In addition, to proveprove the protocol fulfills the security requirementsrequirements, it was analyzedanalyzed usingusing security gamesgames and also formally verifiedverified usingusing the AVISPA and ProVerif tools.
DA  - 2019/04/02/
PY  - 2019
DO  - 10.1145/3274667
DP  - ACM Digital Library
VL  - 18
IS  - 3
SP  - 27:1
EP  - 27:27
J2  - ACM Trans. Embed. Comput. Syst.
SN  - 1539-9087
UR  - https://doi.org/10.1145/3274667
Y2  - 2024/01/03/04:55:42
KW  - active RFID nodes
KW  - Data collection protocol
KW  - data confidentiality
KW  - lightweight cryptography
KW  - serverless protocol
KW  - activeactive RFID nodesnodes
KW  - lightweightlightweight cryptography
ER  - 

TY  - JOUR
TI  - Cloud computing and running code on Google cloud
AU  - Chun, Wesley
T2  - Journal of Computing Sciences in Colleges
AB  - Cloud computing skills are critical for student career readiness, and more cloud should be integrated into the curriculum. This lecture+hands-on workshop shows you how! The lecture begins with a vendor-agnostic tour of cloud computing to ensure common vocabulary and pedagogy focus. An overview of Google Cloud Platform (GCP) and G Suite developer tools follows. Products covered can be applied to many courses. Lecture also covers Google education grants (in qualifying countries) which include both teaching and research grants. In the hands-on component, attendees learn how to use our REST APIs and run code on our serverless platforms via hands-on tutorials
DA  - 2019/10/01/
PY  - 2019
DP  - ACM Digital Library
VL  - 35
IS  - 3
SP  - 19
EP  - 20
J2  - J. Comput. Sci. Coll.
SN  - 1937-4771
UR  - https://dl.acm.org/doi/10.5555/3381569.3381571
ER  - 

TY  - JOUR
TI  - Cloud-native database systems at Alibaba: opportunities and challenges
AU  - Li, Feifei
T2  - Proceedings of the VLDB Endowment
AB  - Cloud-native databases become increasingly important for the era of cloud computing, due to the needs for elasticity and on-demand usage by various applications. These challenges from cloud applications present new opportunities for cloud-native databases that cannot be fully addressed by traditional on-premise enterprise database systems. A cloud-native database leverages software-hardware co-design to explore accelerations offered by new hardware such as RDMA, NVM, kernel bypassing protocols such as DPDK. Meanwhile, new design architectures, such as shared storage, enable a cloud-native database to decouple computation from storage and provide excellent elasticity. For highly concurrent workloads that require horizontal scalability, a cloud-native database can leverage a shared-nothing layer to provide distributed query and transaction processing. Applications also require cloud-native databases to offer high availability through distributed consensus protocols. At Alibaba, we have explored a suite of technologies to design cloud-native database systems. Our storage engine, X-Engine and PolarFS, improves both write and read throughputs by using a LSM-tree design and self-adapted separation of hot and cold data records. Based on these efforts, we have designed and implemented POLARDB and its distributed version POLARDB-X, which has successfully supported the extreme transaction workloads during the 2018 Global Shopping Festival on November 11, 2018, and achieved commercial success on Alibaba Cloud. We have also designed an OLAP system called AnalyticDB (ADB in short) for enabling real-time interactive data analytics for big data. We have explored a self-driving database platform to achieve autoscaling and intelligent database management. We will report key technologies and lessons learned to highlight the technical challenges and opportunities for cloud-native database systems at Alibaba.
DA  - 2019/08/01/
PY  - 2019
DO  - 10.14778/3352063.3352141
DP  - ACM Digital Library
VL  - 12
IS  - 12
SP  - 2263
EP  - 2272
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - Cloud-native database systems at Alibaba
UR  - https://doi.org/10.14778/3352063.3352141
Y2  - 2024/01/03/04:57:40
ER  - 

TY  - JOUR
TI  - Formal foundations of serverless computing
AU  - Jangda, Abhinav
AU  - Pinckney, Donald
AU  - Brun, Yuriy
AU  - Guha, Arjun
T2  - Proceedings of the ACM on Programming Languages
AB  - Serverless computing (also known as functions as a service) is a new cloud computing abstraction that makes it easier to write robust, large-scale web services. In serverless computing, programmers write what are called serverless functions, which are programs that respond to external events. When demand for the serverless function spikes, the platform automatically allocates additional hardware and manages load-balancing; when demand falls, the platform silently deallocates idle resources; and when the platform detects a failure, it transparently retries affected requests. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major cloud computing platforms. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting λλ, an operational semantics of the essence of serverless computing. Despite being a small (half a page) core calculus, λλ models all the low-level details that serverless functions can observe. To show that λλ is useful, we present three applications. First, to ease reasoning about code, we present a simplified naive semantics of serverless execution and precisely characterize when the naive semantics and λλ coincide. Second, we augment λλ with a key-value store to allow reasoning about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend λλ with a composition language and show that our implementation can outperform prior work.
DA  - 2019/10/10/
PY  - 2019
DO  - 10.1145/3360575
DP  - ACM Digital Library
VL  - 3
IS  - OOPSLA
SP  - 149:1
EP  - 149:26
J2  - Proc. ACM Program. Lang.
UR  - https://dl.acm.org/doi/10.1145/3360575
Y2  - 2024/01/03/05:02:03
L1  - https://dl.acm.org/doi/pdf/10.1145/3360575
L1  - https://arxiv.org/pdf/1902.05870
KW  - serverless computing
KW  - distributed computing
KW  - formal language semantics
ER  - 

TY  - JOUR
TI  - Initialize once, start fast: application initialization at build time
AU  - Wimmer, Christian
AU  - Stancu, Codrut
AU  - Hofer, Peter
AU  - Jovanovic, Vojin
AU  - Wögerer, Paul
AU  - Kessler, Peter B.
AU  - Pliss, Oleg
AU  - Würthinger, Thomas
T2  - Proceedings of the ACM on Programming Languages
AB  - Arbitrary program extension at run time in language-based VMs, e.g., Java's dynamic class loading, comes at a startup cost: high memory footprint and slow warmup. Cloud computing amplifies the startup overhead. Microservices and serverless cloud functions lead to small, self-contained applications that are started often. Slow startup and high memory footprint directly affect the cloud hosting costs, and slow startup can also break service-level agreements. Many applications are limited to a prescribed set of pre-tested classes, i.e., use a closed-world assumption at deployment time. For such Java applications, GraalVM Native Image offers fast startup and stable performance. GraalVM Native Image uses a novel iterative application of points-to analysis and heap snapshotting, followed by ahead-of-time compilation with an optimizing compiler. Initialization code can run at build time, i.e., executables can be tailored to a particular application configuration. Execution at run time starts with a pre-populated heap, leveraging copy-on-write memory sharing. We show that this approach improves the startup performance by up to two orders of magnitude compared to the Java HotSpot VM, while preserving peak performance. This allows Java applications to have a better startup performance than Go applications and the V8 JavaScript VM.
DA  - 2019/10/10/
PY  - 2019
DO  - 10.1145/3360610
DP  - ACM Digital Library
VL  - 3
IS  - OOPSLA
SP  - 184:1
EP  - 184:29
J2  - Proc. ACM Program. Lang.
ST  - Initialize once, start fast
UR  - https://dl.acm.org/doi/10.1145/3360610
Y2  - 2024/01/03/05:04:39
L1  - https://dl.acm.org/doi/pdf/10.1145/3360610
KW  - ahead-of-time compilation
KW  - compiler
KW  - Graal
KW  - GraalVM
KW  - Java
KW  - optimization
KW  - virtual machine
ER  - 

TY  - JOUR
TI  - Jam Today, Jam Tomorrow: Learning in Online Game Jams
AU  - Faas, Travis
AU  - Liu, I-ching
AU  - Dombrowski, Lynn
AU  - Miller, Andrew D.
T2  - Proceedings of the ACM on Human-Computer Interaction
AB  - Game jams, which are game creation events in which developers design and build a game over a short period of time, have been shown to support participatory, active STEM learning. Game jams have expanded from their origins as physically co-located experiences and many are now conducted exclusively online. Though the co-located game jam has been noted as educational, little is known about how learning happens within online game jams. To better understand the ways online game jams support self-development, we interviewed fifteen online jam participants about their learning experiences during their jams. Additionally, we observed and analyzed several jams using activity theory. We found that online game jams support participants' learning through extensive feedback from others during and after the jam. Such feedback sessions were a key social and participatory learning elements for online jams, providing much-need social support. In contrast to the group-focused development in offline jams many participants in our study chose to develop games alone. Many individuals participated in online game jams regularly, treating them as a broader experience than singular events. We use these findings to discuss how we might better design for self-directed learning online and offer suggestions on how to better attune online game jams to the needs of participants.
DA  - 2019/12/05/
PY  - 2019
DO  - 10.1145/3361121
DP  - ACM Digital Library
VL  - 3
IS  - GROUP
SP  - 240:1
EP  - 240:27
J2  - Proc. ACM Hum.-Comput. Interact.
ST  - Jam Today, Jam Tomorrow
UR  - https://doi.org/10.1145/3361121
Y2  - 2024/01/03/05:05:15
KW  - connectivism
KW  - critique
KW  - feedback
KW  - game jam
KW  - indie
KW  - self-directed learning
ER  - 

TY  - JOUR
TI  - Stateful functions as a service in action
AU  - Akhter, Adil
AU  - Fragkoulis, Marios
AU  - Katsifodimos, Asterios
T2  - Proceedings of the VLDB Endowment
AB  - In the serverless model, users upload application code to a cloud platform and the cloud provider undertakes the deployment, execution and scaling of the application, relieving users from all operational aspects. Although very popular, current serverless offerings offer poor support for the management of local application state, the main reason being that managing state and keeping it consistent at large scale is very challenging. As a result, the serverless model is inadequate for executing stateful, latency-sensitive applications. In this paper we present a high-level programming model for developing stateful functions and deploying them in the cloud. Our programming model allows functions to retain state as well as call other functions. In order to deploy stateful functions in a cloud infrastructure, we translate functions and their data exchanges into a stateful dataflow graph. With this paper we aim at demonstrating that using a modified version of an open-source dataflow engine as a runtime for stateful functions, we can deploy scalable and stateful services in the cloud with surprisingly low latency and high throughput.
DA  - 2019/08/01/
PY  - 2019
DO  - 10.14778/3352063.3352092
DP  - ACM Digital Library
VL  - 12
IS  - 12
SP  - 1890
EP  - 1893
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3352063.3352092
Y2  - 2024/01/03/05:06:17
ER  - 

TY  - JOUR
TI  - Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems
AU  - Tsigkanos, Christos
AU  - Garriga, Martin
AU  - Baresi, Luciano
AU  - Ghezzi, Carlo
T2  - ACM Transactions on Internet Technology
AB  - Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.
DA  - 2020/05/03/
PY  - 2020
DO  - 10.1145/3381452
DP  - ACM Digital Library
VL  - 20
IS  - 2
SP  - 17:1
EP  - 17:23
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
UR  - https://doi.org/10.1145/3381452
Y2  - 2024/01/03/05:07:40
L1  - https://arxiv.org/pdf/2004.11428
ER  - 

TY  - JOUR
TI  - Cloudburst: stateful functions-as-a-service
AU  - Sreekanti, Vikram
AU  - Wu, Chenggang
AU  - Lin, Xiayue Charles
AU  - Schleier-Smith, Johann
AU  - Gonzalez, Joseph E.
AU  - Hellerstein, Joseph M.
AU  - Tumanov, Alexey
T2  - Proceedings of the VLDB Endowment
AB  - Function-as-a-Service (FaaS) platforms and "serverless" cloud computing are becoming increasingly popular due to ease-of-use and operational simplicity. Current FaaS offerings are targeted at stateless functions that do minimal I/O and communication. We argue that the benefits of serverless computing can be extended to a broader range of applications and algorithms while maintaining the key benefits of existing FaaS offerings. We present the design and implementation of Cloudburst, a stateful FaaS platform that provides familiar Python programming with low-latency mutable state and communication, while maintaining the autoscaling benefits of serverless computing. Cloudburst accomplishes this by leveraging Anna, an autoscaling key-value store, for state sharing and overlay routing combined with mutable caches co-located with function executors for data locality. Performant cache consistency emerges as a key challenge in this architecture. To this end, Cloudburst provides a combination of lattice-encapsulated state and new definitions and protocols for distributed session consistency. Empirical results on benchmarks and diverse applications show that Cloudburst makes stateful functions practical, reducing the state-management overheads of current FaaS platforms by orders of magnitude while also improving the state of the art in serverless consistency.
DA  - 2020/07/01/
PY  - 2020
DO  - 10.14778/3407790.3407836
DP  - ACM Digital Library
VL  - 13
IS  - 12
SP  - 2438
EP  - 2452
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - Cloudburst
UR  - https://doi.org/10.14778/3407790.3407836
Y2  - 2024/01/03/05:08:01
L1  - https://arxiv.org/pdf/2001.04592
ER  - 

TY  - JOUR
TI  - From Virtual Strangers to IRL Friends: Relationship Development in Livestreaming Communities on Twitch
AU  - Sheng, Jeff T.
AU  - Kairam, Sanjay R.
T2  - Proceedings of the ACM on Human-Computer Interaction
AB  - Accounts of the social experience within livestreaming channels vary widely, from the frenetic "crowdroar" offered in some channels to the close-knit, "participatory communities" within others. What kinds of livestreaming communities enable the types of meaningful conversation and connection that support relationship development, and how? In this paper, we explore how personal relationships develop within Twitch, a popular livestreaming service. Interviews with 21 pairs who met initially within Twitch channels illustrate how interactions originating in Twitch's text-based, pseudonymous chat environment can evolve into close relationships, marked by substantial trust and support. Consistent with Walther's hyperpersonal model, these environments facilitate self-disclosure and conversation by reducing physical cues and emphasizing common ground, while frequent, low-stakes interaction allow relationships to deepen over time. Our findings also highlight boundaries of the hyperpersonal model. As group size increases, participants leverage affordances for elevated visibility to spark interactions; as relationships deepen, they incorporate complementary media channels to increase intimacy. Often, relationships become so deep through purely computer-mediated channels that face-to-face meetings become yet another step in a continuum of relationship development. Findings from a survey of 1,367 members of Twitch communities demonstrate how the suitability of these spaces as venues for relational interaction decreases as communities increase in size. Together, these findings illustrate vividly how hyperpersonal interaction functions in the context of real online communities. We consider implications for the design and management of online communities, including their potential for supporting "strong bridges," relationships which combine the benefits of strong ties and network bridges.
DA  - 2020/10/15/
PY  - 2020
DO  - 10.1145/3415165
DP  - ACM Digital Library
VL  - 4
IS  - CSCW2
SP  - 94:1
EP  - 94:34
J2  - Proc. ACM Hum.-Comput. Interact.
ST  - From Virtual Strangers to IRL Friends
UR  - https://doi.org/10.1145/3415165
Y2  - 2024/01/03/05:08:27
KW  - livestreaming
KW  - networks
KW  - online communities
KW  - relationships
KW  - strong bridges
KW  - tie strength
KW  - twitch
ER  - 

TY  - JOUR
TI  - Set the Configuration for the Heart of the OS: On the Practicality of Operating System Kernel Debloating
AU  - Kuo, Hsuan-Chi
AU  - Chen, Jianyan
AU  - Mohan, Sibin
AU  - Xu, Tianyin
T2  - Proceedings of the ACM on Measurement and Analysis of Computing Systems
AB  - This paper presents a study on the practicality of operating system (OS) kernel debloating---reducing kernel code that is not needed by the target applications---in real-world systems. Despite their significant benefits regarding security (attack surface reduction) and performance (fast boot times and reduced memory footprints), the state-of-the-art OS kernel debloating techniques are seldom adopted in practice, especially in production systems. We identify the limitations of existing kernel debloating techniques that hinder their practical adoption, including both accidental and essential limitations. To understand these limitations, we build an advanced debloating framework named \tool which enables us to conduct a number of experiments on different types of OS kernels (including Linux and the L4 microkernel) with a wide variety of applications (including HTTPD, Memcached, MySQL, NGINX, PHP and Redis). Our experimental results reveal the challenges and opportunities towards making kernel debloating techniques practical for real-world systems. The main goal of this paper is to share these insights and our experiences to shed light on addressing the limitations of kernel debloating in future research and development efforts.
DA  - 2020/05/27/
PY  - 2020
DO  - 10.1145/3379469
DP  - ACM Digital Library
VL  - 4
IS  - 1
SP  - 03:1
EP  - 03:27
J2  - Proc. ACM Meas. Anal. Comput. Syst.
ST  - Set the Configuration for the Heart of the OS
UR  - https://dl.acm.org/doi/10.1145/3379469
Y2  - 2024/01/03/05:08:46
L1  - https://dl.acm.org/doi/pdf/10.1145/3379469
KW  - configuration
KW  - debloating
KW  - kernel
KW  - operating system
KW  - os
ER  - 

TY  - JOUR
TI  - A Tale of Creativity and Struggles: Team Practices for Bottom-Up Innovation in Virtual Game Jams
AU  - Freeman, Guo
AU  - McNeese, Nathan J.
T2  - Proceedings of the ACM on Human-Computer Interaction
AB  - Game jams are intense and time-sensitive online or face-to-face game creation events where a digital game is developed in a relatively short time frame (typically 48 to 72 hours) exploring given design constraints and end results are shared publicly. They have increasingly become emerging sites where non-professional game developers, amateurs, and hobbyists engage in bottom-up technological innovation by collaboratively designing and developing more creative and novel digital products. Drawing on 28 interviews, in this paper we focus on how game developers collaborate as small teams to innovate game design and development from the bottom up in virtual game jams (i.e., exclusively online) and the unique role of virtual game jams in their technological innovation. We contribute to CSCW by providing new empirical evidence of how team practices for innovation may emerge in a novel technology community that is not widely studied before. We also expand a growing research agenda in CSCW on explicating nuanced social behaviors, processes, and consequences of bottom-up technological innovation.
DA  - 2021/04/22/
PY  - 2021
DO  - 10.1145/3449150
DP  - ACM Digital Library
VL  - 5
IS  - CSCW1
SP  - 76:1
EP  - 76:27
J2  - Proc. ACM Hum.-Comput. Interact.
ST  - A Tale of Creativity and Struggles
UR  - https://dl.acm.org/doi/10.1145/3449150
Y2  - 2024/01/03/05:09:12
L1  - https://dl.acm.org/doi/pdf/10.1145/3449150
KW  - computer-mediated collaboration
KW  - game development
KW  - indie game development
KW  - team practices
KW  - technological innovation
KW  - virtual game jams
ER  - 

TY  - JOUR
TI  - Centralized, Distributed, and Everything in between: Reviewing Access Control Solutions for the IoT
AU  - Dramé-Maigné, Sophie
AU  - Laurent, Maryline
AU  - Castillo, Laurent
AU  - Ganem, Hervé
T2  - ACM Computing Surveys
AB  - The Internet of Things is taking hold in our everyday life. Regrettably, the security of IoT devices is often being overlooked. Among the vast array of security issues plaguing the emerging IoT, we decide to focus on access control, as privacy, trust, and other security properties cannot be achieved without controlled access. This article classifies IoT access control solutions from the literature according to their architecture (e.g., centralized, hierarchical, federated, distributed) and examines the suitability of each one for access control purposes. Our analysis concludes that important properties such as auditability and revocation are missing from many proposals while hierarchical and federated architectures are neglected by the community. Finally, we provide an architecture-based taxonomy and future research directions: a focus on hybrid architectures, usability, flexibility, privacy, and revocation schemes in serverless authorization.
DA  - 2021/09/17/
PY  - 2021
DO  - 10.1145/3465170
DP  - ACM Digital Library
VL  - 54
IS  - 7
SP  - 138:1
EP  - 138:34
J2  - ACM Comput. Surv.
SN  - 0360-0300
ST  - Centralized, Distributed, and Everything in between
UR  - https://doi.org/10.1145/3465170
Y2  - 2024/01/03/05:09:29
KW  - Internet of Things
KW  - IoT
KW  - Access control
KW  - security
KW  - survey
ER  - 

TY  - JOUR
TI  - DBOS: a DBMS-oriented operating system
AU  - Skiadopoulos, Athinagoras
AU  - Li, Qian
AU  - Kraft, Peter
AU  - Kaffes, Kostis
AU  - Hong, Daniel
AU  - Mathew, Shana
AU  - Bestor, David
AU  - Cafarella, Michael
AU  - Gadepally, Vijay
AU  - Graefe, Goetz
AU  - Kepner, Jeremy
AU  - Kozyrakis, Christos
AU  - Kraska, Tim
AU  - Stonebraker, Michael
AU  - Suresh, Lalith
AU  - Zaharia, Matei
T2  - Proceedings of the VLDB Endowment
AB  - This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.
DA  - 2021/09/01/
PY  - 2021
DO  - 10.14778/3485450.3485454
DP  - ACM Digital Library
VL  - 15
IS  - 1
SP  - 21
EP  - 30
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - DBOS
UR  - https://doi.org/10.14778/3485450.3485454
Y2  - 2024/01/03/05:10:41
ER  - 

TY  - JOUR
TI  - Durable functions: semantics for stateful serverless
AU  - Burckhardt, Sebastian
AU  - Gillum, Chris
AU  - Justo, David
AU  - Kallas, Konstantinos
AU  - McMahon, Connor
AU  - Meiklejohn, Christopher S.
T2  - Proceedings of the ACM on Programming Languages
AB  - Serverless, or Functions-as-a-Service (FaaS), is an increasingly popular paradigm for application development, as it provides implicit elastic scaling and load based billing. However, the weak execution guarantees and intrinsic compute-storage separation of FaaS create serious challenges when developing applications that require persistent state, reliable progress, or synchronization. This has motivated a new generation of serverless frameworks that provide stateful abstractions. For instance, Azure's Durable Functions (DF) programming model enhances FaaS with actors, workflows, and critical sections. As a programming model, DF is interesting because it combines task and actor parallelism, which makes it suitable for a wide range of serverless applications. We describe DF both informally, using examples, and formally, using an idealized high-level model based on the untyped lambda calculus. Next, we demystify how the DF runtime can (1) execute in a distributed unreliable serverless environment with compute-storage separation, yet still conform to the fault-free high-level model, and (2) persist execution progress without requiring checkpointing support by the language runtime. To this end we define two progressively more complex execution models, which contain the compute-storage separation and the record-replay, and prove that they are equivalent to the high-level model.
DA  - 2021/10/15/
PY  - 2021
DO  - 10.1145/3485510
DP  - ACM Digital Library
VL  - 5
IS  - OOPSLA
SP  - 133:1
EP  - 133:27
J2  - Proc. ACM Program. Lang.
ST  - Durable functions
UR  - https://dl.acm.org/doi/10.1145/3485510
Y2  - 2024/01/03/05:10:59
L1  - https://dl.acm.org/doi/pdf/10.1145/3485510
KW  - Serverless
KW  - Durable Functions
KW  - Programming
KW  - Reliable
KW  - Service Composition
KW  - Services
KW  - Workflows
ER  - 

TY  - JOUR
TI  - Hey Alexa! a fun introduction to AWS Lambda, serverless functions, and JavaScript with conversational AI: conference workshop
AU  - Case, Denise M.
T2  - Journal of Computing Sciences in Colleges
AB  - Learn how to design and create engaging cutting-edge apps! We'll introduce principles of designing apps that apply freely available conversational AI tools (e.g., Alexa, Google Home, and Siri) [2]. We'll explain the powerful and easy 'serverless' functions that power them - no programming experience required! We'll introduce cloud computing and show the benefits of serverless functions [1]. Along the way, participants will open an Amazon Web Services account and get started as an Alexa developer for free! We'll show how to use AWS Lambda to run code without worrying about servers, complex file transfers, or setup. We'll show you how to easily upload and edit simple web-enabled functions. You'll learn how Alexa can be used to illustrate basic programming concepts like programming flow and enumerated types. Participants get access to the project code for the reference app, checklists for working through the steps to publish, and if they like, can leave with a newly submitted Alexa skill customized for their organization or institution.
DA  - 2021/04/01/
PY  - 2021
DP  - ACM Digital Library
VL  - 36
IS  - 6
SP  - 66
J2  - J. Comput. Sci. Coll.
SN  - 1937-4771
ST  - Hey Alexa! a fun introduction to AWS Lambda, serverless functions, and JavaScript with conversational AI
UR  - https://dl.acm.org/doi/10.5555/3469567.3469574
ER  - 

TY  - JOUR
TI  - Operating Systems for Resource-adaptive Intelligent Software: Challenges and Opportunities
AU  - Liu, Xuanzhe
AU  - Wang, Shangguang
AU  - Ma, Yun
AU  - Zhang, Ying
AU  - Mei, Qiaozhu
AU  - Liu, Yunxin
AU  - Huang, Gang
T2  - ACM Transactions on Internet Technology
AB  - The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is “resource adaptive,” i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the “monolithic OS” that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of “Software-as-a-Service” that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners.
DA  - 2021/03/15/
PY  - 2021
DO  - 10.1145/3425866
DP  - ACM Digital Library
VL  - 21
IS  - 2
SP  - 27:1
EP  - 27:19
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
ST  - Operating Systems for Resource-adaptive Intelligent Software
UR  - https://doi.org/10.1145/3425866
Y2  - 2024/01/03/05:11:59
KW  - machine learning
KW  - Operating systems
KW  - resource disaggregation
KW  - service-oriented
ER  - 

TY  - JOUR
TI  - Query-driven video event processing for the internet of multimedia things
AU  - Yadav, Piyush
AU  - Salwala, Dhaval
AU  - Pontes, Felipe Arruda
AU  - Dhingra, Praneet
AU  - Curry, Edward
T2  - Proceedings of the VLDB Endowment
AB  - Advances in Deep Neural Network (DNN) techniques have revolutionized video analytics and unlocked the potential for querying and mining video event patterns. This paper details GNOSIS, an event processing platform to perform near-real-time video event detection in a distributed setting. GNOSIS follows a serverless approach where its component acts as independent microservices and can be deployed at multiple nodes. GNOSIS uses a declarative query-driven approach where users can write customize queries for spatiotemporal video event reasoning. The system converts the incoming video streams into a continuous evolving graph stream using machine learning (ML) and DNN models pipeline and applies graph matching for video event pattern detection. GNOSIS can perform both stateful and stateless video event matching. To improve Quality of Service (QoS), recent work in GNOSIS incorporates optimization techniques like adaptive scheduling, energy efficiency, and content-driven windows. This paper demonstrates the Occupational Health and Safety query use cases to show the GNOSIS efficacy.
DA  - 2021/07/01/
PY  - 2021
DO  - 10.14778/3476311.3476360
DP  - ACM Digital Library
VL  - 14
IS  - 12
SP  - 2847
EP  - 2850
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3476311.3476360
Y2  - 2024/01/03/05:12:16
L1  - https://aran.library.nuigalway.ie/bitstream/10379/16899/1/p2847-yadav_vldb.pdf
ER  - 

TY  - JOUR
TI  - SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks
AU  - Krishnan, Prabhakar
AU  - Jain, Kurunandan
AU  - Jose, Pramod George
AU  - Achuthan, Krishnashree
AU  - Buyya, Rajkumar
T2  - ACM Transactions on Multimedia Computing, Communications, and Applications
AB  - The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.
DA  - 2021/04/21/
PY  - 2021
DO  - 10.1145/3377390
DP  - ACM Digital Library
VL  - 17
IS  - 2
SP  - 39:1
EP  - 39:29
J2  - ACM Trans. Multimedia Comput. Commun. Appl.
SN  - 1551-6857
UR  - https://doi.org/10.1145/3377390
Y2  - 2024/01/03/05:12:59
KW  - NFV
KW  - SDN
KW  - lightweight cryptography
KW  - 5th Generation network (5G)
KW  - Multi-Access Edge Computing (MEC)
KW  - multimedia communication
KW  - network security
KW  - network slicing
KW  - QoE
ER  - 

TY  - JOUR
TI  - BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications
AU  - Damiani, Andrea
AU  - Fiscaletti, Giorgia
AU  - Bacis, Marco
AU  - Brondolin, Rolando
AU  - Santambrogio, Marco D.
T2  - ACM Transactions on Reconfigurable Technology and Systems
AB  - “Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.
DA  - 2022/01/11/
PY  - 2022
DO  - 10.1145/3472958
DP  - ACM Digital Library
VL  - 15
IS  - 2
SP  - 17:1
EP  - 17:27
J2  - ACM Trans. Reconfigurable Technol. Syst.
SN  - 1936-7406
ST  - BlastFunction
UR  - https://doi.org/10.1145/3472958
Y2  - 2024/01/03/05:13:30
KW  - cloud-native
KW  - autoscaling
KW  - Field Programmable Gate Arrays (FPGAs)
KW  - hardware acceleration
KW  - time-sharing
ER  - 

TY  - JOUR
TI  - CDI-E: an elastic cloud service for data engineering
AU  - Das, Prakash
AU  - Srivastava, Shivangi
AU  - Moskovich, Valentin
AU  - Chaturvedi, Anmol
AU  - Mittal, Anant
AU  - Xiao, Yongqin
AU  - Chowdhury, Mosharaf
T2  - Proceedings of the VLDB Endowment
AB  - We live in the gilded age of data-driven computing. With public clouds offering virtually unlimited amounts of compute and storage, enterprises collecting data about every aspect of their businesses, and advances in analytics and machine learning technologies, data driven decision making is now timely, cost-effective, and therefore, pervasive. Alas, only a handful of power users can wield today's powerful data engineering tools. For one thing, most solutions require knowledge of specific programming interfaces or libraries. Furthermore, running them requires complex configurations and knowledge of the underlying cloud for cost-effectiveness. We decided that a fundamental redesign is in order to democratize data engineering for the masses at cloud scale. The result is Informatica Cloud Data Integration - Elastic (CDI-E). Since the early 1990s, Informatica has been a pioneer and industry leader in building no-code data engineering tools. Non-experts can express complex data engineering tasks using a graphical user interface (GUI). Informatica CDI-E is built to incorporate the simplicity of GUI in the design layer with an elastic and highly scalable run time to handle data in any format without little to no user input using automated optimizations. Users upload their data to the cloud in any format and can immediately use them in conjunction with their data management and analytic tools of choice using CDI-E GUI. Implementation began in the Spring of 2017, and Informatica CDI-E has been generally available since the Summer of 2019. Today, CDI-E is used in production by a growing number of small and large enterprises to make sense of data in arbitrary formats. In this paper, we describe the architecture of Informatica CDI-E and its novel no-code data engineering interface. The paper highlights some of the key features of CDI-E: simplicity without loss in productivity and extreme elasticity. It concludes with lessons we learned and an outlook of the future.
DA  - 2022/08/01/
PY  - 2022
DO  - 10.14778/3554821.3554825
DP  - ACM Digital Library
VL  - 15
IS  - 12
SP  - 3319
EP  - 3331
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - CDI-E
UR  - https://doi.org/10.14778/3554821.3554825
Y2  - 2024/01/03/05:14:02
ER  - 

TY  - JOUR
TI  - Cloud databases: new techniques, challenges, and opportunities
AU  - Li, Guoliang
AU  - Dong, Haowen
AU  - Zhang, Chao
T2  - Proceedings of the VLDB Endowment
AB  - As database vendors are increasingly moving towards the cloud data service, i.e., databases as a service (DBaaS), cloud databases have become prevalent. Compared with the early cloud-hosted databases, the new generation of cloud databases, also known as cloud-native databases, seek for higher elasticity and lower cost by developing new techniques, e.g., compute-storage disaggregation and the log is the database. To better harness the power of these cloud databases, it is important to study and compare the pros and cons of their key techniques. In this tutorial, we offer a comprehensive survey of cloud-native databases. Based on various system architectures, we introduce a taxonomy for the state-of-the-art cloud-native OLTP databases and OLAP databases, respectively. We then take a deep dive into their key techniques regarding storage management, transaction processing, analytical processing, data replication, serverless computing, database recovery, and security. Finally, we discuss the research challenges and opportunities.
DA  - 2022/08/01/
PY  - 2022
DO  - 10.14778/3554821.3554893
DP  - ACM Digital Library
VL  - 15
IS  - 12
SP  - 3758
EP  - 3761
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - Cloud databases
UR  - https://doi.org/10.14778/3554821.3554893
Y2  - 2024/01/03/05:16:21
ER  - 

TY  - JOUR
TI  - Fog Computing Platforms for Smart City Applications: A Survey
AU  - Da Silva, Thiago Pereira
AU  - Batista, Thais
AU  - Lopes, Frederico
AU  - Neto, Aluizio Rocha
AU  - Delicato, Flávia C.
AU  - Pires, Paulo F.
AU  - Da Rocha, Atslands R.
T2  - ACM Transactions on Internet Technology
AB  - Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.
DA  - 2022/12/22/
PY  - 2022
DO  - 10.1145/3488585
DP  - ACM Digital Library
VL  - 22
IS  - 4
SP  - 96:1
EP  - 96:32
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
ST  - Fog Computing Platforms for Smart City Applications
UR  - https://doi.org/10.1145/3488585
Y2  - 2024/01/03/05:17:26
KW  - edge computing
KW  - Fog computing
KW  - smart cities
ER  - 

TY  - JOUR
TI  - FuncPipe: A Pipelined Serverless Framework for Fast and Cost-Efficient Training of Deep Learning Models
AU  - Liu, Yunzhuo
AU  - Jiang, Bo
AU  - Guo, Tian
AU  - Huang, Zimeng
AU  - Ma, Wenhao
AU  - Wang, Xinbing
AU  - Zhou, Chenghu
T2  - Proceedings of the ACM on Measurement and Analysis of Computing Systems
AB  - Training deep learning (DL) models in the cloud has become a norm. With the emergence of serverless computing and its benefits of true pay-as-you-go pricing and scalability, systems researchers have recently started to provide support for serverless-based training. However, the ability to train DL models on serverless platforms is hindered by the resource limitations of today's serverless infrastructure and DL models' explosive requirement for memory and bandwidth. This paper describes FuncPipe, a novel pipelined training framework specifically designed for serverless platforms that enable fast and low-cost training of DL models. FuncPipe is designed with the key insight that model partitioning can be leveraged to bridge both memory and bandwidth gaps between the capacity of serverless functions and the requirement of DL training. Conceptually simple, we have to answer several design questions, including how to partition the model, configure each serverless function, and exploit each function's uplink/downlink bandwidth. In particular, we tailor a micro-batch scheduling policy for the serverless environment, which serves as the basis for the subsequent optimization. Our Mixed-Integer Quadratic Programming formulation automatically and simultaneously configures serverless resources and partitions models to fit within the resource constraints. Lastly, we improve the bandwidth efficiency of storage-based synchronization with a novel pipelined scatter-reduce algorithm. We implement FuncPipe on two popular cloud serverless platforms and show that it achieves 7%-77% cost savings and 1.3X-2.2X speedup compared to state-of-the-art serverless-based frameworks.
DA  - 2022/12/08/
PY  - 2022
DO  - 10.1145/3570607
DP  - ACM Digital Library
VL  - 6
IS  - 3
SP  - 47:1
EP  - 47:30
J2  - Proc. ACM Meas. Anal. Comput. Syst.
ST  - FuncPipe
UR  - https://dl.acm.org/doi/10.1145/3570607
Y2  - 2024/01/03/05:17:44
L1  - https://dl.acm.org/doi/pdf/10.1145/3570607
L1  - https://arxiv.org/pdf/2204.13561
KW  - distributed training
KW  - pipeline parallelism
KW  - serverless function
ER  - 

TY  - JOUR
TI  - Integration of DevOps Practices on a Noise Monitor System with CircleCI and Terraform
AU  - Romero, Esteban Elias
AU  - Camacho, Carlos David
AU  - Montenegro, Carlos Enrique
AU  - Acosta, Óscar Esneider
AU  - Crespo, Rubén González
AU  - Gaona, Elvis Eduardo
AU  - Martínez, Marcelo Herrera
T2  - ACM Transactions on Management Information Systems
AB  - Lowering pollution levels is one of the main principles of Sustainable Development goals dictated by the United Nations. Consequently, developments on noise monitoring contribute in great manner to this purpose, since they give the opportunity to governments and institutions to maintain track on the matter. While developing a software product for this purpose, with the growth in terms of functional and non-functional requirements, elements such as infrastructure, source code, and others also scale up. Consequently if there are not good practices to face the new challenges of the software product, then it could become more complex to refactor, maintain, and scale, causing a decrease on delivery rate and the quality of the product. DevOps is an emerging concept but still hazy, which involves a set of practices that helps organizations to speed up delivery time, improve software quality and collaboration between teams. The aim of this article is to document the implementation of some DevOps practices such as IaC, continuous integration and deployment, code quality control, and collaboration on a noise monitor system to increase the product quality and automation of deployment. The final result is a set of automated pipelines that represents the entire integration and deployment cycle of the software integrated with platforms to improve quality and maintainability of the software components.
DA  - 2022/08/10/
PY  - 2022
DO  - 10.1145/3505228
DP  - ACM Digital Library
VL  - 13
IS  - 4
SP  - 36:1
EP  - 36:24
J2  - ACM Trans. Manage. Inf. Syst.
SN  - 2158-656X
UR  - https://doi.org/10.1145/3505228
Y2  - 2024/01/03/05:18:05
KW  - serverless
KW  - CI/CD
KW  - classification
KW  - DeVOps
KW  - sound
ER  - 

TY  - JOUR
TI  - Moneyball: proactive auto-scaling in Microsoft Azure SQL database serverless
AU  - Poppe, Olga
AU  - Guo, Qun
AU  - Lang, Willis
AU  - Arora, Pankaj
AU  - Oslake, Morgan
AU  - Xu, Shize
AU  - Kalhan, Ajay
T2  - Proceedings of the VLDB Endowment
AB  - Microsoft Azure SQL Database is among the leading relational database service providers in the cloud. Serverless compute automatically scales resources based on workload demand. When a database becomes idle its resources are reclaimed. When activity returns, resources are resumed. Customers pay only for resources they used. However, scaling is currently merely reactive, not proactive, according to customers' workloads. Therefore, resources may not be immediately available when a customer comes back online after a prolonged idle period. In this work, we focus on reducing this delay in resource availability by predicting the pause/resume patterns and proactively resuming resources for each database. Furthermore, we avoid taking away resources for short idle periods to relieve the back-end from ineffective pause/resume workflows. Results of this study are currently being used worldwide to find the middle ground between quality of service and cost of operation.
DA  - 2022/02/01/
PY  - 2022
DO  - 10.14778/3514061.3514073
DP  - ACM Digital Library
VL  - 15
IS  - 6
SP  - 1279
EP  - 1287
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - Moneyball
UR  - https://doi.org/10.14778/3514061.3514073
Y2  - 2024/01/03/05:18:26
ER  - 

TY  - JOUR
TI  - Netherite: efficient execution of serverless workflows
AU  - Burckhardt, Sebastian
AU  - Chandramouli, Badrish
AU  - Gillum, Chris
AU  - Justo, David
AU  - Kallas, Konstantinos
AU  - McMahon, Connor
AU  - Meiklejohn, Christopher S.
AU  - Zhu, Xiangfeng
T2  - Proceedings of the VLDB Endowment
AB  - Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck. To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts. Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.
DA  - 2022/04/01/
PY  - 2022
DO  - 10.14778/3529337.3529344
DP  - ACM Digital Library
VL  - 15
IS  - 8
SP  - 1591
EP  - 1604
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - Netherite
UR  - https://doi.org/10.14778/3529337.3529344
Y2  - 2024/01/03/05:18:54
ER  - 

TY  - JOUR
TI  - NLUBroker: A QoE-driven Broker System for Natural Language Understanding Services
AU  - Xu, Lanyu
AU  - Iyengar, Arun
AU  - Shi, Weisong
T2  - ACM Transactions on Internet Technology
AB  - Cloud-based Natural Language Understanding (NLU) services are becoming more popular with the development of artificial intelligence. More applications are integrated with cloud-based NLU services to enhance the way people communicate with machines. However, with NLU services provided by different companies powered by unrevealed AI technology, how to choose the best one is a problem for developers. Existing tools that can provide guidance to developers and make recommendations based on their needs are severely limited. This article comprehensively evaluates multiple state-of-the-art NLU services, and the results indicate that there is no absolute winner for different usage requirements. Motivated by this observation, we provide several insights and propose NLUBroker, a Quality of Experience-driven (QoE-driven) broker system, to select the proper service according to the environment. NLUBroker senses the client and service status and leverages a solution to the multi-armed bandit problem to conduct online learning, aiming to achieve maximum expected QoE. The performance of NLUBroker is evaluated in both simulation and real-world environments, and the evaluation results demonstrate that NLUBroker is an efficient solution for selecting NLU services. It is adaptive to changes in the environment, outperforms three baseline methods we evaluated and improves overall QoE up to 1.5× for the evaluated state-of-the-art NLU services.
DA  - 2022/02/01/
PY  - 2022
DO  - 10.1145/3497807
DP  - ACM Digital Library
VL  - 22
IS  - 3
SP  - 69:1
EP  - 69:29
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
ST  - NLUBroker
UR  - https://doi.org/10.1145/3497807
Y2  - 2024/01/03/05:19:47
KW  - Quality of experience
KW  - service broker
ER  - 

TY  - JOUR
TI  - Optimizing inference serving on serverless platforms
AU  - Ali, Ahsan
AU  - Pinciroli, Riccardo
AU  - Yan, Feng
AU  - Smirni, Evgenia
T2  - Proceedings of the VLDB Endowment
AB  - Serverless computing is gaining popularity for machine learning (ML) serving workload due to its autonomous resource scaling, easy to use and pay-per-use cost model. Existing serverless platforms work well for image-based ML inference, where requests are homogeneous in service demands. That said, recent advances in natural language processing could not fully benefit from existing serverless platforms as their requests are intrinsically heterogeneous. Batching requests for processing can significantly increase ML serving efficiency while reducing monetary cost, thanks to the pay-per-use pricing model adopted by serverless platforms. Yet, batching heterogeneous ML requests leads to additional computation overhead as small requests need to be "padded" to the same size as large requests within the same batch. Reaching effective batching decisions (i.e., which requests should be batched together and why) is non-trivial: the padding overhead coupled with the serverless auto-scaling forms a complex optimization problem. To address this, we develop Multi-Buffer Serving (MBS), a framework that optimizes the batching of heterogeneous ML inference serving requests to minimize their monetary cost while meeting their service level objectives (SLOs). The core of MBS is a performance and cost estimator driven by analytical models supercharged by a Bayesian optimizer. MBS is prototyped and evaluated on AWS using bursty workloads. Experimental results show that MBS preserves SLOs while outperforming the state-of-the-art by up to 8 x in terms of cost savings while minimizing the padding overhead by up to 37 x with 3 x less number of serverless function invocations.
DA  - 2022/06/01/
PY  - 2022
DO  - 10.14778/3547305.3547313
DP  - ACM Digital Library
VL  - 15
IS  - 10
SP  - 2071
EP  - 2084
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3547305.3547313
Y2  - 2024/01/03/05:20:04
ER  - 

TY  - JOUR
TI  - Reducing Minor Page Fault Overheads through Enhanced Page Walker
AU  - Tirumalasetty, Chandrahas
AU  - Chou, Chih Chieh
AU  - Reddy, Narasimha
AU  - Gratz, Paul
AU  - Abouelwafa, Ayman
T2  - ACM Transactions on Architecture and Code Optimization
AB  - Application virtual memory footprints are growing rapidly in all systems from servers down to smartphones. To address this growing demand, system integrators are incorporating ever larger amounts of main memory, warranting rethinking of memory management. In current systems, applications produce page fault exceptions whenever they access virtual memory regions that are not backed by a physical page. As application memory footprints grow, they induce more and more minor page faults. Handling of each minor page fault can take a few thousands of CPU cycles and blocks the application till the OS kernel finds a free physical frame. These page faults can be detrimental to the performance when their frequency of occurrence is high and spread across application runtime. Specifically, lazy allocation-induced minor page faults are increasingly impacting application performance. Our evaluation of several workloads indicates an overhead due to minor page faults as high as 29% of execution time. In this article, we propose to mitigate this problem through a hardware, software co-design approach. Specifically, we first propose to parallelize portions of the kernel page allocation to run ahead of fault time in a separate thread. Then we propose the Minor Fault Offload Engine (MFOE), a per-core hardware accelerator for minor fault handling. MFOE is equipped with a pre-allocated page frame table that it uses to service a page fault. On a page fault, MFOE quickly picks a pre-allocated page frame from this table, makes an entry for it in the TLB, and updates the page table entry to satisfy the page fault. The pre-allocation frame tables are periodically refreshed by a background kernel thread, which also updates the data structures in the kernel to account for the handled page faults. We evaluate this system in the gem5 architectural simulator with a modified Linux kernel running on top of simulated hardware containing the MFOE accelerator. Our results show that MFOE improves the average critical path fault handling latency by 33× and tail critical path latency by 51×. Among the evaluated applications, we observed an improvement of runtime by an average of 6.6%.
DA  - 2022/09/16/
PY  - 2022
DO  - 10.1145/3547142
DP  - ACM Digital Library
VL  - 19
IS  - 4
SP  - 57:1
EP  - 57:26
J2  - ACM Trans. Archit. Code Optim.
SN  - 1544-3566
UR  - https://dl.acm.org/doi/10.1145/3547142
Y2  - 2024/01/03/05:20:26
L1  - https://dl.acm.org/doi/pdf/10.1145/3547142
L1  - https://arxiv.org/pdf/2112.14013
KW  - Function-as-a-Service (FaaS)
KW  - Paging
KW  - Translate Look-aside Buffer (TLB)
KW  - virtualization
ER  - 

TY  - JOUR
TI  - Stateful Serverless Computing with Crucial
AU  - Barcelona-Pons, Daniel
AU  - Sutra, Pierre
AU  - Sánchez-Artigas, Marc
AU  - París, Gerard
AU  - García-López, Pedro
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial, a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k-means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18%–40% faster). We also use Crucial to port (part of) a state-of-the-art multi-threaded ML library to serverless. The ported application is up to 30% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6% of changes in the code bases of the evaluated applications.
DA  - 2022/03/07/
PY  - 2022
DO  - 10.1145/3490386
DP  - ACM Digital Library
VL  - 31
IS  - 3
SP  - 39:1
EP  - 39:38
J2  - ACM Trans. Softw. Eng. Methodol.
SN  - 1049-331X
UR  - https://dl.acm.org/doi/10.1145/3490386
Y2  - 2024/01/03/05:21:11
L1  - https://dl.acm.org/doi/pdf/10.1145/3490386
KW  - FaaS
KW  - Serverless
KW  - in-memory
KW  - stateful
KW  - synchronization
ER  - 

TY  - JOUR
TI  - Tikkoun Sofrim: Making Ancient Manuscripts Digitally Accessible: The Case of Midrash Tanhuma
AU  - Wecker, Alan J.
AU  - Raziel-Kretzmer, Vered
AU  - Kiessling, Benjamin
AU  - Ezra, Daniel Stökl Ben
AU  - Lavee, Moshe
AU  - Kuflik, Tsvi
AU  - Elovits, Dror
AU  - Schorr, Moshe
AU  - Schor, Uri
AU  - Jablonski, Pawel
T2  - Journal on Computing and Cultural Heritage
AB  - Making ancient handwritten manuscripts accessible to the general public is challenging, for several reasons. Foremost, they are handwritten. Each and every one is unique, so there is a need for manual transcription for providing enough examples for training a machine-learning-based algorithm to automatically transcribe the handwritten text. Moreover, the quality of the text is diverse—over time the ink faded, pages were damaged, and so forth. Furthermore, the boundaries of the textual regions on a page and the lines of text are not standard. Sometimes there are corrections above the lines, the lines are curved, there are comments and annotations on the margins, and more. A possible solution for these challenges is having a “person in the loop.” However, manual correction brings with it another challenge—how to address disagreement between annotations (as usually several corrections are considered before a decision is taken about the correct transcription). Tikkoun-Sofrim is a system that integrates automatic handwritten text recognition with manual, crowdsourced error correction, introducing an automatic decision process about when to stop asking for additional transcription and selecting the best transcription, declaring it as the recommended agreed reading. The system was applied to several manuscripts of “Midrash Tanhuma,” a medieval Hebrew rabbinic homiletic text, achieving a high level of success.
DA  - 2022/04/07/
PY  - 2022
DO  - 10.1145/3476776
DP  - ACM Digital Library
VL  - 15
IS  - 2
SP  - 20:1
EP  - 20:20
J2  - J. Comput. Cult. Herit.
SN  - 1556-4673
ST  - Tikkoun Sofrim
UR  - https://doi.org/10.1145/3476776
Y2  - 2024/01/03/05:22:11
KW  - CATTI
KW  - crowd-sourcing
KW  - handwritten text recognition
KW  - HTR
KW  - transcription
ER  - 

TY  - JOUR
TI  - WISEFUSE: Workload Characterization and DAG Transformation for Serverless Workflows
AU  - Mahgoub, Ashraf
AU  - Yi, Edgardo Barsallo
AU  - Shankar, Karthick
AU  - Minocha, Eshaan
AU  - Elnikety, Sameh
AU  - Bagchi, Saurabh
AU  - Chaterji, Somali
T2  - Proceedings of the ACM on Measurement and Analysis of Computing Systems
AB  - We characterize production workloads of serverless DAGs at a major cloud provider. Our analysis highlights two major factors that limit performance: (a) lack of efficient communication methods between the serverless functions in the DAG, and (b) stragglers when a DAG stage invokes a set of parallel functions that must complete before starting the next DAG stage. To address these limitations, we propose WISEFUSE, an automated approach to generate an optimized execution plan for serverless DAGs for a user-specified latency objective or budget. We introduce three optimizations: (1) Fusion combines in-series functions together in a single VM to reduce the communication overhead between cascaded functions. (2) Bundling executes a group of parallel invocations of a function in one VM to improve resource sharing among the parallel workers to reduce skew. (3) Resource Allocation assigns the right VM size to each function or function bundle in the DAG to reduce the E2E latency and cost. We implement WISEFUSE to evaluate it experimentally using three popular serverless applications with different DAG structures, memory footprints, and intermediate data sizes. Compared to competing approaches and other alternatives, WISEFUSE shows significant improvements in E2E latency and cost. Specifically, for a machine learning pipeline, WISEFUSE achieves P95 latency that is 67% lower than Photons, 39% lower than Faastlane, and 90% lower than SONIC without increasing the cost.
DA  - 2022/06/06/
PY  - 2022
DO  - 10.1145/3530892
DP  - ACM Digital Library
VL  - 6
IS  - 2
SP  - 26:1
EP  - 26:28
J2  - Proc. ACM Meas. Anal. Comput. Syst.
ST  - WISEFUSE
UR  - https://dl.acm.org/doi/10.1145/3530892
Y2  - 2024/01/03/05:22:58
L1  - https://dl.acm.org/doi/pdf/10.1145/3530892
KW  - serverless
KW  - dag transformation
KW  - workload characterization
ER  - 

TY  - JOUR
TI  - A Flexible and Modular Architecture for Edge Digital Twin: Implementation and Evaluation
AU  - Picone, Marco
AU  - Mamei, Marco
AU  - Zambonelli, Franco
T2  - ACM Transactions on Internet of Things
AB  - IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.
DA  - 2023/02/23/
PY  - 2023
DO  - 10.1145/3573206
DP  - ACM Digital Library
VL  - 4
IS  - 1
SP  - 8:1
EP  - 8:32
J2  - ACM Trans. Internet Things
ST  - A Flexible and Modular Architecture for Edge Digital Twin
UR  - https://dl.acm.org/doi/10.1145/3573206
Y2  - 2024/01/03/05:23:26
L1  - https://dl.acm.org/doi/pdf/10.1145/3573206
KW  - Internet of Things
KW  - edge computing
KW  - Digital Twin
ER  - 

TY  - JOUR
TI  - A Low-code Development Framework for Cloud-native Edge Systems
AU  - Zhang, Wenzhao
AU  - Zhang, Yuxuan
AU  - Fan, Hongchang
AU  - Gao, Yi
AU  - Dong, Wei
T2  - ACM Transactions on Internet Technology
AB  - Customizing and deploying an edge system are time-consuming and complex tasks because of hardware heterogeneity, third-party software compatibility, diverse performance requirements, and so on. In this article, we present TinyEdge, a holistic framework for the low-code development of edge systems. The key idea of TinyEdge is to use a top-down approach for designing edge systems. Developers select and configure TinyEdge modules to specify their interaction logic without dealing with the specific hardware or software. Taking the configuration as input, TinyEdge automatically generates the deployment package and estimates the performance with sufficient profiling. TinyEdge provides a unified development toolkit to specify module dependencies, functionalities, interactions, and configurations. We implement TinyEdge and evaluate its performance using real-world edge systems. Results show that: (1) TinyEdge achieves rapid customization of edge systems, reducing 44.15% of development time and 67.79% of lines of code on average compared with the state-of-the-art edge computing platforms; (2) TinyEdge builds compact modules and optimizes the latent circular dependency detection and message routing efficiency; (3) TinyEdge performance estimation has low absolute errors in various settings.
DA  - 2023/02/27/
PY  - 2023
DO  - 10.1145/3563215
DP  - ACM Digital Library
VL  - 23
IS  - 1
SP  - 15:1
EP  - 15:22
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
UR  - https://doi.org/10.1145/3563215
Y2  - 2024/01/03/05:23:37
KW  - Edge computing
KW  - cloud-native
KW  - low-code development
ER  - 

TY  - JOUR
TI  - A survey on social-physical sensing: An emerging sensing paradigm that explores the collective intelligence of humans and machines
AU  - Rashid, Md Tahmid
AU  - Wei, Na
AU  - Wang, Dong
T2  - Collective Intelligence
AB  - Propelled by the omnipresence of versatile data capture, communication, and computing technologies, physical sensing has revolutionized the avenue for decisively interpreting the real world. However, various limitations hinder physical sensing’s effectiveness in critical scenarios such as disaster response and urban anomaly detection. Meanwhile, social sensing is contriving as a pervasive sensing paradigm leveraging observations from human participants equipped with portable devices and ubiquitous Internet connectivity to perceive the environment. Despite its virtues, social sensing also inherently suffers from a few drawbacks (e.g., inconsistent reliability and uncertain data provenance). Motivated by the complementary strengths of the two sensing modes, social-physical sensing (SPS) is protruding as an emerging sensing paradigm that explores the collective intelligence of humans and machines to reconstruct the “state of the world,” both physically and socially. While a good number of interesting SPS applications have been studied, several critical unsolved challenges still exist in SPS. In this paper, we provide a comprehensive survey of SPS, emphasizing its definition, key enablers, state-of-the-art applications, potential research challenges, and roadmap for future work. This paper intends to bridge the knowledge gap of existing sensing-focused survey papers by thoroughly examining the various aspects of SPS crucial for building potent SPS systems.
DA  - 2023/04/01/
PY  - 2023
DO  - 10.1177/26339137231170825
DP  - ACM Digital Library
VL  - 2
IS  - 2
SP  - 26339137231170825
LA  - en
SN  - 2633-9137
ST  - A survey on social-physical sensing
UR  - https://doi.org/10.1177/26339137231170825
Y2  - 2024/01/03/05:23:58
L1  - https://journals.sagepub.com/doi/pdf/10.1177/26339137231170825
KW  - collective intelligence
KW  - crowdsensing
KW  - physical sensing
KW  - social media
KW  - Social sensing
KW  - social-physical sensing
ER  - 

TY  - JOUR
TI  - Cloud Analytics Benchmark
AU  - van Renen, Alexander
AU  - Leis, Viktor
T2  - Proceedings of the VLDB Endowment
AB  - The cloud facilitates the transition to a service-oriented perspective. This affects cloud-native data management in general, and data analytics in particular. Instead of managing a multi-node database cluster on-premise, end users simply send queries to a managed cloud data warehouse and receive results. While this is obviously very attractive for end users, database system architects still have to engineer systems for this new service model. There are currently many competing architectures ranging from self-hosted (Presto, PostgreSQL), over managed (Snowflake, Amazon Redshift) to query-as-a-service (Amazon Athena, Google BigQuery) offerings. Benchmarking these architectural approaches is currently difficult, and it is not even clear what the metrics for a comparison should be. To overcome these challenges, we first analyze a real-world query trace from Snowflake and compare its properties to that of TPC-H and TPC-DS. Doing so, we identify important differences that distinguish traditional benchmarks from real-world cloud data warehouse workloads. Based on this analysis, we propose the Cloud Analytics Benchmark (CAB). By incorporating workload fluctuations and multi-tenancy, CAB allows evaluating different designs in terms of user-centered metrics such as cost and performance.
DA  - 2023/02/01/
PY  - 2023
DO  - 10.14778/3583140.3583156
DP  - ACM Digital Library
VL  - 16
IS  - 6
SP  - 1413
EP  - 1425
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3583140.3583156
Y2  - 2024/01/03/05:24:13
ER  - 

TY  - JOUR
TI  - Enhancing Blockchain Adoption through Tailored Software Engineering: An Industrial-grounded Study in Education Credentialing
AU  - Li, Zoey Ziyi
AU  - Wang, Han
AU  - Gasevic, Dragan
AU  - Yu, Jiangshan
AU  - Liu, Joseph K.
T2  - Distributed Ledger Technologies: Research and Practice
AB  - Recent years have witnessed a marked increase in both academic proposals and industrial adoptions of blockchain technology. However, a majority of the projects remain at the stage of prototype proposals and their real-world deployment has not met the anticipated level. This gap can be attributed to three major barriers - technical difficulties, human factors, and social context. Most of the existing research leans towards addressing the technical challenges, leaving the human and social aspects inadequately explored. Moreover, a lack of practical insights in the existing blockchain software engineering frameworks further exacerbates the adoption problem. To address these gaps, we introduce a Blockchain-oriented Software Engineering Approach for Higher Adoption Possibility (BOSE-HAP). This approach emphasizes collaboration, reflective thinking, and iterative development, aiming to bolster implementation consistency and stimulate industry adoption. We have applied this approach in the design, development, and launch of a blockchain credentialing product, CValid.org, in the context of a university-level summer school. The product achieves industry-accepted System Usability Score and has seen successful real-world deployment. In addition, this study embed usability considerations throughout the process, involved a total of 112 stakeholders across different development stages, with 25 of them participating in our in-depth interviews and usability testing. Drawing from our firsthand experience and industrial-grounded findings, we deliver eight reflections and propose five best practice suggestions relevant to blockchain adoption. We believe these insights will provide invaluable guidance for both academic researchers and industry practitioners involved in the field of blockchain technology.
DA  - 2023/12/14/
PY  - 2023
DO  - 10.1145/3632532
DP  - ACM Digital Library
VL  - 2
IS  - 4
SP  - 27:1
EP  - 27:24
J2  - Distrib. Ledger Technol.
ST  - Enhancing Blockchain Adoption through Tailored Software Engineering
UR  - https://doi.org/10.1145/3632532
Y2  - 2024/01/03/05:24:21
KW  - Blockchain
KW  - design-based thinking
KW  - education credentialing
KW  - software engineering
KW  - STGT
KW  - usability testing
ER  - 

TY  - JOUR
TI  - Executing Microservice Applications on Serverless, Correctly
AU  - Kallas, Konstantinos
AU  - Zhang, Haoran
AU  - Alur, Rajeev
AU  - Angel, Sebastian
AU  - Liu, Vincent
T2  - Proceedings of the ACM on Programming Languages
AB  - While serverless platforms substantially simplify the provisioning, configuration, and management of cloud applications, implementing correct services on top of these platforms can present significant challenges to programmers. For example, serverless infrastructures introduce a host of failure modes that are not present in traditional deployments. Individual serverless instances can fail while others continue to make progress, correct but slow instances can be killed by the cloud provider as part of resource management, and providers will often respond to such failures by re-executing requests. For functions with side-effects, these scenarios can create behaviors that are not observable in serverful deployments. In this paper, we propose mu2sls, a framework for implementing microservice applications on serverless using standard Python code with two extra primitives: transactions and asynchronous calls. Our framework orchestrates user-written services to address several challenges, such as failures and re-executions, and provides formal guarantees that the generated serverless implementations are correct. To that end, we present a novel service specification abstraction and formalization of serverless implementations that facilitate reasoning about the correctness of a given application’s serverless implementation. This formalization forms the basis of the mu2sls prototype, which we then use to develop a few real-world microservice applications and show that the performance of the generated serverless implementations achieves significant scalability (3-5× the throughput of a sequential implementation) while providing correctness guarantees in the context of faults, re-execution, and concurrency.
DA  - 2023/01/11/
PY  - 2023
DO  - 10.1145/3571206
DP  - ACM Digital Library
VL  - 7
IS  - POPL
SP  - 13:367
EP  - 13:395
J2  - Proc. ACM Program. Lang.
UR  - https://dl.acm.org/doi/10.1145/3571206
Y2  - 2024/01/03/05:24:50
L1  - https://dl.acm.org/doi/pdf/10.1145/3571206
KW  - microservices
KW  - stateful serverless
KW  - transactions
ER  - 

TY  - JOUR
TI  - FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing
AU  - Liu, Xuanzhe
AU  - Wen, Jinfeng
AU  - Chen, Zhenpeng
AU  - Li, Ding
AU  - Chen, Junkai
AU  - Liu, Yi
AU  - Wang, Haoyu
AU  - Jin, Xin
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency. In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency.
DA  - 2023/07/22/
PY  - 2023
DO  - 10.1145/3585007
DP  - ACM Digital Library
VL  - 32
IS  - 5
SP  - 119:1
EP  - 119:29
J2  - ACM Trans. Softw. Eng. Methodol.
SN  - 1049-331X
ST  - FaaSLight
UR  - https://doi.org/10.1145/3585007
Y2  - 2024/01/03/05:24:57
L1  - https://arxiv.org/pdf/2207.08175
KW  - Serverless computing
KW  - cold start
KW  - optional function elimination
KW  - performance optimization
ER  - 

TY  - JOUR
TI  - Facilitating Serverless Match-based Online Games with Novel Blockchain Technologies
AU  - Wu, Feijie
AU  - Yuen, Ho Yin
AU  - Chan, Henry
AU  - Leung, Victor C. M.
AU  - Cai, Wei
T2  - ACM Transactions on Internet Technology
AB  - Applying peer-to-peer (P2P) architecture to online video games has already attracted both academic and industrial interests, since it removes the need for expensive server maintenance. However, there are two major issues preventing the use of a P2P architecture, namely how to provide an effective distributed data storage solution, and how to tackle potential cheating behaviors. Inspired by emerging blockchain techniques, we propose a novel consensus model called Proof-of-Play (PoP) to provide a decentralized data storage system that incorporates an anti-cheating mechanism for P2P games, by rewarding players that interact with the game as intended, along with consideration of security measures to address the Nothing-at-stake Problem and the Long-range Attack. To validate our design, we utilize a game-theory model to show that under certain assumptions, the integrity of the PoP system would not be undermined due to the best interests of any user. Then, as a proof-of-concept, we developed a P2P game (Infinity Battle) to demonstrate how a game can be integrated with PoP in practice. Finally, experiments were conducted to study PoP in comparison with Proof-of-Work (PoW) to show its advantages in various aspects.
DA  - 2023/02/23/
PY  - 2023
DO  - 10.1145/3565884
DP  - ACM Digital Library
VL  - 23
IS  - 1
SP  - 10:1
EP  - 10:26
J2  - ACM Trans. Internet Technol.
SN  - 1533-5399
UR  - https://doi.org/10.1145/3565884
Y2  - 2024/01/03/05:25:06
KW  - blockchain
KW  - consensus model
KW  - Peer-to-peer game
ER  - 

TY  - JOUR
TI  - Flexible Resource Allocation for Relational Database-as-a-Service
AU  - Arora, Pankaj
AU  - Chaudhuri, Surajit
AU  - Das, Sudipto
AU  - Dong, Junfeng
AU  - George, Cyril
AU  - Kalhan, Ajay
AU  - König, Arnd Christian
AU  - Lang, Willis
AU  - Li, Changsong
AU  - Li, Feng
AU  - Liu, Jiaqi
AU  - Maas, Lukas M.
AU  - Mata, Akshay
AU  - Menache, Ishai
AU  - Moeller, Justin
AU  - Narasayya, Vivek
AU  - Olma, Matthaios
AU  - Oslake, Morgan
AU  - Rezai, Elnaz
AU  - Shan, Yi
AU  - Syamala, Manoj
AU  - Xu, Shize
AU  - Zois, Vasileios
T2  - Proceedings of the VLDB Endowment
AB  - Oversubscription is an essential cost management strategy for cloud database providers, and its importance is magnified by the emerging paradigm of serverless databases. In contrast to general purpose techniques used for oversubscription in hypervisors, operating systems and cluster managers, we develop techniques that leverage our understanding of how DBMSs use resources and how resource allocations impact database performance. Our techniques are designed to flexibly redistribute resources across database tenants at the node and cluster levels with low overhead. We have implemented our techniques in a commercial cloud database service: Azure SQL Database. Experiments using microbenchmarks, industry-standard benchmarks and real-world resource usage traces show that using our approach, it is possible to tightly control the impact on database performance even with a relatively high degree of oversubscription.
DA  - 2023/09/01/
PY  - 2023
DO  - 10.14778/3625054.3625058
DP  - ACM Digital Library
VL  - 16
IS  - 13
SP  - 4202
EP  - 4215
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3625054.3625058
Y2  - 2024/01/03/05:25:15
ER  - 

TY  - JOUR
TI  - Foreign Keys Open the Door for Faster Incremental View Maintenance
AU  - Svingos, Christoforos
AU  - Hernich, Andre
AU  - Gildhoff, Hinnerk
AU  - Papakonstantinou, Yannis
AU  - Ioannidis, Yannis
T2  - Proceedings of the ACM on Management of Data
AB  - Serverless cloud-based warehousing systems enable users to create materialized views in order to speed up predictable and repeated query workloads. Incremental view maintenance (IVM) minimizes the time needed to bring a materialized view up-to-date. It allows the refresh of a materialized view solely based on the base table changes since the last refresh. In serverless cloud-based warehouses, IVM uses computations defined as SQL scripts that update the materialized view based on updates to its base tables. However, the scripts set up for materialized views with inner joins are not optimal in the presence of foreign key constraints. For instance, for a join of two tables, the state of the art IVM computations use a UNION ALL operator of two joins - one computing the contributions to the join from updates to the first table and the other one computing the remaining contributions from the second table. Knowing that one of the join keys is a foreign-key would allow us to prune all but one of the UNION ALL branches and obtain a more efficient IVM script. In this work, we explore ways of incorporating knowledge about foreign key into IVM in order to speed up its performance. Experiments in Redshift showed that the proposed technique improved the execution times of the whole refresh process up to 2 times, and up to 2.7 times the process of calculating the necessary changes that will be applied into the materialized view.
DA  - 2023/05/30/
PY  - 2023
DO  - 10.1145/3588720
DP  - ACM Digital Library
VL  - 1
IS  - 1
SP  - 40:1
EP  - 40:25
J2  - Proc. ACM Manag. Data
UR  - https://dl.acm.org/doi/10.1145/3588720
Y2  - 2024/01/03/05:25:29
L1  - https://dl.acm.org/doi/pdf/10.1145/3588720
KW  - foreign key constraints
KW  - incrementally updated materialized views
KW  - relational databases
ER  - 

TY  - JOUR
TI  - Hardware Hardened Sandbox Enclaves for Trusted Serverless Computing
AU  - Park, Joongun
AU  - Kang, Seunghyo
AU  - Lee, Sanghyeon
AU  - Kim, Taehoon
AU  - Park, Jongse
AU  - Kwon, Youngjin
AU  - Huh, Jaehyuk
T2  - ACM Transactions on Architecture and Code Optimization
AB  - In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this paper proposes an extended TEE model called Cloister, designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves which is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this paper shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware.
DA  - 2023/11/14/
PY  - 2023
DO  - 10.1145/3632954
DP  - ACM Digital Library
J2  - ACM Trans. Archit. Code Optim.
SN  - 1544-3566
UR  - https://dl.acm.org/doi/10.1145/3632954
Y2  - 2024/01/03/05:25:42
L1  - https://dl.acm.org/doi/pdf/10.1145/3632954
KW  - Security
KW  - Serverless computing
KW  - Hardware
KW  - Trusted Execution Environment
ER  - 

TY  - JOUR
TI  - InfiniStore: Elastic Serverless Cloud Storage
AU  - Zhang, Jingyuan
AU  - Wang, Ao
AU  - Ma, Xiaolong
AU  - Carver, Benjamin
AU  - Newman, Nicholas John
AU  - Anwar, Ali
AU  - Rupprecht, Lukas
AU  - Tarasov, Vasily
AU  - Skourtis, Dimitrios
AU  - Yan, Feng
AU  - Cheng, Yue
T2  - Proceedings of the VLDB Endowment
AB  - Cloud object storage such as AWS S3 is cost-effective and highly elastic but relatively slow, while high-performance cloud storage such as AWS ElastiCache is expensive and provides limited elasticity. We present a new cloud storage service called ServerlessMemory, which stores data using the memory of serverless functions. ServerlessMemory employs a sliding-window-based memory management strategy inspired by the garbage collection mechanisms used in the programming language to effectively segregate hot/cold data and provides fine-grained elasticity, good performance, and a pay-per-access cost model with extremely low cost. We then design and implement InfiniStore, a persistent and elastic cloud storage system, which seamlessly couples the function-based ServerlessMemory layer with a persistent, inexpensive cloud object store layer. InfiniStore enables durability despite function failures using a fast parallel recovery scheme built on the auto-scaling functionality of a FaaS (Function-as-a-Service) platform. We evaluate InfiniStore extensively using both microbenchmarking and two real-world applications. Results show that InfiniStore has more performance benefits for objects larger than 10 MB compared to AWS ElastiCache and Anna, and InfiniStore achieves 26.25% and 97.24% tenant-side cost reduction compared to InfiniCache and ElastiCache, respectively.
DA  - 2023/03/01/
PY  - 2023
DO  - 10.14778/3587136.3587139
DP  - ACM Digital Library
VL  - 16
IS  - 7
SP  - 1629
EP  - 1642
J2  - Proc. VLDB Endow.
SN  - 2150-8097
ST  - InfiniStore
UR  - https://doi.org/10.14778/3587136.3587139
Y2  - 2024/01/03/05:26:00
L1  - https://arxiv.org/pdf/2209.01496
ER  - 

TY  - JOUR
TI  - NEPTUNE: a Comprehensive Framework for Managing Serverless Functions at the Edge
AU  - Baresi, Luciano
AU  - Hu, Davide Yi Xian
AU  - Quattrocchi, Giovanni
AU  - Terracciano, Luca
T2  - ACM Transactions on Autonomous and Adaptive Systems
AB  - Applications that are constrained by low-latency requirements can hardly be executed on cloud infrastructures, given the high network delay required to reach remote servers. Multi-access Edge Computing (MEC) is the reference architecture for executing applications on nodes that are located close to users (i.e., at the edge of the network). This way, the network overhead is reduced but new challenges emerge. The resources available on edge nodes are limited, workloads fluctuate since users can rapidly change location, and complex tasks are becoming widespread (e.g., machine learning inference). To address these issues, this article presents NEPTUNE, a serverless-based framework that automates the management of large-scale MEC infrastructures. In particular, NEPTUNE provides i) the placement of serverless functions on MEC nodes according to users’ location, ii) the resolution of resource contention scenarios by avoiding that single nodes be saturated, and iii) the dynamic allocation of CPUs and GPUs to meet foreseen execution times. To assess NEPTUNE, we built a prototype based on K3S, an edge-dedicated version of Kubernetes, and executed a comprehensive set of experiments. Results show that NEPTUNE obtains a significant reduction in terms of response time, network overhead, and resource consumption compared to five state-of-the-art solutions.
DA  - 2023/12/04/
PY  - 2023
DO  - 10.1145/3634750
DP  - ACM Digital Library
J2  - ACM Trans. Auton. Adapt. Syst.
SN  - 1556-4665
ST  - NEPTUNE
UR  - https://dl.acm.org/doi/10.1145/3634750
Y2  - 2024/01/03/05:26:07
L1  - https://dl.acm.org/doi/pdf/10.1145/3634750
KW  - edge computing
KW  - control theory
KW  - dynamic resource allocation
KW  - GPU
KW  - k3s
KW  - kubernetes
KW  - placement
KW  - serverless
KW  - vertical scaling
ER  - 

TY  - JOUR
TI  - Performance Testing of a Web Application Using Azure Serverless Functions and Apache JMeter
AU  - Dhalla, Hardeep Kaur
T2  - Journal of Computing Sciences in Colleges
AB  - Software testing is one of the most critical phases in the software development life cycle. It is of utmost importance to learn how to verify and validate a software application. Moreover, the performance of the software is also pivotal for the success of any software application. This tutorial aims to provide participants with a comprehensive understanding of performance testing methodologies using Apache JMeter, with a specific focus on leveraging Azure Serverless Functions as endpoints. I will demonstrate the use of Apache JMeter automation testing tool to create test plans and to generate artificial workload for real-world scenarios of performance testing. Moreover, the participants will learn the impact of resource allocation variations using cloud infrastructure on the performance of web applications.
DA  - 2023/10/01/
PY  - 2023
DP  - ACM Digital Library
VL  - 39
IS  - 3
SP  - 26
J2  - J. Comput. Sci. Coll.
SN  - 1937-4771
UR  - https://dl.acm.org/doi/10.5555/3636988.3636995
ER  - 

TY  - JOUR
TI  - Using Cloud Functions as Accelerator for Elastic Data Analytics
AU  - Bian, Haoqiong
AU  - Sha, Tiannan
AU  - Ailamaki, Anastasia
T2  - Proceedings of the ACM on Management of Data
AB  - Cloud function (CF) services, such as AWS Lambda, have been applied as the new computing infrastructure in implementing analytical query engines. For bursty and sparse workloads, CF-based query engine is more elastic than the traditional query engines running in servers, i.e., virtual machines (VMs), and might provide a higher performance/price ratio. However, it is still controversial whether CF services are good suites for general analytical workloads, in respect of the limitations of CFs in storage, network, and lifetime, as well as the much higher resource unit prices than VMs. In this paper, we first present micro-benchmark evaluations of the features of CF and VM. We reveal that for query processing, though CF is more elastic than VM, it is less scalable and is more expensive for continuous workloads. Then, to get the best of both worlds, we propose Pixels-Turbo - a hybrid query engine that processes queries in a scalable VM cluster by default and invokes CFs to accelerate the processing of unpredictable workload spikes. In the query engine, we propose several optimizations to improve the performance and scalability of the CF-based operators and a cost-based optimizer to select the appropriate algorithm and parallelism for the physical query plan. Evaluations on TPC-H and real-world workload show that our query engine has a 1-2 orders of magnitude higher performance/price ratio than state-of-the-art serverless query engines for sustained workloads while not compromising the elasticity for workload spikes.
DA  - 2023/06/20/
PY  - 2023
DO  - 10.1145/3589306
DP  - ACM Digital Library
VL  - 1
IS  - 2
SP  - 161:1
EP  - 161:27
J2  - Proc. ACM Manag. Data
UR  - https://doi.org/10.1145/3589306
Y2  - 2024/01/03/05:27:19
KW  - data lake
KW  - OLAP
KW  - serverless
KW  - cloud databases
KW  - cloud function
KW  - cloud storage
KW  - column store
KW  - cost efficiency
KW  - data warehouse
KW  - elasticity
KW  - FAAS
KW  - QAAS
KW  - query optimization
KW  - query processing
ER  - 

TY  - JOUR
TI  - The Story of AWS Glue
AU  - Saxena, Mohit
AU  - Sowell, Benjamin
AU  - Alamgir, Daiyan
AU  - Bahadur, Nitin
AU  - Bisht, Bijay
AU  - Chandrachood, Santosh
AU  - Keswani, Chitti
AU  - Krishnamoorthy, G.
AU  - Lee, Austin
AU  - Li, Bohou
AU  - Mitchell, Zach
AU  - Porwal, Vaibhav
AU  - Chappidi, Maheedhar Reddy
AU  - Ross, Brian
AU  - Sekiyama, Noritaka
AU  - Zaki, Omer
AU  - Zhang, Linchi
AU  - Shah, Mehul A.
T2  - Proceedings of the VLDB Endowment
AB  - AWS Glue is Amazon's serverless data integration cloud service that makes it simple and cost effective to extract, clean, enrich, load, and organize data. Originally launched in August 2017, AWS Glue began as an extract-transform-load (ETL) service designed to relieve developers and data engineers of the undifferentiated heavy lifting needed to load databases, data warehouses, and build data lakes on Amazon S3. Since then, it has evolved to serve a larger audience including ETL specialists and data scientists, and includes a broader suite of data integration capabilities. Today, hundreds of thousands of customers use AWS Glue every month. In this paper, we describe the use cases and challenges cloud customers face in preparing data for analytics and the tenets we chose to drive Glue's design. We chose early on to focus on ease-of-use, scale, and extensibility. At its core, Glue offers serverless Apache Spark and Python engines backed by a purpose-built resource manager for fast startup and auto-scaling. In Spark, it offers a new data structure --- DynamicFrames --- for manipulating messy schema-free semi-structured data such as event logs, a variety of transformations and tooling to simplify data preparation, and a new shuffle plugin to offload to cloud storage. It also includes a Hivemetastore compatible Data Catalog with Glue crawlers to build and manage metadata, e.g. for data lakes on Amazon S3. Finally, Glue Studio is its visual interface for authoring Spark and Python-based ETL jobs. We describe the innovations that differentiate AWS Glue and drive its popularity and how it has evolved over the years.
DA  - 2023/08/01/
PY  - 2023
DO  - 10.14778/3611540.3611547
DP  - ACM Digital Library
VL  - 16
IS  - 12
SP  - 3557
EP  - 3569
J2  - Proc. VLDB Endow.
SN  - 2150-8097
UR  - https://doi.org/10.14778/3611540.3611547
Y2  - 2024/01/03/05:27:43
ER  - 

TY  - JOUR
TI  - A Cloud-Based Computing Framework for Artificial Intelligence Innovation in Support of Multidomain Operations
AU  - Robertson, James
AU  - Fossaceca, John M
AU  - Bennett, Kelly W.
T2  - IEEE Transactions on Engineering Management
AB  - The DoD’s artificial intelligence (AI) strategy requires the delivery of transformative and disruptive capabilities that impact the “character of the future battlefield and the pace of threats” that US forces must be prepared to handle. Candidate frameworks must also address key mission areas while enabling partnerships with the private sector, academia, and global allies. To meet these challenges, a flexible, cost-effective, and scalable computing infrastructure that incorporates cutting edge technologies and complies with stringent information assurance requirements is necessary. The DoD AI strategy mandates the agile employment of innovative AI capabilities that “rapidly and iteratively” execute experimentation with new operating concepts, and leverage lessons learned in subsequent experiments. Using cloud computing, we present a flexible approach to solve complex systems problems. Promoting “rapid experimentation” and collaboration on problems such as recursive algorithm implementation, deep learning, and inference in neural networks has enabled inherent advantages over existing computing frameworks. Leveraging the cloud to implement shared responsibility security models, serverless architectures, and high-performance virtual machines, aspects of the AI lifecycle including build, deploy, and monitor have resulted in an adaptable and scalable computing framework that is not only disruptive to the current computing paradigm but also promotes enhanced and productive collaboration.
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/TEM.2021.3088382
DP  - IEEE Xplore
VL  - 69
IS  - 6
SP  - 3913
EP  - 3922
J2  - IEEE Transactions on Engineering Management
SN  - 1558-0040
UR  - https://ieeexplore.ieee.org/document/9497678
Y2  - 2024/01/06/04:31:45
L2  - https://ieeexplore.ieee.org/document/9497678
ER  - 

TY  - JOUR
TI  - A Decentralized Federated Learning Framework via Committee Mechanism With Convergence Guarantee
AU  - Che, Chunjiang
AU  - Li, Xiaoli
AU  - Chen, Chuan
AU  - He, Xiaoyu
AU  - Zheng, Zibin
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this article, we propose a novel serverless federated learning framework Committee Mechanism based Federated Learning (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results.
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/TPDS.2022.3202887
DP  - IEEE Xplore
VL  - 33
IS  - 12
SP  - 4783
EP  - 4800
J2  - IEEE Transactions on Parallel and Distributed Systems
SN  - 1558-2183
UR  - https://ieeexplore.ieee.org/document/9870745
Y2  - 2024/01/06/04:41:17
L1  - https://arxiv.org/pdf/2108.00365
L2  - https://ieeexplore.ieee.org/document/9870745
ER  - 

TY  - JOUR
TI  - A Fast Blockchain-Based Federated Learning Framework With Compressed Communications
AU  - Cui, Laizhong
AU  - Su, Xiaoxin
AU  - Zhou, Yipeng
T2  - IEEE Journal on Selected Areas in Communications
AB  - Recently, blockchain-based federated learning (BFL) has attracted intensive research attention due to that the training process is auditable and the architecture is serverless avoiding the single point failure of the parameter server in vanilla federated learning (VFL). Nevertheless, BFL tremendously escalates the communication traffic volume because all local model updates (i.e., changes of model parameters) obtained by BFL clients will be transmitted to all miners for verification and to all clients for aggregation. In contrast, the parameter server and clients in VFL only retain aggregated model updates. Consequently, the huge communication traffic in BFL will inevitably impair the training efficiency and hinder the deployment of BFL in reality. To improve the practicality of BFL, we are among the first to propose a fast blockchain-based communication-efficient federated learning framework by compressing communications in BFL, called BCFL. Meanwhile, we derive the convergence rate of BCFL with non-convex loss. To maximize the final model accuracy, we further formulate the problem to minimize the training loss of the convergence rate subject to a limited training time with respect to the compression rate and the block generation rate, which is a bi-convex optimization problem and can be efficiently solved. To the end, to demonstrate the efficiency of BCFL, we carry out extensive experiments with standard CIFAR-10 and FEMNIST datasets. Our experimental results not only verify the correctness of our analysis, but also manifest that BCFL can remarkably reduce the communication traffic by 95–98% or shorten the training time by 90–95% compared with BFL.
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/JSAC.2022.3213345
DP  - IEEE Xplore
VL  - 40
IS  - 12
SP  - 3358
EP  - 3372
J2  - IEEE Journal on Selected Areas in Communications
SN  - 1558-0008
UR  - https://ieeexplore.ieee.org/document/9917527
Y2  - 2024/01/06/04:41:51
L1  - https://arxiv.org/pdf/2208.06095
L2  - https://ieeexplore.ieee.org/document/9917527
ER  - 

TY  - CONF
TI  - A Scalable Design Approach for State Propagation in Serverless Workflow
AU  - Bharti, Urmil
AU  - Goel, Anita
AU  - Gupta, S. C.
T2  - 2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)
AB  - Serverless development is challenging as applications are composed of stateless and short-lived functions. Many workflows require time-bound functions to transfer their state to other function before termination. The serverless Function-as-a-Service offerings lack state management support; therefore, it must be handled at application-level. In this paper, we propose a scalable design approach that simplifies development of workflows that require sharing of ephemeral intermediate data. Our design uses object serialization/deserialization with cloud object storage to share state across functions. It provides a mechanism for fine-grained support for state propagation and synchronization in a serverless workflow. This solution is cost-effective and efficient as it does not depend on any external database or cache for state management. The design has been validated by implementing ‘Word Count’- a classic MapReduce use case. Our results show that the proposed scalable design can process input of any size and can handle state propagation in complex serverless workflow.
C3  - 2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/GCAT55367.2022.9972158
DP  - IEEE Xplore
SP  - 1
EP  - 7
UR  - https://ieeexplore.ieee.org/document/9972158
Y2  - 2024/01/06/04:42:48
L2  - https://ieeexplore.ieee.org/document/9972158
KW  - FaaS
KW  - Serverless
KW  - Function-as-a-service
KW  - Serverless workflow
KW  - Work-flows
KW  - Digital storage
KW  - Faas
KW  - Cost effectiveness
KW  - Design
KW  - Design approaches
KW  - MapReduce
KW  - Scalable design
KW  - Serverless composition
KW  - Serverless Composition
KW  - Serverless stateful application
KW  - Serverless Stateful Application
KW  - Serverless Workflows
KW  - State management
KW  - State Management
ER  - 

TY  - CONF
TI  - A Serverless Computing Fabric for Edge & Cloud
AU  - Nastic, Stefan
AU  - Raith, Philipp
AU  - Furutanpey, Alireza
AU  - Pusztai, Thomas
AU  - Dustdar, Schahram
T2  - 2022 IEEE 4th International Conference on Cognitive Machine Intelligence (CogMI)
AB  - Serverless computing has been establishing itself as a compelling paradigm for the development and of modern cloud-native applications. Serverless represents the next step in the evolution of cloud programming models, services and platforms, which is especially appealing due to its low management overhead, easy deployment, scale-to-zero and the promise of optimized costs. Recently, due to the advantages it offers, the serverless paradigm has been growing beyond traditional clouds, making its way to the Edge. The natural evolutionary step for serverless computing is to unify the Edge and the Cloud into what we refer to as Edge-Cloud Continuum. In this paper, we outline our vision of the Serverless Computing Fabric (SCF) for the Edge-Cloud continuum. We introduce the reference architecture for the SCF and show how it unlocks the full potential of the Edge-Cloud continuum. We also discuss main opportunities and challenges, which need to be overcome in order to achieve the vision of the Serverless Computing Fabric. Finally, we introduce key design principles together with core enabling runtime mechanisms, which are intended to serve as a research road map towards the Serverless Computing Fabric for Edge-Cloud continuum.
C3  - 2022 IEEE 4th International Conference on Cognitive Machine Intelligence (CogMI)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/CogMI56440.2022.00011
DP  - IEEE Xplore
SP  - 1
EP  - 12
UR  - https://ieeexplore.ieee.org/document/10063372
Y2  - 2024/01/06/04:45:59
L2  - https://ieeexplore.ieee.org/document/10063372
ER  - 

TY  - CONF
TI  - A Serverless Engine for High Energy Physics Distributed Analysis
AU  - Kuśnierz, Jacek
AU  - Padulano, Vincenzo E.
AU  - Malawski, Maciej
AU  - Burkiewicz, Kamil
AU  - Saavedra, Enric Tejedor
AU  - Alonso-Jordá, Pedro
AU  - Pitt, Michael
AU  - Avati, Valentina
T2  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - The Large Hadron Collider (LHC) at CERN has generated in the last decade an unprecedented volume of data for the High-Energy Physics (HEP) field. Scientific collaborations interested in analysing such data very often require computing power beyond a single machine. This issue has been tackled traditionally by running analyses in distributed environments using stateful, managed batch computing systems. While this approach has been effective so far, current estimates for future computing needs of the field present large scaling challenges. Such a managed approach may not be the only viable way to tackle them and an interesting alternative could be provided by serverless architectures, to enable an even larger scaling potential. This work describes a novel approach to running real HEP scientific applications through a distributed serverless computing engine. The engine is built upon ROOT, a well-established HEP data analysis software, and distributes its computations to a large pool of concurrent executions on Amazon Web Services Lambda Serverless Platform. Thanks to the developed tool, physicists are able to access datasets stored at CERN (also those that are under restricted access policies) and process it on remote infrastructures outside of their typical environment. The analysis of the serverless functions is monitored at runtime to gather performance metrics, both for data- and computation-intensive workloads.
C3  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/CCGrid54584.2022.00067
DP  - IEEE Xplore
SP  - 575
EP  - 584
UR  - https://ieeexplore.ieee.org/document/9826036
Y2  - 2024/01/06/04:47:37
L1  - https://arxiv.org/pdf/2206.00942
L2  - https://ieeexplore.ieee.org/document/9826036
ER  - 

TY  - CONF
TI  - Actor-Oriented Scalable Domain-Specific Cluster Architecture for Cloud-Applications
AU  - Bauer, David Alessandro
AU  - Mäkiö, Juho
T2  - IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society
AB  - Nowadays, applications in the cloud are based on a microservice architecture. Depending on the problem to be solved, they tend to grow complex, and the maintenance is more complicated. For this, a scalable domain-specific (application-related) cluster architecture is conceptualized, which should fulfill the requirements of flexibility, scalability and elasticity, cost-effectiveness, and reliability. Each instance within the cluster covers an application domain. It is possible to upload service subdomains dynamically to an application domain at runtime. A subdomain consists of a group of actors (also called a pod). The development of a subdomain can be assigned to a team. A service subdomain can be scaled individually through replication or sharding (within the instance or the cluster). The proposed solution is expected to achieve a better result than a microservice or FaaS architecture. A single instance prototype was already developed, and a demo application was created.
C3  - IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/IECON49645.2022.9968983
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2577-1647
UR  - https://ieeexplore.ieee.org/document/9968983
Y2  - 2024/01/06/04:48:46
L2  - https://ieeexplore.ieee.org/document/9968983
ER  - 

TY  - CONF
TI  - Adaptive Auto-Scaling of Delay-Sensitive Serverless Services with Reinforcement Learning
AU  - Zhang, Zhiyu
AU  - Wang, Tao
AU  - Li, An
AU  - Zhang, Wenbo
T2  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
AB  - Serverless services such as image recognition and natural language processing have strict response-time constraints. The incoming workloads and resource requirements of a newly deployed serverless service are always unpredictable due to the lack of available historical tracing data. Therefore, making effective auto-scaling decisions for these services is challenging. Open source serverless platforms often work in a best-effort manner, which cannot guarantee the response delay. Moreover, existing studies usually adopt threshold-based methods by configuring additional resource, which cannot well balance the trade-off between the quality of service and resource efficiency. To address the above issues, we propose an adaptive auto-scaling approach for delay-sensitive serverless services with reinforcement learning. First, we characterize the service's resource profile by exploring the performance improvement of different resource allocations with the reinforcement learning method. Then, we propose an adaptive auto-scaling method combining both horizontal and vertical scaling strategies based on the characterized profile to dynamically adjust the resource allocation. Finally, we select three typical services to validate our approach by comparing with two existing state-of-the-art auto-scaling methods. The experimental results show that our approach can accurately characterize services' resource profile, and effectively ensure the response delay constraints while achieving about 10.50% reduction of cost on average.
C3  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/COMPSAC54236.2022.00137
DP  - IEEE Xplore
SP  - 866
EP  - 871
SN  - 0730-3157
UR  - https://ieeexplore.ieee.org/document/9842605
Y2  - 2024/01/06/04:49:21
L2  - https://ieeexplore.ieee.org/document/9842605
ER  - 

TY  - CONF
TI  - AIBLOCK: Blockchain based Lightweight Framework for Serverless Computing using AI
AU  - Golec, Muhammed
AU  - Chowdhury, Deepraj
AU  - Jaglan, Shivam
AU  - Gill, Sukhpal Singh
AU  - Uhlig, Steve
T2  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - Artificial intelligence (AI)-based studies have been carried out recently for the early detection of COVID-19. The goal is to prevent the spread of the disease and the number of fatal cases. In AI-based COVID-19 diagnostic studies, the integrity of the data is critical to obtain reliable results. In this paper, we propose a Blockchain-based framework called AIBLOCK, to offer the data integrity required for applications such as Industry 4.0, healthcare, and online banking. In addition, the proposed framework is integrated with Google Cloud Platform (GCP)-Cloud Functions, a serverless computing platform that automatically manages resources by offering dynamic scalability. The performance of five different machine learning models is evaluated and compared in terms of Accuracy, Precision, Recall, F-Score and Area under the curve (AUC). The experimental results show that decision trees gives the best results in terms of accuracy (98.4 %). Further, it has been identified that utilization of Blockchain technology can increase the load on memory.
C3  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/CCGrid54584.2022.00106
DP  - IEEE Xplore
SP  - 886
EP  - 892
ST  - AIBLOCK
UR  - https://ieeexplore.ieee.org/document/9826025
Y2  - 2024/01/06/04:50:32
L1  - https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/79966/2/Gill%20AIBLOCK%20Blockchain%20based%202022%20Accepted.pdf
L2  - https://ieeexplore.ieee.org/document/9826025
ER  - 

TY  - CONF
TI  - An Architectural Approach for Heterogeneous Data Access in Serverless Platforms
AU  - Sabbioni, Andrea
AU  - Bujari, Armir
AU  - Romeo, Stefano
AU  - Foschini, Luca
AU  - Corradi, Antonio
T2  - GLOBECOM 2022 - 2022 IEEE Global Communications Conference
AB  - The continuous digitalization and application of modern ICT technologies in the Smart City and Tourism domains are paving the way to new, integrated and connected experiences with strong economic and social impact. However, the integration of services and data coming from different providers is hindered by a rapidly evolving ecosystem of tools and techniques, introducing substantial delays and costs in solution design, deployment, testing, and refinement. Serverless computing is a novel cloud computing model where the customers' business logic, structured as lightweight functions, is automatically put into execution in response to an incoming event. This emergent paradigm promises to be an appealing solution, lowering the development and management barrier for the adaptation, integration, and roll-out of services. However, the ephemeral nature of serverless functions clashes with the necessity of creating and maintaining persistent connections to the various data providers. In this work, we present the Serverless Persistence Support (SPS) service, a novel, distributed, and scalable service implementing an adaptation layer able to improve data access performance from serverless functions. To validate our proposal, we conduct a thorough analysis on a realistic testbed, contrasting various data layer supports and assessing SPS performance when compared to the classic approach where connections and operations are executed inside the business logic. The experimental analysis shows that our solution exhibits better performance both in terms of latency and throughput, while also improving resource utilization.
C3  - GLOBECOM 2022 - 2022 IEEE Global Communications Conference
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/GLOBECOM48099.2022.10000963
DP  - IEEE Xplore
SP  - 129
EP  - 134
UR  - https://ieeexplore.ieee.org/document/10000963
Y2  - 2024/01/06/20:50:03
L2  - https://ieeexplore.ieee.org/document/10000963
ER  - 

TY  - CONF
TI  - An Optimization Approach of Container Startup Times for Time-Sensitive Embedded Systems
AU  - Stahlbock, Lukas
AU  - Weber, Jan
AU  - Köster, Frank
T2  - 2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
AB  - Containers are a lightweight virtualization method that is widely adopted in Cloud Environments, Internet of Things (IoT) and embedded devices. Apart from virtualization, containers provide an efficient way of building, storing and distributing layered filesystems for applications. This is one reason why containers became state of the art for service development, deployment and administration in IT systems. Although containers are lightweight, the image layering and container creation add overhead to the application startup time causing a cold start problem in serverless computing. Therefore, the overhead must be kept as small as possible, especially for resource-constrained devices in time-sensitive systems. We present a concept to decrease container startup times and perform analysis on a resource constrained device, a Renesas RCar H3. The study shows that using our concept, the average container startup time can be reduced by up to 64% on target hardware. Furthermore, we evaluate how the solution scales on different hardware setups and determine the impact of container configuration parameters on startup times.
C3  - 2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00300
DP  - IEEE Xplore
SP  - 2019
EP  - 2026
UR  - https://ieeexplore.ieee.org/document/10074694
Y2  - 2024/01/06/20:51:15
L2  - https://ieeexplore.ieee.org/document/10074694
ER  - 

TY  - CONF
TI  - Analysis Of Cloud Computing Security Threats and Countermeasures
AU  - Kumari, Sushila
AU  - Solanki, Kamna
AU  - Dalal, Sandeep
AU  - Dhankhar, Amita
T2  - 2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
AB  - The demand for Cloud Computing (CC) is increasing at a higher growth rate because of the emerging technologies like machine learning, artificial intelligence, big data analytics, IoT, mobile supercomputing, serverless computing, and the industrial need for reducing operational costs. Cloud computing features such as customized offerings, pay-as-you-go or pay-per-use model, availability, seamless scalability, and flexibility have accelerated the demand for cloud services. Despite these many benefits, security is the top challenge in the adoption of cloud services. However, the recent research studies lack the gap between analyzing potential security threats and an exhaustive list of countermeasures proposed for mitigating the threats. The main objective of this paper is to provide a comprehensive view of cloud security threats and to identify potential (more vulnerable) threats. Further, the taxonomy of proposed solutions is provided for data security and privacy on the basis of analysis of research papers in the last decade.
C3  - 2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/ICRITO56286.2022.9964632
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9964632
Y2  - 2024/01/06/20:57:33
L2  - https://ieeexplore.ieee.org/document/9964632
ER  - 

TY  - CONF
TI  - Application Deployment Strategies for Reducing the Cold Start Delay of AWS Lambda
AU  - Dantas, Jaime
AU  - Khazaei, Hamzeh
AU  - Litoiu, Marin
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Serverless computing has emerged in recent years as the new computing paradigm adopted by key players in the industry for software development. This new paradigm has seen rapid growth in adoption due to its unique billing model and scaling characteristics. Public cloud providers such as Amazon Web Services (AWS) offer several configurations and language runtimes for their serverless functions. Although extensively explored by the research community, this field still lacks current studies that address the many challenges developers face when leveraging serverless functions for real-world applications. One of these challenges that are often overseen by many programmers is the cold start problem which is present in any serverless application. For this reason, we propose the first study to characterize the underlying cold start impacts caused by the choice of language runtime, application size, memory size and deployment type on AWS Lambda. In this paper, we analyze the performance of the container-based deployment and ZIP-based deployment of AWS Lambda using a variety of language runtimes and applications running with different function configurations; then we propose guidelines for developers and cloud managers to consider when deploying/managing the workloads on the cloud.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00016
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2159-6190
UR  - https://ieeexplore.ieee.org/document/9860368
Y2  - 2024/01/06/20:58:31
L2  - https://ieeexplore.ieee.org/document/9860368
ER  - 

TY  - JOUR
TI  - Astrea: Auto-Serverless Analytics Towards Cost-Efficiency and QoS-Awareness
AU  - Jarachanthan, Jananie
AU  - Chen, Li
AU  - Xu, Fei
AU  - Li, Bo
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astrea, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21% to 69% under a given budget constraint, while saving cost by 20% to 84% without violating performance requirements.
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/TPDS.2022.3172069
DP  - IEEE Xplore
VL  - 33
IS  - 12
SP  - 3833
EP  - 3849
J2  - IEEE Transactions on Parallel and Distributed Systems
SN  - 1558-2183
ST  - Astrea
UR  - https://ieeexplore.ieee.org/document/9767624
Y2  - 2024/01/06/20:59:50
L2  - https://ieeexplore.ieee.org/document/9767624
ER  - 

TY  - CONF
TI  - AutoDECK: Automated Declarative Performance Evaluation and Tuning Framework on Kubernetes
AU  - Choochotkaew, Sunyanan
AU  - Chiba, Tatsuhiro
AU  - Trent, Scott
AU  - Yoshimura, Takeshi
AU  - Amaral, Marcelo
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Containerization and application variety bring many challenges in automating evaluations for performance tuning and comparison among infrastructure choices. Due to the tightly-coupled design of benchmarks and evaluation tools, the present automated tools on Kubernetes are limited to trivial microbenchmarks and cannot be extended to complex cloudnative architectures such as microservices and serverless, which are usually managed by customized operators for setting up workload dependencies. In this paper, we propose AutoDECK, a performance evaluation framework with a fully declarative manner. The proposed framework automates configuring, deploying, evaluating, summarizing, and visualizing the benchmarking workload. It seamlessly integrates mature Kubernetes-native systems and extends multiple functionalities such as tracking the image-build pipeline, and auto-tuning. We present five use cases of evaluations and analysis through various kinds of bench-marks including microbenchmarks and HPC/AI benchmarks. The evaluation results can also differentiate characteristics such as resource usage behavior and parallelism effectiveness between different clusters. Furthermore, the results demonstrate the benefit of integrating an auto-tuning feature in the proposed framework, as shown by the 10% transferred memory bytes in the Sysbench benchmark.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00053
DP  - IEEE Xplore
SP  - 309
EP  - 314
SN  - 2159-6190
ST  - AutoDECK
UR  - https://ieeexplore.ieee.org/document/9860408
Y2  - 2024/01/06/21:05:15
L2  - https://ieeexplore.ieee.org/document/9860408
ER  - 

TY  - CONF
TI  - Automatic Test Case Generation for Serverless Applications
AU  - Winzinger, Stefan
AU  - Wirtz, Guido
T2  - 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)
AB  - Testing is an important part of software development helping detect faults and gain confidence in the quality of the software. However, serverless computing, a new cloud computing model which emerged with Amazon’s introduction of AWS Lambda, still lacks testing support. In serverless computing, applications consist of several components mainly built around serverless functions whose integration has to be tested. In particular for regression tests and stress testing, many test cases are needed which have to cover different parts of the application. Therefore, we investigated how test cases can be built for serverless applications and what is needed for an automatic test case generation. Based on these requirements represented by a model, we built a tool which is able to create integration tests for some coverage criteria automatically and evaluated their potential on two applications. The results showed that integration test cases can be built for the supported criteria with a high coverage by our tool which can be helpful for regression or stress tests where many different test cases are needed.
C3  - 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)
DA  - 2022/08//
PY  - 2022
DO  - 10.1109/SOSE55356.2022.00015
DP  - IEEE Xplore
SP  - 77
EP  - 84
SN  - 2642-6587
UR  - https://ieeexplore.ieee.org/document/9912636
Y2  - 2024/01/06/21:19:57
L2  - https://ieeexplore.ieee.org/document/9912636
ER  - 

TY  - CONF
TI  - Autoscaling cracker: an efficient asymmetric DDoS attack on serverless functions
AU  - Wang, Dengzhe
AU  - Chen, Xingshu
AU  - Wang, Qixu
AU  - Wang, Shengkai
AU  - Xu, Feiyu
AU  - Zheng, Tao
T2  - GLOBECOM 2022 - 2022 IEEE Global Communications Conference
AB  - Serverless computing has brought new changes to cloud computing. The decoupled serverless functions have more flexible scheduling methods and use resources efficiently with the help of autoscaling. However, it exposes more attack surfaces. If an insecure function becomes a serverless function, a significant security risk will be brought to its service. This paper analyzes the risk of asymmetric DDoS attacks faced by insecure serverless functions. These attacks can occupy a large amount of CPU or memory resources without redundant connections. They can affect the quality of service, delay response time, or even interrupt the service. Autoscaling lacks resilience to such attacks. We test the effects of these attacks in experimental environments and Alibaba Cloud's serverless application engine (SAE). In SAE, we increase the response time from 0.2 seconds to 25 seconds or crash the target function within 6 seconds. Compared with traditional DDoS attacks, asymmetric DDoS attacks are more effective for serverless applications. Finally, we design solutions to mitigate asymmetric DDoS attacks for applications with long and short response times in serverless environments.
C3  - GLOBECOM 2022 - 2022 IEEE Global Communications Conference
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/GLOBECOM48099.2022.10001386
DP  - IEEE Xplore
SP  - 4179
EP  - 4184
ST  - Autoscaling cracker
UR  - https://ieeexplore.ieee.org/document/10001386
Y2  - 2024/01/06/21:20:32
L2  - https://ieeexplore.ieee.org/document/10001386
ER  - 

TY  - CONF
TI  - BabelFish: A Seamless Solution to Communicate with Multi-Lingual Individuals
AU  - Keisel, Clay
AU  - Sajal, Sayeed
T2  - 2022 Intermountain Engineering, Technology and Computing (IETC)
AB  - The goal of our work is to build a tool that allows any number of people to communicate comfortably in their native language with anyone around the globe in real time. As globalization and the mixing of economies grows throughout the world people from different locales will need to constantly be communicating with each other to collaborate and get work done. This collaboration will oftentimes include crucial conversations where language barriers might be an obstacle that prevents efficient work from getting done. We want to demonstrate that a server-less implementation of the MQTT protocol and by leveraging google’s translation engine, that seamless native communication can happen between multiple individuals who speak different languages. To accomplish this goal we will develop a rudimentary chat application which will allow one user to type a message in their native language and have that language translate in transit to the other users preferred language. By offering this feature users from around the globe will be able to feel confident that they can communicate complex thoughts and ideas to anyone anywhere and will be a huge boon to industry as our global economy continues to get more tightly coupled.
C3  - 2022 Intermountain Engineering, Technology and Computing (IETC)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/IETC54973.2022.9796846
DP  - IEEE Xplore
SP  - 1
EP  - 5
ST  - BabelFish
UR  - https://ieeexplore.ieee.org/document/9796846
Y2  - 2024/01/06/21:20:59
L2  - https://ieeexplore.ieee.org/document/9796846
ER  - 

TY  - CONF
TI  - Call Scheduling to Reduce Response Time of a FaaS System
AU  - Zuk, Paweł
AU  - Przybylski, Bartłomiej
AU  - Rzadca, Krzysztof
T2  - 2022 IEEE International Conference on Cluster Computing (CLUSTER)
AB  - In an overloaded FaaS cluster, individual worker nodes strain under lengthening queues of requests. Although the cluster might be eventually horizontally-scaled, adding a new node takes dozens of seconds. As serving applications are tuned for tail serving latencies, and these greatly increase under heavier loads, the current workaround is resource over-provisioning. In fact, even though a service can withstand a steady load of, e.g., 70% CPU utilization, the autoscaler is triggered at, e.g., 30–40% (thus the service uses twice as many nodes as it would be needed). We propose an alternative: a worker-level method handling heavy load without increasing the number of nodes. FaaS executions are not interactive, compared to, e.g., text editors: end-users do not benefit from the CPU allocated to processes often, yet for short periods. Inspired by scheduling methods for High Performance Computing, we take a radical step of replacing the classic OS preemption by (1) queuing requests based on their historical characteristics; (2) once a request is being processed, setting its CPU limit to exactly one core (with no CPU oversubscription). We extend OpenWhisk and measure the efficiency of the proposed solutions using the SeBS benchmark. In a loaded system, our method decreases the average response time by a factor of 4. The improvement is even higher for shorter requests, as the average stretch is decreased by a factor of 18. This leads us to show that we can provide better response-time statistics with 3 machines compared to a 4-machine baseline.
C3  - 2022 IEEE International Conference on Cluster Computing (CLUSTER)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/CLUSTER51413.2022.00031
DP  - IEEE Xplore
SP  - 172
EP  - 182
SN  - 2168-9253
UR  - https://ieeexplore.ieee.org/document/9912714
Y2  - 2024/01/06/21:22:12
L1  - https://arxiv.org/pdf/2207.13168
L2  - https://ieeexplore.ieee.org/document/9912714
ER  - 

TY  - CONF
TI  - CardioHPC: Serverless Approaches for Real-Time Heart Monitoring of Thousands of Patients
AU  - Gusev, Marjan
AU  - Ristov, Sashko
AU  - Amza, Andrei
AU  - Hohenegger, Armin
AU  - Prodan, Radu
AU  - Mileski, Dimitar
AU  - Gushev, Pano
AU  - Temelkov, Goran
T2  - 2022 IEEE/ACM Workshop on Workflows in Support of Large-Scale Science (WORKS)
AB  - We analyze a heart monitoring center for patients wearing electrocardiogram sensors outside hospitals. This prevents serious heart damages and increases life expectancy and health-care efficiency. In this paper, we address a problem to provide a scalable infrastructure for the real-time processing scenario for at least 10,000 patients simultaneously, and efficient fast processing architecture for the postponed scenario when patients upload data after realized measurements. CardioHPC is a project to realize a simulation of these two scenarios using digital signal processing algorithms and artificial intelligence-based detection and classification software for automated reporting and alerting. We elaborate the challenges we met in experimenting with different serverless implementations: 1) container-based on Google Cloud Run, and 2) Function-as-a-Service (FaaS) on AWS Lambda. Experimental results present the effect of overhead in the request and transfer time, and speedup achieved by analyzing the response time and throughput on both container-based and FaaS implementations as serverless workflows.
C3  - 2022 IEEE/ACM Workshop on Workflows in Support of Large-Scale Science (WORKS)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/WORKS56498.2022.00015
DP  - IEEE Xplore
SP  - 76
EP  - 83
ST  - CardioHPC
UR  - https://ieeexplore.ieee.org/document/10023939
Y2  - 2024/01/06/21:24:33
L2  - https://ieeexplore.ieee.org/document/10023939
ER  - 

TY  - JOUR
TI  - CharmSeeker: Automated Pipeline Configuration for Serverless Video Processing
AU  - Zhang, Miao
AU  - Zhu, Yifei
AU  - Liu, Jiangchuan
AU  - Wang, Feng
AU  - Wang, Fangxin
T2  - IEEE/ACM Transactions on Networking
AB  - Video processing plays an essential role in a wide range of cloud-based applications. It typically involves multiple pipelined stages, which well fits the latest fine-grained serverless computing paradigm if properly configured to match the cost and delay constraints of video. Existing configuration tools, however, are primarily developed for traditional virtual machine clusters with general workloads. This paper presents CharmSeeker, an automated configuration tuning tool for serverless video processing pipelines. We first carefully examine the key steps and the performance bottlenecks for video processing over modern serverless platforms. Then, we identify the configuration space for processing pipelines and leverage a carefully designed Sequential Bayesian Optimization search scheme to identify promising configurations. We further address the practical challenges toward integrating our solution into real-world systems and develop a prototype with AWS Lambda. Evaluation results show that CharmSeeker can find out the optimal or near-optimal configurations that improve the relative processing time up to 408.77%. It is also more robust and scalable to various video processing pipelines compared with state-of-the-art solutions.
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/TNET.2022.3183231
DP  - IEEE Xplore
VL  - 30
IS  - 6
SP  - 2730
EP  - 2743
J2  - IEEE/ACM Transactions on Networking
SN  - 1558-2566
ST  - CharmSeeker
UR  - https://ieeexplore.ieee.org/document/9802908
Y2  - 2024/01/06/21:26:42
L2  - https://ieeexplore.ieee.org/document/9802908
ER  - 

TY  - CONF
TI  - Cloud-based Serverless Solution for Facilitating the Organisation of Athletics Competitions
AU  - Nagy, Tifani Franciska
AU  - Csibi, Zsolt
AU  - Jánosi, Borbála
AU  - Simon, Károly
AU  - Hegedüs, Hunor
AU  - Szász, Erika
T2  - 2022 IEEE 20th Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
AB  - The Athletimeter project was inspired by the annually held Béla Török Memorial athletics competition in Odorheiu Secuiesc, Romania, which brings together children from the surrounding areas to compete. So far, the organizers have not used any digital solution to maintain the data related to the competition. It was managed on paper, which did not provide the necessary transparency and secure maintenance for the organizers, and there was no opportunity for the spectators to follow the results. Therefore, the authors' aim was to develop a cloud-based system that could facilitate the data management needed to run an athletics competition by digitizing the process. The system provides a web and a mobile interface to its users, and is based on a cloud-based server that introduces the concept of serverless architecture. The server is only accessible during the competition, so the amount spent on resources is significantly reduced. Through the web interface, organizers can manage age groups, contestants, events, and results. In addition, the platform allows spectators to track the results of the different events in real-time. The mobile application provides an efficient alternative for recording results and identifying contestants. It is equipped with a built-in stopwatch to record the results of time-based events and a QR code reader that can associate a competitor with a result by scanning the code on the jersey. The paper aims to present the system, detailing its functionalities, architecture, and different components, as well as the technologies and tools used.
C3  - 2022 IEEE 20th Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/SISY56759.2022.10036272
DP  - IEEE Xplore
SP  - 000145
EP  - 000150
SN  - 1949-0488
UR  - https://ieeexplore.ieee.org/document/10036272
Y2  - 2024/01/06/21:28:23
L2  - https://ieeexplore.ieee.org/document/10036272
ER  - 

TY  - CONF
TI  - Clustering-Based Serverless Edge Computing Assisted Federated Learning for Energy Procurement
AU  - Zou, Luyao
AU  - Munir, Md. Shirajum
AU  - Tun, Ye Lin
AU  - Hong, Choong Seon
T2  - 2022 23rd Asia-Pacific Network Operations and Management Symposium (APNOMS)
AB  - Prosumers nowadays are capable of consuming and generating renewable energy along with providing charging services for public electric vehicles (EVs) through EV support equipment (EVSE). However, the energy demand of prosumers and EVs as well as the renewable energy generation of prosumers have uncertain nature, which causes difficulty for each prosumer to purchase the proper energy at a lower price in advance. Thus, it is paramount important to do energy procurement prediction (EPP) for each prosumer. Nevertheless, submitting data from each prosumer to a centralized server for EPP will result in communication delay and need to consume a huge amount of network bandwidth and energy. Therefore, in this paper, a clustering-based serverless edge computing-assisted federated learning (FL) approach is proposed for EPP, where the objective is to minimize the Huber loss between the predicted and the real value per prosumer. In particular, firstly, normalized Laplacian-based spectral clustering is leveraged to group the prosumers with a similar energy procurement pattern to solve the problem of biased energy procurement forecast caused by updating the model among all the clients. Secondly, long short-term memory (LSTM) in the federated learning setting is utilized to train the global model of each clustered group, where the model aggregation occurs in the serverless edge computing ability-enhanced local edge server with the best performance. The evaluation results demonstrate the proposed method can achieve the lowest Huber loss compared with the baseline methods.
C3  - 2022 23rd Asia-Pacific Network Operations and Management Symposium (APNOMS)
DA  - 2022/09//
PY  - 2022
DO  - 10.23919/APNOMS56106.2022.9919944
DP  - IEEE Xplore
SP  - 01
EP  - 06
SN  - 2576-8565
UR  - https://ieeexplore.ieee.org/document/9919944
Y2  - 2024/01/06/21:29:31
L2  - https://ieeexplore.ieee.org/document/9919944
ER  - 

TY  - JOUR
TI  - Code Layering for the Detection of Network Covert Channels in Agentless Systems
AU  - Zuppelli, Marco
AU  - Repetto, Matteo
AU  - Schaffhauser, Andreas
AU  - Mazurczyk, Wojciech
AU  - Caviglione, Luca
T2  - IEEE Transactions on Network and Service Management
AB  - The growing interest in agentless and serverless environments for the implementation of virtual/container network functions makes monitoring and inspection of network services challenging tasks. A major requirement concerns the agility of deploying security agents at runtime, especially to effectively address emerging and advanced attack patterns. This work investigates a framework leveraging the extended Berkeley Packet Filter to create ad-hoc security layers in virtualized architectures without the need of embedding additional agents. To prove the effectiveness of the approach, we focus on the detection of network covert channels, i.e., hidden/parasitic network conversations difficult to spot with legacy mechanisms. Experimental results demonstrate that different types of covert channels can be revealed with a good accuracy while using limited resources compared to existing cybersecurity tools (i.e., Zeek and libpcap).
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/TNSM.2022.3176752
DP  - IEEE Xplore
VL  - 19
IS  - 3
SP  - 2282
EP  - 2294
J2  - IEEE Transactions on Network and Service Management
SN  - 1932-4537
UR  - https://ieeexplore.ieee.org/document/9779347
Y2  - 2024/01/06/21:31:54
L1  - https://ieeexplore.ieee.org/ielx7/4275028/9917435/09779347.pdf
L2  - https://ieeexplore.ieee.org/document/9779347
ER  - 

TY  - CONF
TI  - Creation of serverless applications in the cloud
AU  - Rodríguez, Mario Alberto Negrete
AU  - Martínez, Felipe Uriel Infante
T2  - 2022 11th International Conference On Software Process Improvement (CIMPS)
AB  - Serverless computing has gained much importance in the last decade, offering features such as cost reduction, low latency, scalability, and avoiding server administration [1]. Serverless computing was introduced in 2014 by Amazon with lambdas. Later it was adopted by other providers [2]. This model allows developers to focus on the application logic, because a traditional architecture contains a server as a monolithic system, while serverless works by events being different pieces of software working independently. This article presents the advantages and disadvantages of working with a cloud server from the perspective of quality development.
C3  - 2022 11th International Conference On Software Process Improvement (CIMPS)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/CIMPS57786.2022.10035685
DP  - IEEE Xplore
SP  - 216
EP  - 218
UR  - https://ieeexplore.ieee.org/document/10035685
Y2  - 2024/01/06/21:32:51
L2  - https://ieeexplore.ieee.org/document/10035685
ER  - 

TY  - CONF
TI  - CrossFit: Fine-grained Benchmarking of Serverless Application Performance across Cloud Providers
AU  - Scheuner, Joel
AU  - Deng, Rui
AU  - Steghöfer, Jan-Philipp
AU  - Leitner, Philipp
T2  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
AB  - Serverless computing emerged as a promising cloud computing paradigm for deploying cloud-native applications but raises new performance challenges. Existing performance evaluation studies focus on micro-benchmarking to measure an individual aspect of serverless functions, such as CPU speed, but lack an in-depth analysis of differences in application performance across cloud providers. This paper presents CrossFit, an approach for detailed and fair cross-provider performance benchmarking of serverless applications based on a providerindependent tracing model. Our case study demonstrates how detailed distributed tracing enables drill-down analysis to explain performance differences between two leading cloud providers, AWS and Azure. The results for an asynchronous application show that trigger time contributes most delay to the end-to-end latency and explains the main performance difference between cloud providers. Our results further reveal how increasing and bursty workloads affect performance stability, median latency, and tail latency.
C3  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/UCC56403.2022.00016
DP  - IEEE Xplore
SP  - 51
EP  - 60
ST  - CrossFit
UR  - https://ieeexplore.ieee.org/document/10061777
Y2  - 2024/01/06/21:41:47
L2  - https://ieeexplore.ieee.org/document/10061777
ER  - 

TY  - CONF
TI  - DataX Allocator: Dynamic resource management for stream analytics at the Edge
AU  - Benedetti, Priscilla
AU  - Coviello, Giuseppe
AU  - Rao, Kunal
AU  - Chakradhar, Srimat
T2  - 2022 9th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)
AB  - Serverless edge computing aims to deploy and manage applications so that developers are unaware of challenges associated with dynamic management, sharing, and maintenance of the edge infrastructure. However, this is a non-trivial task because the resource usage by various edge applications varies based on the content in their input sensor data streams. We present a novel reinforcement-learning (RL) technique to maximize the processing rates of applications by dynamically allocating resources (like CPU cores or memory) to microservices in these applications. We model applications as analytics pipelines consisting of several microservices, and a pipeline’s processing rate directly impacts the accuracy of insights from the application. In our unique problem formulation, the state space or the number of actions of RL is independent of the type of workload in the microservices, the number of microservices in a pipeline, or the number of pipelines. This enables us to learn the RL model only once and use it many times to improve the accuracy of insights for a diverse set of AI/ML engines like action recognition or face recognition and applications with varying microservices.Our experiments with real-world applications, i.e., face recognition and action recognition, show that our approach outperforms other widely-used alternative approaches and achieves up to 2.5X improvement in the overall application processing rate. Furthermore, when we apply our RL model trained on a face recognition pipeline to a different and more complex action recognition pipeline, we obtain a 2X improvement in processing rate, thus showing the versatility and robustness of our RL model to pipeline changes.
C3  - 2022 9th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/IOTSMS58070.2022.10061998
DP  - IEEE Xplore
SP  - 1
EP  - 8
SN  - 2832-3033
ST  - DataX Allocator
UR  - https://ieeexplore.ieee.org/document/10061998
Y2  - 2024/01/06/21:42:12
L2  - https://ieeexplore.ieee.org/document/10061998
ER  - 

TY  - CONF
TI  - DayDream: Executing Dynamic Scientific Workflows on Serverless Platforms with Hot Starts
AU  - Roy, Rohan Basu
AU  - Patel, Tirthak
AU  - Tiwari, Devesh
T2  - SC22: International Conference for High Performance Computing, Networking, Storage and Analysis
AB  - HPC applications are increasingly being designed as dynamic workflows for the ease of development and scaling. This work demonstrates how the serverless computing model can be leveraged for efficient execution of complex, real-world scientific workflows, although serverless computing was not originally designed for executing scientific workflows. This work characterizes, quantifies, and improves the execution of three real-world, complex, dynamic scientific workflows: ExaFEL (workflow for investigating the molecular structures via X-Ray diffraction), Cosmoscout-Vr(workflow for large scale virtual reality simulation), and Core Cosmology Library (a cosmology workflow for investigating dark matter). The proposed technique, DayDream, employs the hot start mechanism for warming up the components of the workflows by decoupling the runtime environment from the component function code to mitigate cold start overhead. DayDream optimizes the service time and service cost jointly to reduce the service time by 45% and service cost by 23% over the state-of-the-art HPC workload manager.
C3  - SC22: International Conference for High Performance Computing, Networking, Storage and Analysis
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/SC41404.2022.00027
DP  - IEEE Xplore
SP  - 1
EP  - 18
SN  - 2167-4337
ST  - DayDream
UR  - https://ieeexplore.ieee.org/document/10046081
Y2  - 2024/01/06/21:46:33
L2  - https://ieeexplore.ieee.org/document/10046081
ER  - 

TY  - JOUR
TI  - Dependent Function Embedding for Distributed Serverless Edge Computing
AU  - Deng, Shuiguang
AU  - Zhao, Hailiang
AU  - Xiang, Zhengzhe
AU  - Zhang, Cheng
AU  - Jiang, Rong
AU  - Li, Ying
AU  - Yin, Jianwei
AU  - Dustdar, Schahram
AU  - Zomaya, Albert Y.
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - Edge computing is booming as a promising paradigm to extend service provisioning from the centralized cloud to the network edge. Benefit from the development of serverless computing, an edge server can be configured as a carrier of limited serverless functions, in the way of deploying Docker runtime and Kubernetes engine. Meanwhile, an application generally takes the form of directed acyclic graphs (DAGs), where vertices represent dependent functions and edges represent data traffic. The status quo of minimizing the completion time (a.k.a. makespan) of the application motivates the study on optimal function placement. However, current approaches lose sight of proactively splitting and mapping the traffic to the logical data paths between the heterogeneous edge servers, which could affect the makespan significantly. To remedy that, we propose an algorithm, termed as Dependent Function Embedding (DPE), to get the optimal edge server for each function to execute and the moment it starts executing. DPE finds the best segmentation of each data traffic by exquisitely solving several infinity norm minimization problems. DPE is theoretically verified to achieve the global optimality. Extensive experiments on Alibaba cluster trace show that DPE significantly outperforms two baseline algorithms in makespan by 43.19% and 40.71%, respectively.
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/TPDS.2021.3137380
DP  - IEEE Xplore
VL  - 33
IS  - 10
SP  - 2346
EP  - 2357
J2  - IEEE Transactions on Parallel and Distributed Systems
SN  - 1558-2183
UR  - https://ieeexplore.ieee.org/document/9665233
Y2  - 2024/01/06/21:49:47
L2  - https://ieeexplore.ieee.org/document/9665233
ER  - 

TY  - CONF
TI  - Development of Low Cost 3 Phases IoT Electrical Power Meter
AU  - Yodjaiphet, Anusorn
AU  - Ayutaya, Ronnachai Sretawat Na
AU  - Chansareewittaya, Suppakarn
T2  - 2022 6th International Conference on Information Technology (InCIT)
AB  - In this paper, 3 phases of IoT electrical power meter is developed. The aim of this development is to create a 3 phases low-cost meter with IoT that can measure the true RMS value. The design of both hardware and software focuses on easy design, fewer components, uncomplicated processing, power component values, and using calculations according to numerical calculation method according to the basic theoretical principles. The dsPIC30F4011 16-bits microcontroller is used as the main microcontroller. The ESP8266 is used as the transmitting device. The information of the measurement is sent from dsPIC30F4011 to the Thinkspeak platform due to the serverless concept. After that, the information is displayed on the website via the dashboard. By comparing with the commercial product, this DIY is cheaper and has more flexible that the user can adjust the function and dash board as wish.
C3  - 2022 6th International Conference on Information Technology (InCIT)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/InCIT56086.2022.10067539
DP  - IEEE Xplore
SP  - 78
EP  - 82
UR  - https://ieeexplore.ieee.org/document/10067539
Y2  - 2024/01/06/21:50:09
L2  - https://ieeexplore.ieee.org/document/10067539
ER  - 

TY  - CONF
TI  - DGSF: Disaggregated GPUs for Serverless Functions
AU  - Fingler, Henrique
AU  - Zhu, Zhiting
AU  - Yoon, Esther
AU  - Jia, Zhipeng
AU  - Witchel, Emmett
AU  - Rossbach, Christopher J.
T2  - 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - Ease of use and transparent access to elastic resources have attracted many applications away from traditional platforms toward serverless functions. Many of these applications, such as machine learning, could benefit significantly from GPU acceleration. Unfortunately, GPUs remain inaccessible from serverless functions in modern production settings. We present DGSF, a platform that transparently enables serverless functions to use GPUs through general purpose APIs such as CUDA. DGSF solves provisioning and utilization challenges with disaggregation, serving the needs of a potentially large number of functions through virtual GPUs backed by a small pool of physical GPUs on dedicated servers. Disaggregation allows the provider to decouple GPU provisioning from other resources, and enables significant benefits through consolidation. We describe how DGSF solves GPU disaggregation challenges including supporting API transparency, hiding the latency of communication with remote GPUs, and load-balancing access to heavily shared GPUs. Evaluation of our prototype on six workloads shows that DGSF's API remoting optimizations can improve the runtime of a function by up to 50% relative to unoptimized DGSF. Such optimizations, which aggressively remove GPU runtime and object management latency from the critical path, can enable functions running over DGSF to have a lower end-to-end time than when running on a GPU natively. By enabling GPU sharing, DGSF can reduce function queueing latency by up to 53%. We use DGSF to augment AWS Lambda with GPU support, showing similar benefits.
C3  - 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/IPDPS53621.2022.00077
DP  - IEEE Xplore
SP  - 739
EP  - 750
SN  - 1530-2075
ST  - DGSF
UR  - https://ieeexplore.ieee.org/document/9820659
Y2  - 2024/01/06/21:54:18
L2  - https://ieeexplore.ieee.org/document/9820659
ER  - 

TY  - CONF
TI  - Distributed Denial of Wallet Attack on Serverless Pay-as-you-go Model
AU  - Mileski, Dimitar
AU  - Mihajloska, Hristina
T2  - 2022 30th Telecommunications Forum (℡FOR)
AB  - The serverless pay-as-you-go model in the cloud enables payment of services during execution and resources used at the smallest, most granular level, as was the initial idea when setting the foundations and concepts of the pay-as-you-go model in the cloud. The disadvantage of this method of payment during execution and the resources used is that it is subject to financial damage if we have an attack on serverless services. This paper defines notions for three types of attacks that can cause financial damage to the serverless pay-as-you-go model and are experimentally validated. The first attack is Blast DDoW - Distributed Denial of Wallet, the second attack is Continual Inconspicuous DDoW, and the third one is Background Chained DDoW. We discussed financial damages and the consequences of each type of attack.
C3  - 2022 30th Telecommunications Forum (℡FOR)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/℡FOR56187.2022.9983732
DP  - IEEE Xplore
SP  - 1
EP  - 4
UR  - https://ieeexplore.ieee.org/document/9983732
Y2  - 2024/01/06/21:54:36
L2  - https://ieeexplore.ieee.org/document/9983732
ER  - 

TY  - JOUR
TI  - Distributed Task Scheduling in Serverless Edge Computing Networks for the Internet of Things: A Learning Approach
AU  - Tang, Qinqin
AU  - Xie, Renchao
AU  - Yu, Fei Richard
AU  - Chen, Tianjiao
AU  - Zhang, Ran
AU  - Huang, Tao
AU  - Liu, Yunjie
T2  - IEEE Internet of Things Journal
AB  - By delegating the infrastructure management, such as provisioning or scaling to third-party providers, serverless edge computing has recently been widely adopted in several applications, especially Internet of Things (IoT) applications. Task scheduling is a critical issue in serverless edge computing as it significantly impacts the quality of user experience. In contrast to the centralized scheduling in the cloud center, serverless edge task scheduling is more challenging due to the heterogeneous and resource-constrained nature of edge resources. This article aims to study the distributed task scheduling for the IoT in serverless edge computing networks, in which heterogeneous serverless edge computing nodes are rational individuals with interests to optimize their own scheduling utility while the nodes only have access to local observations. The task scheduling competition process is formulated as a partially observable stochastic game (POSG) to enable serverless edge computing nodes to noncooperatively schedule tasks and allocate computing resources depending on their locally observed system state, which takes into account the associated task generation state, data queue state, communication channel state, and previous computing resource allocation state. To solve the proposed POSG and deal with the partial observability, a multiagent task scheduling algorithm based on the dueling double deep recurrent Q -network (D3RQN) method is developed to approximate the optimal task scheduling and resource allocation solution. Finally, extensive simulation experiments are conducted to validate the effectiveness and superiority of the proposed scheme.
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/JIOT.2022.3167417
DP  - IEEE Xplore
VL  - 9
IS  - 20
SP  - 19634
EP  - 19648
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - Distributed Task Scheduling in Serverless Edge Computing Networks for the Internet of Things
UR  - https://ieeexplore.ieee.org/document/9757233
Y2  - 2024/01/06/21:55:12
L2  - https://ieeexplore.ieee.org/document/9757233
ER  - 

TY  - CONF
TI  - DSServe - Data Science using Serverless
AU  - Patel, Dhaval
AU  - Lin, Shuxin
AU  - Kalagnanam, Jayant
T2  - 2022 IEEE International Conference on Big Data (Big Data)
AB  - AI Applications uses various data science tools such as Jupyter notebook to prescribe a series of steps, commonly referred as workflow, for building AI Solutions. The steps in workflow can be as simple as loading the data from remote storage, visualize the data for better understanding or conducting data quality study, or it can be as complex as generating features for modeling, best model discovery processes, etc. Clearly, different steps of the data science workflow has varying requirement of compute resources. Moreover, the execution of steps in workflow are Adhoc and Subjective. With wider availability of various Serverless technology, in this paper, we demonstrate a generalized framework that can be used to provide on demand scale out capability for the Data Science Workflow. In particular, we selected the most common AI operation, namely Automatic Model Selection, as an example to demonstrate benefits of serverless computing. We conducted a detailed experimental results using IBM Code Engine technology to validate the benefits of our proposed approach.
C3  - 2022 IEEE International Conference on Big Data (Big Data)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/BigData55660.2022.10020441
DP  - IEEE Xplore
SP  - 2343
EP  - 2345
UR  - https://ieeexplore.ieee.org/document/10020441
Y2  - 2024/01/06/21:55:53
L2  - https://ieeexplore.ieee.org/document/10020441
KW  - Work-flows
KW  - Digital storage
KW  - Remote storage
KW  - On demands
KW  - AI applications
KW  - Best model
KW  - Compute resources
KW  - Data quality
KW  - Data Science
KW  - Model discoveries
KW  - Science tools
KW  - Simple++
ER  - 

TY  - CONF
TI  - EdgeFaaSBench: Benchmarking Edge Devices Using Serverless Computing
AU  - Rajput, Kaustubh Rajendra
AU  - Kulkarni, Chinmay Dilip
AU  - Cho, Byungjin
AU  - Wang, Wei
AU  - Kim, In Kee
T2  - 2022 IEEE International Conference on Edge Computing and Communications (EDGE)
AB  - Due to the development of small-size, energy-efficient, and powerful CPUs and GPUs for single board computers, various edge devices are widely adopted for hosting real-world applications, including real-time object detection, autonomous driving, and sensor stream processing. At the same time, serverless computing receives increasing attention as a new application deployment model because of its simplicity, scalability, event-driven processing, and short-lived computation. Therefore, there is a growing demand for applying serverless computing to edge computing environments. However, due to the lack of characterization of serverless edge computing (e.g., application performance and impact from resource heterogeneity), researchers and practitioners have to conduct tedious measurements to understand the performance of serverless applications on edge devices in non-systematic ways.We create EdgeFaaSBench, a novel benchmark suite for serverless computing on edge devices, to bridge this gap. EdgeFaaSBench is developed on top of Apache OpenFaaS with Docker Swarm and can run various serverless benchmark workloads on edge devices with different hardware specifications (e.g., GPUs). EdgeFaaSBench contains 14 different benchmark workloads running on heterogeneous edge devices and captures various system-level, application-level, and serverless-specific metrics, including system utilization, response time, cold/warm start times, and impact of concurrent function executions. Experimental studies are conducted on two widely used edge devices, Raspberry Pi 4B and Jetson Nano, to show EdgeFaaSBench’s capabilities to benchmark serverless computing on edge devices.
C3  - 2022 IEEE International Conference on Edge Computing and Communications (EDGE)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/EDGE55608.2022.00024
DP  - IEEE Xplore
SP  - 93
EP  - 103
SN  - 2767-9918
ST  - EdgeFaaSBench
UR  - https://ieeexplore.ieee.org/document/9860334
Y2  - 2024/01/06/21:56:48
L2  - https://ieeexplore.ieee.org/document/9860334
ER  - 

TY  - CONF
TI  - Energy-Aware Resource Scheduling for Serverless Edge Computing
AU  - Aslanpour, Mohammad Sadegh
AU  - Toosi, Adel N.
AU  - Cheema, Muhammad Aamir
AU  - Gaire, Raj
T2  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - In this paper, we present energy-aware scheduling for Serverless edge computing. Energy awareness is critical since edge nodes, in many Internet of Things (IoT) domains, are meant to be powered by renewable energy sources that are variable, making low-powered and/or overloaded (bottleneck) nodes unavailable and not operating their services. This awareness is also required since energy challenges have not been previously addressed by Serverless, largely due to its origin in cloud computing. To achieve this, we formally model an energy-aware resource scheduling problem in Serverless edge computing, given a cluster of battery-operated and renewable-energy powered nodes. Then, we devise zone-oriented and priority-based algorithms to improve the operational availability of bottleneck nodes. As assets, our algorithm coins terms “sticky offloading” and “warm scheduling” in the interest of the Quality of Service (QoS). We evaluate our proposal against well-known benchmarks using real-world implementations on a cluster of Raspberry Pis enabled with container orchestration, Kubernetes, and Serverless computing, OpenFaaS, where edge nodes are powered by real-world solar irradiation. Experimental results achieve significant improvements, up to 33%, in helping bottleneck node's operational availability while preserving the QoS. With energy awareness, now Serverless can unconditionally offer its resource efficiency and portability at the edge.
C3  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/CCGrid54584.2022.00028
DP  - IEEE Xplore
SP  - 190
EP  - 199
UR  - https://ieeexplore.ieee.org/document/9826101
Y2  - 2024/01/06/22:00:39
L2  - https://ieeexplore.ieee.org/document/9826101
ER  - 

TY  - CONF
TI  - Exploring Tradeoffs in Federated Learning on Serverless Computing Architectures
AU  - Baughman, Matt
AU  - Foster, Ian
AU  - Chard, Kyle
T2  - 2022 IEEE 18th International Conference on e-Science (e-Science)
AB  - Federated learning is driving the development of new techniques to efficiently and securely use data across multiple sites while using diverse resources. One of these techniques is the use of the serverless computing paradigm to abstract away resource specific configurations, allowing federated learning across heterogeneous environments. However, deploying federated learning across edge resources, the cloud, and traditional HPC sites will require specialized approaches in order to best account for the weaknesses and strengths of each resource. In this work, we explore the new tradeoffs presented by managing a federated learning task across heterogeneous resources and demonstrate these tradeoffs with experiments using a serverless federated learning framework.
C3  - 2022 IEEE 18th International Conference on e-Science (e-Science)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/eScience55777.2022.00074
DP  - IEEE Xplore
SP  - 433
EP  - 434
UR  - https://ieeexplore.ieee.org/document/9973734
Y2  - 2024/01/06/22:24:02
L1  - https://zenodo.org/records/7160310/files/escience_poster_pres.pdf
L2  - https://ieeexplore.ieee.org/document/9973734
ER  - 

TY  - CONF
TI  - FaDO: FaaS Functions and Data Orchestrator for Multiple Serverless Edge-Cloud Clusters
AU  - Smith, Christopher Peter
AU  - Jindal, Anshul
AU  - Chadha, Mohak
AU  - Gerndt, Michael
AU  - Benedict, Shajulin
T2  - 2022 IEEE 6th International Conference on Fog and Edge Computing (ICFEC)
AB  - Function-as-a-Service (FaaS) is an attractive cloud computing model that simplifies application development and deployment. However, current serverless compute platforms do not consider data placement when scheduling functions. With the growing demand for edge-cloud continuum, multi-cloud, and multi-serverless applications, this flaw means serverless technologies are still ill-suited to latency-sensitive operations like media streaming. This work proposes a solution by presenting a tool called FaDO: FaaS Functions and Data Orchestrator, designed to allow data-aware functions scheduling across multi-serverless compute clusters present at different locations, such as at the edge and in the cloud. FaDO works through header-based HTTP reverse proxying and uses three load-balancing algorithms: 1) The Least Connections, 2) Round Robin, and 3) Random for load balancing the invocations of the function across the suitable serverless compute clusters based on the set storage policies. FaDO further provides users with an abstraction of the serverless compute cluster’s storage, allowing users to interact with data across different storage services through a unified interface. In addition, users can configure automatic and policy-aware granular data replications, causing FaDO to spread data across the clusters while respecting location constraints. Load testing results show that it is capable of load balancing high-throughput workloads, placing functions near their data without contributing any significant performance overhead.
C3  - 2022 IEEE 6th International Conference on Fog and Edge Computing (ICFEC)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/ICFEC54809.2022.00010
DP  - IEEE Xplore
SP  - 17
EP  - 25
ST  - FaDO
UR  - https://ieeexplore.ieee.org/document/9799194
Y2  - 2024/01/06/22:48:05
L2  - https://ieeexplore.ieee.org/document/9799194
KW  - Serverless
KW  - Function-as-a-service
KW  - Edge computing
KW  - Orchestration
KW  - Scheduling
KW  - Digital storage
KW  - Edge clouds
KW  - Data-aware
KW  - Edge-computing
KW  - Load-Balancing
KW  - Media streaming
KW  - Multi-cloud
KW  - Multi-clouds
KW  - Service data
KW  - Service functions
ER  - 

TY  - CONF
TI  - Function Memory Optimization for Heterogeneous Serverless Platforms with CPU Time Accounting
AU  - Cordingly, Robert
AU  - Xu, Sonia
AU  - Lloyd, Wes
T2  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Serverless Function-as-a-Service (FaaS) platforms often abstract the underlying infrastructure configuration into the single option of specifying a function's memory reservation size. This resource abstraction of coupling configurations options (e.g. vCPUs, memory, disk), combined with the lack of profiling, leaves developers to make ad hoc decisions on how to configure functions. Solutions are needed to mitigate exhaustive brute force searches of large parameter input spaces to find optimal configurations which can incur high costs. To address these challenges, we propose CPU Time Accounting Memory Selection (CPU-TAMS). CPU-TAMS is a workload agnostic memory selection method that utilizes CPU time accounting principles and regression modeling to recommend memory settings that reduce function runtime and subsequently, cost. Comparing CPU-TAMS to eight existing selection methods, we find that CPU-TAMS finds maximum value memory settings with only 8% runtime and 5% cost error compared to brute force testing while only requiring a single profiling run to evaluate function resource requirements. We adapt CPU-TAMS for use on four commercial FaaS platforms demonstrating efficacy to optimize function memory configurations where platforms feature heterogeneous infrastructure management policies.
C3  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/IC2E55432.2022.00019
DP  - IEEE Xplore
SP  - 104
EP  - 115
UR  - https://ieeexplore.ieee.org/document/9946331
Y2  - 2024/01/06/22:48:24
KW  - Serverless Computing
KW  - Serverless computing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - Service platforms
KW  - Function evaluation
KW  - Performance Evaluation
KW  - Performances evaluation
KW  - CPU time
KW  - Memory optimization
KW  - Performance Modeling
KW  - Resource abstraction
KW  - Runtimes
KW  - Selection methods
ER  - 

TY  - CONF
TI  - Fusionize: Improving Serverless Application Performance through Feedback-Driven Function Fusion
AU  - Schirmer, Trever
AU  - Scheuner, Joel
AU  - Pfandzelter, Tobias
AU  - Bermbach, David
T2  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Serverless computing increases developer productivity by removing operational concerns such as managing hardware or software runtimes. Developers, however, still need to partition their application into functions, which can be error-prone and adds complexity: Using a small function size where only the smallest logical unit of an application is inside a function maximizes flexibility and reusability. Yet, having small functions leads to invocation overheads, additional cold starts, and may increase cost due to double billing during synchronous invocations. In this paper we present Fusionize, a framework that removes these concerns from developers by automatically fusing the application code into a multi-function orchestration with varying function size. Developers only need to write the application code following a lightweight programming model and do not need to worry how the application is turned into functions. Our framework automatically fuses different parts of the application into functions and manages their interactions. Leveraging monitoring data, the framework optimizes the distribution of application parts to functions to optimize deployment goals such as end-to-end latency and cost. Using two example applications, we show that Fusionizecan automatically and iteratively improve the deployment artifacts of the application.
C3  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/IC2E55432.2022.00017
DP  - IEEE Xplore
SP  - 85
EP  - 95
ST  - Fusionize
UR  - https://ieeexplore.ieee.org/document/9946241
Y2  - 2024/01/06/22:48:37
L1  - https://arxiv.org/pdf/2204.11533
L2  - https://ieeexplore.ieee.org/document/9946241
ER  - 

TY  - JOUR
TI  - General and Fast Inter-Process Communication via Bypassing Privileged Software
AU  - Mi, Zeyu
AU  - Zhuang, Haoqi
AU  - Zang, Binyu
AU  - Chen, Haibo
T2  - IEEE Transactions on Computers
AB  - IPC (Inter-Process Communication) is a widely used operating system (OS) technique that allows one process to invoke the services of other processes. The IPC participants may share the same OS (internal IPC) or use a separate OS (external IPC). Even though a long line of researches has optimized the performance of IPC, it is still a major factor of the run-time overhead of IPC-intensive applications. Furthermore, there is no one-size-fits-all solution for both internal and external IPC. This paper presents SkyBridge, a general communication technique designed and optimized for both types of IPC. SkyBridge requires no involvement of the privileged software (the kernel or the hypervisor) and enables a process to directly switch to the virtual address space of the target process, regardless of whether they are running on the same OS or not. We have implemented SkyBridge on two microkernels (seL4 and Google Zircon) as well as an open-source serverless hypervisor (Firecracker). The evaluation results show that SkyBridge improves the latency of internal IPC and external IPC by up to 19.6x and 1265.7x, respectively.
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/TC.2021.3130751
DP  - IEEE Xplore
VL  - 71
IS  - 10
SP  - 2435
EP  - 2448
J2  - IEEE Transactions on Computers
SN  - 1557-9956
UR  - https://ieeexplore.ieee.org/document/9627571
Y2  - 2024/01/06/22:49:19
L2  - https://ieeexplore.ieee.org/document/9627571
ER  - 

TY  - CONF
TI  - GoDeploy: Portable Deployment of Serverless Functions in Federated FaaS
AU  - Ristov, Sashko
AU  - Brandacher, Simon
AU  - Felderer, Michael
AU  - Breu, Ruth
T2  - 2022 IEEE Cloud Summit
AB  - Federated Function-as-a-Service (FaaS) offers higher scalability, better resilience and cost-performance trade-off than running serverless applications in a single cloud region. However, existing Infrastructure-as-Code (IaC) tools are mainly focused on the FaaS provider, rather than on applications, which increases developer effort to code multiple times the same data in order to deploy a serverless function on various cloud regions in federated FaaS. To bridge this gap, this paper introduces GoDeploy, a framework that simplifies coding the deployment of serverless functions in Federated FaaS. Using the design principle “code once, deploy everywhere”, GoDeploy offers developers a domain-specific language, which introduces a three-levels hierarchy serverless function → FaaS providers →cloud regions of FaaS provider, rather than existing either the two-levels hierarchy FaaS provider → serverless functions or flat horizontal structure. Moreover, GoDeploy hides the complexity and requirements of each FaaS provider to store deployment packages (zip) of serverless functions on their storages. With this approach, GoDeploy reduces deployment script length measured in lines of code (LoC) compared to the recent FaaSifier M2FaaS by up to 33.33% for deployment on three cloud regions of AWS. When deploying a single function on three cloud regions of each of three FaaS providers AWS, IBM, Google, LoC are reduced by up to 72.34% compared to the state-of-the-art IaC tool Terraform. The improvement is higher when a serverless function needs to be deployed on multiple cloud regions because GoDeploy's three-level hierarchy requires a single LoC per cloud region, compared to multiple LoC in Terraform's and M2FaaS DSLs.
C3  - 2022 IEEE Cloud Summit
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/CloudSummit54781.2022.00012
DP  - IEEE Xplore
SP  - 38
EP  - 43
ST  - GoDeploy
UR  - https://ieeexplore.ieee.org/document/9973070
Y2  - 2024/01/06/23:38:59
L2  - https://ieeexplore.ieee.org/document/9973070
KW  - Automation
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - Portability
KW  - Infrastructure as a service (IaaS)
KW  - Economic and social effects
KW  - Cost performance
KW  - Domain Specific Language
KW  - Domains specific languages
KW  - High scalabilities
KW  - Infrastructure-as-a-code
KW  - Infrastructure-as-a-Code
KW  - Line of codes
KW  - portability
KW  - Problem oriented languages
KW  - Service provider
KW  - Terraform
KW  - Three-level
ER  - 

TY  - CONF
TI  - HARDLESS: A Generalized Serverless Compute Architecture for Hardware Processing Accelerators
AU  - Werner, Sebastian
AU  - Schirmer, Trever
T2  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
AB  - The increasing use of hardware processing accelerators tailored for specific applications, such as the Vision Processing Unit (VPU) for image recognition, further increases developers' configuration, development, and management over-head. Developers have successfully used fully automated elastic cloud services such as serverless computing to counter these additional efforts and shorten development cycles for applications running on CPUs. Unfortunately, current cloud solutions do not yet provide these simplifications for applications that require hardware acceleration. However, as the development of special-ized hardware acceleration continues to provide performance and cost improvements, it will become increasingly important to enable ease of use in the cloud. In this paper, we present an initial design and implemen-tation of Hardless, an extensible and generalized serverless computing architecture that can support workloads for arbitrary hardware accelerators. We show how Hardless can scale across different commodity hardware accelerators and support a variety of workloads using the same execution and programming model common in serverless computing today.
C3  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/IC2E55432.2022.00016
DP  - IEEE Xplore
SP  - 79
EP  - 84
ST  - HARDLESS
UR  - https://ieeexplore.ieee.org/document/9946318?denied=
Y2  - 2024/01/06/23:54:03
L1  - https://arxiv.org/pdf/2208.03192
L2  - https://ieeexplore.ieee.org/document/9946318?denied=
ER  - 

TY  - CONF
TI  - Implementation of Serverless E-Commerce Mobile Application
AU  - Athreya, Shriram
AU  - Kurian, Sanmith
AU  - Dange, Afrin
AU  - Bhatsangave, Suvarna
T2  - 2022 2nd International Conference on Intelligent Technologies (CONIT)
AB  - In recent years, the internet ecosystem has improved many folds giving rise to multiple avenues for businesses to reach their customers. Technology adoption in the global arena has exponentially increased due to the evolution of smartphones. Due to the above-mentioned factors, E-Commerce has become one of the most widely used transaction methods worldwide to purchase products and services. Traditional Established Businesses can manage E-Commerce without a technology team with the help of EaaS (E-Commerce as a Service) Providers like Shopify, WooCommerce, BigCommerce and Magento which have very high development, subscription and ongoing cloud costs. Small and Medium Scale businesses need a cost-effective, high-performance solution to build the business. To build such a system we have performed research and analysis on available technology paradigms (monolith vs microservices) and Cloud paradigm (IaaS vs PaaS) for finalizing the architecture of the E-Commerce application. In this paper, we showcase the analysis results and elaborate on the implementation of our E-Commerce Application.
C3  - 2022 2nd International Conference on Intelligent Technologies (CONIT)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/CONIT55038.2022.9847829
DP  - IEEE Xplore
SP  - 1
EP  - 5
UR  - https://ieeexplore.ieee.org/document/9847829
Y2  - 2024/01/06/23:54:36
L2  - https://ieeexplore.ieee.org/document/9847829
ER  - 

TY  - CONF
TI  - IoT based Chatbots using NLP and SVM Algorithms
AU  - Wahal, Asthha
AU  - Aggarwal, Muskan
AU  - Poongodi, T.
T2  - 2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)
AB  - As of late, there has been a developing revenue in creating AI-empowered chatbot-based indication checker (CSC) applications in the medical services market. CSC applications give expected determinations to clients and help them with self-triaging in light of Artificial Intelligence (AI) procedures utilizing human-like discussions. The review presents an original PC application going about as an individual virtual specialist that has been ideally planned and broadly prepared to collaborate with patients like people. This application depends on a serverless engineering and it totals the administrations of a specialist by giving preventive measures, home cures, intuitive directing meetings, medical care tips, and side effects covering the most pervasive sicknesses in provincial India. Man-made reasoning (AI) is progressively being utilized in medical services. Here, AI-based chatbot frameworks can go about as robotized conversational specialists, equipped for advancing wellbeing, giving schooling, and conceivably provoking conduct change. Investigating the inspiration to utilize wellbeing chatbots is expected to foresee take-up. This conversational application has brought about diminishing the hindrances for admittance to medical services offices and secures shrewd interviews from a distance to permit opportune consideration and quality therapy, along these lines really helping the general public. This conversational application has achieved decreasing the obstructions for permission to clinical benefits workplaces and ties down canny meetings from a distance to allow perfect thought and quality treatment, thusly truly helping the overall population [1]. Therefore, in this research paper, we have discussed about the different roles of artificial intelligence, the way it uniquely involves itself in chatbots and cover it and make it to the most important working part of the model. Specifically, in this, we have discussed about the role in medical healthcare related chatbots along with the integration of IoT devices such as batteries and sensors in this case with the chatbot.
C3  - 2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)
DA  - 2022/04//
PY  - 2022
DO  - 10.1109/ICIEM54221.2022.9853095
DP  - IEEE Xplore
SP  - 484
EP  - 489
UR  - https://ieeexplore.ieee.org/document/9853095
Y2  - 2024/01/06/23:58:01
L2  - https://ieeexplore.ieee.org/document/9853095
ER  - 

TY  - CONF
TI  - Keynote: Designing Serverless Platforms to Support Emerging Applications
AU  - Abad, Cristina L.
T2  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
AB  - Serverless computing offerings from cloud providers have gained significant traction in recent years due to the advantages that these platforms bring with their flexible pricing models, built-in scalability, and minimal operational requirements. In a recent survey of serverless use cases, we found a wide variety of applications that depend on these services, including implementing the core functionality at the backend of mobile applications, automating the DevOps tasks of complex distributed applications, real-time processing of IoT streaming data, and scientific applications. To properly support these applications, the platforms should be fast, self-managing, and provide support for diverse QoS requirements. As a result, novel improvements to serverless platforms are rapidly being proposed and adopted. Evaluating these solutions necessitates application-based, workload-aware benchmarking tools that the community can rely on. This talk addresses these challenges and our research efforts on tackling them, presenting a performance engineering perspective about the current state and future challenges of serverless computing research. I will describe our solutions in autonomic resource management for serverless platforms, focusing on solutions that improve performance or reduce costs via scheduling, caching, and right-sizing of resources, along with our ongoing efforts in developing an application-driven serverless benchmark.
C3  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
DA  - 2022/03//
PY  - 2022
DO  - 10.1109/PerComWorkshops53856.2022.9775198
DP  - IEEE Xplore
SP  - 1
EP  - 1
ST  - Keynote
UR  - https://ieeexplore.ieee.org/document/9775198
Y2  - 2024/01/07/00:00:59
L2  - https://ieeexplore.ieee.org/document/9775198
ER  - 

TY  - CONF
TI  - Leaps and bounds: Analyzing WebAssembly’s performance with a focus on bounds checking
AU  - Szewczyk, Raven
AU  - Stonehouse, Kimberley
AU  - Barbalace, Antonio
AU  - Spink, Tom
T2  - 2022 IEEE International Symposium on Workload Characterization (IISWC)
AB  - WebAssembly is gaining more and more popularity, finding applications beyond the Web browser for which it was initially designed. However, its performance, which developers intended to be comparable with native, has not been extensively studied to identify overheads and pinpoint their causes. This paper identifies that WebAssembly’s bounds-checked memory access safety mechanism may introduce up to a 650% overhead, and requires further tuning. Based on that, we extend four popular WebAssembly runtimes with modern bounds checking mechanisms and compare the performance of each with native compiled code. The runtimes are evaluated on three different instruction set architectures: x86-64, Armv8, and RISC-V RV64GC. We show that, for simple numerical kernels from Poly-Bench/C, there are no significant differences in the bounds checking performance overheads across different instruction set architectures. With the default bounds checking mechanism, performance-oriented runtimes are able to achieve execution times within 20% of native on x86-64 platforms, within 35% on Armv8 platforms, and within 17% on RISC-V. We also show that, when scaling the tested runtimes to multiple threads, the default bounds checking approach taken by WAVM, Wasmtime, and V8 of using the mprotect syscall to resize memory can cause excessive locking in the Linux kernel. Such scaling might be used to quickly start up serverless instances for a single function without the overhead of spawning new processes. We present an alternative userfaultf-based solution to mitigate this issue. We share our results, tools, and scripts under an open source license for other researchers to replicate and use to monitor the progress that WebAssembly runtimes make as they evolve.
C3  - 2022 IEEE International Symposium on Workload Characterization (IISWC)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/IISWC55918.2022.00030
DP  - IEEE Xplore
SP  - 256
EP  - 268
ST  - Leaps and bounds
UR  - https://ieeexplore.ieee.org/document/9975418
Y2  - 2024/01/07/00:04:40
L1  - https://research-repository.st-andrews.ac.uk/bitstream/10023/27004/1/iiswc22_paper90.pdf
L2  - https://ieeexplore.ieee.org/document/9975418
ER  - 

TY  - CONF
TI  - Local Area Network Based Collaboration Using Distributed Computing
AU  - Mutengeni, Joseph
AU  - Musasa, Alec
AU  - Mutunhu, Belinda
T2  - 2022 1st Zimbabwe Conference of Information and Communication Technologies (ZCICT)
AB  - Collab is an application that uses distributed computing techniques for effective, real-time collaboration over a Local Area Network (LAN) in either a home, educational, or workplace environment. Users can collaborate by joining a secure collaboration session in the absence of an internet connection. Within a session, users can share messages, digital content, and reviews. Most existing LANbased applications lack important collaboration features. It is against this backdrop that we propose the use of distributed computing techniques in a small-scale, serverless LAN to empower users to collaborate effectively in local teams. Thus, we design and implement a computer application that can be used for collaboration on a LAN in the absence of a centralised web service.
C3  - 2022 1st Zimbabwe Conference of Information and Communication Technologies (ZCICT)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/ZCICT55726.2022.10045858
DP  - IEEE Xplore
SP  - 1
EP  - 7
UR  - https://ieeexplore.ieee.org/document/10045858
Y2  - 2024/01/07/00:04:58
L2  - https://ieeexplore.ieee.org/document/10045858
ER  - 

TY  - CONF
TI  - MDA: Multiple Decentralized Anchors for Hiding Communication Information
AU  - Abdelbari, Amr
T2  - 2022 International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)
AB  - This paper presents a novel network architecture based on The Onion Routing (TOR) network and BitTorrent concepts to advance privacy and anonymity in communication networks. In the traditional TOR network, when the transmitter sends a message, any external intruder or the first proxy knows that a message has been sent from a specific location or IP address. The same issue is on the receiver side. This can reveal some privacy about who using the network, the time of activity, and the density of communications. Another issue with the services that provide end-to-end encryption is that the powerful entities can force the service's owner to deliver their encryption keys and other related data that can reveal information about the communications within the network. Therefore, the new architecture decentralizes the network entirely in which nodes are the only fundamental component of the network. The nodes themselves act as hops and control the rules of the network by using a distributed hash table (DHT). There are no proxy servers working as hops in TOR networks or servers to hold DHTs as trackers in the BitTorrent network. The nodes act as an end client and a hop server for other nodes that want to join, leave, and communicate. The actual implementation of the MDA network is under development for peer-to-peer (P2P) messaging. Further development can be made for web browsing, file sharing and video streaming.
C3  - 2022 International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/ISMSIT56059.2022.9932770
DP  - IEEE Xplore
SP  - 591
EP  - 596
SN  - 2770-7962
ST  - MDA
UR  - https://ieeexplore.ieee.org/document/9932770
Y2  - 2024/01/07/00:06:34
L2  - https://ieeexplore.ieee.org/document/9932770
ER  - 

TY  - CONF
TI  - MedTator: A Serverless Web-based Tool for Corpus Annotation
AU  - He, Huan
AU  - Fu, Sunyang
AU  - Wang, Liwei
AU  - Wen, Andrew
AU  - Liu, Sijia
AU  - Liu, Hongfang
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
AB  - Annotation tools play an important role in building high-quality annotation corpus. Although existing text annotation tools may provide many features to meet different needs for text annotation, an easy-to-install tool for annotating clinical narrative documents is still demanded. In response, we developed MedTator, a serverless web-based tool for corpus annotation.
C3  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/ICHI54592.2022.00099
DP  - IEEE Xplore
SP  - 530
EP  - 531
SN  - 2575-2634
ST  - MedTator
UR  - https://ieeexplore.ieee.org/document/9874521
Y2  - 2024/01/07/00:07:32
L2  - https://ieeexplore.ieee.org/document/9874521
ER  - 

TY  - CONF
TI  - MicroFaaS: Energy-efficient Serverless on Bare-metal Single-board Computers
AU  - Byrne, Anthony
AU  - Pang, Yanni
AU  - Zou, Allen
AU  - Nadgowda, Shripad
AU  - Coskun, Ayse K.
T2  - 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)
AB  - Serverless function-as-a-service (FaaS) platforms offer a radically-new paradigm for cloud software development, yet the hardware infrastructure underlying these platforms is based on a decades-old design pattern. The rise of FaaS presents an opportunity to reimagine cloud infrastructure to be more energy-efficient, cost-effective, reliable, and secure. In this paper, we show how replacing handfuls of x86-based rack servers with hundreds of ARM-based single-board computers could lead to a virtualization-free, energy-proportional cloud that achieves this vision. We call our systematically-designed implementation MicroFaaS, and we conduct a thorough evaluation and cost analysis comparing MicroFaaS to a throughput-matched FaaS platform implemented in the style of conventional virtualization-based cloud systems. Our results show a 5.6x increase in energy efficiency and 34.2% decrease in total cost of ownership compared to our baseline.
C3  - 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)
DA  - 2022/03//
PY  - 2022
DO  - 10.23919/DATE54114.2022.9774688
DP  - IEEE Xplore
SP  - 754
EP  - 759
SN  - 1558-1101
ST  - MicroFaaS
UR  - https://ieeexplore.ieee.org/document/9774688
Y2  - 2024/01/07/00:12:58
L2  - https://ieeexplore.ieee.org/document/9774688
ER  - 

TY  - CONF
TI  - Mobility-aware Seamless Virtual Function Migration in Deviceless Edge Computing Environments
AU  - Huang, Yaodong
AU  - Lin, Zelin
AU  - Yao, Tingting
AU  - Shang, Xiaojun
AU  - Cui, Laizhong
AU  - Huang, Joshua Zhexue
T2  - 2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)
AB  - Serverless Computing and Function-as-a-Service (FaaS) offer convenient and transparent services to developers and users. The deployment and resource allocation of services are managed by the cloud service providers. Meanwhile, the development of smart mobile devices and network technology enables the collection and transmission of a huge amount of data, which creates the mobile edge computing shifting tasks to the network edge for mobile users. In this paper, we propose a deviceless edge computing system targeting the mobility of end users. We focus on the migration of virtual functions to provide uninterrupted services to mobile users. We introduce the deviceless edge computing model and propose a seamless migration scheme of virtual functions with limited involvement of function developers. We formulate the migration decision problem into integer linear programming and use receding horizon control (RHC) for online solutions. We implement the migration system and algorithm to support delay-sensitive scenarios over real edge devices and develop a streaming game as the virtual function to test the performance. Extensive experiments in real scenarios exhibit the system has the ability to support high-mobility and delay-sensitive application scenarios. Extensive simulation results also show its applicability over large-scale networks.
C3  - 2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/ICDCS54860.2022.00050
DP  - IEEE Xplore
SP  - 447
EP  - 457
SN  - 2575-8411
UR  - https://ieeexplore.ieee.org/document/9912171
Y2  - 2024/01/07/00:13:32
L2  - https://ieeexplore.ieee.org/document/9912171
ER  - 

TY  - JOUR
TI  - Monitoring Platform Evolution Toward Serverless Computing for 5G and Beyond Systems
AU  - Perez, Ramon
AU  - Benedetti, Priscilla
AU  - Pergolesi, Matteo
AU  - Garcia-Reinoso, Jaime
AU  - Zabala, Aitor
AU  - Serrano, Pablo
AU  - Femminella, Mauro
AU  - Reali, Gianluca
AU  - Steenhaut, Kris
AU  - Banchs, Albert
T2  - IEEE Transactions on Network and Service Management
AB  - Fifth generation (5G) and beyond systems require flexible and efficient monitoring platforms to guarantee optimal key performance indicators (KPIs) in various scenarios. Their applicability in Edge computing environments requires lightweight monitoring solutions. This work evaluates different candidate technologies to implement a monitoring platform for 5G and beyond systems in these environments. For monitoring data plane technologies, we evaluate different virtualization technologies, including bare metal servers, virtual machines, and orchestrated containers. We show that containers not only offer superior flexibility and deployment agility, but also allow obtaining better throughput and latency. In addition, we explore the suitability of the Function-as-a-Service (FaaS) serverless paradigm for deploying the functions used to manage the monitoring platform. This is motivated by the event oriented nature of those functions, designed to set up the monitoring infrastructure for newly created services. When the FaaS warm start mode is used, the platform gives users the perception of resources that are always available. When a cold start mode is used, containers running the application’s modules are automatically destroyed when the application is not in use. Our analysis compares both of them with the standard deployment of microservices. The experimental results show that the cold start mode produces a significant latency increase, along with potential instabilities. For this reason, its usage is not recommended despite the potential savings of computing resources. Conversely, when the warm start mode is used for executing configuration tasks of monitoring infrastructure, it can provide similar execution times to a microservice-based deployment. In addition, the FaaS approach significantly simplifies the code logic in comparison with microservices, reducing lines of code to less than 38%, thus reducing development time. Thus, FaaS in warm start mode represents the best candidate technology to implements such management functions.
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/TNSM.2022.3150586
DP  - IEEE Xplore
VL  - 19
IS  - 2
SP  - 1489
EP  - 1504
J2  - IEEE Transactions on Network and Service Management
SN  - 1932-4537
UR  - https://ieeexplore.ieee.org/document/9709528
Y2  - 2024/01/07/01:02:50
L1  - https://e-archivo.uc3m.es/bitstream/10016/34283/2/monitoring_IEEE-TNSM_2022_ps.pdf
L2  - https://ieeexplore.ieee.org/document/9709528
ER  - 

TY  - CONF
TI  - NEPTUNE: Network- and GPU-aware Management of Serverless Functions at the Edge
AU  - Baresi, Luciano
AU  - Hu, Davide Yi Xian
AU  - Quattrocchi, Giovanni
AU  - Terracciano, Luca
T2  - 2022 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)
AB  - Nowadays a wide range of applications is constrained by low-latency requirements that cloud infrastructures cannot meet. Multi-access Edge Computing (MEC) has been proposed as the reference architecture for executing applications closer to users and reducing latency, but new challenges arise: edge nodes are resource-constrained, the workload can vary significantly since users are nomadic, and task complexity is increasing (e.g., machine learning inference). To overcome these problems, the paper presents NEPTUNE, a serverless-based framework for managing complex MEC solutions. NEPTUNE i) places functions on edge nodes according to user locations, ii) avoids the saturation of single nodes, iii) exploits GPUs when available, and iv) allocates resources (CPU cores) dynamically to meet foreseen execution times. A prototype, built on top of K3S, was used to evaluate NEPTUNE on a set of experiments that demonstrate a significant reduction in terms of response time, network overhead, and resource consumption compared to three well-known approaches. CCS CONCEPTS • Theory of computation → Scheduling algorithms; • Computing methodologies →Distributed computing methodologies; • Computer systems organization →Distributed architectures.
C3  - 2022 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)
DA  - 2022/05//
PY  - 2022
DO  - 10.1145/3524844.3528051
DP  - IEEE Xplore
SP  - 144
EP  - 155
SN  - 2157-2321
ST  - NEPTUNE
UR  - https://ieeexplore.ieee.org/document/9799943
Y2  - 2024/01/07/01:04:07
L1  - https://dl.acm.org/doi/pdf/10.1145/3524844.3528051
L1  - https://dl.acm.org/doi/pdf/10.1145/3524844.3528051
L2  - https://ieeexplore.ieee.org/document/9799943
L2  - https://ieeexplore.ieee.org/document/9799943
ER  - 

TY  - CONF
TI  - On the Joint Optimization of Function Assignment and Communication Scheduling toward Performance Efficient Serverless Edge Computing
AU  - Li, Yuepeng
AU  - Zeng, Deze
AU  - Gu, Lin
AU  - Wang, Kun
AU  - Guo, Song
T2  - 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)
AB  - Serverless edge computing is booming as an efficient carrier of deploying complex applications composed of dependent functions, whose assignment decisions highly influence the application performance. Although similar problem has been widely studied, none of existing approaches considers the diversity of communication styles, which is specially introduced in serverless computing and also imposes high influence to the performance efficiency. We compare two communication styles, called direct-passing and remote-storage, to transmit intermediate data between functions. We find that there is no single communication style that can prevail under all scenarios and the optimal selection depends on several factors, such as fanout degree, data size, and network bandwidth. Hence, how to select the appropriate communication style for each inter-function communication link, together with the function assignment decision, is essential to the application performance. To this end, we propose a Priority-based ASsignment and Selection (PASS) algorithm with joint consideration of function assignment and communication style selection. We theoretically analyze the approximation ratio of PASS algorithm and extensive experiments on real-world applications show that PASS can averagely reduce the completion time by 24.1% in comparison with state-of-the-art approaches.
C3  - 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/IWQoS54832.2022.9812887
DP  - IEEE Xplore
SP  - 1
EP  - 9
SN  - 1548-615X
UR  - https://ieeexplore.ieee.org/document/9812887
Y2  - 2024/01/07/01:04:36
L2  - https://ieeexplore.ieee.org/document/9812887
ER  - 

TY  - CONF
TI  - On the Power Consumption of Serverless Functions: An Evaluation of OpenFaaS
AU  - Alhindi, Abdulaziz
AU  - Djemame, Karim
AU  - Heravan, Fatemeh Banaie
T2  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
AB  - The rapid growth in cloud-based technologies has introduced the need for very large data centres to meet the increasing demand for cloud services. One of the main challenges in managing these data centres is the sharp increase of power consumption. Research has therefore tackled the issue of power/energy efficiency in cloud data centres. Serverless computing is a cloud computing execution model that gives software developers the option to deploy their code without the need to configure servers, operating systems or runtime libraries, thus allowing them to invest less effort and capital in infrastructure management. This paper investigates whether serverless computing has the ability to support power efficiency. To this aim, a number of experiments are conducted to compare the power consumption of a serverless platform, OpenFaaS, against Docker containers with the consideration of applications and benchmarks. The experimental results show that OpenFaaS is more power efficient than Docker when the processor and memory are under stress.
C3  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/UCC56403.2022.00064
DP  - IEEE Xplore
SP  - 366
EP  - 371
ST  - On the Power Consumption of Serverless Functions
UR  - https://ieeexplore.ieee.org/document/10061779
Y2  - 2024/01/07/01:05:12
L1  - https://eprints.whiterose.ac.uk/193714/1/cifs2022-final.pdf
L2  - https://ieeexplore.ieee.org/document/10061779
ER  - 

TY  - CONF
TI  - OpenWolf: A Serverless Workflow Engine for Native Cloud-Edge Continuum
AU  - Sicari, Christian
AU  - Carnevale, Lorenzo
AU  - Galletta, Antonino
AU  - Villari, Massimo
T2  - 2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
AB  - Nowadays, Serverless computing is emerging as one of the most used Cloud services. In particular, the Function as Service (FaaS) is bringing to Cloud consumers, developers, and devops many advantages in terms of service costs, speed of development, and ease of deployment. In fact, it stands to be a key technology for enabling the Cloud-Edge Continuum. Regardless of these features, it is still not possible to build FaaS native applications without a Cloud broker that coordinates the functions. Therefore, FaaS usage is limited to very simple and specific jobs. In this work, we brush up on the concept of Scientific Workflow using the FaaS paradigm, in order to realize full Native Serverless Workflows-based applications. We define a custom Workflow Manifest DSL used to describe function interactions, then we describe the implementation of an agent able to deploy architecture-independent functions and coordinate them according to the Manifest. Finally, federating the Cloud-Fog-Edge tiers in a single Continuum environment, we allow functions to take advantage of the Continuum tier’s characteristics where they are deployed. This project is called OpenWolf, it’s repository is published on GitHub, under GNU General Public License v3.0.
C3  - 2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927926
DP  - IEEE Xplore
SP  - 1
EP  - 8
ST  - OpenWolf
UR  - https://ieeexplore.ieee.org/document/9927926
Y2  - 2024/01/07/01:05:36
L2  - https://ieeexplore.ieee.org/document/9927926
ER  - 

TY  - CONF
TI  - Performance Analysis of Geospatial Serverless Computing for Geospatial Big Data Analysis
AU  - Nanda, Sandeep
AU  - Ranjan Mishra, Manoj
AU  - Narayan Brahma, Aditya
AU  - Chandra Swain, Sarat
AU  - Shekhar Patra, Sudhansu
AU  - Kumar Barik, Rabindra
T2  - 2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)
AB  - Nowadays, real-world applications are primarily concerned with collecting the physical characteristics that correspond to various geographical phenomena, and information plays a critical role in the analysis and forecast of various occurrences that occur. Real-time analysis is used in a variety of application domains, including traffic flow monitoring, healthcare monitoring, and so on. This research study has considered the existing geospatial serverless computing framework and proposed an analytical model to enable the geospatial serverless platform to compute and operate at different workloads and the tradeoff between the cost and performance are then determined by the users’ preferences. Here, the accuracy of the model is validated in Amazon Web Service (AWS) Lambda environment and the model calculations are shown based on the performance metrics by including average response time, probability of cold start as well as the average amount of functional instances in the steady state by using the queuing theory. Here, the performance modelling is carried out in the geospatial serverless environment for different workloads, which results in delivering better performance at a minimal cost.
C3  - 2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)
DA  - 2022/08//
PY  - 2022
DO  - 10.1109/ICESC54411.2022.9885470
DP  - IEEE Xplore
SP  - 716
EP  - 720
UR  - https://ieeexplore.ieee.org/document/9885470
Y2  - 2024/01/07/01:05:57
L2  - https://ieeexplore.ieee.org/document/9885470
ER  - 

TY  - CONF
TI  - Performance Evaluation of Serverless Edge Computing for Machine Learning Applications
AU  - Trieu, Quoc Lap
AU  - Javadi, Bahman
AU  - Basilakis, Jim
AU  - Toosi, Adel N.
T2  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
AB  - Next generation technologies such as smart health-care, self-driving cars, and smart cities require new approaches to deal with the network traffic generated by the Internet of Things (IoT) devices, as well as efficient programming models to deploy machine learning techniques. Serverless edge computing is an emerging computing paradigm from the integration of two recent technologies, edge computing and serverless computing, that can possibly address these challenges. However, there is little work to explore the capability and performance of such a technology. In this paper, a comprehensive performance analysis of a serverless edge computing system using popular open-source frameworks, namely, Kubeless, OpenFaaS, Fission, and funcX is presented. The experiments considered different programming languages, workloads, and the number of concurrent users. The machine learning workloads have been used to evaluate the performance of the system under different working conditions to provide insights into the best practices. The evaluation results revealed some of the current challenges in serverless edge computing and open research opportunities in this emerging technology for machine learning applications.
C3  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/UCC56403.2022.00025
DP  - IEEE Xplore
SP  - 139
EP  - 144
UR  - https://ieeexplore.ieee.org/document/10061771
Y2  - 2024/01/07/05:17:28
L1  - https://arxiv.org/pdf/2210.10331
L2  - https://ieeexplore.ieee.org/document/10061771
ER  - 

TY  - JOUR
TI  - Performance Modeling of Serverless Computing Platforms
AU  - Mahmoudi, Nima
AU  - Khazaei, Hamzeh
T2  - IEEE Transactions on Cloud Computing
AB  - Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user's preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/TCC.2020.3033373
DP  - IEEE Xplore
VL  - 10
IS  - 4
SP  - 2834
EP  - 2847
J2  - IEEE Transactions on Cloud Computing
SN  - 2168-7161
UR  - https://ieeexplore.ieee.org/document/9238484
Y2  - 2024/01/07/05:18:26
L2  - https://ieeexplore.ieee.org/document/9238484
ER  - 

TY  - CONF
TI  - Performance Optimization in Serverless Edge Computing Environment using DRL-Based Function Offloading
AU  - Yao, Xuyi
AU  - Chen, Ningjiang
AU  - Yuan, Xuemei
AU  - Ou, Pingjie
T2  - 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)
AB  - Serverless computing/Function as a Service (FaaS) has emerged as a new paradigm for running short-lived applications in the cloud. Serverless edge computing is recently adopting serverless computing at edge to run event-driven tasks and supporting application running of resource-constrained Internet of Things (IoT) devices by offloading their tasks to the edge. However, traditional task offloading methods are mainly based on heuristic algorithms for one-shot optimization, which leads to performance degradation in long-term operation. Fortunately, deep reinforcement learning techniques combining reinforcement learning and deep neural networks provide a promising alternative. Therefore, a function offloading algorithm DRLFO is proposed with a deep reinforcement learning algorithm based on actor-critic framework in this paper. The function offloading process in serverless edge computing environment is modeled as a Markov Decision Process. Finally, the experimental results show that the proposed algorithm can successfully converge and outperform the compared baseline algorithm in terms of function success rate and reduce the average latency by 4.6%-22.6%.
C3  - 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/CSCWD54268.2022.9776166
DP  - IEEE Xplore
SP  - 1390
EP  - 1395
UR  - https://ieeexplore.ieee.org/document/9776166
Y2  - 2024/01/07/05:18:56
L2  - https://ieeexplore.ieee.org/document/9776166
ER  - 

TY  - CONF
TI  - Predictable Open Source FaaS Function Chains
AU  - Balla, David
AU  - Maliosz, Markosz
AU  - Simon, Csaba
T2  - 2022 IEEE 11th International Conference on Cloud Networking (CloudNet)
AB  - Function as a Service (FaaS) provides a way to execute modular pieces of code in cloud environments. FaaS also supports the implementation of distributed cloud applications by deploying function chains.In this paper, we introduce a simulation framework for function chains using asynchronous invocations. By using our simulator the completion time distribution of the individual function invocations can be estimated as well as the completion time of the distribution of the whole function chain.We evaluate the results of our simulation framework by using compute-intensive, Python3 based function chains that implement image transforming operations. We also propose cost decreasing methods for function chains with asynchronous invocations.
C3  - 2022 IEEE 11th International Conference on Cloud Networking (CloudNet)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/CloudNet55617.2022.9978820
DP  - IEEE Xplore
SP  - 186
EP  - 194
SN  - 2771-5663
UR  - https://ieeexplore.ieee.org/document/9978820
Y2  - 2024/01/07/05:23:49
L2  - https://ieeexplore.ieee.org/document/9978820
ER  - 

TY  - CONF
TI  - Predictive function placement for distributed serverless environments
AU  - Martinez, Maria Mora
AU  - Pandey, Sanjeet Raj
T2  - 2022 25th Conference on Innovation in Clouds, Internet and Networks (ICIN)
AB  - Serverless computing has gained popularity in recent years, and it is being used in an increasing number of use cases. It is still limited by some of its challenges, such as high latency, statelessness, vendor lock-in or difficulty to test. However, it presents advantages that could open new opportunities for more applications, such as event-driven IoT or mobile applications that could benefit from the serverless elasticity and resilience and reduce operating costs. Networks are becoming more geographically distributed and adopting a hybrid infrastructure of cloud and edge nodes, for example on 5G base stations. If the latency is high, a wide range of mobile applications cannot make use of serverless computing. In this paper, we propose an approach to reduce the end-to-end latency perceived by an application using serverless computing by predicting the resource utilization of the available nodes to decide the location to deploy the serverless function instances. In this first approach, we apply a Kalman filter to predict the CPU load of each of the nodes. The results of the experiments with two serverless nodes show a latency reduction that increases as the requested computation becomes more complex, reaching a 17% reduction compared to resource-based load balancer with direct measurement.
C3  - 2022 25th Conference on Innovation in Clouds, Internet and Networks (ICIN)
DA  - 2022/03//
PY  - 2022
DO  - 10.1109/ICIN53892.2022.9758140
DP  - IEEE Xplore
SP  - 86
EP  - 90
SN  - 2472-8144
UR  - https://ieeexplore.ieee.org/document/9758140
Y2  - 2024/01/07/05:25:53
L2  - https://ieeexplore.ieee.org/document/9758140
ER  - 

TY  - CONF
TI  - ProProv: A Language and Graphical Tool for Specifying Data Provenance Policies
AU  - Dennis, Kevin
AU  - Engram, Shamaria
AU  - Kaczmarek, Tyler
AU  - Ligatti, Jay
T2  - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
AB  - The Function-as-a-Service cloud computing paradigm has made large-scale application development convenient and efficient as developers no longer need to deploy or manage the necessary infrastructure themselves. However, as a consequence of this abstraction, developers lose insight into how their code is executed and data is processed. Cloud providers currently offer little to no assurance of the integrity of customer data. One approach to robust data integrity verification is the analysis of data provenance—logs that describe the causal history of data, applications, users, and non-person entities. This paper introduces ProProv, a new domain-specific language and graphical user interface for specifying policies over provenance metadata to automate provenance analyses.To evaluate the convenience and usability of the new ProProv interface, 61 individuals were recruited to construct provenance policies using both ProProv and the popular, general-purpose policy specification language Rego—used as a baseline for comparison. We found that, compared to Rego, the ProProv interface greatly increased the number of policies successfully constructed, improved the time taken to construct those policies, and reduced the failed-attempt rate. Participants successfully constructed 73% of the requested policies using ProProv, compared to 41% using Rego. To further evaluate the usability of the tools, participants were given a 10-question questionnaire measured using the System Usability Scale (SUS). The median SUS score for the graphical ProProv interface was above average and fell into the “excellent” category, compared to below average and “OK” for Rego. These results highlight the impacts that graphical domain-specific tools can have on the accuracy and speed of policy construction.
C3  - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/TPS-ISA56441.2022.00040
DP  - IEEE Xplore
SP  - 266
EP  - 275
ST  - ProProv
UR  - https://ieeexplore.ieee.org/document/10063380
Y2  - 2024/01/07/05:33:17
L2  - https://ieeexplore.ieee.org/document/10063380
ER  - 

TY  - CONF
TI  - QKD-Secure ETSI MEC
AU  - Cicconetti, Claudio
AU  - Conti, Marco
AU  - Passarella, Andrea
T2  - 2022 IEEE 15th Workshop on Low Temperature Electronics (WOLTE)
AB  - Edge computing is a leading trend in the deployment of computation and networking infrastructures, where processing is done closer to the users, compared to more traditional cloud-based solutions relying on remote data centers. On the other hand, Quantum Key Distribution (QKD) technologies, offering unconditional security between the communicating parties, is expected to become affordable in the near future and it will unlock new opportunities, e.g., for applications with long-term confidentiality requirements. In this paper we study how to enable ETSI ISG QKD key delivery within an ETSI MEC edge computing scenario, in the practical use case of cloud-native applications based on Function-as-a-Service (FaaS). This work is part of the activities of the PON project “Development of quantum systems and technologies for IT security in communication networks” (QUANCOM) which aims to the realization of a metropolitan quantum communication network through the collaboration between universities, research centers and companies operating in the communication market area.
C3  - 2022 IEEE 15th Workshop on Low Temperature Electronics (WOLTE)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/WOLTE55422.2022.9882872
DP  - IEEE Xplore
SP  - 1
EP  - 4
UR  - https://ieeexplore.ieee.org/document/9882872
Y2  - 2024/01/07/05:34:22
L2  - https://ieeexplore.ieee.org/document/9882872
ER  - 

TY  - CONF
TI  - QoS aware FaaS for Heterogeneous Edge-Cloud continuum
AU  - Sheshadri, K R
AU  - Lakshmi, J
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Function as a Service (FaaS) is one of the widely used serverless computing service offerings to build and deploy applications on the Cloud. The platform is popular for its "pay-as-you-go" billing model, microservice-based design, event-driven executions, and autonomous scaling. Although it has its firm roots in Cloud computing service offerings, it is considerably explored in the Edge computing layer. The efficient resource management of FaaS is attractive to Edge computing because of the limited nature of resources. Existing literature on Edge-Cloud FaaS platforms orchestrates compute workloads based on factors such as data locality, resource availability, network costs, and bandwidth. However, the state-of-the-art platforms lack a comprehensive way to address the challenges of managing heterogeneous resources in the FaaS platform. The resource specification in a heterogeneous setting, lack of Quality of Service (QoS) driven resource provisioning, and function deployment exacerbate the problem of resource selection, and function deployment in FaaS platforms with a heterogeneous resource pool. To address these gaps, the current work presents a novel heterogeneous FaaS platform that deduces function resource specification using Machine Learning (ML) methods, performs smart function placement on Edge/Cloud based on a user-specified QoS requirement, and exploit data locality by caching appropriate data for function executions. Experimental results based on real-world workloads on a video surveillance application show that the proposed platform brings efficient resource utilization and cost savings at the Cloud by reducing the resource usage by up to 30%, while improving the performance of function executions by up to 25% at Edge and Cloud.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00023
DP  - IEEE Xplore
VL  - 2022-July
SP  - 70
EP  - 80
SN  - 2159-6190
UR  - https://ieeexplore.ieee.org/document/9860835
Y2  - 2024/01/07/05:34:44
L2  - https://ieeexplore.ieee.org/document/9860835
KW  - serverless computing
KW  - Serverless computing
KW  - FaaS
KW  - Function as a Service
KW  - Function as a service
KW  - Edge computing
KW  - Quality of service
KW  - Service platforms
KW  - Cloud platforms
KW  - Edge Cloud
KW  - Edge cloud continuum
KW  - Edge Cloud continuum
KW  - Edge Cloud FaaS
KW  - Edge cloud function as a service
KW  - Edge clouds
KW  - Heterogeneous edge cloud platform
KW  - Heterogeneous Edge Cloud platforms
KW  - Heterogeneous FaaS platforms
KW  - Heterogeneous function as a service platform
KW  - QoS
KW  - QoS aware FaaS for Heterogeneous Edge Cloud continuum
KW  - Quality of Service
KW  - Quality of service aware function as a service for heterogeneous edge cloud continuum
KW  - Quality-of-service
KW  - Security systems
KW  - Service-aware
KW  - Specifications
ER  - 

TY  - CONF
TI  - Real-Time Object Detection with Tensorflow Model Using Edge Computing Architecture
AU  - N, Mahiban Lindsay
AU  - Rao, Alla Eswara
AU  - Kalyan, Madaka Pavan
T2  - 2022 8th International Conference on Smart Structures and Systems (ICSSS)
AB  - This paper presents the capturing of objects using Wi-Fi enabled modular esp32 camera and processes the captured stream of data using machine learning and computer vision techniques, then sends the processed data to the cloud, there are major cloud providers in the market who occupied more than 80% of the global public market the cloud providers are Google Cloud, Amazon AWS, Microsoft Azure. Google Cloud Platform (GCP) is been our primary choice because of its good documentation availability, The Cloud IoT-Core Gateway, as well as a serverless cloud layer to store all of the data. The cloud functions help to trigger the notifications to the users when the cameras detect what we have trained the model. The Edge computing project uses an ESP32 With cameras as a device listener and a raspberry pi as an edge server which has an image classifier model trained with TensorFlow.
C3  - 2022 8th International Conference on Smart Structures and Systems (ICSSS)
DA  - 2022/04//
PY  - 2022
DO  - 10.1109/ICSSS54381.2022.9782169
DP  - IEEE Xplore
SP  - 01
EP  - 04
UR  - https://ieeexplore.ieee.org/document/9782169
Y2  - 2024/01/07/05:35:23
L2  - https://ieeexplore.ieee.org/document/9782169
ER  - 

TY  - CONF
TI  - Reflekt: a Library for Compile-Time Reflection in Kotlin
AU  - Birillo, Anastasiia
AU  - Lyulina, Elena
AU  - Malysheva, Maria
AU  - Tankov, Vladislav
AU  - Bryksin, Timofey
T2  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)
AB  - Reflection in Kotlin is a powerful mechanism to introspect program behavior during its execution at run-time. However, among the variety of practical tasks involving reflection, there are scenarios when the poor performance of run-time approaches becomes a significant disadvantage. This problem manifests itself in Kotless, a popular framework for developing serverless applications, because the faster the applications launch, the less their cloud infrastructure costs. In this paper, we present Reflekt - a compile-time reflection library which allows to perform the search among classes, object expressions (which in Kotlin are implemented as singleton classes), and functions in Kotlin code based on the given search query. It comes with a convenient DSL and better performance comparing to the existing run-time reflection approaches. Our experiments show that replacing run-time reflection calls with Reflekt in serverless applications created with Kotless resulted in a significant performance boost in start-up time of these applications.
C3  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)
DA  - 2022/05//
PY  - 2022
DO  - 10.1145/3510457.3513053
DP  - IEEE Xplore
SP  - 231
EP  - 240
ST  - Reflekt
UR  - https://ieeexplore.ieee.org/document/9793932
Y2  - 2024/01/07/05:35:44
L1  - https://arxiv.org/pdf/2202.06033
L2  - https://ieeexplore.ieee.org/document/9793932
ER  - 

TY  - CONF
TI  - Repository Platform for RESTful Web Services
AU  - Nikolov, Alexander
AU  - Petrova-Antonova, Dessislava
T2  - 2022 International Conference Automatics and Informatics (ICAI)
AB  - With the growing usage of web services across the globe, demand for advanced repositories has emerged that manage web service discovery and facilitate their use by different domains. The rapid development of mobile applications and the adoption of Internet of Things technologies and serverless computing have led to the popularity of the Representational State Transfer (REST) architecture, a preferable approach. Due to their lightweight nature, the RESTful web services area is preferable over Simple Object Access Protocol (SOAP) web services for the development of distributed applications. Thus, new approaches for the design and development of web service repositories are needed to overcome the limitations of the current web service discovery mechanisms and consider the advanced RESTful technologies. This paper addresses such a need by proposing a repository platform for RESTful web services. It supports the validation of web service definitions against OpenAPI specification, the generation of client SDKs and server stubs, the invocation of web service operations for testing purposes and the assessment of web service availability, customer support level and rating. The architecture, functionality and validation of the platform are presented, giving insight into its features, and proving its feasibility.
C3  - 2022 International Conference Automatics and Informatics (ICAI)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/ICAI55857.2022.9960066
DP  - IEEE Xplore
SP  - 292
EP  - 297
UR  - https://ieeexplore.ieee.org/document/9960066
Y2  - 2024/01/07/05:36:08
L2  - https://ieeexplore.ieee.org/document/9960066
ER  - 

TY  - CONF
TI  - Resource Scaling Strategies for Open-Source FaaS Platforms compared to Commercial Cloud Offerings
AU  - Manner, Johannes
AU  - Wirtz, Guido
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Open-source offerings are often investigated when comparing their features to commercial cloud offerings. However, performance benchmarking is rarely executed for open-source tools hosted on-premise nor is it possible to conduct a fair cost comparison due to a lack of resource settings equivalent to cloud scaling strategies.Therefore, we firstly list implemented resource scaling strategies for public and open-source FaaS platforms. Based on this we propose a methodology to calculate an abstract performance measure to compare two platforms with each other. Since all open-source platforms suggest a Kubernetes deployment, we use this measure for a configuration of open-source FaaS platforms based on Kubernetes limits. We tested our approach with CPU intensive functions, considering the difference between single-threaded and multi-threaded functions to avoid wasting resources. With regard to this, we also address the noisy neighbor problem for open-source FaaS platforms by conducting an instance parallelization experiment. Our approach to limit resources leads to consistent results while avoiding an overbooking of resources.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00020
DP  - IEEE Xplore
SP  - 40
EP  - 48
SN  - 2159-6190
UR  - https://ieeexplore.ieee.org/document/9860370
Y2  - 2024/01/07/05:36:47
L2  - https://ieeexplore.ieee.org/document/9860370
ER  - 

TY  - CONF
TI  - SAPPARCHI: an Osmotic Platform to Execute Scalable Applications on Smart City Environments
AU  - Souza, Arthur
AU  - Cacho, Nélio
AU  - Batista, Thais
AU  - Ranjan, Rajiv
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - In the Smart Cities context, a plethora of Middle-ware Platforms had been proposed to support applications execution and data processing. Despite all the progress already made, the vast majority of solutions have not met the requirements of Applications’ Runtime, Development, and Deployment when related to Scalability. Some studies point out that just 1 of 97 (1%) reported platforms reach this all this set of requirements at same time. This small number of platforms may be explained by some reasons: i) Big Data: The huge amount of processed and stored data with various data sources and data types, ii) Multi-domains: many domains involved (Economy, Traffic, Health, Security, Agronomy, etc.), iii) Multiple processing methods like Data Flow, Batch Processing, Services, and Microservices, and 4) High Distributed Degree: The use of multiple IoT and BigData tools combined with execution at various computational levels (Edge, Fog, Cloud) leads applications to present a high level of distribution. Aware of those great challenges, we propose Sapparchi, an integrated architectural model for Smart Cities applications that defines multi-processing levels (Edge, Fog, and Cloud). Also, it presents the Sapparchi middleware platform for developing, deploying, and running applications in the smart city environment with an osmotic multi-processing approach that scales applications from Cloud to Edge. Finally, an experimental evaluation exposes the main advantages of adopting Sapparchi.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00051
DP  - IEEE Xplore
SP  - 289
EP  - 298
SN  - 2159-6190
ST  - SAPPARCHI
UR  - https://ieeexplore.ieee.org/document/9860412
Y2  - 2024/01/07/05:37:06
L2  - https://ieeexplore.ieee.org/document/9860412
ER  - 

TY  - CONF
TI  - Scorpius: Proactive Code Preparation to Accelerate Function Startup
AU  - Yu, Heng
AU  - Shen, Junxian
AU  - Zhang, Han
AU  - Wang, Jilong
AU  - Miao, Congcong
AU  - Xu, Mingwei
T2  - 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)
AB  - Massive enterprises deploy their applications on public clouds to relieve infrastructure management burden. However, applications are faced with highly fluctuating workloads, while clouds provision exclusive resources at coarse time granularity, resulting in severely low resource efficiency. Function-as-a-Service (FaaS) platform enables fine-grained resource multiplexing, which has the potential to improve efficiency. However, FaaS platforms could consume several seconds to start functions and the long startup latency can severely hurt the performance of applications. In this paper, we measure the FaaS platforms and find that most startup latency is occupied by code preparation. To reduce the code preparation latency with little resource overhead, we propose Scorpius, a FaaS platform that proactively prepares code based on the historical data of functions. It combines two optimization categories: (1) To reduce the code size, Scorpius proposes to proactively prepare partial libraries over servers and run functions on the server with most library sharing. (2) To advance the start time, Scorpius proposes to predict the function overload with a simple model and proactively scale code to more servers. We have implemented a prototype of Scorpius and conducted extensive experiments. Evaluation results demonstrate that compared with state-of-the-art methods, Scorpius can reduce the code preparation latency by 87.6% with only 9.3% storage overhead.
C3  - 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/IWQoS54832.2022.9812868
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 1548-615X
ST  - Scorpius
UR  - https://ieeexplore.ieee.org/document/9812868
Y2  - 2024/01/07/05:37:32
ER  - 

TY  - CONF
TI  - Sequence Clock: A Dynamic Resource Orchestrator for Serverless Architectures
AU  - Fakinos, Ioannis
AU  - Tzenetopoulos, Achilleas
AU  - Masouros, Dimosthenis
AU  - Xydis, Sotirios
AU  - Soudris, Dimitrios
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Function-as-a-service (FaaS) represents the next frontier in the evolution of cloud computing being an emerging paradigm that removes the burden of configuration and management issues from users. This is achieved by replacing the well-established monolithic approach with graphs of standalone, small, stateless, event-driven components called functions. At the same time, from the cloud providers’ perspective, problems such as availability, load balancing and scalability need to be resolved without being aware of the functionality, behavior or resource requirements of their tenants’ code. However, in this context, functions’ containers coexist with others inside a host of finite resources, where a passive resource allocation technique does not guarantee a well-defined quality of service (QoS) in regards to time latency. In this paper, we present Sequence Clock, an expandable latency targeting tool that actively monitors serverless invocations in a cluster and offers execution of a sequential chain of functions, also known as pipelines or sequences, while achieving the targeted time latency. Two regulation methods were utilized, with one of them achieving up to 82% decrease in the severity of time violations and in some cases even eliminating them completely.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00024
DP  - IEEE Xplore
SP  - 81
EP  - 90
SN  - 2159-6190
ST  - Sequence Clock
UR  - https://ieeexplore.ieee.org/document/9860832
Y2  - 2024/01/07/05:41:11
L1  - https://zenodo.org/record/7941063/files/IEEE_CLOUD_2022___Sequence_Clock__A_Dynamic_Resource_Orchestrator_for_Serverless_Architectures.pdf
L2  - https://ieeexplore.ieee.org/document/9860832
ER  - 

TY  - CONF
TI  - Serverless Blockchain-based AI-Powered Financial Transaction Management System on Cloud
AU  - B, Sri Sathya K
AU  - A, Nithyasri
AU  - E, Amirtha
AU  - S, Fowjiya
T2  - 2022 Third International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)
AB  - The Finance Manager Application is used to automate the existing finance tracking system and to offer the user to have a more engaging and aesthetic user experience with the bill tracking feature. The existing systems are used to record several financial transactions and events. They are not giving an overview of the amount spent and in which category the user has transacted more amount and hence it does not support expense management. Mainly the existing finance tracking systems are not using AI-based authentication which is more convenient for the user. Our web application is proposed to give transaction charts and graphs that give more clarity of the income, expenses, and transfers. In addition, we can analyse the transactions in category-wise, and we can track the history. Along with form-data authentication, we are providing OAuth and Zero-effort authentication that is more user friendly. Our SPA provides the facility to add multiple transactions at a time and it is to show the balance in the wallet on the future according to the planned transactions. Also, it can automate the data entry for recurrent transactions.
C3  - 2022 Third International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/ICSTCEE56972.2022.10100227
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10100227
Y2  - 2024/01/07/05:41:48
L2  - https://ieeexplore.ieee.org/document/10100227
ER  - 

TY  - CONF
TI  - Serverless Computing and FaaS for Airport Meteorology
AU  - Hluchý, Ladislav
AU  - Habala, Ondrej
AU  - Bobák, Martin
AU  - Tran, Viet
AU  - Ivica, Lukáš
T2  - 2022 Cybernetics & Informatics (K&I)
AB  - Serverless computing and Function-as-a-Service are programming paradigms that have many advantages for modern, distributed and highly modular applications. However, the situation for older applications with more monolithic architectures is more complex. It may be questionable whether the obvious advantages received from their porting into cloud and FaaS can outweigh the effort and resources spent on the transformation itself. In this paper, we present our initial effort aimed at transforming one such application, successfully being used for several years for visibility determination at airports - a significant component of air traffic safety. We have chosen to modularize the application, divide it into parts that can be implemented as functions in the FaaS paradigm, and implement simple cloud-based data management. The tools that we are using in the initial stage are OpenWhisk for FaaS and Airflow for workflow management (events production for OpenWhisk, which has an event-driven programming model), while we are also exploring the application of OpenFaaS and OSCAR. The work is still in initial stages and only a small part of the application has been implemented as OpenWhisk actions (FaaS functions) so far.
C3  - 2022 Cybernetics & Informatics (K&I)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/KI55792.2022.9925943
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9925943
Y2  - 2024/01/07/05:42:39
L2  - https://ieeexplore.ieee.org/document/9925943
ER  - 

TY  - CONF
TI  - Serverless Computing Approach for Deploying Machine Learning Applications in Edge Layer
AU  - Bac, Ta Phuong
AU  - Tran, Minh Ngoc
AU  - Kim, YoungHan
T2  - 2022 International Conference on Information Networking (ICOIN)
AB  - Serverless computing-a stateless cloud computing model, is an emerging solution that has shown significant benefits to efficiency and cost for event-driven applications in the cloud environment, including artificial intelligence (AI), machine learning applications. With serverless computing, the machine learning system’s complexity is minimized, flexible and straightforward in management. However, operating and managing serverless machine learning services on clouds faces many limitations such as latency and data privacy. Local distributed edge computing nodes which are closed to users can address these challenges of cloud-serverless AI applications. Based on this motivation, in this paper, we propose an architecture for deploying machine learning workload as serverless functions in the edge environment. We illustrate our proposed approach and evaluate its performance and effectiveness by exploiting a holistic end-to-end image classifier, a famous machine learning use case in the MNIST dataset. Our proof of concept provides comprehensive assessments that prove its effectiveness in latency reduction and distributed machine learning deployment.
C3  - 2022 International Conference on Information Networking (ICOIN)
DA  - 2022/01//
PY  - 2022
DO  - 10.1109/ICOIN53446.2022.9687209
DP  - IEEE Xplore
SP  - 396
EP  - 401
SN  - 1976-7684
UR  - https://ieeexplore.ieee.org/document/9687209
Y2  - 2024/01/07/05:42:52
L2  - https://ieeexplore.ieee.org/document/9687209
ER  - 

TY  - CONF
TI  - Serverless Computing at the Edge for AIoT Applications
AU  - Mora, H.
AU  - Mora-Gimeno, F.J.
AU  - Jimeno-Morenilla, A.
AU  - Macia-Lillo, A.
AU  - Elouali, A.
T2  - 2022 International Conference on Artificial Intelligence of Things (ICAIoT)
AB  - Serverless computing is a new trend for developing Cloud hosted applications. This paradigm takes advantage of the scalability and flexibility of the management of infrastructure from the Cloud provider to offer advantages in cost and maintenance of Cloud applications. Although it was originally designed for Cloud deployments, some proposals have been made to give its benefits closer where the data is generated and provide service to IoT applications. However, some important challenges remain in order to provide a suitable solution for all cases. This work proposes a computational model for designing a serverless architecture at the edge side of the network. The model is especially focused on Artificial Intelligence (AI) functions of IoT applications, where serverless could achieve its full potential. The results show that the proposal allows to overcome the main drawbacks raised while maintains the cloud processing support.
C3  - 2022 International Conference on Artificial Intelligence of Things (ICAIoT)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/ICAIoT57170.2022.10121879
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10121879
Y2  - 2024/01/07/05:43:14
L2  - https://ieeexplore.ieee.org/document/10121879
ER  - 

TY  - CONF
TI  - Serverless Implementations of Real-time Embarrassingly Parallel Problems
AU  - Mileski, Dimitar
AU  - Gusev, Marjan
T2  - 2022 30th Telecommunications Forum (℡FOR)
AB  - In this paper, we conduct experiments to deploy a scalable serverless computing solution for real-time monitoring of thousands of patients with streaming electrocardiograms as an example of embarrassingly parallel tasks originally executed on two virtual machines. The research question is to find the speedup of such solution versus classical virtual machine approaches with sequential or parallel threads.The challenge of migrating an existing service to a serverless solution is to adapt and reconfigure the code for serverless platform, to write the code to invoke the service in parallel and asynchronously, and to use other services in the cloud that are needed for the whole solution to be functional and scalable. Evaluation of developing various solutions matching migration challenges to Google Cloud Run, Google Cloud Compute Engine, and Google Cloud Storage (customization of code, the configuration of services) shows that greater speedups can be achieved by dividing the Embarrassingly Parallel tasks into sub-tasks executed as a serverless service. We achieved highest speedup of almost 40 for Serverless solution compared to a sequential execution on a virtual machine solution, and speedup of 23 for Serverless solution compared to a Parallel execution using virtual machines.
C3  - 2022 30th Telecommunications Forum (℡FOR)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/℡FOR56187.2022.9983710
DP  - IEEE Xplore
SP  - 1
EP  - 4
UR  - https://ieeexplore.ieee.org/document/9983710
Y2  - 2024/01/07/05:43:58
L2  - https://ieeexplore.ieee.org/document/9983710
ER  - 

TY  - CONF
TI  - Serverless Video Analysis Pipeline for Autonomous Remote Monitoring System
AU  - Rohan, Mohammad
AU  - Ahmed, Shurjeel
AU  - Kaleem, Mohammad
AU  - Nazir, Sajid
T2  - 2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)
AB  - Cloud computing is the delivery of computing services including servers, storage, databases, networking, software, and analytics over the Internet. The businesses can store the data and run applications in the cloud with a high availability, accessibility from anywhere with a web browser, and on pay-per-use basis. Public cloud platforms make it possible for anyone to deploy end-to-end applications easily and economically. The cloud hosting for business applications provides the additional benefits of security, elasticity, and logging compared to an in-house private cloud.In this paper, we describe the implementation of a serverless remote video monitoring solution on the Amazon Web Services (AWS) cloud. It can provide surveillance of an area against unauthorized access, identification of objects in the scene, and investigating security incidents. This autonomous prototype application provides a frame-by-frame object detection of the live video. A Short Message Service (SMS) notification alert is generated and sent to the third-party on detecting an object of interest. It provides a flexible and economical solution for remote monitoring.
C3  - 2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/ICETECC56662.2022.10068884
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10068884
Y2  - 2024/01/07/05:44:18
L1  - https://researchonline.gcu.ac.uk/files/60693437/20221013_Final_AWS_paper_camera_ready.pdf
L2  - https://ieeexplore.ieee.org/document/10068884
ER  - 

TY  - CONF
TI  - Serverless: From Bad Practices to Good Solutions
AU  - Taibi, Davide
AU  - Kehoe, Ben
AU  - Poccia, Danilo
T2  - 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)
AB  - Serverless computing is increasing its popularity in the industry. However, practitioners still have issues when using it, also because serverless bad practices, bad smells, and anti-patterns have not been deeply investigated. In this work, we identify the main bad practices experienced by practitioners during the development of serverless-based applications. We interviewed 91 experienced practitioners and we performed a focus group to analyze the solutions they adopted to solve the issues generated by the bad practice. Moreover, we propose the most appropriate solutions based on our professional experience. The results can be helpful to other practitioners to avoid facing the same issues, to understand how to overcome them, and to researchers that can better validate them and propose alternative solutions.
C3  - 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)
DA  - 2022/08//
PY  - 2022
DO  - 10.1109/SOSE55356.2022.00016
DP  - IEEE Xplore
SP  - 85
EP  - 92
SN  - 2642-6587
ST  - Serverless
UR  - https://ieeexplore.ieee.org/document/9912641
Y2  - 2024/01/07/05:44:46
L2  - https://ieeexplore.ieee.org/document/9912641
ER  - 

TY  - CONF
TI  - SLAM: SLO-Aware Memory Optimization for Serverless Applications
AU  - Safaryan, Gor
AU  - Jindal, Anshul
AU  - Chadha, Mohak
AU  - Gerndt, Michael
T2  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
AB  - Serverless computing paradigm has become more ingrained into the industry, as it offers a cheap alternative for application development and deployment. This new paradigm has also created new kinds of problems for the developer, who needs to tune memory configurations for balancing cost and performance. Many researchers have addressed the issue of minimizing cost and meeting Service Level Objective (SLO) requirements for a single FaaS function, but there has been a gap for solving the same problem for an application consisting of many FaaS functions, creating complex application workflows.In this work, we designed a tool called SLAM to address the issue. SLAM uses distributed tracing to detect the relationship among the FaaS functions within a serverless application. By modeling each of them, it estimates the execution time for the application at different memory configurations. Using these estimations, SLAM determines the optimal memory configuration for the given serverless application based on the specified SLO requirements and user-specified objectives (minimum cost or minimum execution time). We demonstrate the functionality of SLAM on AWS Lambda by testing on four applications. Our results show that the suggested memory configurations guarantee that more than 95% of requests are completed within the predefined SLOs.
C3  - 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)
DA  - 2022/07//
PY  - 2022
DO  - 10.1109/CLOUD55607.2022.00019
DP  - IEEE Xplore
SP  - 30
EP  - 39
SN  - 2159-6190
ST  - SLAM
UR  - https://ieeexplore.ieee.org/document/9860980
Y2  - 2024/01/07/05:45:20
L1  - https://arxiv.org/pdf/2207.06183
L2  - https://ieeexplore.ieee.org/document/9860980
ER  - 

TY  - CONF
TI  - Smart Water Flow Meter for Improved Measurement of Water Usage in a Smart City
AU  - Sushma, N.
AU  - Suresh, H. N.
AU  - Lakshmi, J. Mohana
T2  - 2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)
AB  - Water shortage is one of the predominant problems which influences billions of humans throughout the world. With the cities transforming to dwelling places of smart infrastructure, controlling, analyzing and reducing the water consumption of the houses has proved to be challenging. Existing smart water monitoring devices are prone to theft, damage and can be quite expensive. As a result, developing countries are facing a difficult time in acquiring and maintaining high quality monitoring tools. Thereby water information stays erroneous and generally depends on isolated estimations. Hence, there is an increased demand for low-cost, reliable and precise flow sensors that are easy to use. To meet this necessity a low-cost, remote monitored smart water meter is proposed in this article to monitor water usage in urban private residences. A water flow sensor along with Raspberry Pi Pico is used that continuously monitors water usage. With the ability to monitor and visualize data with ease, the designed smart water meter is capable of sensing even moderate pipeline flow rates with a serverless architecture. The proposed meter also records monthly average water requirement of a house.
C3  - 2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)
DA  - 2022/04//
PY  - 2022
DO  - 10.1109/ICAECT54875.2022.9808041
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/9808041
Y2  - 2024/01/07/05:45:41
L2  - https://ieeexplore.ieee.org/document/9808041
ER  - 

TY  - CONF
TI  - SMPI: Scalable Serverless MPI Computing
AU  - Yuan, Yuxin
AU  - Shi, Xiao
AU  - Lei, Zhengyu
AU  - Wang, Xiaohong
AU  - Zhao, Xiaofang
T2  - 2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)
AB  - Running HPC in the cloud has gained more and more practice. As a new cloud paradigm, serverless is highly attractive for HPC service providers due to its distinctive benefits such as scalability. However, it is difficult for serverless to meet the demands of MPI programming and running, resulting in that MPI programs cannot scale with serverless functions. We introduce a serverless parallel function model to solve the problems. It divides parallelism at function and worker levels to bridge gaps in programming and running between serverless and MPI. Then we present the SMPI framework atop the model. For programming, SMPI redefines the function generation pipeline for parallel functions to prepare metadata for MPI parallel functions. For running, SMPI employs the parallel function gateway and scheduler to realize parallel function invocation and instantiation for MPI parallel functions. It is implemented and evaluated with OpenFaaS. Experiments show that SMPI supports MPI programming and running in a complete serverless manner. Compared to server-centric methods, it reduces efforts on cluster maintenance, provides scalable serverless MPI computing with competitive performance (0.559–1.048s slower of start-up time, and 0.145s–0.945s slower of computing time than best-behaved baseline), and is potential to scale on multiple clusters for higher scalability.
C3  - 2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/IPCCC55026.2022.9894339
DP  - IEEE Xplore
SP  - 275
EP  - 282
SN  - 2374-9628
ST  - SMPI
UR  - https://ieeexplore.ieee.org/document/9894339
Y2  - 2024/01/07/05:46:02
L2  - https://ieeexplore.ieee.org/document/9894339
ER  - 

TY  - CONF
TI  - Streaming vs. Functions: A Cost Perspective on Cloud Event Processing
AU  - Pfandzelter, Tobias
AU  - Henning, Sören
AU  - Schirmer, Trever
AU  - Hasselbring, Wilhelm
AU  - Bermbach, David
T2  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
AB  - In cloud event processing, data generated at the edge is processed in real-time by cloud resources. Both distributed stream processing (DSP) and Function-as-a-Service (FaaS) have been proposed to implement such event processing applications. FaaS emphasizes fast development and easy operation, while DSP emphasizes efficient handling of large data volumes. Despite their architectural differences, both can be used to model and implement loosely-coupled job graphs. In this paper, we consider the selection of FaaS and DSP from a cost perspective. We implement stateless and stateful workflows from the Theodolite benchmarking suite using cloud FaaS and DSP. In an extensive evaluation, we show how application type, cloud service provider, and runtime environment can influence the cost of application deployments and derive decision guidelines for cloud engineers.
C3  - 2022 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2022/09//
PY  - 2022
DO  - 10.1109/IC2E55432.2022.00015
DP  - IEEE Xplore
SP  - 67
EP  - 78
ST  - Streaming vs. Functions
UR  - https://ieeexplore.ieee.org/document/9946366
Y2  - 2024/01/07/05:46:37
L1  - https://arxiv.org/pdf/2204.11509
L2  - https://ieeexplore.ieee.org/document/9946366
ER  - 

TY  - CONF
TI  - Supporting Multi-Cloud in Serverless Computing
AU  - Zhao, Haidong
AU  - Benomar, Zakaria
AU  - Pfandzelter, Tobias
AU  - Georgantas, Nikolaos
T2  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
AB  - Serverless computing is a widely adopted cloud execution model composed of Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS) offerings. The increased level of abstraction makes vendor lock-in inherent to serverless computing, raising more concerns than previous cloud paradigms. Multicloud serverless is a promising emerging approach against vendor lock-in, yet multiple challenges must be overcome to tap its potential. First, we need to be aware of both the performance and cost of each FaaS provider. Second, a multi-cloud architecture needs to be proposed before deploying a multi-cloud workflow. Domain-specific serverless offerings must then be integrated into the multi-cloud architecture to improve performance and/or save costs. Finally, we require workload portability support for serverless multi-cloud. In this paper, we present a multi-cloud library for crossserverless offerings. We develop an analysis system to support comparison among public FaaS providers in terms of performance and cost. Moreover, we present how to alleviate data gravity with domain-specific serverless offerings. Finally, we deploy workloads on these architectures to evaluate several public FaaS offerings.
C3  - 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/UCC56403.2022.00051
DP  - IEEE Xplore
SP  - 285
EP  - 290
UR  - https://ieeexplore.ieee.org/document/10061782
Y2  - 2024/01/07/05:52:17
L1  - https://arxiv.org/pdf/2209.09367
L2  - https://ieeexplore.ieee.org/document/10061782
KW  - Computer architecture
KW  - Serverless
KW  - Performance
KW  - serverless
KW  - Lock-in
KW  - Locks (fasteners)
KW  - vendor lock-in
KW  - Vendor lock-in
KW  - Multi-clouds
KW  - Service provider
KW  - Architecture
KW  - Cloud architectures
KW  - Cloud computing architecture
KW  - Cost benefit analysis
KW  - Domain specific
KW  - multi-cloud
KW  - performance and cost
KW  - Performance and cost
KW  - Service offering
KW  - Taps
ER  - 

TY  - JOUR
TI  - TEMPOS: QoS Management Middleware for Edge Cloud Computing FaaS in the Internet of Things
AU  - Garbugli, Andrea
AU  - Sabbioni, Andrea
AU  - Corradi, Antonio
AU  - Bellavista, Paolo
T2  - IEEE Access
AB  - Several classes of advanced Internet of Things (IoT) applications, e.g., in the industrial manufacturing domain, call for Quality of Service (QoS) management to guarantee/control performance indicators, even in presence of many sources of “stochastic noise” in real deployment environments, from scarcely available bandwidth in a time window to concurrent usage of virtualized processing resources. This paper proposes a novel IoT-oriented middleware that i) considers and coordinates together different aspects of QoS monitoring, control, and management for different kinds of virtualized resources (from networking to processing) in a holistic way, and ii) specifically targets deployment environments where edge cloud resources are employed to enable the Serverless paradigm in the cloud continuum. The reported experimental results show how it is possible to achieve the desired QoS differentiation by coordinating heterogeneous mechanisms and technologies already available in the market. This demonstrates the feasibility of effective QoS-aware management of virtualized resources in the cloud-to-things continuum when considering a Serverless provisioning scenario, which is completely original in the related literature to the best of our knowledge.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3173434
DP  - IEEE Xplore
VL  - 10
SP  - 49114
EP  - 49127
J2  - IEEE Access
SN  - 2169-3536
ST  - TEMPOS
UR  - https://ieeexplore.ieee.org/document/9770777
Y2  - 2024/01/07/05:58:11
L1  - https://ieeexplore.ieee.org/ielx7/6287639/9668973/09770777.pdf
L2  - https://ieeexplore.ieee.org/document/9770777
ER  - 

TY  - CONF
TI  - Testing Approaches And Tools For AWS Lambda Serverless-Based Applications
AU  - Rinta-Jaskari, Eetu
AU  - Allen, Christopher
AU  - Meghla, Tamara
AU  - Taibi, Davide
T2  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
AB  - With serverless-based applications are increasing their popularity, little is known on testing practices and tools available to test serverless functions. This work aims to identify testing approaches for serverless functions built for the Amazon Web Services cloud platform, and to demonstrate how to implement them to a full-stack application. For this purpose, we implemented unit, integration and system tests to an existing open source application providing insights of the testing practices and and tools applicable. Results shows that all the testing practices are applicable, even if there is a lack of tools to support end-to-end tests, especially for debugging.
C3  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
DA  - 2022/03//
PY  - 2022
DO  - 10.1109/PerComWorkshops53856.2022.9767473
DP  - IEEE Xplore
SP  - 686
EP  - 692
UR  - https://ieeexplore.ieee.org/document/9767473
Y2  - 2024/01/07/05:58:37
L2  - https://ieeexplore.ieee.org/document/9767473
KW  - Testing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - serverless function
KW  - Infrastructure as a service (IaaS)
KW  - Open source software
KW  - Lambda's
KW  - Application programs
KW  - Amazon web services
KW  - Amazon Web Services
KW  - faas test
KW  - Faas test
KW  - Infrastructure-as-code
KW  - Infrastructure-as-Code
KW  - Integration testing
KW  - Lambda
KW  - lambda test
KW  - Lambda test
KW  - Open systems
KW  - Serverless function
KW  - serverless testing
KW  - Serverless testing
KW  - software testing
KW  - Software testings
KW  - Web services
KW  - Websites
ER  - 

TY  - CONF
TI  - The Next Generation Edge Technologies: Architectures, Challenges, and Research Directions
AU  - Patel, Akash
AU  - Nayak, Amit
T2  - 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)
AB  - Advanced computing technology like Edge Computing moves the data to the edge of the network for processing and storage. With the advancement of communication technologies such as 5th Generation (5G) networks, the number of computing devices connected over internet and the data generated by these devices are also increased, which demand for this variation of cloud computing (CC). Edge computing was introduced with the goal to overcomes the limitations of cloud computing such as significant communication overhead and overall communication time in networks with many computing devices that produce and consume huge amounts of data. Moreover, Edge Computing boosts network support for mobility and confidentiality. The article discusses Edge Computing, its architectures and gives brief summary of its concept, core characteristics and challenges.
C3  - 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/ICAISS55157.2022.10011000
DP  - IEEE Xplore
SP  - 851
EP  - 855
ST  - The Next Generation Edge Technologies
UR  - https://ieeexplore.ieee.org/document/10011000
Y2  - 2024/01/07/05:59:09
L2  - https://ieeexplore.ieee.org/document/10011000
ER  - 

TY  - JOUR
TI  - The Shape of Your Cloud: How to Design and Run Polylithic Cloud Applications
AU  - Toka, Laszlo
T2  - IEEE Access
AB  - Nowadays the major trend in IT dictates deploying applications in the cloud, cutting the monolithic software into small, easily manageable and developable components, and running them in a microservice scheme. With these choices come the questions: which cloud service types to choose from the several available options, and how to distribute the monolith in order to best resonate with the selected cloud features. We propose a model that presents monolithic applications in a novel way and focuses on key properties that are crucial in the development of cloud-native applications. The model focuses on the organization of scaling units, and it accounts for the cost of provisioned resources in scale-out periods and invocation delays among the application components. We analyze dis-aggregated monolithic applications that are deployed in the cloud, offering both Container-as-a-Service (CaaS) and Function-as-a-Service (FaaS) platforms. We showcase the efficiency of our proposed optimization solution by presenting the reduction in operation costs as an illustrative example. We propose to group similarly low scale components together in CaaS, while running dynamically scaled components in FaaS. By doing so, the price is decreased as unnecessary memory provisioning is eliminated, while application response time does not show any degradation.
DA  - 2022///
PY  - 2022
DO  - 10.1109/ACCESS.2022.3206433
DP  - IEEE Xplore
VL  - 10
SP  - 97971
EP  - 97982
J2  - IEEE Access
SN  - 2169-3536
ST  - The Shape of Your Cloud
UR  - https://ieeexplore.ieee.org/document/9889714
Y2  - 2024/01/07/06:01:26
L1  - https://ieeexplore.ieee.org/ielx7/6287639/6514899/09889714.pdf
ER  - 

TY  - CONF
TI  - Toward Scientific Workflows in a Serverless World
AU  - Khochare, Aakash
AU  - Simmhan, Yogesh
AU  - Mehta, Sameep
AU  - Agarwal, Arvind
T2  - 2022 IEEE 18th International Conference on e-Science (e-Science)
AB  - Serverless computing and FaaS have gained popularity due to their ease of design, deployment, scaling and billing on clouds. However, when used to compose and orchestrate scientific workflows, they pose limitations due to cold starts, message indirection, vendor lock-in and lack of provenance support. Here, we propose a design for a Ser verless Scientific Workflow Orchestrator that overcomes these challenges using techniques like function fusion, pilot invocations and data fabrics.
C3  - 2022 IEEE 18th International Conference on e-Science (e-Science)
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/eScience55777.2022.00057
DP  - IEEE Xplore
SP  - 399
EP  - 400
UR  - https://ieeexplore.ieee.org/document/9973585
Y2  - 2024/01/07/06:01:42
L1  - https://eprints.iisc.ac.in/79644/1/eScience_2022.pdf
L2  - https://ieeexplore.ieee.org/document/9973585
ER  - 

TY  - CONF
TI  - Towards a Model-Based Serverless Platform for the Cloud-Edge-IoT Continuum
AU  - Ferry, Nicolas
AU  - Dautov, Rustem
AU  - Song, Hui
T2  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - One of the most prominent implementations of the serverless programming model is Function-as-a-Service (FaaS). Using FaaS, application developers provide source code of serverless functions, typically describing only parts of a larger application, and define triggers for executing these functions on infrastructure components managed by the FaaS provider. There are still challenges that hinder the wider adoption of the FaaS model across the whole Cloud-Edge-IoT continuum. These include the high heterogeneity of the Edge and IoT infrastructure, vendor lock-in, the need to deploy and adapt serverless functions as well as their supporting services and software stacks into their cyber-physical execution environment. As a first step towards addressing these challenges, we introduce the SERVERLEss4I0T platform for the design, deployment, and maintenance of applications over the Cloud-Edge-IoT continuum. In particular, our platform enables the specification and deployment of serverless functions on Cloud and Edge resources, as well as the deployment of their supporting services and software stacks over the whole Cloud-Edge-IoT continuum.
C3  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2022/05//
PY  - 2022
DO  - 10.1109/CCGrid54584.2022.00101
DP  - IEEE Xplore
SP  - 851
EP  - 858
UR  - https://ieeexplore.ieee.org/document/9826113
Y2  - 2024/01/07/06:02:07
L1  - https://hal.inria.fr/hal-03729308/file/Serverless4IoT-19.pdf
L2  - https://ieeexplore.ieee.org/document/9826113
KW  - Internet of Things
KW  - Cloud Computing
KW  - Edge computing
KW  - Internet of things
KW  - Deployment
KW  - Cloud-computing
KW  - Application developers
KW  - Edge Computing
KW  - Model-based OPC
KW  - Model-driven Engineering
KW  - Model-Driven Engineering
KW  - Programming models
KW  - Service stack
KW  - Services applications
KW  - Software stacks
ER  - 

TY  - CONF
TI  - Towards Extreme and Sustainable Graph Processing for Urgent Societal Challenges in Europe
AU  - Prodan, Radu
AU  - Kimovski, Dragi
AU  - Bartolini, Andrea
AU  - Cochez, Michael
AU  - Iosup, Alexandru
AU  - Kharlamov, Evgeny
AU  - Rožanec, Jože
AU  - Vasiliu, Laurenţiu
AU  - Vărbănescu, Ana Lucia
T2  - 2022 IEEE Cloud Summit
AB  - The Graph-Massivizer project, funded by the Horizon Europe research and innovation program, researches and develops a high-performance, scalable, and sustainable platform for information processing and reasoning based on the massive graph (MG) representation of extreme data. It delivers a toolkit of five open-source software tools and FAIR graph datasets covering the sustainable lifecycle of processing extreme data as MGs. The tools focus on holistic usability (from extreme data ingestion and MG creation), automated intelligence (through analytics and reasoning), performance modelling, and environmental sustainability tradeoffs, supported by credible data-driven evidence across the computing continuum. The automated operation uses the emerging serverless computing paradigm for efficiency and event responsiveness. Thus, it supports experienced and novice stakeholders from a broad group of large and small organisations to capitalise on extreme data through MG programming and processing. Graph-Massivizer validates its innovation on four complementary use cases considering their extreme data properties and coverage of the three sustainability pillars (economy, society, and environment): sustainable green finance, global environment protection foresight, green AI for the sustainable automotive industry, and data centre digital twin for exascale computing. Graph-Massivizer promises 70% more efficient analytics than AliGraph, and 30 % improved energy awareness for extract, transform and load storage operations than Amazon Redshift. Furthermore, it aims to demonstrate a possible two-fold improvement in data centre energy efficiency and over 25 % lower greenhouse gas emissions for basic graph operations.
C3  - 2022 IEEE Cloud Summit
DA  - 2022/10//
PY  - 2022
DO  - 10.1109/CloudSummit54781.2022.00010
DP  - IEEE Xplore
SP  - 23
EP  - 30
UR  - https://ieeexplore.ieee.org/document/9973125
Y2  - 2024/01/07/06:04:49
ER  - 

TY  - CONF
TI  - Towards QoS-Aware Function Composition Scheduling in Apache OpenWhisk
AU  - Russo, Gabriele Russo
AU  - Milani, Alfredo
AU  - Iannucci, Stefano
AU  - Cardellini, Valeria
T2  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
AB  - Function-as-a-Service (FaaS) is increasingly popular thanks to the benefits provided to application developers and operators. Besides commercial Cloud-based offerings, open-source solutions have emerged enabling FaaS deployment on private infrastructures and possibly at the edge of the network. When moving from the Cloud to Fog/Edge environments, optimizing resource allocation for function execution becomes a critical challenge. Unfortunately, existing FaaS platforms have little or no support for fine-grained scheduling and resource allocation, nor allow users to enforce Quality-of-Service (QoS) requirements. We take a first step towards the development of a QoS-aware FaaS platform. We design and implement new mechanisms to support differentiated classes of services within Apache OpenWhisk, a popular open-source FaaS framework. Our experiments show that our prototype efficiently supports state-of-the-art scheduling policies and provides throughput improvements when dealing with function compositions under high load scenarios.
C3  - 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
DA  - 2022/03//
PY  - 2022
DO  - 10.1109/PerComWorkshops53856.2022.9767299
DP  - IEEE Xplore
SP  - 693
EP  - 698
UR  - https://ieeexplore.ieee.org/document/9767299
Y2  - 2024/01/07/06:05:00
L2  - https://ieeexplore.ieee.org/document/9767299
ER  - 

TY  - CONF
TI  - TUFA: A TOSCA extension for the specification of accelerator-aware applications in the Cloud Continuum
AU  - Spătaru, Adrian
AU  - Iuhasz, Gabriel
AU  - Panica, Silviu
T2  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
AB  - A Distributed Application Topology is a valuable commodity built on the strength of a long and iterative design process. A topology is generally refined over time, other topologies can use it as a component, and the community may share it. To reproduce a deployment, several properties must be recorded such as data origin, processing steps, configuration settings, and hardware requirements. This paper presents an extension to the TOSCA specification that allows for the definition of accelerator-aware services that can span from Cloud to Edge. Additionally, we introduce the concept of Abstract Applications that contain at least one abstract service definition. The process of Service Optimization replaces the abstract sertvices, creating an explicit topology deployable under hybrid deployment models (Virtual Machines, Containers, HPC) residing on the Cloud Continuum spectrum.
C3  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/COMPSAC54236.2022.00185
DP  - IEEE Xplore
SP  - 1178
EP  - 1183
SN  - 0730-3157
ST  - TUFA
UR  - https://ieeexplore.ieee.org/document/9842714
Y2  - 2024/01/07/06:05:31
L2  - https://ieeexplore.ieee.org/document/9842714
ER  - 

TY  - CONF
TI  - Workflow Scheduling Using Hybrid PSO-GA Algorithm in Serverless Edge Computing for the Internet of Things
AU  - Xie, Renchao
AU  - Gu, Dier
AU  - Tang, Qinqin
AU  - Huang, Tao
AU  - Yu, F. Richard
T2  - 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)
AB  - In this paper, we design a task scheduling scheme for Internet of Things (IoT) workflow applications in serverless edge computing. Notice the fact that complex applications in traditional serverless computing are decomposed into several stateless, dependent functions, whose execution environments are pre-deployed at the resource-finite edge domain, we model the workflow application as Directed Acyclic Graph (DAG) by considering the distribution of edge resources and the deployment of serverless functions. We further formulate the scheduling problem as a multi-objective optimization problem to reduce the time consumption, energy consumption, and cost simultaneously. Then, considering the diversity of solution space and the fast convergence to optimal solutions, an improved hybrid algorithm that combines Particle Swarm Optimization and Genetic Algorithm (PSO–GA) is introduced and utilized to make the scheduling decision. Finally, extensive simulation experiments are conducted to validate the superiority of the proposed scheme.
C3  - 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)
DA  - 2022/06//
PY  - 2022
DO  - 10.1109/VTC2022-Spring54318.2022.9860395
DP  - IEEE Xplore
SP  - 1
EP  - 7
SN  - 2577-2465
UR  - https://ieeexplore.ieee.org/document/9860395
Y2  - 2024/01/07/06:06:02
L2  - https://ieeexplore.ieee.org/document/9860395
ER  - 

TY  - CONF
TI  - Workflow Sensitive Access Management in Serverless Computing
AU  - Kumari, Anisha
AU  - Akram Khan, Md.
AU  - Sahoo, Bibhudatta
T2  - 2022 IEEE 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)
AB  - In recent years, serverless computing has been emerging as a most profitable cloud framework, which drastically improves the development and deployment policy of online services, but as a result, it is highly exposed to tempting targets for attackers. These attackers are proposing innovative strategies to get beyond the transitory nature of serverless activities by taking advantage of container reuse for the execution of stateless functions. The external request for function invocation must be extensively verified to protect the valuable resources from attackers. Traditional access management policy usually checks the individual inbound request for function invocation by ignoring other dependencies associated with the complete workflow. In this paper, we have proposed a two-phase workflow sensitive access management (WAM) policy that provides authentication tokens and checks whether the incoming request possesses all the necessary permission or not. WAM is based on a state-dependency graph which is a representation of allowable permission required to make transitions among the functions in the workflow. AWS Lambda is considered as the base framework where WAM policy is integrated. The effectiveness of WAM is verified using four real-world serverless applications and the performance is extensively compared with other standard serverless frameworks like Openwhisk, Openfaas, and Microsoft Azure.
C3  - 2022 IEEE 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)
DA  - 2022/12//
PY  - 2022
DO  - 10.1109/iSSSC56467.2022.10051255
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10051255
Y2  - 2024/01/07/06:06:13
L2  - https://ieeexplore.ieee.org/document/10051255
ER  - 

TY  - CONF
TI  - WoS Bibliometric-based Review on Serverless Computing model
AU  - Kumar, Ajay
AU  - Gupta, Rohan
AU  - Bhandari, Rahul
T2  - 2022 Seventh International Conference on Parallel, Distributed and Grid Computing (PDGC)
AB  - The Serverless architecture is a cloud-native development paradigm that allows developers to build and run programs. Servers are still there in serverless, but they are not involved in the creation of apps. A cloud provider often takes care of the server infrastructure’s provisioning, upkeep, and expansion. Developers merely need to bundle their code in containers in order to deploy it. It also aids in the construction of "cloud-naive computing model". The term "cloud-naive" describes the possible advantages for the environment that IT services provided online may present to society. The number of resources used by the IT services is causing a resource deficit. We use the last 6 years of WoS articles, which were published between 2017 and 2022. Overall analysis is performed in a RStudio environment using the "bibliometrix" library. Even so, we use AMSTAR and PRISMA check-lists to evaluate the methodological quality of the systematic review technique. We compute the outcomes based on eight different factors, including publisher, year, citation, sources, technology, country adoption, research impact, and research gaps.
C3  - 2022 Seventh International Conference on Parallel, Distributed and Grid Computing (PDGC)
DA  - 2022/11//
PY  - 2022
DO  - 10.1109/PDGC56933.2022.10053142
DP  - IEEE Xplore
SP  - 600
EP  - 605
SN  - 2573-3079
UR  - https://ieeexplore.ieee.org/document/10053142
Y2  - 2024/01/07/06:06:31
L2  - https://ieeexplore.ieee.org/document/10053142
ER  - 

TY  - JOUR
TI  - λDNN: Achieving Predictable Distributed DNN Training With Serverless Architectures
AU  - Xu, Fei
AU  - Qin, Yiling
AU  - Chen, Li
AU  - Zhou, Zhi
AU  - Liu, Fangming
T2  - IEEE Transactions on Computers
AB  - Serverless computing is becoming a promising paradigm for Distributed Deep Neural Network (DDNN) training in the cloud, as it allows users to decompose complex model training into a number of functions without managing virtual machines or servers. Though provided with a simpler resource interface (i.e., function number and memory size), inadequate function resource provisioning (either under-provisioning or over-provisioning) easily leads to unpredictable DDNN training performance in serverless platforms. Our empirical studies on AWS Lambda indicate that, such unpredictable performance of serverless DDNN training is mainly caused by the resource bottleneck of Parameter Servers (PS) and small local batch size. In this article, we design and implement λλDNN, a cost-efficient function resource provisioning framework to provide predictable performance for serverless DDNN training workloads, while saving the budget of provisioned functions. Leveraging the PS network bandwidth and function CPU utilization, we build a lightweight analytical DDNN training performance model to enable our design of λλDNN resource provisioning strategy, so as to guarantee DDNN training performance with serverless functions. Extensive prototype experiments on AWS Lambda and complementary trace-driven simulations demonstrate that, λλDNN can deliver predictable DDNN training performance and save the monetary cost of function resources by up to 66.7 percent, compared with the state-of-the-art resource provisioning strategies, yet with an acceptable runtime overhead.
DA  - 2022/02//
PY  - 2022
DO  - 10.1109/TC.2021.3054656
DP  - IEEE Xplore
VL  - 71
IS  - 2
SP  - 450
EP  - 463
J2  - IEEE Transactions on Computers
SN  - 1557-9956
ST  - λDNN
UR  - https://ieeexplore.ieee.org/document/9336272
Y2  - 2024/01/07/06:06:51
L2  - https://ieeexplore.ieee.org/document/9336272
ER  - 

TY  - CONF
TI  - 11 things about Securing Microservice
AU  - Madheswaran, Yuvaraj
T2  - 2023 IEEE Secure Development Conference (SecDev)
AB  - Microservices are a modern-era software development approach to creating REST APIs as a small independent process that is loosely coupled, performs business specific operation or capabilities, and is owned by a small team. Microservices are light weight components that are easy to develop, deploy and scale based on business requirements. Some of our customers use microservice with container-based deployment, some implement regular legacy process build and deploy on the webserver and some serverless in private, public, or hybrid cloud. Microservices are evolving and distributing the business requirement logic to multiple services which increases application complexity, maintainability and security while interacting with other services. Each organization follows multiple ways to secure their Microservices. Here, we will go through some of the security implementations and finally will review securing Microservices in zero trust.
C3  - 2023 IEEE Secure Development Conference (SecDev)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/SecDev56634.2023.00019
DP  - IEEE Xplore
SP  - 51
EP  - 53
LA  - English
PB  - IEEE Computer Society
SN  - 9798350331325
UR  - https://ieeexplore.ieee.org/document/10305626
Y2  - 2024/01/07/18:08:08
L2  - https://ieeexplore.ieee.org/document/10305626
ER  - 

TY  - CONF
TI  - A Comparison of Distributed Tracing Tools in Serverless Applications
AU  - Eder, Christina
AU  - Winzinger, Stefan
AU  - Lichtenthäler, Robin
T2  - 2023 IEEE International Conference on Service-Oriented System Engineering (SOSE)
AB  - Serverless computing can favor the emergence of complex and error-prone applications. In order to gain observability in such applications, distributed tracing can be used. However, as serverless computing relies on the pay-per-use billing model, utilizing distributed tracing tools can have a noticeable impact on the resulting costs. Therefore, this paper investigates the impact of distributed tracing in serverless applications by exploring and comparing the efficiency characteristics of three selected distributed tracing tools - Zipkin, OpenTelemetry, and SkyWalking. In particular, the runtime, the memory usage, and the initialization duration were examined by benchmarking AWS Lambda function invocations. In the experiments, Zipkin imposed the lowest runtime overhead with an average of 10.73 %, while SkyWalking introduced the highest overhead with an average runtime overhead of 50.67 %. OpenTelemetry added 24.19 % additional runtime. Besides runtime overheads, significantly higher memory usage and initialization durations were detected for all tools. Therefore, the results suggest that distributed tracing can significantly impact the efficiency of serverless applications. Nevertheless, differences could be observed concerning tracing mechanisms and use cases. This helps developers to carefully select the most suitable tracing tool considering factors such as runtime overhead, memory usage, and initialization durations.
C3  - 2023 IEEE International Conference on Service-Oriented System Engineering (SOSE)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/SOSE58276.2023.00018
DP  - IEEE Xplore
SP  - 98
EP  - 105
SN  - 2642-6587
UR  - https://ieeexplore.ieee.org/document/10254754
Y2  - 2024/01/07/18:09:02
L2  - https://ieeexplore.ieee.org/document/10254754
ER  - 

TY  - CONF
TI  - A Review of Multimedia Video Services Based on Serverless Cloud Computing
AU  - Su, Yue
AU  - Sang, Liu
AU  - Zhao, Weibo
AU  - Li, Wei
AU  - Yuan, Changqing
T2  - 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
AB  - Cloud computing technology has achieved rapid growth in recent years. Serverless cloud computing has been applied in more and more scenarios due to its pay-as-you-go, event-driven, and flexible features. This article introduces the application of serverless cloud computing technology in multimedia video services, analyzes the key capabilities, and looks forward to the key development directions in the future.
C3  - 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/BMSB58369.2023.10211228
DP  - IEEE Xplore
SP  - 1
EP  - 5
SN  - 2155-5052
UR  - https://ieeexplore.ieee.org/document/10211228
Y2  - 2024/01/07/18:23:50
L2  - https://ieeexplore.ieee.org/document/10211228
ER  - 

TY  - CONF
TI  - A Design of Edge Distributed Video Analysis System Based on Serverless Computing Service
AU  - Wang, Yutong
AU  - Yang, Mingchuan
AU  - Ding, Peng
AU  - Shen, Yun
AU  - Shi, Xiaohou
AU  - Dai, Meiling
T2  - 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
AB  - The paper introduces the design of an edge distributed video analysis system based on serverless computing services. Firstly, we briefly describe the challenges faced by the traditional multimedia video intelligent analysis system based on centralize-cloud, and analyze the idea of deploying it as a edge distributed system. Secondly, we proposed a serverless edge distributed video intelligent analysis system, introduced the system architecture and key components. Meanwhile the resource matching scheme between the stateless video analysis function and the actual edge computing device is presented. Finally, by comparing with the small-scale experimental cluster and centralized video analysis system, we verified the efficiency and effectiveness of the system design scheme by this paper.
C3  - 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/BMSB58369.2023.10211177
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2155-5052
UR  - https://ieeexplore.ieee.org/document/10211177
Y2  - 2024/01/07/18:24:11
L2  - https://ieeexplore.ieee.org/document/10211177
ER  - 

TY  - CONF
TI  - A Review on Machine Learning Methods for Workload Prediction in Cloud Computing
AU  - Yekta, Mohammad
AU  - Shahhoseini, Hadi Shahriar
T2  - 2023 13th International Conference on Computer and Knowledge Engineering (ICCKE)
AB  - Workload prediction is one of the critical parts of resource provisioning in cloud computing and its evolved branches such as serverless and edge computing. Effective resource provisioning stands as a crucial element within the realm of edge-cloud computing. Accurate prediction of cloud workloads is essential for the effective allocation of resources. Workload prediction plays a crucial role in enhancing efficiency, reducing costs, optimizing cloud performance, maintaining a high level of quality of service, and minimizing energy consumption. In this paper, we conduct a comprehensive review of state-of-the-art Machine Learning (ML) and Deep Learning (DL) algorithms employed in workload prediction in cloud computing and other similar platforms such as edge computing. We compared the selected papers in terms of utilized methods and techniques, predicted factors, accuracy metrics, and the dataset. Additionally, to facilitate usability and comparison, articles sharing similar advantages and disadvantages are organized into a table. Finally, the paper concludes by addressing current challenges and future research directions.
C3  - 2023 13th International Conference on Computer and Knowledge Engineering (ICCKE)
DA  - 2023/11//
PY  - 2023
DO  - 10.1109/ICCKE60553.2023.10326297
DP  - IEEE Xplore
SP  - 306
EP  - 311
SN  - 2643-279X
UR  - https://ieeexplore.ieee.org/document/10326297
Y2  - 2024/01/07/18:24:37
L2  - https://ieeexplore.ieee.org/document/10326297
ER  - 

TY  - CONF
TI  - A Node Agent for Fast and Safe Execution of Computing Tasks under Kubernetes
AU  - Zhang, Zhiwei
AU  - Pan, Qijun
AU  - Zhou, Jun
AU  - Zhang, Chen
T2  - 2023 International Conference on Mobile Internet, Cloud Computing and Information Security (MICCIS)
AB  - Current cloud platforms host workloads by creating a container instance and destroying the instance when the call is complete. However, the use of containers brings much overhead, which is not suitable for performance-sensitive computing tasks such as emerging IoT applications with real-time latency constraints. In this paper, we discuss the existing solutions for performing computational tasks in Kubernetes, a software that can automatically manage, scale, and maintain the required state of multiple workloads, as well as unresolved performance challenges in resource-constrained nodes. To achieve higher performance, we propose and implement a node agent that performs computing tasks quickly and safely. Compared with traditional container-based node agent, the performance of our node agent is improved by about 100 times and 1.1 times in the cold start and hot start workload scenarios, respectively. Compared with the latest solution using Webassembly, it improves the cold start and hot start time by about 10%-50%, and can be applied to richer scenarios.
C3  - 2023 International Conference on Mobile Internet, Cloud Computing and Information Security (MICCIS)
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/MICCIS58901.2023.00020
DP  - IEEE Xplore
SP  - 86
EP  - 94
UR  - https://ieeexplore.ieee.org/document/10242630
Y2  - 2024/01/07/18:25:07
ER  - 

TY  - CONF
TI  - A Secure Resale Management System using Cloud Services and ReactJS
AU  - Baseer, K. K.
AU  - Jahir Pasha, M.
AU  - Srinivasulu, B. V.
AU  - Moon, Shaik Ali
T2  - 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)
AB  - Resale Management System (RMS) is a convenient approach to buy or to sell products from the comfort of home through the internet. Current Scenario of reselling products involves a person who buys the products from customers and sells them to others which impacts on price. In most of the cases customers need to manually visit the store to buy or sell the products which leads to more time consuming, man power, marketing skills, cost ineffective, etc. Generally, few stores allow customers to place an order through phone call then the product is sent to the customer but there is no guarantee that it satisfies the customer. So, the resale management system is a platform for customers to find right products they need or to sell the products. This project aims to create a customized e-commerce system that allows users to buy or to sell products such as books, phones, gadgets, etc. Resale Management System (RMS) developed by using Amazon Web Services (Cognito, API Gateway, S3 Bucket, Lambdas, DynamoDB, Node.js, Serverless and Express) and ReactJS . To test the system, a case study has been considered on resale of books. Now-a-days people are finding difficulties in buying books at reasonable-prices and cannot even afford to buy books due to their high price in present market and if they can get them in second hand, the price will be favourable to the needy. Finally, a comparative study with respect to properties of existing and proposed system has been measured with predefined metrics and suggested that our proposed RMS with Cloud services will provide an optimal solution to resolve the existing issues.
C3  - 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/ICEARS56392.2023.10085477
DP  - IEEE Xplore
SP  - 727
EP  - 734
UR  - https://ieeexplore.ieee.org/document/10085477
Y2  - 2024/01/07/18:25:21
L2  - https://ieeexplore.ieee.org/document/10085477
ER  - 

TY  - CONF
TI  - A Novel Technique to Improve Latency and Response Time of AI Models using Serverless Infrastructure
AU  - Ravi, Bhanu Sankar
AU  - Madhukanthi, Chepuri
AU  - Sivasankar, P
AU  - Prasanna, John Deva
T2  - 2023 International Conference on Inventive Computation Technologies (ICICT)
AB  - Response time for AI-based models in time-critical applications is always a matter of concern. The situation is challenging when the model is deployed in a cloud-based infrastructure. To address this issue, a cloud-native development methodology called serverless infrastructure enables developers to create and execute applications without having to worry about managing servers. Generally, traditional systems require manual scaling, constant maintenance, and dedicated hardware resulting in high costs. To solve this, the AI model is deployed in serverless infrastructure services for hosting APIs that is to identify marine animals which have the potential to attack human beings at the seashores. The serverless infrastructure suffers from an initializing delay called cold start for occasional requests and hence the response time will be delayed even if the lambda function is free. The problem of cold starts is mitigated using the scheduler in the lambda function. The scheduler sends dummy requests to the server to keep the server warm and active. The AI model used as a test case utilizes Convolutional Neural Network Algorithms and Transfer Learning Technique, for detecting predators near the seashore. The model is deployed in a serverless infrastructure and has the benefits of automatic scaling, pay-per-use pricing, and decreased operational costs. The AI model is implemented in both serverless infrastructure and in Elastic Cloud Compute EC2. The performance of both systems was done for cost, latency, and response time. The proposed system provides promising results when compared to traditional server systems.
C3  - 2023 International Conference on Inventive Computation Technologies (ICICT)
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/ICICT57646.2023.10134379
DP  - IEEE Xplore
SP  - 428
EP  - 433
SN  - 2767-7788
UR  - https://ieeexplore.ieee.org/document/10134379
Y2  - 2024/01/07/18:26:03
L2  - https://ieeexplore.ieee.org/document/10134379
ER  - 

TY  - CONF
TI  - A Serverless Electroencephalogram Data Retrieval and Preprocessing Framework
AU  - Farrow, Bathsheba
AU  - Jayarathna, Sampath
T2  - 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)
AB  - Electroencephalogram (EEG) research continues to rely heavily on data silos used in isolated physical lab environments. However, as a part of the digital transformation, the EEG community has begun its exploration of the public cloud to determine how it can be best utilized to increase collaboration and accelerate research outcomes. The growing number of online repositories for data and tools has provided additional computational resources but the process of downloading data and software along with the installation and configuration requirements is cumbersome and prone to error. To break away from this research paradigm, we present a novel application of cloud technologies to provide reusable EEG data acquisition and preprocessing software as a service (SaaS) that eliminates data and software downloading prerequisites. We utilize the Amazon Web Services (AWS) cloud platform and serverless technologies to create a distributed, highly scalable and extensible solution for EEG signal data preprocessing that is more conducive to effective collaboration and data reproducibility with the potential to expedite neurotechnology breakthroughs.
C3  - 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/IRI58017.2023.00045
DP  - IEEE Xplore
SP  - 221
EP  - 226
SN  - 2835-5776
UR  - https://ieeexplore.ieee.org/document/10229382
Y2  - 2024/01/07/18:26:23
L2  - https://ieeexplore.ieee.org/document/10229382
KW  - AWS
KW  - SaaS
KW  - Serverless
KW  - Microservices
KW  - Computer software reusability
KW  - Application programs
KW  - Amazon web services
KW  - Web services
KW  - Data acquisition
KW  - Data preprocessing
KW  - Data retrieval
KW  - Data silos
KW  - Digital transformation
KW  - Electroencephalography
KW  - Electrophysiology
KW  - Microservice
KW  - Pipeline
KW  - PREP
KW  - Saa
KW  - Software as a service (SaaS)
KW  - Software Reuse
KW  - Software-reuse
ER  - 

TY  - CONF
TI  - An Alternative to FaaS Cold Start Latency of Low Request Frequency Applications
AU  - Ferreira Dos Santos, Paulo Otavio
AU  - Jorge de Moura Costa, Humberto
AU  - Leithardt, Valderi R. Q.
AU  - Jorge Silveira Ferreira, Paulo
T2  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
AB  - Serverless applications are those that do not require the developer to provide or manage any servers. The developer can focus on core product logic and development and just release their code into a container at the service provider. However, when initializing a container there may be a latency called cold start. This article aims to use the Node.js language as an alternative to the Java language as a prevention against the cold start scenario in applications that have a low frequency of use. With the development of lambda functions with the same functionality in both languages. Noting that the node.js language had an 82% reduction in startup time compared to java.
C3  - 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICECCME57830.2023.10253389
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10253389
Y2  - 2024/01/07/18:27:24
L2  - https://ieeexplore.ieee.org/document/10253389
ER  - 

TY  - CONF
TI  - An improved method for increasing maintainability in terms of serverless architecture application
AU  - Lakhai, Vladyslav
AU  - Kuzmych, Oleh
AU  - Seniv, Maksym
T2  - 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)
AB  - The use of new and improvement of existing software development methods becomes a necessity due to high quality requirements, complexity and importance of ease of maintenance. Based on the most common software development approach that improves maintainability, this paper develops a proprietary approach to improve maintainability and demonstrates its advantages over the existing approach.
C3  - 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/CSIT61576.2023.10324273
DP  - IEEE Xplore
SP  - 1
EP  - 4
SN  - 2766-3639
UR  - https://ieeexplore.ieee.org/document/10324273
Y2  - 2024/01/07/18:27:59
L2  - https://ieeexplore.ieee.org/document/10324273
ER  - 

TY  - CONF
TI  - An OpenStack Cloud Solution for a Community Database with Handwritten Characters Used in Developing OCR Algorithms
AU  - Pavăl, Mihaela-Irina
AU  - Alexandrescu, Adrian
AU  - Archip, Alexandru
T2  - 2023 27th International Conference on System Theory, Control and Computing (ICSTCC)
AB  - Most research addressing OCR through machine learning techniques is focused on the actual algorithms and on using the MNIST data set as the de facto benchmark. Little effort was made to extend the data set or to build an entirely new one. Furthermore, support for characters other than English ones is mostly limited. This paper presents an OpenStack based approach that aims to overcome this last limitation by providing a community-oriented solution for developing and maintaining richer, language agnostic, community-shared data sets for OCR based applications. The proposed architecture is integrated with OpenStack services and relies on new Cloud perspectives, such as Function-as-a-Service (FaaS), to achieve a greater degree of flexibility. The included modules allow users to upload their own data sets, select or fine-tune their desired pre-processing methods, and derive the required features for their target character set. Both the input and the output data are stored using OpenStack specific data services and are shared for all the users of the Cloud deployment. An interesting feature is that the underlying FaaS functionality would also allow interested parties to upload their own pre-processing and feature extraction stages.
C3  - 2023 27th International Conference on System Theory, Control and Computing (ICSTCC)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/ICSTCC59206.2023.10308492
DP  - IEEE Xplore
SP  - 327
EP  - 332
SN  - 2473-5698
UR  - https://ieeexplore.ieee.org/document/10308492
Y2  - 2024/01/07/18:28:45
L2  - https://ieeexplore.ieee.org/document/10308492
ER  - 

TY  - CONF
TI  - A Survey and Implementation on Using A Runtime Overhead To Enable Serverless Deployment
AU  - Saravana Kumar, N.
AU  - Selvakumara Samy, S.
T2  - 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
AB  - Serverless computing is an Important paradigm for cloud computing that may greatly decrease application development complexity and releasing programmers of the burden of operational responsibilities. This more sophisticated form of serverless computing, also known as “Pay what is used,” runs charges users and clusters on a “Pay as you go” basis. It is attracting a lot of attention from researchers as a symbol of the development of cloud programming paradigms. This new methodology’s acceptance as the industry standard, its effect, and the transfer of older applications are all covered. In this essay, we assess the descriptions of serverless apps in the literature. We divide applications into categories and go into great length on the aims, viability, and difficulties that the serverless paradigm poses in each of those disciplines. Evaluate the performance profile of the serverless ecosystem in a low latency, rising environment and offer conclusions about overall performance for enhancing serverless designs. Our investigation is restricted to One element of AWS Lambda’s serverless architecture is available. Our results show that the Lambda-based architectures' performance characteristics can be changed, and we cover aspects such as chilly beginnings and possible delay characteristics that can be caused by a variety of factors, incorporating systems and external events. To eventually offer a set of explicit references that will let the use of serverless computing to address a wider range of issues, We offer a wide range of tactics, suggestions, and techniques that, Simultaneously work to its advantages when utilized and executed properly. We cover both the challenging challenges and the subjects that the science community must continue studying.
C3  - 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/ICACCS57279.2023.10113032
DP  - IEEE Xplore
VL  - 1
SP  - 497
EP  - 501
SN  - 2575-7288
UR  - https://ieeexplore.ieee.org/document/10113032
Y2  - 2024/01/07/18:30:04
L2  - https://ieeexplore.ieee.org/document/10113032
ER  - 

TY  - CONF
TI  - Automation of FaaS Serverless Frameworks OpenFaaS and OpenWhisk in Private Cloud
AU  - Tiwari, Pankaj
AU  - Sharma, Sangeeta
T2  - 2023 World Conference on Communication & Computing (WCONF)
AB  - A methodology for cloud-native development called serverless facilitates the creation and execution of programmes by developers without worrying about maintaining servers. As no automation is available to implement FaaS serverless frameworks in private clouds and manual implementation is a tedious job. If the same tasks are performed manually, there is a greater risk of human error as well as an increase in cognitive strain that can take up to 4-5 hours for installation and configuration. Therefore, we are going to provide a command line tool to automate the process to implement FaaS serverless frameworks Open-FaaS and OpenWhisk in private clouds, which is easy to configure and straight forward to use. The proposed tool has the potential to save significant time and cost for customers compared to hiring a DevOps engineer to manually perform the installation and configuration. Additionally, the potential revenue generated from the tool’s use in the financial industry alone suggests a promising market for this type of automation tool. Overall, this paper highlights the usefulness and importance of automation in facilitating the adoption and implementation of serverless architecture in private clouds.
C3  - 2023 World Conference on Communication & Computing (WCONF)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/WCONF58270.2023.10235008
DP  - IEEE Xplore
SP  - 1
EP  - 11
UR  - https://ieeexplore.ieee.org/document/10235008
Y2  - 2024/01/07/18:30:36
L2  - https://ieeexplore.ieee.org/document/10235008
ER  - 

TY  - CONF
TI  - ACTS: Autonomous Cost-Efficient Task Orchestration for Serverless Analytics
AU  - Jarachanthan, Jananie
AU  - Chen, Li
AU  - Xu, Fei
T2  - 2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)
AB  - Serverless computing has become increasingly popular for cloud applications, due to its compelling properties of high-level abstractions, lightweight runtime, high elasticity and pay-per-use billing. In this revolutionary computing paradigm shift, challenges arise when adapting data analytics applications to the serverless environment, due to the lack of support for efficient state sharing, which attract ever-growing research attention. In this paper, we aim to exploit the advantages of task-level orchestration and fine-grained resource provisioning for data analytics on serverless platforms, with the hope of fulfilling the promise of serverless deployment to the maximum extent. To this end, we present ACTS, an autonomous cost-efficient task orchestration framework for serverless analytics. ACTS judiciously schedules and coordinates function tasks to mitigate cold-start latency and state sharing overhead. In addition, ACTS explores the optimization space of fine-grained workload distribution and function resource configuration for cost efficiency. We have deployed and implemented ACTS on AWS Lambda, evaluated with various data analytics workloads. Results from extensive experiments demonstrate that ACTS achieves up to 98% monetary cost reduction while maintaining superior job completion time performance, in comparison with the state-of-the-art baselines.
C3  - 2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/IWQoS57198.2023.10188782
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2766-8568
ST  - ACTS
UR  - https://ieeexplore.ieee.org/document/10188782
Y2  - 2024/01/07/18:31:28
L2  - https://ieeexplore.ieee.org/document/10188782
ER  - 

TY  - CONF
TI  - Behavior Tree-based Workflow Modeling and Scheduling for Serverless Edge Computing
AU  - Luo, Ke
AU  - Ouyang, Tao
AU  - Zhou, Zhi
AU  - Chen, Xu
T2  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
AB  - Despite the popularity of Serverless computing, there are insufficient efforts dedicated to Serverless workflows (i.e., Serverless function orchestration), particularly for Serverless edge computing. In this paper, we first identify the challenges of deploying the state-of-the-art cloud-oriented Serverless workflow scheduling on resource-constrained edge devices, then propose to model Serverless workflows with behavior trees, and finally reveal our key observations and preliminary results for behavior tree-based Serverless workflow scheduling.
C3  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICDCS57875.2023.00100
DP  - IEEE Xplore
SP  - 955
EP  - 956
SN  - 2575-8411
UR  - https://ieeexplore.ieee.org/document/10272475
Y2  - 2024/01/07/18:32:48
L2  - https://ieeexplore.ieee.org/document/10272475
ER  - 

TY  - CONF
TI  - “Challenges and Solution for Security & Living in High Altitude Area Siachen”
AU  - Singh, Anirudh Pratap
AU  - Rajput, Anushree
AU  - Kumar, Anil
AU  - Jain, Dhyanendra
AU  - Verma, Pranshi
T2  - 2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET)
AB  - This abstract discusses the challenges faced by soldiers at high-altitude border fronts and proposes a solution to address these issues. The soldiers endure extreme weather conditions, including strong winds of 300 KM/Hour and bone-chilling temperatures of -60 degrees Celsius, along with blizzards and avalanches. Food scarcity is common, necessitating snow boiling for drinking water, and limited time for eating due to freezing conditions. Communication with loved ones is challenging due to the lack of mobile networks in such remote locations. To tackle these problems, the author introduces a Multipurpose cable system that provides electrical power, telecommunication lines for base camp connectivity, and optical fiber for data sharing. The cable is designed to withstand extreme weather conditions and is spread within the snow using specific alloys to ensure stability at temperatures ranging from -75°C to -100°C. The cable includes self-security features to prevent tampering and provides GPS navigation capabilities. The proposed solution enables the supply of frozen food to soldiers, which can be cooked on electric heaters, making the process environmentally friendly. Additionally, an Indestructible Secure Communication Network is suggested, utilizing optical and telecommunication cables covered with metal repair paste for added security. This network establishes an unhackable serverless system at critical locations, facilitating secure communication for defense forces, research labs, and ministries. The proposed system utilizes two specially designed cables for different terrains and risk levels, effectively connecting high-altitude posts to bases without disturbing the environment. By implementing this solution, the aim is to improve the living conditions and security of armed forces stationed at challenging high-altitude border fronts.
C3  - 2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/ICSEIET58677.2023.10303613
DP  - IEEE Xplore
SP  - 140
EP  - 143
UR  - https://ieeexplore.ieee.org/document/10303613
Y2  - 2024/01/07/18:33:14
L2  - https://ieeexplore.ieee.org/document/10303613
ER  - 

TY  - CONF
TI  - Cloud Computing Based (Serverless computing) using Serverless architecture for Dynamic Web Hosting and cost Optimization
AU  - Veuvolu, Rakesh
AU  - Suryadevar, Anirudh
AU  - Vignesh, T.
AU  - Avthu, Nikhil Reddy
T2  - 2023 International Conference on Computer Communication and Informatics (ICCCI)
AB  - Serverless is a cloud-based code execution model where cloud carrie copes with servers and computing useful resource control instead of builders. There are not any virtual machines or bodily servers: they’re deployed routinely inside the cloud by companies. Cloud carriers cope with provisioning, keeping, and scaling the serverless architecture. What’s more, the serverless structure permits launching apps as wished: you don’t pay for ‘constantly-on’ server components to run your app when it’s no longer getting used. Instead, on a few occasiotriggerers app code, and the resources are dynamically allocated for that code. You forestall paying as quickly as the code is carried out. So, in a nutshell, serverless architecture is a manner to construct your cloud-primarily based software without coping with infrastructure. It eliminates the want for ordinary duties like protection patches, ability control, load balancing, scaling, and many others. Still, serverless does not mean there are no servers in any respect. The time period is truly elusive. Servers are definitely removed from the app development seeing that they are managed by the companies.
C3  - 2023 International Conference on Computer Communication and Informatics (ICCCI)
DA  - 2023/01//
PY  - 2023
DO  - 10.1109/ICCCI56745.2023.10128286
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2473-7577
UR  - https://ieeexplore.ieee.org/document/10128286
Y2  - 2024/01/07/18:33:53
L2  - https://ieeexplore.ieee.org/document/10128286
ER  - 

TY  - CONF
TI  - Cloud Services Enable Efficient AI-Guided Simulation Workflows across Heterogeneous Resources
AU  - Ward, Logan
AU  - Pauloski, J. Gregory
AU  - Hayot-Sasson, Valerie
AU  - Chard, Ryan
AU  - Babuji, Yadu
AU  - Sivaraman, Ganesh
AU  - Choudhury, Sutanay
AU  - Chard, Kyle
AU  - Thakur, Rajeev
AU  - Foster, Ian
T2  - 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
AB  - Applications that fuse machine learning and simulation can benefit from the use of multiple computing resources, with, for example, simulation codes running on highly parallel supercomputers and AI training and inference tasks on specialized accelerators. Here, we present our experiences deploying two AI-guided simulation workflows across such heterogeneous systems. A unique aspect of our approach is our use of cloud-hosted management services to manage challenging aspects of cross-resource authentication and authorization, function-as-a-service (FaaS) function invocation, and data transfer. We show that these methods can achieve performance parity with systems that rely on direct connection between resources. We achieve parity by integrating the FaaS system and data transfer capabilities with a system that passes data by reference among managers and workers, and a user-configurable steering algorithm to hide data transfer latencies. We anticipate that this ease of use can enable routine use of heterogeneous resources in computational science.
C3  - 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/IPDPSW59300.2023.00018
DP  - IEEE Xplore
SP  - 32
EP  - 41
UR  - https://ieeexplore.ieee.org/document/10196576
Y2  - 2024/01/07/18:35:59
L1  - https://arxiv.org/pdf/2303.08803
L2  - https://ieeexplore.ieee.org/document/10196576
ER  - 

TY  - CONF
TI  - COGNIT: Challenges and Vision for a Serverless and Multi-Provider Cognitive Cloud-Edge Continuum
AU  - Townend, Paul
AU  - Martí, Alberto P.
AU  - De La Iglesia, Idoia
AU  - Matskanis, Nikolaos
AU  - Timoudas, Thomas Ohlson
AU  - Hallmann, Torsten
AU  - Lalaguna, Antonio
AU  - Swat, Kaja
AU  - Renzi, Francesco
AU  - Bocheński, Dominik
AU  - Mancini, Marco
AU  - Bhuyan, Monowar
AU  - González-Hierro, Marco
AU  - Dupont, Sébastien
AU  - Kristiansson, Johan
AU  - Montero, Rubén S.
AU  - Elmroth, Erik
AU  - Valdés, Iván
AU  - Massonet, Philippe
AU  - Olsson, Daniel
AU  - Llorente, Ignacio M.
AU  - Östberg, Per-Olov
AU  - Abdou, Michael
T2  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
AB  - Use of the serverless paradigm in cloud application development is growing rapidly, primarily driven by its promise to free developers from the responsibility of provisioning, operating, and scaling the underlying infrastructure. However, modern cloud-edge infrastructures are characterized by large numbers of disparate providers, constrained resource devices, platform heterogeneity, infrastructural dynamicity, and the need to orchestrate geographically distributed nodes and devices over public networks. This presents significant management complexity that must be addressed if serverless technologies are to be used in production systems. This position paper introduces COGNIT, a major new European initiative aiming to integrate AI technology into cloud-edge management systems to create a Cognitive Cloud reference framework and associated tools for serverless computing at the edge. COGNIT aims to: 1) support an innovative new serverless paradigm for edge application management and enhanced digital sovereignty for users and developers; 2) enable on-demand deployment of large-scale, highly distributed and self-adaptive serverless environments using existing cloud resources; 3) optimize data placement according to changes in energy efficiency heuristics and application demands and behavior; 4) enable secure and trusted execution of serverless runtimes. We identify and discuss seven research challenges related to the integration of serverless technologies with multi-provider Edge infrastructures and present our vision for how these challenges can be solved. We introduce a high-level view of our reference architecture for serverless cloud-edge continuum systems, and detail four motivating real-world use cases that will be used for validation, drawing from domains within Smart Cities, Agriculture and Environment, Energy, and Cybersecurity.
C3  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/EDGE60047.2023.00015
DP  - IEEE Xplore
VL  - 2023-July
SP  - 12
EP  - 22
SN  - 2767-9918
ST  - COGNIT
UR  - https://ieeexplore.ieee.org/document/10234326
Y2  - 2024/01/07/18:37:21
L2  - https://ieeexplore.ieee.org/document/10234326
KW  - FaaS
KW  - Serverless
KW  - edge computing
KW  - Edge computing
KW  - Resource management
KW  - serverless
KW  - Application development
KW  - Faas
KW  - Scalings
KW  - Cloud applications
KW  - Open systems
KW  - cognitive cloud
KW  - Cognitive cloud
KW  - Cognitive systems
KW  - Energy efficiency
KW  - Information management
KW  - multi-provider
KW  - Multi-provider
KW  - open source
KW  - Open-source
KW  - resource management
ER  - 

TY  - CONF
TI  - Cost-optimal Operation of Latency Constrained Serverless Applications: From Theory to Practice
AU  - Czentye, János
AU  - Pelle, István
AU  - Sonkoly, Balázs
T2  - NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium
AB  - Serverless computing and the function as a service model are new paradigms enabling the fine granular, bottomup construction of cloud-native applications. It can significantly reduce operating costs while shifting the management tasks from developers and application providers towards the cloud operators. But these benefits are provided at the cost of less control over the underlying infrastructure and the application performance, including the end-to-end latency. However, grouping of functions into deployable serverless software artifacts remains still under our control, which has a considerable impact on performance and operation costs. In this paper, we propose fast and efficient algorithms that can partition an application’s functions into separate deployment artifacts in a cost-optimal way while meeting user-defined average end-to-end latency bounds. Moreover, our approach supports the dynamic redesign and reconfiguration of the current deployment setup in response to changes in monitored metrics. Our main contribution is threefold. First, we establish the relevant theoretical models capturing the behavior of the serverless ecosystem and we define the main problem. In addition, the concept of the integrated application management is introduced. Second, we propose novel algorithms providing optimal solutions for different variants of the core problem and the complexity of the methods are analyzed. Third, we demonstrate the applicability and the benefits of our solution by evaluating different deployment scenarios of a realistic use case in Amazon’s public cloud environment.
C3  - NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/NOMS56928.2023.10154412
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2374-9709
ST  - Cost-optimal Operation of Latency Constrained Serverless Applications
UR  - https://ieeexplore.ieee.org/document/10154412
Y2  - 2024/01/07/18:37:42
L2  - https://ieeexplore.ieee.org/document/10154412
ER  - 

TY  - CONF
TI  - Cloud-Based Serverless Computing Enables Accelerated Monte Carlo-Based Scatter Correction for Positron Emission Tomography
AU  - Bayerlein, R.
AU  - Swarnakar, V.
AU  - Spencer, B.
AU  - Selfridge, A.
AU  - Leung, E. K.S.
AU  - Nardo, L.
AU  - Cherry, S. R.
AU  - Badawi, R. D.
T2  - 2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)
AB  - This study investigates the potential of cloud-based serverless computing to accelerate Monte Carlo (MC)-based scatter correction (SC) in positron emission tomography (PET) imaging. Especially with longer axial field of view, wider acceptance angles and increased number of lines-of-response, SC can pose high computational burden for image reconstruction methods – even when executed on a modern multi-core computing server. In this work we investigate the computational performance of a cloud-based serverless MC simulation using Amazon Web Service (AWS) Lambda and provide a comparison to the computational performance of modern on-premises multi-thread reconstruction server. We find that for our specific application cloud-based SC can outperform local server-based computations by more than an order of magnitude. Adopting cloud-based serverless computing in PET imaging facilities can significantly improve processing times and overall workflow efficiency, with future research exploring additional enhancements through optimized configurations and computational methods.
C3  - 2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)
DA  - 2023/11//
PY  - 2023
DO  - 10.1109/NSSMICRTSD49126.2023.10337966
DP  - IEEE Xplore
SP  - 1
EP  - 1
SN  - 2577-0829
UR  - https://ieeexplore.ieee.org/document/10337966
Y2  - 2024/01/07/18:38:26
L2  - https://ieeexplore.ieee.org/document/10337966
ER  - 

TY  - CONF
TI  - Cost-Sensitive Cold Start Latency Optimization Mechanism in Function-as-a-Service
AU  - Liu, Ruiyan
AU  - Ma, Tengchao
AU  - Huang, Yiting
AU  - An, Qingzhao
AU  - Yan, Lin
AU  - Li, Jiangyuan
T2  - 2023 International Conference on Networking and Network Applications (NaNA)
AB  - Function-as-a-Service (FaaS), as a cloud computing service, builds, runs, and manages application packages directly in a functional way, greatly improving development and delivery efficiency, and is a major trend in the future development of cloud services. However, FaaS is executed via event-driven execution and has a cold-start problem at runtime. Most of the existing research focuses on function runtime optimization and ignores cold start time. For business scenarios with high real-time requirements, prolonged cold starts can affect business results. Therefore, cold-start optimization is particularly important for the application of function computing in latency-sensitive scenarios. To reduce the impact of cold start latency on services, this paper proposes a memory configuration to reduce cold start. Firstly, a memory-cost model is constructed based on memory resource rules and service computation time rules, and the model is optimized using a gradient descent algorithm. The results of large-scale simulations show that the memory selection scheme proposed in this paper can reduce the cold start latency by about 25% compared to the memory selection of conventional function services in existing cases.
C3  - 2023 International Conference on Networking and Network Applications (NaNA)
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/NaNA60121.2023.00081
DP  - IEEE Xplore
SP  - 453
EP  - 458
UR  - https://ieeexplore.ieee.org/document/10284800
Y2  - 2024/01/07/18:39:25
L2  - https://ieeexplore.ieee.org/document/10284800
ER  - 

TY  - CONF
TI  - Container Image Similarity-Aware Resource Provisioning for Serverless Edge Computing
AU  - Zhou, Ao
AU  - Li, Sisi
AU  - Ma, Xiao
AU  - Zhang, Yiran
AU  - Wang, Shangguang
T2  - 2023 IEEE International Conference on Web Services (ICWS)
AB  - Container-enabled serverless computing has become a widely adopted approach for resource provisioning in the edge cloud. However, traffic incurred by container image pulling heavily burdens the already congested back-haul network. To relieve the problem, we do an analysis on Docker Hub, and find that instance deployment strategy has a significant impact on the back-haul traffic due to the varying similarity levels of different images. We incorporate this feature into task offloading decision and resource provisioning, and formulate the problem with a mixed integer non-linear programming (MINLP) problem. To address the challenges arising from the coupling and contradiction of instance deployment, image pulling, offloading decision, and resource allocation, we employ multi-agent deep reinforcement learning to decompose the problem into several simpler sub-problems, and design an algorithm for each sub-problem individually by exploiting convex optimization and fractional programming techniques. Simulations are conducted to validate the effectiveness of the proposed algorithm. The experiment results illustrate that our algorithm outperforms current notable solutions and improves the global utility by 13%–74%.
C3  - 2023 IEEE International Conference on Web Services (ICWS)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICWS60048.2023.00047
DP  - IEEE Xplore
SP  - 278
EP  - 288
SN  - 2836-3868
UR  - https://ieeexplore.ieee.org/document/10248269
Y2  - 2024/01/07/18:39:50
L2  - https://ieeexplore.ieee.org/document/10248269
ER  - 

TY  - CONF
TI  - Crow API: Cross-device I/O Sharing in Web Applications
AU  - Park, Seonghoon
AU  - Lee, Jeho
AU  - Cha, Hojung
T2  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
AB  - Although cross-device input/output (I/O) sharing is useful for users who own multiple computing devices, previous solutions had a platform-dependency problem. The meta-platform characteristics of web applications could provide a viable solution. In this paper, we propose the Crow application programming interface (API) that allows web applications to access other devices’ I/O through standard web APIs without modifying operating systems or browsers. The provision of cross-device I/O should resolve two key challenges. First, the web environment lacks support for device discovery when making a device-to-device connection. This requires a significant effort for developers to implement and maintain signaling servers. To address this challenge, we propose a serverless Crow connectivity mechanism using devices’ I/O-specific communication schemes. Second, JavaScript runtimes have limitations in supporting cross-device inter-process communication (IPC). To solve the problem, we propose a web IPC scheme, called Crow IPC, which introduces a proxy interface that relays the cross-device IPC connection. Crow IPC also provides a mechanism for ensuring functional consistency. We implemented the Crow API as a JavaScript library with which developers can easily develop their applications. An extensive evaluation showed that the Crow API provides cross-device I/O sharing functionality effectively and efficiently on various web applications and platforms.
C3  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/INFOCOM53939.2023.10228950
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2641-9874
ST  - Crow API
UR  - https://ieeexplore.ieee.org/document/10228950
Y2  - 2024/01/07/18:41:06
L2  - https://ieeexplore.ieee.org/document/10228950
ER  - 

TY  - CONF
TI  - DisProTrack: Distributed Provenance Tracking over Serverless Applications
AU  - Satapathy, Utkalika
AU  - Thakur, Rishabh
AU  - Chattopadhyay, Subhrendu
AU  - Chakraborty, Sandip
T2  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
AB  - Provenance tracking has been widely used in the recent literature to debug system vulnerabilities and find the root causes behind faults, errors, or crashes over a running system. However, the existing approaches primarily developed graph-based models for provenance tracking over monolithic applications running directly over the operating system kernel. In contrast, the modern DevOps-based service-oriented architecture relies on distributed platforms, like serverless computing that uses container-based sandboxing over the kernel. Provenance tracking over such a distributed micro-service architecture is challenging, as the application and system logs are generated asynchronously and follow heterogeneous nomenclature and logging formats. This paper develops a novel approach to combining system and micro-services logs together to generate a Universal Provenance Graph (UPG) that can be used for provenance tracking over serverless architecture. We develop a Loadable Kernel Module (LKM) for runtime unit identification over the logs by intercepting the system calls with the help from the control flow graphs over the static application binaries. Finally, we design a regular expression-based log optimization method for reverse query parsing over the generated UPG. A thorough evaluation of the proposed UPG model with different benchmarked serverless applications shows the system’s effectiveness.
C3  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/INFOCOM53939.2023.10228884
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2641-9874
ST  - DisProTrack
UR  - https://ieeexplore.ieee.org/document/10228884
Y2  - 2024/01/07/18:41:49
L2  - https://ieeexplore.ieee.org/document/10228884
ER  - 

TY  - JOUR
TI  - Decentralized Federated Learning With Markov Chain Based Consensus for Industrial IoT Networks
AU  - Du, Mengxuan
AU  - Zheng, Haifeng
AU  - Feng, Xinxin
AU  - Chen, Youjia
AU  - Zhao, Tiesong
T2  - IEEE Transactions on Industrial Informatics
AB  - Federated learning (FL) provides a novel framework to collaboratively train a shared model in a distribution fashion by virtue of a central server. However, FL is inappropriate for a serverless scenario and also suffers from some major drawbacks in Industrial Internet of Things (IIoT) networks, such as unresilience to network failures and communication bottleneck effect. In this article, we propose a novel decentralized federated learning (DFL) approach for IIoT devices to achieve model consensus by exchanging model parameters only with their neighbors rather than a central server. We firstly formulate the problem of model consensus in DFL as a fastest mixing Markov chain problem and then optimize the consensus matrix to improve the convergence rate. Meanwhile, a practical medium access control protocol with time slotted channel hopping is taken into account to implement the proposed approach. Furthermore, we also propose an accumulated update compression method to alleviate communication cost. Finally, extensive simulation results demonstrate that the proposed approach improves accuracy and reduces communication cost especially under the nonindependent identically distribution data distribution.
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/TII.2022.3192297
DP  - IEEE Xplore
VL  - 19
IS  - 4
SP  - 6006
EP  - 6015
J2  - IEEE Transactions on Industrial Informatics
SN  - 1941-0050
UR  - https://ieeexplore.ieee.org/document/9833351
Y2  - 2024/01/07/18:42:49
L2  - https://ieeexplore.ieee.org/document/9833351
ER  - 

TY  - CONF
TI  - Decentralized Serverless IoT Dataflow Architecture for the Cloud-to-Edge Continuum
AU  - Escobar, Juan José López
AU  - Gil-Castiñeira, Felipe
AU  - Díaz Redondo, Rebeca P.
T2  - 2023 26th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)
AB  - The advent of new computing and communication trends that link pervasive data sources and consumers, such as Edge Computing, 5G and IIoT, has led to the development of the Cloud-to-Edge Continuum in order to take advantage of the resources available in massive IoT scenarios, and to conduct data analysis to leverage intelligence at all levels. This paper outlines the challenging requirements of this novel IoT context and presents an innovative IoT framework to develop dataflow applications for data-centric environments. The proposed design takes advantage of decentralized Pub/Sub communication and serverless nanoservice architecture, using novel technologies such as Zenoh and WebAssembly, respectively, to implement lightweight services along the Cloud-to-Edge infrastructure. We also describe some use cases to illustrate the benefits and concerns of the coming IoT generation.
C3  - 2023 26th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/ICIN56760.2023.10073502
DP  - IEEE Xplore
SP  - 42
EP  - 49
SN  - 2472-8144
UR  - https://ieeexplore.ieee.org/document/10073502
Y2  - 2024/01/07/18:43:12
L2  - https://ieeexplore.ieee.org/document/10073502
ER  - 

TY  - JOUR
TI  - Distributed Sketching for Randomized Optimization: Exact Characterization, Concentration, and Lower Bounds
AU  - Bartan, Burak
AU  - Pilanci, Mert
T2  - IEEE Transactions on Information Theory
AB  - We consider distributed optimization methods for problems where forming the Hessian is computationally challenging and communication is a significant bottleneck. We leverage randomized sketches for reducing the problem dimensions as well as preserving privacy and improving straggler resilience in asynchronous distributed systems. We derive novel approximation guarantees for classical sketching methods and establish tight concentration results that serve as both upper and lower bounds on the error. We then extend our analysis to the accuracy of parameter averaging for distributed sketches. Furthermore, we develop unbiased parameter averaging methods for randomized second order optimization in regularized problems that employ sketching of the Hessian. Existing works do not take the bias of the estimators into consideration, which limits their application to massively parallel computation. We provide closed-form formulas for regularization parameters and step sizes that provably minimize the bias for sketched Newton directions. Additionally, we demonstrate the implications of our theoretical findings via large scale experiments on a serverless cloud computing platform.
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/TIT.2023.3247559
DP  - IEEE Xplore
VL  - 69
IS  - 6
SP  - 3850
EP  - 3879
J2  - IEEE Transactions on Information Theory
SN  - 1557-9654
ST  - Distributed Sketching for Randomized Optimization
UR  - https://ieeexplore.ieee.org/document/10049613
Y2  - 2024/01/07/18:48:56
L1  - https://arxiv.org/pdf/2203.09755
L2  - https://ieeexplore.ieee.org/document/10049613
ER  - 

TY  - CONF
TI  - Energy Efficient Scheduling for Serverless Systems
AU  - Tsenos, Michail
AU  - Peri, Aristotelis
AU  - Kalogeraki, Vana
T2  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
AB  - Serverless computing, also referred to as Function-as-a-Service (FaaS), is a cloud computing model that has attracted significant attention and has been widely adopted in recent years. The serverless computing model offers an intuitive, event-based interface that makes the development and deployment of scalable cloud-based applications easier and cost-effective. An important aspect that has not been examined in these systems is their energy consumption during the application execution. One way to deal with this issue is to schedule the function invocations in an energy-efficient way. However, efficient scheduling of applications in a multi-tenant environment, like FaaS systems, poses significant challenges. The trade-off between the server’s energy usage and the hosted functions’ performance requirements needs to be taken into consideration. In this work, we propose an Energy Efficient Scheduler for orchestrating the execution of serverless functions so that it minimizes energy consumption while it satisfies the applications’ performance demands. Our approach considers real-time performance measurements and historical data and applies a novel DVFS technique to minimize energy consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach.
C3  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/ACSOS58161.2023.00020
DP  - IEEE Xplore
SP  - 27
EP  - 36
UR  - https://ieeexplore.ieee.org/document/10336044
Y2  - 2024/01/07/18:52:05
L2  - https://ieeexplore.ieee.org/document/10336044
ER  - 

TY  - CONF
TI  - Enhancing Smart Agriculture Scenarios with Low-code, Pattern-oriented functionalities for Cloud/Edge collaboration
AU  - Fatouros, Georgios
AU  - Kousiouris, George
AU  - Lohier, Theophile
AU  - Makridis, Georgios
AU  - Polyviou, Ariana
AU  - Soldatos, John
AU  - Kyriazis, Dimosthenis
T2  - 2023 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)
AB  - The integration of cloud computing and Internet of Things (IoT) technologies has brought significant advancements in the agriculture domain. However, the implementation of such systems often requires significant time and resources, making it challenging for smart agriculture providers to offer optimized yet affordable services for small and medium-sized farmers at scale. Low-code development platforms can be a viable solution to address these challenges, enabling non-experts to adapt or enhance existing applications with minimal coding. This paper presents a low-code approach to enhance smart agriculture scenarios with pattern-oriented functionality blocks for cloud/edge collaboration. It highlights the usage of a pattern collection for redesigning the implementation of smart agriculture applications that can enhance the data collection process as well as real-time decision-making and efficient resource management in the continuum. The effectiveness of the presented approach is demonstrated through the implementation of a case study in smart agriculture greenhouses. Evaluation results show that this approach can significantly reduce the time and effort required to deploy smart agriculture applications and provide data resilience.
C3  - 2023 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/DCOSS-IoT58021.2023.00055
DP  - IEEE Xplore
SP  - 285
EP  - 292
SN  - 2325-2944
UR  - https://ieeexplore.ieee.org/document/10257242
Y2  - 2024/01/07/18:56:05
L2  - https://ieeexplore.ieee.org/document/10257242
ER  - 

TY  - CONF
TI  - Edge-Assisted Adaptive Configuration for Serverless-Based Video Analytics
AU  - Wang, Ziyi
AU  - Zhang, Songyu
AU  - Cheng, Jing
AU  - Wu, Zhixiong
AU  - Cao, Zhen
AU  - Cui, Yong
T2  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
AB  - The growth of video volumes and increased DNN capabilities have led to a growing desire for video analytics, which demands intensive computation resources. Traditional resource provisioning strategies, such as configuring a cluster per peak utilization, lead to low resource efficiency. Serverless computing is a promising way to avoid wasteful resource provisioning since video analytics regularly encounters bursty input workloads and finegrained video content dynamics. For serverless-based video analytics, the application configuration (frame rate, detection model, and computation resources) will impact several metrics, such as computation cost and analytics accuracy. In this paper, we investigate the joint configuration adjustment problem for video knobs and computation resources provided by the serverless platform. We propose an algorithm that can efficiently adapt configurations for video streams to address two key challenges in serverless-based video analytics systems, including the complex relationships between the configurations and the key performance metrics, and the dynamically best configuration. Our algorithm is developed based on Markov approximation to minimize the computation cost within an accuracy constraint. We have developed a prototype over AWS Lambda and conducted extensive experiments with real-world video streams. The results show that our algorithm can greatly reduce the computation cost under the constraint of target accuracy.
C3  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICDCS57875.2023.00058
DP  - IEEE Xplore
SP  - 248
EP  - 258
SN  - 2575-8411
UR  - https://ieeexplore.ieee.org/document/10272441
Y2  - 2024/01/07/18:56:23
L2  - https://ieeexplore.ieee.org/document/10272441
ER  - 

TY  - CONF
TI  - Duo: Improving Data Sharing of Stateful Serverless Applications by Efficiently Caching Multi-Read Data
AU  - Huang, Zhuo
AU  - Fan, Hao
AU  - Cheng, Chaoyi
AU  - Wu, Song
AU  - Jin, Hai
T2  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - A growing number of applications are moving to serverless architectures for high elasticity and fine-grained billing. For stateful applications, however, the use of serverless architectures is likely to lead to significant performance degradation, as frequent data sharing between different execution stages involves time-consuming remote storage access. Current platforms leverage memory cache to speed up remote access. However, conventional caching strategies show limited performance improvement. We experimentally find that the reason is that current strategies overlook the stage-dependent access patterns of stateful serverless applications, i.e., data that are read multiple times across stages (denoted as multi-read data) are wrongly evicted by data that are read only once (denoted as read-once data), causing a high cache miss ratio.Accordingly, we propose a new caching strategy, Duo, whose design principle is to cache multi-read data as long as possible. Specifically, Duo contains a large cache list and a small cache list, which act as Leader list and Wingman list, respectively. Leader list ignores the data that is read for the first time to prevent itself from being polluted by massive read-once data at each stage. Wingman list inspects the data that are ignored or evicted by Leader list, and pre-fetches the data that will probably be read again based on the observation that multi-read data usually appear periodically in groups. Compared to the state-of-the-art works, Duo improves hit ratio by 1.1×-2.1× and reduces the data sharing overhead by 25%-62%.
C3  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/IPDPS54959.2023.00092
DP  - IEEE Xplore
SP  - 875
EP  - 885
SN  - 1530-2075
ST  - Duo
UR  - https://ieeexplore.ieee.org/document/10177481
Y2  - 2024/01/07/19:00:49
L2  - https://ieeexplore.ieee.org/document/10177481
ER  - 

TY  - CONF
TI  - EdgeOrcher: Predictive Function Orchestration for Serverless-Based Edge Native Applications
AU  - Liang, Yunkai
AU  - Zhou, Zhi
AU  - Chen, Xu
T2  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
AB  - Serverless computing is becoming prevalent to develop resource-demanding and delay-sensitive edge native applications across the edge and cloud. The unique pricing mechanism of serverless computing brings new opportunities to reduce the cost of edge native applications, by orchestrating function fusion and placement across the edge and cloud. However, function fusion potentially increases the latency of the serverless workflow. To navigate this performance-cost tradeoff, we present an online predictive function orchestration framework which leverages predictions to dynamically optimize the function fusion and placement. Preliminary evaluation results verify the efficacy of the proposed framework.
C3  - 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICDCS57875.2023.00094
DP  - IEEE Xplore
SP  - 1
EP  - 2
SN  - 2575-8411
ST  - EdgeOrcher
UR  - https://ieeexplore.ieee.org/document/10272539
Y2  - 2024/01/07/19:01:04
L2  - https://ieeexplore.ieee.org/document/10272539
ER  - 

TY  - CONF
TI  - Enabling Age-Aware Big Data Analytics in Serverless Edge Clouds
AU  - Xu, Zichuan
AU  - Fu, Yuexin
AU  - Xia, Qiufen
AU  - Li, Hao
T2  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
AB  - With the fast development of artificial intelligence applications, large-volume big data generated in the edge of networks are waiting for real-time analysis, such that the valuable information is unveiled. Analytic developers for big data applications usually face the burden of managing the underlying cloud resources, which greatly drags the speed of analytic development. Serverless Computing is envisioned as an enabling technology to release the management burden of developers and to enable agile big data analytics. That is, big data analytics can be implemented in short-lived functions via the Function-as-a-Service (FaaS) programming paradigm. In this paper, we aim to fill the gap between serverless computing and mobile edge computing, via enabling query evaluations for big data analytics in short-lived functions of a serverless edge cloud (SEC). Specifically, we formulate novel age-aware big data query evaluation problems in an SEC so that the age of data is minimized, where the age of data is defined as the time difference between the current time and the generation time of the dataset. We propose approximation algorithms for the age-aware big data query evaluation problem with a single query, by proposing a novel parameterized virtualization technique that strives for a fine trade-off between short-lived functions and large resource demands of big data queries. We also devise an online learning algorithm with a bounded regret for the problem with multiple queries arriving dynamically and without prior knowledge of resource demands of the queries. We finally evaluate the performance of the proposed algorithms by extensive simulations. Simulation results show that the performance of our algorithms is promising.
C3  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/INFOCOM53939.2023.10228905
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2641-9874
UR  - https://ieeexplore.ieee.org/document/10228905
Y2  - 2024/01/07/19:01:29
L2  - https://ieeexplore.ieee.org/document/10228905
ER  - 

TY  - CONF
TI  - Enabling Serverless Sky Computing
AU  - Cordingly, Robert
AU  - Lloyd, Wes
T2  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
AB  - The Sky Computing vision represents a unified multi-cloud environment where applications can be deployed to utilize resources from different cloud regions, resource configurations, and cloud providers. Serverless computing platforms have recently emerged, offering automatic elastic scaling, high performance, and reduced costs but often utilize proprietary deployment tools and services locking users into platform-specific services. This research aims to apply Sky Computing to serverless computing platforms offered by major cloud providers such as Amazon Web Services, Google Cloud, Azure, and more. This research will build a serverless sky architecture to enable the aggregation of serverless resources to achieve service-level objectives such as low hosting costs, high performance, fault tolerance, high throughput, and low carbon footprint. The research will focus on evaluating the performance implications of serverless aggregation (Thrust-1), design and evaluation of Sky Computing architectures and aggregation strategies (Thrust-2), and finally autonomous resource aggregation for intelligent self-management of applications deployed to the sky (Thrust-3).
C3  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IC2E59103.2023.00038
DP  - IEEE Xplore
SP  - 232
EP  - 235
SN  - 2694-0825
UR  - https://ieeexplore.ieee.org/document/10305821
Y2  - 2024/01/07/19:02:15
L2  - https://ieeexplore.ieee.org/document/10305821
ER  - 

TY  - CONF
TI  - Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning
AU  - Barrak, Amine
AU  - Trabelsi, Ranim
AU  - Jaafar, Fehmi
AU  - Petrillo, Fabio
T2  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
AB  - The increasing demand for computational power in big data and machine learning has driven the development of distributed training methodologies. Among these, peer-to-peer (P2P) networks provide advantages such as enhanced scalability and fault tolerance. However, they also encounter challenges related to resource consumption, costs, and communication overhead as the number of participating peers grows. In this paper, we introduce a novel architecture that combines serverless computing with P2P networks for distributed training and present a method for efficient parallel gradient computation under resource constraints.Our findings show a significant enhancement in gradient computation time, with up to a 97.34% improvement compared to conventional P2P distributed training methods. As for costs, our examination confirmed that the serverless architecture could incur higher expenses, reaching up to 5.4 times more than instance-based architectures. It is essential to consider that these higher costs are associated with marked improvements in computation time, particularly under resource-constrained scenarios.Despite the cost-time trade-off, the serverless approach still holds promise due to its pay-as-you-go model. Utilizing dynamic resource allocation, it enables faster training times and optimized resource utilization, making it a promising candidate for a wide range of machine learning applications.
C3  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IC2E59103.2023.00024
DP  - IEEE Xplore
SP  - 141
EP  - 152
SN  - 2694-0825
UR  - https://ieeexplore.ieee.org/document/10305831
Y2  - 2024/01/07/19:05:54
L1  - https://arxiv.org/pdf/2309.14139
L2  - https://ieeexplore.ieee.org/document/10305831
ER  - 

TY  - CONF
TI  - Exploring the Serverless First Strategy in Cloud Application Development
AU  - Cavalheiro, Adriano Prado
AU  - Schepke, Claudio
T2  - 2023 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)
AB  - This paper explores the Serverless First strategy in cloud application development. Serverless computing has gained popularity due to its flexibility and scalability. In our work, we provide a systematic review of the literature about the Serverless paradigm in cloud computing and an evaluation of the advantages of this approach by performing a comparative analysis among three ways for the implementation of an application: AWS Lambda, AWS Lambda with Chalice framework, and the traditional form using the Flask framework. The literature review results show the gains in scaling, cost reduction, and ease of maintenance achieved with the Serverless First strategy. However, some limitations and challenges were also highlighted, such as the greater complexity of the environment, less control over resources, resource limitations imposed by the cloud provider, and difficulties in debugging and managing the infrastructure. The case study demonstrates in practice that the Chalice framework provided the most straightforward and rapid implementation, the AWS Lambda without Chalice offered greater flexibility and control, and the Flask version allowed local testing and total control but required more manual setup and lacked automatic scalability.
C3  - 2023 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/SBAC-PADW60351.2023.00023
DP  - IEEE Xplore
SP  - 89
EP  - 94
UR  - https://ieeexplore.ieee.org/document/10306039
Y2  - 2024/01/07/19:06:15
L2  - https://ieeexplore.ieee.org/document/10306039
ER  - 

TY  - CONF
TI  - FaaSCTDO: Collaborative Task-Data Orchestration for Serverless Workflows
AU  - Yang, Neng
AU  - Zhang, Haitao
AU  - Zhang, Yepeng
T2  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
AB  - The use of Function-as-a-Service (FaaS) platforms for executing complex serverless workflows has gained significant popularity. However, the stateless nature of FaaS requires functions to rely on remote storage to store their state, leading to performance overhead and reduced efficiency. Previous approaches using excess memory resources as caches can cause resource contention and decreased task execution performance. Moreover, the impact of workflow orchestration approaches on latency has not been fully explored. To address these challenges, we propose FaaSCTDO, a framework for collaborative task and data orchestration in serverless workflows. FaaSCTDO provides developers with comprehensive lifecycle management capabilities, treating data as orchestratable objects. It groups tasks and data with high data correlation together and allocates them on the same server node, leveraging data locality to expedite function access to data.
C3  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/CLOUD60044.2023.00070
DP  - IEEE Xplore
VL  - 2023-July
SP  - 526
EP  - 528
SN  - 2159-6190
ST  - FaaSCTDO
UR  - https://ieeexplore.ieee.org/document/10254972
Y2  - 2024/01/07/19:06:43
L2  - https://ieeexplore.ieee.org/document/10254972
KW  - FaaS
KW  - Function-as-a-service
KW  - Orchestration
KW  - Service platforms
KW  - Serverless workflow
KW  - Work-flows
KW  - Collaborative tasks
KW  - data locality
KW  - Data locality
KW  - Digital storage
KW  - Life cycle
KW  - orchestration
KW  - Remote storage
KW  - serverless workflows
KW  - workflow grouping
KW  - Workflow grouping
ER  - 

TY  - JOUR
TI  - FaaSDeliver: Cost-Efficient and QoS-Aware Function Delivery in Computing Continuum
AU  - Yu, Guangba
AU  - Chen, Pengfei
AU  - Zheng, Zibin
AU  - Zhang, Jingrun
AU  - Li, Xiaoyun
AU  - He, Zilong
T2  - IEEE Transactions on Services Computing
AB  - Serverless Function-as-a-Service (FaaS) is a rapidly growing computing paradigm in the cloud era. To provide rapid service response and save network bandwidth, traditional cloud-based FaaS platforms have been extended to the edge. However, launching functions in a heterogeneous computing continuum (HCC) that includes the cloud, fog, and the edge brings new challenges: determining where functions should be delivered and how many resources should be allocated. To optimize the cost of running functions in the HCC, we propose an adaptive and efficient function delivery engine, named FaaSDeliver, which automatically unearths a cost-efficient function delivery policy (FDP) for each function, including the FaaS platform selection and resource allocation. Real system implementation and evaluations in a practical HCC demonstrate that FaaSDeliver can unearth the most cost-efficient FDPs from among 180,200 FDPs after a few trials. FaaSDeliver reduces the average cost of function execution from 38% to 78% compared to some state-of-the-art approaches.
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/TSC.2023.3274769
DP  - IEEE Xplore
VL  - 16
IS  - 5
SP  - 3332
EP  - 3347
J2  - IEEE Transactions on Services Computing
SN  - 1939-1374
ST  - FaaSDeliver
UR  - https://ieeexplore.ieee.org/document/10122727
Y2  - 2024/01/07/19:08:32
L2  - https://ieeexplore.ieee.org/document/10122727
ER  - 

TY  - CONF
TI  - FCloudless: A Performance-Aware Collaborative Mechanism for JointCloud Serverless
AU  - Liu, Jianfei
AU  - Wang, Huaimin
AU  - Shi, Peichang
AU  - Li, Yaojie
AU  - Ma, Penghui
AU  - Yi, Guodong
T2  - 2023 IEEE International Conference on Joint Cloud Computing (JCC)
AB  - As a new stage in the development of the cloud computing paradigm, serverless computing has the high-level abstraction characteristic of shielding underlying details. This makes it extremely challenging for users to choose a suitable serverless platform. To address this, targeting the jointcloud computing scenario of heterogeneous serverless platforms across multiple clouds, this paper presents a jointcloud collaborative mechanism called FCloudless with cross-cloud detection of the full lifecycle performance of serverless platforms. Based on the benchmark metrics set that probe performance critical stages of the full lifecycle, this paper proposes a performance optimization algorithm based on detected performance data that takes into account all key stages that affect the performance during the lifecycle of a function and predicts the overall performance by combining the scores of local stages and dynamic weights. We evaluate FCloudless on AWS, AliYun, and Azure. The experimental results show that FCloudless can detect the underlying performance of serverless platforms hidden in the black box and its optimization algorithm can select the optimal scheduling strategy for various applications in a jointcloud environment. FCloudless reduces the runtime by 23.3% and 24.7% for cold and warm invocations respectively under cost constraints.
C3  - 2023 IEEE International Conference on Joint Cloud Computing (JCC)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/JCC59055.2023.00019
DP  - IEEE Xplore
SP  - 93
EP  - 94
ST  - FCloudless
UR  - https://ieeexplore.ieee.org/document/10229726
Y2  - 2024/01/07/19:08:56
L2  - https://ieeexplore.ieee.org/document/10229726
ER  - 

TY  - CONF
TI  - Feature and Performance Comparison of FaaS Platforms
AU  - Ma, Penghui
AU  - Shi, Peichang
AU  - Yi, Guodong
T2  - 2023 IEEE 14th International Conference on Software Engineering and Service Science (ICSESS)
AB  - With serverless computing offering more efficient and cost-effective application deployment, the diversity of serverless platforms presents challenges to users, including platform lock-in and costly migration. Moreover, due to the black box nature of function computing, traditional performance benchmarking methods are not applicable, necessitating new studies. This article presents a detailed comparison of six major public cloud function computing platforms and introduces a benchmarking framework for function computing performance. This framework aims to help users make comprehensive comparisons and select the most suitable platform for their specific needs.
C3  - 2023 IEEE 14th International Conference on Software Engineering and Service Science (ICSESS)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/ICSESS58500.2023.10293015
DP  - IEEE Xplore
SP  - 239
EP  - 243
SN  - 2327-0594
UR  - https://ieeexplore.ieee.org/document/10293015
Y2  - 2024/01/07/19:09:23
L2  - https://ieeexplore.ieee.org/document/10293015
ER  - 

TY  - CONF
TI  - FIRST: Exploiting the Multi-Dimensional Attributes of Functions for Power-Aware Serverless Computing
AU  - Zhang, Lu
AU  - Li, Chao
AU  - Wang, Xinkai
AU  - Feng, Weiqi
AU  - Yu, Zheng
AU  - Chen, Quan
AU  - Leng, Jingwen
AU  - Guo, Minyi
AU  - Yang, Pu
AU  - Yue, Shang
T2  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.In this work, we show that the optimal operating point (OOP) for energy efficiency cannot be attained without synthesizing the multi-dimensional attributes of functions. We present FIRST, a novel mechanism that enables servers to better orchestrate serverless functions. The key feature of FIRST is that it leverages a lightweight Internal Representation and meta-Scheduling (IRS) layer for collecting the maximum potential revenue from the servers. Specifically, FIRST follows a pipeline-style workflow. Its frontend components aim to analyze functions from different angles and expose their key features to the system. Meanwhile, its backend components are able to make informed function assignment decisions to avoid OOP divergence. We further demonstrate the way to create extensions based on FIRST to enable versatile cloud-native power management. In total, our design constitutes a flexible management layer that supports power-aware function deployment. We show that FIRST could allow 94% functions to be processed under the OOP, which brings up to 24% energy efficiency improvements.
C3  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/IPDPS54959.2023.00091
DP  - IEEE Xplore
SP  - 864
EP  - 874
SN  - 1530-2075
ST  - FIRST
UR  - https://ieeexplore.ieee.org/document/10177391
Y2  - 2024/01/07/19:10:19
L2  - https://ieeexplore.ieee.org/document/10177391
ER  - 

TY  - JOUR
TI  - FedDK: Improving Cyclic Knowledge Distillation for Personalized Healthcare Federated Learning
AU  - Xu, Yikai
AU  - Fan, Hongbo
T2  - IEEE Access
AB  - For most healthcare organizations, a significant challenge today is predicting diseases with incomplete data information, often resulting in isolation. Federated learning (FL) solves the issue of data silos by enabling remote local machines to train a globally optimal model collaboratively without the need for sharing data. In this research, we present FedDK, a serverless framework designed to obtain personalized models for each federation through data from local federations using convolutional neural networks and training through FL. Our approach involves using convolutional neural networks (CNNs) to accumulate common knowledge and transfer it using knowledge distillation, which helps prevent common knowledge forgetting. Additionally, the missing common knowledge is filled circularly between each federation, culminating in a personalized model for each group. This novel design leverages federated, deep, and integrated learning methods to produce more accurate machine-learning models. Our federated model exhibits superior performance to local and baseline FL methods, achieving significant advantages.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ACCESS.2023.3294812
DP  - IEEE Xplore
VL  - 11
SP  - 72409
EP  - 72417
J2  - IEEE Access
SN  - 2169-3536
ST  - FedDK
UR  - https://ieeexplore.ieee.org/document/10182241
Y2  - 2024/01/07/19:11:10
L1  - https://ieeexplore.ieee.org/ielx7/6287639/6514899/10182241.pdf
L1  - https://ieeexplore.ieee.org/ielx7/6287639/10005208/10182241.pdf?tp=&arnumber=10182241&isnumber=10005208&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMTgyMjQx
L2  - https://ieeexplore.ieee.org/document/10182241
ER  - 

TY  - CONF
TI  - Firestore: The NoSQL Serverless Database for the Application Developer
AU  - Kesavan, Ram
AU  - Gay, David
AU  - Thevessen, Daniel
AU  - Shah, Jimit
AU  - Mohan, C.
T2  - 2023 IEEE 39th International Conference on Data Engineering (ICDE)
AB  - The recent years have seen an explosive growth in web and mobile application development. Such applications typically have rapid development cycles, and their developers expect mobile-friendly features and serverless characteristics such as rapid deployment capabilities (with minimal initialization), scalability to handle workload spikes, and flexible pay-as-you-go billing. Google’s Firestore is a NoSQL serverless database with real-time notification capability, and together with the Firebase ecosystem greatly simplifies common app development challenges by letting application developers focus primarily on their business logic and user experience. This paper presents the Firestore architecture, how it satisfies the aforementioned requirements, and how its real-time notification system works in tandem with Firebase client libraries to allow mobile applications to provide a smooth user experience even in the presence of network connectivity issues.
C3  - 2023 IEEE 39th International Conference on Data Engineering (ICDE)
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/ICDE55515.2023.00259
DP  - IEEE Xplore
SP  - 3376
EP  - 3388
SN  - 2375-026X
ST  - Firestore
UR  - https://ieeexplore.ieee.org/document/10184529
Y2  - 2024/01/07/19:11:23
L2  - https://ieeexplore.ieee.org/document/10184529
ER  - 

TY  - CONF
TI  - GPU-enabled Function-as-a-Service for Machine Learning Inference
AU  - Zhao, Ming
AU  - Jha, Kritshekhar
AU  - Hong, Sungho
T2  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - Function-as-a-Service (FaaS) is emerging as an important cloud computing service model as it can improve the scalability and usability of a wide range of applications, especially Machine-Learning (ML) inference tasks that require scalable resources and complex software configurations. These inference tasks heavily rely on GPUs to achieve high performance; however, support for GPUs is currently lacking in the existing FaaS solutions. The unique event-triggered and short-lived nature of functions poses new challenges to enabling GPUs on FaaS, which must consider the overhead of transferring data (e.g., ML model parameters and inputs/outputs) between GPU and host memory. This paper proposes a novel GPU-enabled FaaS solution that enables ML inference functions to efficiently utilize GPUs to accelerate their computations. First, it extends existing FaaS frameworks such as OpenFaaS to support the scheduling and execution of functions across GPUs in a FaaS cluster. Second, it provides caching of ML models in GPU memory to improve the performance of model inference functions and global management of GPU memories to improve cache utilization. Third, it offers co-designed GPU function scheduling and cache management to optimize the performance of ML inference functions. Specifically, the paper proposes locality-aware scheduling, which maximizes the utilization of both GPU memory for cache hits and GPU cores for parallel processing. A thorough evaluation based on real-world traces and ML models shows that the proposed GPU-enabled FaaS works well for ML inference tasks, and the proposed locality-aware scheduler achieves a speedup of 48x compared to the default, load balancing only schedulers.
C3  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/IPDPS54959.2023.00096
DP  - IEEE Xplore
SP  - 918
EP  - 928
SN  - 1530-2075
UR  - https://ieeexplore.ieee.org/document/10177435
Y2  - 2024/01/07/19:14:05
L1  - https://arxiv.org/pdf/2303.05601
L2  - https://ieeexplore.ieee.org/document/10177435
ER  - 

TY  - JOUR
TI  - HealthFaaS: AI-Based Smart Healthcare System for Heart Patients Using Serverless Computing
AU  - Golec, Muhammed
AU  - Gill, Sukhpal Singh
AU  - Parlikad, Ajith Kumar
AU  - Uhlig, Steve
T2  - IEEE Internet of Things Journal
AB  - Heart disease is one of the leading causes of death worldwide, and with early detection, mortality rates can be reduced. Well-known studies have shown that the latest artificial intelligence (AI) can be used to determine the risk of heart disease. However, existing studies did not consider dynamic scalability to get the best performance from these AI models in case of an increasing number of users. To solve this problem, we proposed an AI-powered smart healthcare framework called HealthFaaS, using the Internet of Things (IoT) and a Serverless Computing environment to reduce heart disease-related deaths and prevent financial losses by reducing misdiagnoses. HealthFaaS framework collects health data from users via IoT devices and sends it to AI models deployed on a Google Cloud Platform (GCP)-based serverless computing environment due to its advantages, such as dynamic scalability, less operational complexity, and a pay-as-you-go pricing model. The performance of five different AI models for heart disease risk detection is evaluated and compared based on key parameters, such as accuracy, precision, recall, F -Score, and AUC. Experimental results demonstrate that the light gradient boosting machine model gives the highest success in detecting heart diseases with an accuracy rate of 91.80%. Further, we have tested the performance of the HealthFaaS framework in terms of Quality-of-Service (QoS) parameters, such as throughput and latency against the increasing number of users and compared it with a non-serverless platform. In addition, we have also evaluated the cold start latency using a serverless platform which determined that the amount of memory and the software language makes a direct impact on the cold start latency.
DA  - 2023/11//
PY  - 2023
DO  - 10.1109/JIOT.2023.3277500
DP  - IEEE Xplore
VL  - 10
IS  - 21
SP  - 18469
EP  - 18476
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - HealthFaaS
UR  - https://ieeexplore.ieee.org/document/10129153
Y2  - 2024/01/07/19:14:51
L1  - https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/88446/2/Gill%20HealthFaaS%3a%20AI%20based%20Smart%20Healthcare%20System%20for%20Heart%20Patients%20using%20Serverless%20Computing%202023%20Accepted.pdf
L2  - https://ieeexplore.ieee.org/document/10129153
ER  - 

TY  - CONF
TI  - Heterogeneous Reconfigurable Accelerators: Trends and Perspectives
AU  - Luk, Wayne
T2  - 2023 60th ACM/IEEE Design Automation Conference (DAC)
AB  - Heterogeneity and reconfigurability have both been adopted by accelerators to improve their flexibility and efficiency for a wide variety of applications, from cloud computing to embedded systems. This paper provides an overview of the trends of heterogeneous reconfigurable accelerators including field-programmable gate arrays and coarse-grained reconfigurable arrays, and the related design automation approaches for enhancing design quality and designer productivity of these accelerators. We shall also discuss how recent advances in technology, such as multi-level co-design, heterogeneous Function-as-a-Service and meta-programming, would help address the challenges in engineering next-generation heterogeneous reconfigurable accelerators and beyond.
C3  - 2023 60th ACM/IEEE Design Automation Conference (DAC)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/DAC56929.2023.10247723
DP  - IEEE Xplore
SP  - 1
EP  - 2
ST  - Heterogeneous Reconfigurable Accelerators
UR  - https://ieeexplore.ieee.org/document/10247723
Y2  - 2024/01/07/19:15:43
L2  - https://ieeexplore.ieee.org/document/10247723
ER  - 

TY  - CONF
TI  - Hyron: A New Approach for Automating the Network ACL Delivery Pipeline
AU  - Taylor, Jacob Neil
AU  - Le, Ngoc Thuy
AU  - Baek, Joonsang
AU  - Susilo, Willy
T2  - 2023 32nd International Conference on Computer Communications and Networks (ICCCN)
AB  - Automated Access Control List (ACL) configuration has remained an area of research interest for a significant period. However, previous research has not addressed the challenges associated with ACL automation in the context of a complex and evolving industry landscape. We examine the existing research literature on this topic and identify a series of key requirements (“success criteria”) that any new system must achieve to be considered an improvement over the status quo. We then design, develop, and demonstrate an approach to ACL automation that embodies these characteristics by combining model-driven ACL synthesis with a modern DevOps-style deployment system. We explain the rationale that drove the design decisions behind the Hyron ACL generation toolkit and how it can enable a fully automated ACL delivery pipeline when integrated with standard developer tools. In contrast to previous research, our design approach reflects automation trends in the industry to ensure mainstream engineers readily adopt our solution. We provide an analysis comparing our approach's benefits to those of previous research.
C3  - 2023 32nd International Conference on Computer Communications and Networks (ICCCN)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICCCN58024.2023.10230146
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2637-9430
ST  - Hyron
UR  - https://ieeexplore.ieee.org/document/10230146
Y2  - 2024/01/07/19:16:57
L2  - https://ieeexplore.ieee.org/document/10230146
ER  - 

TY  - CONF
TI  - Innovative Approach of Data Encryption Algorithm for Securing Big Data
AU  - Awasthi, Shivani
AU  - Kohli, Narendra
T2  - 2023 8th International Conference on Communication and Electronics Systems (ICCES)
AB  - Nowadays, a large amount of data that is growing exponentially is known as Big data. In other words, big data contains more variety and comes in big volume with more velocity. Traditional processing software cannot handle complex and big amounts of data, but this data addresses a lot of business problems, that could not be able to tackle before so to deal with this data, open-source software HDFS (Hadoop Distributed File System) is used as a cloud storage system. Big Data Security is a collective term used for all measures and tools that are used to guard data and analytics processes used for accessing the data. Big data is a joint term and tool for data and data management systems against attacks, thefts, or other malicious activities that could harm or negatively affect them so server and serverless systems both are major concerned with high security of data. The author’s research presents data security in the cloud computing environment in this paper. HDFS does not provide any scheme of encryption and decryption for securing the data, HDFS act as a storage medium that stores data in Avro format. Users use the encryption key and encryption zone to encrypt their data into HDFS.
C3  - 2023 8th International Conference on Communication and Electronics Systems (ICCES)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/ICCES57224.2023.10192702
DP  - IEEE Xplore
SP  - 528
EP  - 533
UR  - https://ieeexplore.ieee.org/document/10192702
Y2  - 2024/01/07/19:18:01
L2  - https://ieeexplore.ieee.org/document/10192702
ER  - 

TY  - CONF
TI  - Legacy Moderization: A Cloud Migration Strategy With Serverless Microservice Architecture
AU  - Ang, Qi Zhi
AU  - Yau, Peter ChunYu
AU  - Sum, Chin Sean
AU  - Cao, Qi
AU  - Wong, Dennis
T2  - 2023 International Conference on Intelligent Computing and Control (IC&C)
AB  - This paper provides an overview, presents the work-in-progress study of how software architecture can be modernized using the latest technologies. We will discuss the reasons why software modernization is required, how to choose an optimal solution from the architecture perspective, related migration strategy and the related points to note. The paper will also discuss some of the common issues faced while reviewing the architecture. These issues would provide some insights to some frequently faced problems in development and deployment. Finally, a discussion on how containerization technology and load balancer could help in resolving some of the common issues faced. In this paper, we have evaluated our proposed approach in a realistic case study involving an actual organization, which was anonymized for identity protection A certain organization would be used to provide the corresponding review mentioned.
C3  - 2023 International Conference on Intelligent Computing and Control (IC&C)
DA  - 2023/02//
PY  - 2023
DO  - 10.1109/IC-C57619.2023.00017
DP  - IEEE Xplore
SP  - 59
EP  - 63
ST  - Legacy Moderization
UR  - https://ieeexplore.ieee.org/document/10302995
Y2  - 2024/01/07/19:20:03
L2  - https://ieeexplore.ieee.org/document/10302995
ER  - 

TY  - JOUR
TI  - ITS Based on Deep Graph Convolutional Fraud Detection Network Blockchain-Enabled Fog-Cloud
AU  - Lakhan, Abdullah
AU  - Mohammed, Mazin Abed
AU  - Ibrahim, Dheyaa Ahmed
AU  - Kadry, Seifedine
AU  - Abdulkareem, Karrar Hameed
T2  - IEEE Transactions on Intelligent Transportation Systems
AB  - The advancement in transport applications increases at the everyday progress in technologies. Therefore, intelligent transport systems (ITS) gain a lot of progress at the different vehicle levels and in the vehicular area network. However, privacy and security at the network level are critical issues for ITS applications in the existing mechanism. In this paper, the study devises the cost-efficient and secure Serverless Blockchain Enable Task Scheduling (SBETS) ITS system and algorithm framework. The main goal is to reduce processing and security blockchian costs for ITS applications in the system. The processing cost minimizes based on the new proposed function-based price model and secures the data by a suggested deep graph convolutional neural network scheme in the network. The simulation results show that SBETS outperformed all existing ITS systems and minimized processing costs by 10% and fraud detection issues by 50% for transport applications.
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/TITS.2022.3147852
DP  - IEEE Xplore
VL  - 24
IS  - 8
SP  - 8399
EP  - 8408
J2  - IEEE Transactions on Intelligent Transportation Systems
SN  - 1558-0016
UR  - https://ieeexplore.ieee.org/document/9714858
Y2  - 2024/01/07/19:23:29
L2  - https://ieeexplore.ieee.org/document/9714858
ER  - 

TY  - CONF
TI  - Load Testing and Cost Analysis of Deployment Environments of a Backend System in an Electronic Election Recapitulation System Application
AU  - Baskara, Gregorius Dimas
AU  - Perdana, Riza Satria
AU  - Akbar, Saiful
T2  - 2023 IEEE International Conference on Data and Software Engineering (ICoDSE)
AB  - Based on a statement by the General Election Commission of the Republic of Indonesia, 190.022.169 voters will vote in the 2024 election. With a very large number of voters and TPS, this manual recapitulation process raises several problems. As an effort to overcome this problem, an electronic election recapitulation system application is raised to become one of the solutions to be used at the 2024 election. An important part of this application is a scalable backend system. This paper aims to provide a design and also build a backend system that is able to serve every request well. Aspects that are considered in this application are database schemes and flow and implementation at the application level, and application deployment environment to support application performance in dealing with large numbers of requests, but mainly focuses on the aspects of load testing and cost analysis of the deployment environments. The solution that was successfully developed is an application using the Golang programming language connected to the PostgreSQL database and deployed both server-based and serverless Kubernetes on Alibaba Cloud and Google Cloud. After the system has been built, load testing is carried out to test the system's ability to deal with many requests at the same time. The results of load testing show that the system has been able to serve peak requests similar to what will be encountered on election day or regional elections in 2024 with variations in results for each different development environment.
C3  - 2023 IEEE International Conference on Data and Software Engineering (ICoDSE)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/ICoDSE59534.2023.10291494
DP  - IEEE Xplore
SP  - 162
EP  - 167
SN  - 2640-0227
UR  - https://ieeexplore.ieee.org/document/10291494
Y2  - 2024/01/07/19:24:41
L2  - https://ieeexplore.ieee.org/document/10291494
ER  - 

TY  - JOUR
TI  - Machine Learning-Based Non-Intrusive Digital Forensic Service for Smart Homes
AU  - Liu, Xuanyu
AU  - Fu, Xiao
AU  - Du, Xiaojiang
AU  - Luo, Bin
AU  - Guizani, Mohsen
T2  - IEEE Transactions on Network and Service Management
AB  - Security and privacy concerns keep growing with the successful development of Internet of Things (IoT) and the booming deployment of smart homes. IoT devices are utilized cooperatively to enable the interactions between home surroundings and users’ daily lives, containing forensically-valuable information about what happens in smart homes, which can help introduce digital forensics into smart homes to alleviate the growing concerns. However, current IoT devices, apps, and platforms usually do not provide built-in capabilities for digital forensics. To overcome this limitation, we propose a non-intrusive solution (i.e., bringing no modification to IoT devices, apps, and platforms) of digital forensic service to provide Forensics-as-a-Service (FaaS) for smart homes. First, it leverages side-channel analysis on sniffed network traffic to monitor commands, actions, and states of IoT devices. Then, it introduces provenance graphs (i.e., causal graphs) for smart home modeling to provide a holistic and overall explanation of smart homes. Machine learning (ML) techniques are applied to overcome the deficiency of a non-intrusive solution as it suffers from challenges in data collection and smart home modeling. Finally, it conducts forensic analysis based on scalable, reusable policies that are designed for graph-based smart home modeling. We implement a prototype of our forensic service and evaluate it in a real-world smart home. The evaluation results show that our forensic service can effectively collect forensic data for smart home modeling and conduct forensic analysis to explain security risks in smart homes.
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/TNSM.2022.3224863
DP  - IEEE Xplore
VL  - 20
IS  - 2
SP  - 945
EP  - 960
J2  - IEEE Transactions on Network and Service Management
SN  - 1932-4537
UR  - https://ieeexplore.ieee.org/document/9964107
Y2  - 2024/01/07/19:25:12
L2  - https://ieeexplore.ieee.org/document/9964107
ER  - 

TY  - CONF
TI  - Maxwell’s Demon in Tail-tolerant, Resource-efficient Serverless Computing
AU  - Zhang, Huanyu
AU  - Huang, Wenhao
AU  - Zhao, Laiping
AU  - Li, Keqiu
T2  - 2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)
AB  - Computing systems always face a “resource allocation dilemma” that shows the great difficulties in trading off resource efficiency for tail latency, due to the internal uncertainty of cluster status and execution behavior. Inspired by the imaginary “Maxwell’s demon” in thermodynamics who can reduce the uncertainty through a per-gas molecule-level control policy, we consider the “one-to-one mapping” feature of serverless computing and build a novel resource allocator, named Maxwell, that can achieve low tail latency and high resource efficiency in serverless simultaneously. Like the “Maxwell’s demon Maxwell is able to optimize the resource allocation for every request. It observes the state of each request and makes decisions about the minimum resource allocation through a reinforcement learning predictor. As the per-request-grained control incurs significant overhead, we further design a pipeline for avoiding the accumulated effect on a workflow. Experimental results show that Maxwell not only saves up to 31% CPU resources but also reduces the standard deviation of latency by 1.9×. Its time overhead is negligible and the resource overhead is also limited when the query per second \leq500.
C3  - 2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)
DA  - 2023/01//
PY  - 2023
DO  - 10.1109/ICPADS56603.2022.00104
DP  - IEEE Xplore
SP  - 762
EP  - 769
SN  - 2690-5965
UR  - https://ieeexplore.ieee.org/document/10077958
Y2  - 2024/01/07/19:25:59
L2  - https://ieeexplore.ieee.org/document/10077958
ER  - 

TY  - CONF
TI  - Latency and Reliability Aware SDN Controller: A Role Delegation Function as a Service
AU  - Wobuyaga, Dinah
AU  - Arzo, Sisay T
AU  - Kumar, Harsh
AU  - Granelli, Fabrizio
AU  - Devetsikiotis, Michael
T2  - 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)
AB  - The emergency of machine type and ultra-reliable low latency communication is imposing stringent constraints for service provisioning. Addressing such constraints is challenging for network and cloud service providers. As a trending paradigm, software-defined networking (SDN) plays a significant role in future networks and services. However, the classical implementation of the SDN controller has limitations in-terms-of latency and reliability since the controller is decoupled from the forwarding device. Several research works have tried to tackle these challenges by proposing solutions such as Devoflow, DIFANE, and hierarchical and distributed controller deployment. Nonetheless, these approaches are not fully addressing these challenges. This paper tries to address the problem of latency and reliability by proposing a dynamic controller role delegation architecture for forwarding devices. To align with the microservice or multi-agent-based service-based architecture, the role delegation function as a service is proposed. The dynamic role delegation enables to predict and (pre-)installed flow rules in the forwarding devices based on various considerations such as network state, packet type, and service's stringent requirements. The proposed architecture is implemented and evaluated for latency and resiliency performance in comparison to the centralized and distributed deployment of the SDN controller. We used ComNetsEmu, a softwarized network emulation tool, to emulate SDN and NFV (Network Function Virtualization). The result indicated a significant decrease in latency and improved resilience in case of failure, yielding better network performance.
C3  - 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/CCWC57344.2023.10099225
DP  - IEEE Xplore
SP  - 0205
EP  - 0211
ST  - Latency and Reliability Aware SDN Controller
UR  - https://ieeexplore.ieee.org/document/10099225
Y2  - 2024/01/07/19:26:51
L2  - https://ieeexplore.ieee.org/document/10099225
ER  - 

TY  - JOUR
TI  - Least-Privilege Calls to Amazon Web Services
AU  - Gill, Puneet
AU  - Dietl, Werner
AU  - Tripunitara, Mahesh
T2  - IEEE Transactions on Dependable and Secure Computing
AB  - We address least-privilege in a particular context of public cloud computing: calls to Amazon Web Services (AWS) Application Programming Interfaces (APIs). AWS is, by far, the largest cloud provider, and therefore an important context in which to consider the fundamental security design principle of least-privilege, which states that a thread of execution should possess only those privileges it needs. There have been reports of over-privilege being a root cause of attacks against AWS cloud applications, and a least-privilege set for an API call is a necessary building-block in devising a least-privilege policy for a cloud application. We observe that accurate information on a least-privilege set for an invoker of a method to possess is simply not available for most such methods in AWS. We provide a meaningful characterization of least-privilege in this context. We then propose techniques to determine such sets, and discuss a black-box process we have devised and carried out to identify such sets for all 707 API methods we are able to invoke across five AWS services. We discuss a number of interesting discoveries we have made, some of which are surprising and some alarming, that we have reported to AWS. Our work has resulted in a database of least-privilege sets for API calls to AWS, which we make available publicly. Developers can consult our database when configuring security policies for their cloud applications, and we welcome contributors that augment our database. Also, we discuss example uses of our database via an assessment of two repositories and two full-fledged serverless applications that are available publicly and have policies published alongside. We observe that the vast majority of policies are over-privileged. Our work contributes constructively to securing cloud applications in the largest cloud provider.
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/TDSC.2022.3171740
DP  - IEEE Xplore
VL  - 20
IS  - 3
SP  - 2085
EP  - 2096
J2  - IEEE Transactions on Dependable and Secure Computing
SN  - 1941-0018
UR  - https://ieeexplore.ieee.org/document/9767574
Y2  - 2024/01/07/19:27:11
L2  - https://ieeexplore.ieee.org/document/9767574
ER  - 

TY  - JOUR
TI  - Mitigating Cold Start Problem in Serverless Computing: A Reinforcement Learning Approach
AU  - Vahidinia, Parichehr
AU  - Farahani, Bahar
AU  - Aliee, Fereidoon Shams
T2  - IEEE Internet of Things Journal
AB  - Serverless computing has revolutionized the world of cloud-based and event-driven applications with the introduction of Function as a Service (FaaS) as the latest cloud computing model. This computational model increases the level of abstraction from the infrastructure and breaks the program into small units called functions. Thus, it brings benefits, such as ease of development, saving resources, and reducing product launch time for enterprises and developers. Thanks to the scale-to-zero feature of this computational model, idle functions with no traffic will be depreciated from memory. However, this cost-saving approach adversely impacts delay leading to the cold start problem. Unfortunately, the existing solutions to alleviate the cold start delay are not resource efficient as they follow a fixed policy over time. Thereby, this article proposes a novel two-layer adaptive approach to tackle this issue. The first layer utilizes a holistic reinforcement learning algorithm to discover the function invocation patterns over time for determining the best time to keep the containers warm. The second layer is designed based on a long short-term memory (LSTM) to predict the function invocation times in the future to determine the required prewarmed containers. The experimental results on the Openwhisk platform show that the proposed approach reduces the memory consumption by 12.73% and improves the execution invocations on prewarmed containers by 22.65% compared to the Openwhisk platform.
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/JIOT.2022.3165127
DP  - IEEE Xplore
VL  - 10
IS  - 5
SP  - 3917
EP  - 3927
J2  - IEEE Internet of Things Journal
SN  - 2327-4662
ST  - Mitigating Cold Start Problem in Serverless Computing
UR  - https://ieeexplore.ieee.org/document/9749611
Y2  - 2024/01/07/19:28:02
L2  - https://ieeexplore.ieee.org/document/9749611
KW  - Cloud computing
KW  - Internet of Things
KW  - Computational modeling
KW  - Cold start delay
KW  - Containers
KW  - Costs
KW  - Delays
KW  - Function as a Service (FaaS)
KW  - memory consumption
KW  - openwhisk
KW  - reinforcement learning
KW  - serverless computing
KW  - Serverless computing
KW  - serverless platforms
ER  - 

TY  - CONF
TI  - Monitoring in Function-as-a-Service Platforms
AU  - Guerreiro, Beatriz
AU  - Freitas, Filipe
AU  - Simão, José
T2  - 2023 18th Iberian Conference on Information Systems and Technologies (CISTI)
AB  - Functions deployed in FaaS platforms need to be monitored regarding resource consumption, errors, and application-specific metrics. Platforms like Google Cloud Functions or Azure Functions have dashboards and Web APIs that expose information about the execution of functions. However, the current approach collects data of general metrics about the function as an all and imposes a vendor-specific way for monitoring events. To the best of our knowledge, fine-grained function metrics are not available in FaaS platforms, e.g., the time that takes to execute only part of the function’s code.This work aims to explore how to build a system to provide fine-grained monitoring to FaaS platforms for developers. In this paper we present an architecture for this system, introduce the metrics library we are developing for FaaS platforms, and discuss some challenges.
C3  - 2023 18th Iberian Conference on Information Systems and Technologies (CISTI)
DA  - 2023/06//
PY  - 2023
DO  - 10.23919/CISTI58278.2023.10212053
DP  - IEEE Xplore
SP  - 1
EP  - 4
SN  - 2166-0727
UR  - https://ieeexplore.ieee.org/document/10212053
Y2  - 2024/01/07/19:29:02
L2  - https://ieeexplore.ieee.org/document/10212053
ER  - 

TY  - CONF
TI  - Multi-Objective Workflow Scheduling to Serverless Architecture in a Multi-Cloud Environment
AU  - Ramesh, Manju
AU  - Chahal, Dheeraj
AU  - Phalak, Chetan
AU  - Singhal, Rekha
T2  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Many complex workflows consist of multiple tasks represented as a directed acyclic graph (DAG). Optimal deployment of such workflows on a cloud using multiple services requires a judicious selection of compute and storage services to minimize the makespan and the cost of deployment. Multi-cloud deployment is emerging as a preferred choice for the deployment of complex workflows for price competitiveness and freedom from vendor lock-in. However, finding an optimal mapping scheme for heterogeneous tasks of a workflow in a multi-cloud environment is a challenge. Furthermore, each participating cloud service provider (CSP) has a unique cost model and maximum deliverable performance. This makes exploration of the optimal configuration of the chosen service daunting. Many algorithms, frameworks, and tools have been proposed to schedule complex workflows on cloud using virtual machines (VMs) available as Infrastructure-as-a-Service (IaaS). However, the use of scalable and cost-effective serverless platforms offered as Function-as-a-Service (FaaS) is still in its infancy.In this work, we use particle swarm optimization (PSO) for mapping complex workflows to cloud services such as computing, and storage in a multi-cloud scenario. We map complex workflows to the serverless platforms and storage services from popular cloud vendors namely Amazon Web Services (AWS), Azure (AZR), and Google Cloud Platform (GCP). The experimental evaluation shows that our approach results in up to 61% improvement in makespan and 51% improvement in the cost of workflow deployment as compared to naive and intuition-based mapping in a cloud.
C3  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IC2E59103.2023.00027
DP  - IEEE Xplore
SP  - 173
EP  - 183
SN  - 2694-0825
UR  - https://ieeexplore.ieee.org/document/10305826
Y2  - 2024/01/07/19:29:33
L2  - https://ieeexplore.ieee.org/document/10305826
ER  - 

TY  - CONF
TI  - Object as a Service (OaaS): Enabling Object Abstraction in Serverless Clouds
AU  - Lertpongrujikorn, Pawissanutt
AU  - Salehi, Mohsen Amini
T2  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
AB  - Function as a Service (FaaS) paradigm is becoming widespread and is envisioned as the next generation of cloud computing systems that mitigate the burden for programmers and cloud solution architects. However, the FaaS abstraction only makes the cloud resource management aspects transparent but does not deal with the application data aspects. As such, developers have to intervene and undergo the burden of managing the application data, often via separate cloud services (e.g., AWS S3). Similarly, the FaaS abstraction does not natively support function workflow, hence, the developers often have to work with workflow orchestration services (e.g., AWS Step Functions) to build workflows. Moreover, they have to explicitly navigate the data throughout the workflow. To overcome these inherent problems of FaaS, our hypothesis is to design a higher-level cloud programming abstraction that can hide the complexities and mitigate the burden of developing cloud-native application development. Accordingly, in this research, we borrow the notion of object from object-oriented programming and propose a new abstraction level atop the function abstraction, known as Object as a Service (OaaS). OaaS encapsulates the application data and function into the object abstraction and relieves the developers from resource and data management burdens. It also unlocks opportunities for built-in optimization features, such as software reusability, data locality, and caching. OaaS natively supports dataflow programming such that developers define a workflow of functions transparently without getting involved in data navigation, synchronization, and parallelism aspects. We implemented a prototype of the OaaS platform and evaluated it under real-world settings against state-of-the-art platforms regarding the imposed overhead, scalability, and ease of use. The results demonstrate that OaaS streamlines cloud programming and offers scalability with an insignificant overhead to the underlying cloud system.
C3  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/CLOUD60044.2023.00035
DP  - IEEE Xplore
SP  - 238
EP  - 248
SN  - 2159-6190
ST  - Object as a Service (OaaS)
UR  - https://ieeexplore.ieee.org/document/10254994
Y2  - 2024/01/07/19:33:24
L1  - https://arxiv.org/pdf/2206.05361
L2  - https://ieeexplore.ieee.org/document/10254994
ER  - 

TY  - CONF
TI  - Orchestrating the Execution of Serverless Functions in Hybrid Clouds
AU  - Peri, Aristotelis
AU  - Tsenos, Michail
AU  - Kalogeraki, Vana
T2  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
AB  - In recent years, serverless computing, especially Function as a Service (FaaS), is rapidly growing in popularity as a cloud programming model. The serverless computing model provides an intuitive interface for developing cloud-based applications, where the development and deployment of scalable microservices has become easier and cost-effective. An increasing number of batch-processing applications are deployed as pipelines that comprise a sequence of functions that must meet their deadline targets to be practical. In this paper, we present our Hybrid Cloud Scheduler (HCS) for orchestrating the execution of serverless batch-processing pipelines deployed over heterogeneous infrastructures. Our framework enables developers to (i) automatically schedule and execute batch-processing applications in heterogeneous environments such as the private edge and public cloud serverless infrastructures, (ii) benefit from cost reduction through the utilization of their own resources in a private cluster, and (iii) significantly improves the probability of meeting the deadline requirements of their applications. Our experimental evaluation demonstrates the efficiency and benefits of our approach.
C3  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/ACSOS58161.2023.00032
DP  - IEEE Xplore
SP  - 139
EP  - 144
UR  - https://ieeexplore.ieee.org/document/10336026
Y2  - 2024/01/07/19:34:20
L2  - https://ieeexplore.ieee.org/document/10336026
ER  - 

TY  - CONF
TI  - Overcoming Challenges in Migrating Modular Monolith from On-Premises to AWS Cloud
AU  - Olariu, Florin
T2  - 2023 22nd RoEduNet Conference: Networking in Education and Research (RoEduNet)
AB  - Cloud computing has gained popularity for efficient data storage, processing, and application access. This study focuses on migrating Hermit Portal’s modular monolithic web application from on-premises to AWS cloud. The aim is to identify migration patterns, optimize costs, and consider project management constraints (e.g., time, cost, and performance). Considering the Cloud Computing models perspective, the study analyzed Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Serverless models, with a focus on Amazon Web Services (AWS). Initial “lift and shift” using IaaS is recommended, followed by a gradual adoption of PaaS components like AWS Elastic Beanstalk or AWS Lambda for simplified management and reduced infrastructure responsibilities. Additionally, we explored various cost optimization strategies specific to AWS. Utilizing Reserved Instances, optimizing Windows Server and SQL Server licensing, considering PostgreSQL as an alternative to SQL Server, and leveraging Linux-based apps and web servers on AWS, were found to be effective in reducing costs. Also, we realized during the research that conducting a comprehensive analysis within the AWS ecosystem and understanding AWS-specific features before migration is crucial. This avoids unexpected cost increases compared with on premises hosting and accounts for refactoring and acquiring AWS-specific expertise. In conclusion, based on our research, a phased approach with thorough analysis and AWS-Specific strategies is the key to successfully and cost-effectively migrating monolith applications to AWS Cloud. Choosing the right cloud computing model and implementing cost optimization techniques ensures a smooth transition and maximizes the benefits of AWS.
C3  - 2023 22nd RoEduNet Conference: Networking in Education and Research (RoEduNet)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/RoEduNet60162.2023.10274946
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2247-5443
UR  - https://ieeexplore.ieee.org/document/10274946
Y2  - 2024/01/07/19:35:10
L2  - https://ieeexplore.ieee.org/document/10274946
ER  - 

TY  - CONF
TI  - On the Feasibility of Serverless Functions in the Context of Auto-Graders
AU  - Serth, Sebastian
AU  - Paß, Maximilian
AU  - Meinel, Christoph
T2  - 2023 IEEE 2nd German Education Conference (GECon)
AB  - Learners interested in acquiring fundamental programming skills may choose from a variety of different offers, including Massive Open Online Courses (MOOCs). Usually, these courses not only include lecture videos and multiple-choice quizzes, but also feature hands-on programming exercises, allowing learners to apply their newly acquired knowledge right away. Since solving these exercises requires access to a programming tool chain, most MOOCs embed their exercises in a web-based environment supplying necessary tools. One of these so-called auto-graders is CodeOcean, which allows learners to write and run code or receive automated feedback. While a web-based auto-grader lowers the entry barrier for learners to get started, providing sufficient resources for all code executions poses an additional challenge for the MOOC provider, especially during high-demand periods. Therefore, we evaluated serverless functions as offered by cloud computing providers for the use in auto-graders and conducted a Randomized Control Trial. Although serverless functions at first appear to be slower compared to our existing containerized execution of learners' code, they convinced with more constant execution times in high-demand periods.
C3  - 2023 IEEE 2nd German Education Conference (GECon)
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/GECon58119.2023.10295106
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10295106
Y2  - 2024/01/07/19:35:48
L2  - https://ieeexplore.ieee.org/document/10295106
ER  - 

TY  - CONF
TI  - Online Container Scheduling for Data-intensive Applications in Serverless Edge Computing
AU  - Shang, Xiaojun
AU  - Mao, Yingling
AU  - Liu, Yu
AU  - Huang, Yaodong
AU  - Liu, Zhenhua
AU  - Yang, Yuanyuan
T2  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
AB  - Introducing the emerging serverless paradigm into edge computing could avoid over- and under-provisioning of limited edge resources and make complex edge resource management transparent to application developers, which largely facilitates the cost-effectiveness, portability, and short time-to-market of edge applications. However, the computation/data dispersion and device/network heterogeneity of edge environments prevent current serverless computing platforms from acclimating to the network edge. In this paper, we address such challenges by formulating a container placement and data flow routing problem, which fully considers the heterogeneity of edge networks and the overhead of operating serverless platforms on resource-limited edge servers. We design an online algorithm to solve the problem. We further show its local optimum for each arriving container and prove its theoretical guarantee to the optimal offline solution. We also conduct extensive simulations based on practical experiment results to show the advantages of the proposed algorithm over existing baselines.
C3  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/INFOCOM53939.2023.10229034
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2641-9874
UR  - https://ieeexplore.ieee.org/document/10229034
Y2  - 2024/01/07/19:36:09
L2  - https://ieeexplore.ieee.org/document/10229034
ER  - 

TY  - CONF
TI  - Penalty-Enabled Serverless Architecture for Cloud-based Startup Solutions
AU  - Benedict, Shajulin
AU  - Subair, Rubiya
AU  - Gupta, Tanya
AU  - S.P., Vedanta
T2  - 2023 International Conference on Inventive Computation Technologies (ICICT)
AB  - The step to the success of startups is through overcoming competitors by adopting software innovations that improve businesses. Serverless computing model, recently, has intrigued a sizable number of startup professionals belonging to various sectors, including financial or IoT-enabled application developers. One of the main flaws is its heavy dependency on cloud providers, which can still result in hefty pricing to startups and stalling functions in applications. This article proposes a penaltyenabled serverless architecture for startups. The architecture can boost the economy of startups and can analyze the serverlessoriented cost-saving options in applications. The penalty-oriented approach could enable cloud architects, developers, and startups, to rethink the utilization of serverless functions; to gleam of with future innovations.
C3  - 2023 International Conference on Inventive Computation Technologies (ICICT)
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/ICICT57646.2023.10134026
DP  - IEEE Xplore
SP  - 497
EP  - 502
SN  - 2767-7788
UR  - https://ieeexplore.ieee.org/document/10134026
Y2  - 2024/01/07/19:36:38
L2  - https://ieeexplore.ieee.org/document/10134026
ER  - 

TY  - CONF
TI  - Performance Evaluation and Comparison of Microservices and Serverless Deployments in Cloud
AU  - Shrestha, Raju
AU  - Nisha, Beebu
T2  - 2023 IEEE 8th International Conference on Smart Cloud (SmartCloud)
AB  - Microservices and serverless are arguably the two most widely used architectures today for deploying applications in the cloud. With both these technologies, applications can take advantage of faster delivery, lightweight, scalable, and lower development and maintenance costs. However, there are ongoing debates concerning which of these two architectures to use for deploying a given application. This paper evaluates and compares them quantitatively in terms of their performance and cost, based on a use case study for an image processing application. The study was conducted by deploying the application using the two technologies in the two major cloud platforms, Amazon AWS and Google Cloud. Results showed that serverless perform better in terms of performance and cost, whereas microservices show superiority in terms of memory use.
C3  - 2023 IEEE 8th International Conference on Smart Cloud (SmartCloud)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/SmartCloud58862.2023.00043
DP  - IEEE Xplore
SP  - 202
EP  - 207
UR  - https://ieeexplore.ieee.org/document/10349142
Y2  - 2024/01/07/19:37:04
L2  - https://ieeexplore.ieee.org/document/10349142
ER  - 

TY  - JOUR
TI  - Performance Modeling of Metric-Based Serverless Computing Platforms
AU  - Mahmoudi, Nima
AU  - Khazaei, Hamzeh
T2  - IEEE Transactions on Cloud Computing
AB  - Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/TCC.2022.3169619
DP  - IEEE Xplore
VL  - 11
IS  - 2
SP  - 1899
EP  - 1910
J2  - IEEE Transactions on Cloud Computing
SN  - 2168-7161
UR  - https://ieeexplore.ieee.org/document/9763051
Y2  - 2024/01/07/21:29:53
L1  - https://arxiv.org/pdf/2202.11247
L2  - https://ieeexplore.ieee.org/document/9763051
ER  - 

TY  - CONF
TI  - Prediction-driven resource provisioning for serverless container runtimes
AU  - Tomaras, Dimitrios
AU  - Tsenos, Michail
AU  - Kalogeraki, Vana
T2  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
AB  - In recent years Serverless Computing has emerged as a compelling cloud based model for the development of a wide range of data-intensive applications. However, rapid container provisioning introduces non-trivial challenges for FaaS cloud providers, as (i) real-world FaaS workloads may exhibit highly dynamic request patterns, (ii) applications have service-level objectives (SLOs) that must be met, and (iii) container provisioning can be a costly process. In this paper, we present SLOPE, a prediction framework for serverless FaaS platforms to address the aforementioned challenges. Specifically, it trains a neural network model that utilizes knowledge from past runs in order to estimate the number of instances required to satisfy the invocation rate requirements of the serverless applications. In cases that a priori knowledge is not available, SLOPE makes predictions using a graph edit distance approach to capture the similarities among serverless applications. Our experimental results illustrate the efficiency and benefits of our approach, which can reduce the operating costs by 66.25% on average.
C3  - 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/ACSOS58161.2023.00033
DP  - IEEE Xplore
SP  - 1
EP  - 6
UR  - https://ieeexplore.ieee.org/document/10336036
Y2  - 2024/01/07/21:30:27
L2  - https://ieeexplore.ieee.org/document/10336036
ER  - 

TY  - CONF
TI  - Predicut - A Machine Learning Model For Online Prediction of Cut-In Manoeuvre For Autonomous Vehicles
AU  - Sankaranarayanan, Pandeeswari
AU  - Ramanujam, Arvind
AU  - Sathy, Sruthi
AU  - Jayaprakash, Rajesh
T2  - 2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)
AB  - When a vehicle changes lane abruptly resulting in low headway distance to the vehicle behind it, it may trigger emergency braking or it may result in a collision. Such events are called cut-ins and they cause a majority of road accidents in the U.S. Autonomous Vehicles (AVs) have to predict such events in advance and react appropriately to such human behavior. We propose Predicut, a stacking classifier model that uses sensor data of an instrumented host vehicle and predicts if any of the neighboring vehicles would cut in front of it in the next 0.5 to 5.5 seconds. We have built and tested the model using the SPMD and NGSIM (I-80, US-101) driving datasets. The model was able to predict cut-ins with a maximum F1 score of 94.3%. From the datasets, we determine that the model through its advance prediction, avoided up to 90% of emergency braking scenarios. The model is highly performant with average inference time of 26ms when hosted on a cloud based serverless inference service.
C3  - 2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)
DA  - 2023/06//
PY  - 2023
DO  - 10.1109/VTC2023-Spring57618.2023.10199227
DP  - IEEE Xplore
SP  - 1
EP  - 5
SN  - 2577-2465
UR  - https://ieeexplore.ieee.org/document/10199227
Y2  - 2024/01/07/21:30:47
L2  - https://ieeexplore.ieee.org/document/10199227
ER  - 

TY  - CONF
TI  - PrivFlow: Secure and Privacy Preserving Serverless Workflows on Cloud
AU  - Garg, Surabhi
AU  - Dilip Thakur, Meena Singh
AU  - A, Rajan M
AU  - Maddali, Lakshmi Padmaja
AU  - Ramachandran, Vigneswaran
T2  - 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - The recent advancement of serverless computing in the widespread deployment of applications prompts the need to protect serverless workflows against cloud vulnerabilities and threats. We propose PrivFlow, a workflow-centric, privacy preserving framework to protect the information flow in serverless computing applications in semi-honest (S-PrivFlow) and malicious (M-PrivFlow) adversarial settings. An Authenticated Data Structure is used to store the valid workflows encoded in the proposed format. The validation of workflows is performed in a privacy preserving manner that leaks no sensitive information to any unauthorized user. We focus on the two most prevalent attacks on the serverless cloud platforms, namely the Denial-of-Wallet and Wrong Function Invocation attacks. We demonstrate that PrivFlow mitigates both of these attacks. Further, we evaluate PrivFlow on the popular benchmark application- Hello Retail, and a customized scaled application. Though the comparison with the state-of-the-art approaches in terms of the runtime performance shows a latency of 1.6 times for S-PrivFlow and 8 times for M-PrivFlow, the PrivFlow provides high security and privacy. PrivFlow acts as a wrapper to the application resulting in no change to the source code.
C3  - 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/CCGrid57682.2023.00049
DP  - IEEE Xplore
SP  - 447
EP  - 458
ST  - PrivFlow
UR  - https://ieeexplore.ieee.org/document/10171483
Y2  - 2024/01/07/21:31:49
L2  - https://ieeexplore.ieee.org/document/10171483
ER  - 

TY  - CONF
TI  - QoS-Aware and Cost-Efficient Dynamic Resource Allocation for Serverless ML Workflows
AU  - Wu, Hao
AU  - Deng, Junxiao
AU  - Fan, Hao
AU  - Ibrahim, Shadi
AU  - Wu, Song
AU  - Jin, Hai
T2  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
AB  - Machine Learning (ML) workflows are increasingly deployed on serverless computing platforms to benefit from their elasticity and fine-grain pricing. Proper resource allocation is crucial to achieve fast and cost-efficient execution of serverless ML workflows (specially for hyperparameter tuning and model training). Unfortunately, existing resource allocation methods are static, treat functions equally, and rely on offline prediction, which limit their efficiency. In this paper, we introduce CE-scaling – a Cost-Efficient autoscaling framework for serverless ML work-flows. During the hyperparameter tuning, CE-scaling partitions resources across stages according to their exact usage to minimize resource waste. Moreover, it incorporates an online prediction method to dynamically adjust resources during model training. We implement and evaluate CE-scaling on AWS Lambda using various ML models. Evaluation results show that compared to state-of-the-art static resource allocation methods, CE-scaling can reduce the job completion time and the monetary cost by up to 63% and 41% for hyperparameter tuning, respectively; and by up to 58% and 38% for model training.
C3  - 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/IPDPS54959.2023.00093
DP  - IEEE Xplore
SP  - 886
EP  - 896
SN  - 1530-2075
UR  - https://ieeexplore.ieee.org/document/10177446
Y2  - 2024/01/07/21:33:02
L2  - https://ieeexplore.ieee.org/document/10177446
ER  - 

TY  - JOUR
TI  - Randomized Polar Codes for Anytime Distributed Machine Learning
AU  - Bartan, Burak
AU  - Pilanci, Mert
T2  - IEEE Journal on Selected Areas in Information Theory
AB  - We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.
DA  - 2023///
PY  - 2023
DO  - 10.1109/JSAIT.2023.3310931
DP  - IEEE Xplore
VL  - 4
SP  - 393
EP  - 404
J2  - IEEE Journal on Selected Areas in Information Theory
SN  - 2641-8770
UR  - https://ieeexplore.ieee.org/document/10239266
Y2  - 2024/01/07/21:44:56
L1  - https://arxiv.org/pdf/2309.00682
L2  - https://ieeexplore.ieee.org/document/10239266
ER  - 

TY  - JOUR
TI  - Real-Time FaaS: Towards a Latency Bounded Serverless Cloud
AU  - Szalay, Márk
AU  - Mátray, Péter
AU  - Toka, László
T2  - IEEE Transactions on Cloud Computing
AB  - Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgradability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multi-node systems make real-time scheduling significantly complex since guaranteeing real-time task execution and communication is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm suitable for real-time FaaS platforms of multi-node clusters. We show that our task scheduling heuristics could outperform existing algorithms by 55%. Furthermore, we propose three conceptual designs to enable the necessary real-time communications. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/TCC.2022.3151469
DP  - IEEE Xplore
VL  - 11
IS  - 2
SP  - 1636
EP  - 1650
J2  - IEEE Transactions on Cloud Computing
SN  - 2168-7161
ST  - Real-Time FaaS
UR  - https://ieeexplore.ieee.org/document/9714028
Y2  - 2024/01/07/21:45:42
L2  - https://ieeexplore.ieee.org/document/9714028
ER  - 

TY  - CONF
TI  - Reliable Transactions in Serverless-Edge Architecture
AU  - Gupta, Suyash
AU  - Rahnama, Sajjad
AU  - Linsenmayer, Erik
AU  - Nawab, Faisal
AU  - Sadoghi, Mohammad
T2  - 2023 IEEE 39th International Conference on Data Engineering (ICDE)
AB  - Modern edge applications demand novel solutions where edge applications do not have to rely on a single cloud provider (which cannot be in the vicinity of every edge device) or dedicated edge servers (which cannot scale as clouds) for processing compute-intensive tasks. A recent computing philosophy, Sky computing, proposes giving each user ability to select between available cloud providers.In this paper, we present our serverless-edge co-design, which extends the Sky computing vision. In our serverless-edge co-design, we expect edge devices to collaborate and spawn required number of serverless functions. This raises several key challenges: (1) how will this collaboration take place, (2) what if some edge devices are compromised, and (3) what if a selected cloud provider is malicious. Hence, we design ServerlessBFT, the first protocol to guarantee Byzantine fault-tolerant (Bft) transactional flow between edge devices and serverless functions. We present an exhaustive list of attacks and their solutions on our serverless-edge co-design. Further, we extensively benchmark our architecture on a variety of parameters.
C3  - 2023 IEEE 39th International Conference on Data Engineering (ICDE)
DA  - 2023/04//
PY  - 2023
DO  - 10.1109/ICDE55515.2023.00030
DP  - IEEE Xplore
SP  - 301
EP  - 314
SN  - 2375-026X
UR  - https://ieeexplore.ieee.org/document/10184657
Y2  - 2024/01/07/21:46:29
L1  - https://arxiv.org/pdf/2201.00982
L2  - https://ieeexplore.ieee.org/document/10184657
ER  - 

TY  - CONF
TI  - Serverledge: Decentralized Function-as-a-Service for the Edge-Cloud Continuum
AU  - Russo, Gabriele Russo
AU  - Mannucci, Tiziana
AU  - Cardellini, Valeria
AU  - Presti, Francesco Lo
T2  - 2023 IEEE International Conference on Pervasive Computing and Communications (PerCom)
AB  - As the Function-as-a-Service (FaaS) paradigm enjoys growing popularity within Cloud-based systems, there is increasing interest in moving serverless functions towards the Edge, to better support geo-distributed and pervasive applications. However, enjoying both the reduced latency of Edge and the scalability of FaaS requires new architectures and implementations to cope with typical Edge challenges (e.g., nodes with limited computational capacity). While first solutions have been proposed for Edge-based FaaS, including light function sandboxing techniques, we lack a platform with the ability to span both Edge and Cloud and adaptively exploit both. In this paper, we present Serverledge, a FaaS platform designed for the Edge-to-Cloud continuum. Serverledge adopts a decentralized architecture, where function invocation requests can be fully served within Edge nodes. To cope with load peaks, Serverledge also supports vertical (i.e., from Edge to Cloud) and horizontal (i.e., among Edge nodes) computation offloading. Our evaluation shows that Serverledge outperforms Apache OpenWhisk in an Edge-like scenario and has competitive performance with state-of-the-art frameworks optimized for the Edge, with the advantage of built-in support for vertical and horizontal offloading.
C3  - 2023 IEEE International Conference on Pervasive Computing and Communications (PerCom)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/PERCOM56429.2023.10099372
DP  - IEEE Xplore
SP  - 131
EP  - 140
SN  - 2474-249X
ST  - Serverledge
UR  - https://ieeexplore.ieee.org/document/10099372
Y2  - 2024/01/07/21:51:38
L2  - https://ieeexplore.ieee.org/document/10099372
ER  - 

TY  - CONF
TI  - Serverless Computing: Architectural Paradigms, Challenges, and Future Directions in Cloud Technology
AU  - Dey, Niladri Sekhar
AU  - Reddy, Sana Pavan Kumar
AU  - G, Lavanya
T2  - 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
AB  - The concept of serverless computing has emerged as a significant paradigm shift in cloud technology, fundamentally transforming the processes involved in application development and deployment. This research paper offers a complete examination of serverless computing, focusing on its architectural principles, identifying the primary issues it poses, and exploring the potential future developments in this fast-progressing domain. The article begins by providing a comprehensive explanation of the basic ideas behind serverless computing. It places particular emphasis on its event-driven nature, scalability, and the paradigm change from infrastructure management to code-centric development. This study examines the architectural paradigms of serverless computing, specifically focusing on Function as a Service (FaaS) and Backend as a Service (BaaS). It does a comparative analysis of these paradigms, considering their respective use cases and benefits. This study not only discusses the existing issues in serverless computing but also presents potential future possibilities in the field. The paper investigates current developments in the field, including the utilization of serverless orchestration, the integration of hybrid cloud systems, and the advancement of tools for serverless development. Moreover, this study explores the possible ramifications of serverless technology on edge computing, Internet of Things (IoT), and machine learning applications. This research paper is a great resource for practitioners, academics, and organizations in the field of cloud technology that are interested in leveraging the advantages of serverless computing. This study amalgamates existing information, discerns areas of research that need further investigation, and provides valuable perspectives on the dynamic development of serverless technology. Consequently, it facilitates a more knowledgeable and inventive approach to the field of cloud computing.
C3  - 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/I-SMAC58438.2023.10290253
DP  - IEEE Xplore
SP  - 406
EP  - 414
SN  - 2768-0673
ST  - Serverless Computing
UR  - https://ieeexplore.ieee.org/document/10290253
Y2  - 2024/01/07/21:53:32
L2  - https://ieeexplore.ieee.org/document/10290253
KW  - Serverless computing
KW  - Function as a service
KW  - Internet of things
KW  - Application development
KW  - Architectural paradigm
KW  - Baa
KW  - Challenge
KW  - Cloud technologies
KW  - Future direction
KW  - Paradigm shifts
KW  - Research papers
ER  - 

TY  - JOUR
TI  - Serverless Computing: State-of-the-Art, Challenges and Opportunities
AU  - Li, Yongkang
AU  - Lin, Yanying
AU  - Wang, Yang
AU  - Ye, Kejiang
AU  - Xu, Chengzhong
T2  - IEEE Transactions on Services Computing
AB  - Serverless computing is growing in popularity by virtue of its lightweight and simplicity of management. It achieves these merits by reducing the granularity of the computing unit to the function level. Specifically, serverless allows users to focus squarely on the function itself while leaving other cumbersome management and scheduling issues to the platform provider, who is responsible for striking a balance between high-performance scheduling and low resource cost. In this article, we conduct a comprehensive survey of serverless computing with a particular focus on its infrastructure characteristics. Whereby some existing challenges are identified, and the associated cutting-edge solutions are analyzed. With these results, we further investigate some typical open-source frameworks and study how they address the identified challenges. Given the great advantages of serverless computing, it is expected that its deployment would dominate future cloud platforms. As such, we also envision some promising research opportunities that need to be further explored in the future. We hope that our work in this article can inspire those researchers and practitioners who are engaged in related fields to appreciate serverless computing, thereby setting foot in this promising area and making great contributions to its development.
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/TSC.2022.3166553
DP  - IEEE Xplore
VL  - 16
IS  - 2
SP  - 1522
EP  - 1539
J2  - IEEE Transactions on Services Computing
SN  - 1939-1374
ST  - Serverless Computing
UR  - https://ieeexplore.ieee.org/document/9756233
Y2  - 2024/01/07/21:54:02
L2  - https://ieeexplore.ieee.org/document/9756233
ER  - 

TY  - CONF
TI  - Resource Management in Serverless Computing - Review, Research Challenges, and Prospects
AU  - Govindarajan, Kannan
AU  - Tienne, Andrè De
T2  - 2023 12th International Conference on Advanced Computing (ICoAC)
AB  - Serverless Computing has become one of the major trends for running cloud applications. The main advantage of serverless computing is that the infrastructure is being completely taken care of by the providers. Users are spared the burden and the complexity of managing the infrastructure. In serverless computing, cloud applications become for the users a collection of multiple cloud functions. Even though the infrastructure is completely under the control of providers, the resource management is very complex in nature and it is essential to take both the users’ and the consumers’ service requirements into consideration to achieve quality of service (QoS). In this research paper, we conduct a literature review on serverless computing, on various opensource frameworks supporting serverless computing, and on complexities involved in the management of serverless computing platform. The paper ends with a proposal for a rankbased cloud function scheduler for the allocation of cloud functions in the serverless computing environment. Based on the existing works and the preliminary experimentation, we are evident that the proposed mechanism will impact the performance metrics of throughput and the success rate of cloud functions execution.
C3  - 2023 12th International Conference on Advanced Computing (ICoAC)
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/ICoAC59537.2023.10249574
DP  - IEEE Xplore
SP  - 1
EP  - 5
UR  - https://ieeexplore.ieee.org/document/10249574
Y2  - 2024/01/07/21:55:31
L2  - https://ieeexplore.ieee.org/document/10249574
ER  - 

TY  - JOUR
TI  - Restricted Boltzmann Machine Assisted Secure Serverless Edge System for Internet of Medical Things
AU  - Lakhan, Abdullah
AU  - Mohammed, Mazin Abed
AU  - Rashid, Ahmed N.
AU  - Kadry, Seifedine
AU  - Abdulkareem, Karrar Hameed
AU  - Nedoma, Jan
AU  - Martinek, Radek
AU  - Razzak, Imran
T2  - IEEE Journal of Biomedical and Health Informatics
AB  - The Internet of things (IoT) is a network of technologies that support a wide variety of healthcare workflow applications to facilitate users’ obtaining real-time healthcare services. Many patients and doctors’ hospitals use different healthcare services to monitor their healthcare and save their records on the servers. Healthcare sensors are widely linked to the outside world for different disease classifications and questions. These applications are extraordinarily dynamic and use mobile devices to roam several locales. However, healthcare apps confront two significant challenges: data privacy and the cost of application execution services. This work presents the mobility-aware security dynamic service composition (MSDSC) algorithmic framework for workflow healthcare based on serverless, serverless, and restricted Boltzmann machine mechanisms. The study suggests the stochastic deep neural network trains probabilistic models at each phase of the process, including service composition, task sequencing, security, and scheduling. The experimental setup and findings revealed that the developed system-based methods outperform traditional methods by 25% in terms of safety and 35% in application cost.
DA  - 2023/02//
PY  - 2023
DO  - 10.1109/JBHI.2022.3178660
DP  - IEEE Xplore
VL  - 27
IS  - 2
SP  - 673
EP  - 683
J2  - IEEE Journal of Biomedical and Health Informatics
SN  - 2168-2208
UR  - https://ieeexplore.ieee.org/document/9784882
Y2  - 2024/01/07/21:55:45
L2  - https://ieeexplore.ieee.org/document/9784882
ER  - 

TY  - CONF
TI  - Serverless Data Protection in Cloud
AU  - Garde, Anaya
AU  - Gandhale, Siddhi
AU  - Dharankar, Rutuja
AU  - Sangtani, Bhagya Shri
AU  - Deshmukh, Nutan
AU  - Deshpande, Shilpa
AU  - Rathi, Rahul
AU  - Srivastava, Astitva
T2  - 2023 6th International Conference on Information Systems and Computer Networks (ISCON)
AB  - Cloud computing has already gained vast momentum in various fields like healthcare, finance, defense, education, and businesses. With the increasing amount of data generated, most firms have migrated to cloud platforms to reduce operational overhead and infrastructural costs. Corporate data in the cloud is often vulnerable to data breaches and losses. Hence, protecting the data stored on cloud-native assets is quintessential. Over the years, several approaches to disaster recovery have been proposed and most of the solutions focus on a server-based architecture. With the advent of cloud computing, serverless computing is emerging at an exponential rate considering the benefits it offers. Serverless computing lets users focus on developing their business logic rather than managing server configurations. In contrast to a server-based architecture for data protection; a serverless architecture is cost-efficient as it works on a pay-as- you-go model. This paper proposes a microservices-based approach to protect cloud-native assets using a serverless framework. In this approach, cloud assets are first discovered and their snapshot-based backup is taken. The backed-up assets can be later used to recover the asset in case of any disaster or data loss. Further, we also elaborate on the methodology of building a data protection application with Boto3 (Amazon Web Services - Software development Kit for Python) and integrating it with various Amazon Web Services. The proposed serverless and microservices-based approach offers high scalability, reliability, availability with time, and cost-efficiency.
C3  - 2023 6th International Conference on Information Systems and Computer Networks (ISCON)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/ISCON57294.2023.10112206
DP  - IEEE Xplore
SP  - 1
EP  - 6
SN  - 2832-143X
UR  - https://ieeexplore.ieee.org/document/10112206
Y2  - 2024/01/07/21:56:30
L2  - https://ieeexplore.ieee.org/document/10112206
ER  - 

TY  - JOUR
TI  - Serpens: A High Performance FaaS Platform for Network Functions
AU  - Yu, Heng
AU  - Zhang, Han
AU  - Shen, Junxian
AU  - Geng, Yantao
AU  - Wang, Jilong
AU  - Miao, Congcong
AU  - Xu, Mingwei
T2  - IEEE Transactions on Parallel and Distributed Systems
AB  - More and more enterprises deploy applications on Function-as-a-Service (FaaS) platforms to improve resource efficiency and save monetary costs. Network Functions (NFs) suffer from staggered peaks of traffic patterns and could benefit from fine-grained resource multiplexing in FaaS platform. However, naively exploring existing FaaS platforms to support NFs can introduce significant performance overheads in three aspects, including slow instance startup, remote state access for NFs, and costly packet delivery between NFs. To address these problems, we propose \sf SerpensSerpens, a high performance FaaS platform for NFs. First, \sf SerpensSerpens proposes a reusable NF runtime design to slash instance startup overhead. Second, \sf SerpensSerpens designs a novel state management mechanism to support local state access. Third, \sf SerpensSerpens introduces an advanced service chaining approach to avoid extra packet delivery. Besides, \sf SerpensSerpens designs an NF scaling mechanism to minimize performance fluctuation. We have implemented a prototype of \sf SerpensSerpens and conducted comprehensive experiments. Compared with the NFs and Service Function Chains (SFCs) that run on existing FaaS platforms, \sf SerpensSerpens can improve the throughput by more than 10× and reduce the latency by more than 90%.
DA  - 2023/08//
PY  - 2023
DO  - 10.1109/TPDS.2023.3263272
DP  - IEEE Xplore
VL  - 34
IS  - 8
SP  - 2448
EP  - 2463
J2  - IEEE Transactions on Parallel and Distributed Systems
SN  - 1558-2183
ST  - Serpens
UR  - https://ieeexplore.ieee.org/document/10089180
Y2  - 2024/01/07/21:56:54
L2  - https://ieeexplore.ieee.org/document/10089180
ER  - 

TY  - CONF
TI  - Secure Automated Inventory Management system using Abstracted System Design Microservice Architecture
AU  - M, Darshan
AU  - Raswanth, S.R
AU  - Chaitanya, V.S.S.K
AU  - Kalavalapalli, Venkata Sridhar Sai
AU  - Kumar, Priyanka
AU  - Srivastava, Gautam
T2  - 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)
AB  - Managing inventory has forever been a difficult problem for e-commerce sellers. Even today manual labor is used for the major part of the job. Without much automation in this industry growth has been stagnant because of the same reason.The proposed system works on a microservice-based architecture that enables automated inventory management with the help of a social-media messaging bot in an abstract manner. The bot comprises an integrated hybrid recommender system at the customer’s end for better recommendations based on image and textual data. The entire system is deployed on the cloud following a serverless architecture for resilient service. The proposed solution plays a key role in taking research forward in the direction of micro-service-based system design involving real-world data. The results of the experiment prove to be beneficial for customers and retailers since they are now connected using the micro-service.
C3  - 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/ICCCNT56998.2023.10306924
DP  - IEEE Xplore
SP  - 1
EP  - 7
SN  - 2473-7674
UR  - https://ieeexplore.ieee.org/document/10306924
Y2  - 2024/01/07/21:57:13
L2  - https://ieeexplore.ieee.org/document/10306924
ER  - 

TY  - CONF
TI  - Serverless Functions in the Cloud-Edge Continuum: Challenges and Opportunities
AU  - Russo, Gabriele Russo
AU  - Cardellini, Valeria
AU  - Presti, Francesco Lo
T2  - 2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)
AB  - The Function-as-a-Service (FaaS) paradigm is increasingly adopted for the development of Cloud-native applications, which especially benefit from the seamless scalability and attractive pricing models of serverless deployments. With the continuous emergence of latency-sensitive applications and services, including Internet-of-Things and augmented reality, it is now natural to wonder whether and how the FaaS paradigm can be efficiently exploited in the Cloud-Edge Continuum, where serverless functions may benefit from reduced network delay between their invoking users and the FaaS platform. In this paper, we illustrate the key challenges that must be faced to effectively deploy serverless functions in the Cloud-Edge Continuum and review recent contributions proposed by the research community towards overcoming those challenges. We also discuss the key issues that currently remain unsolved and highlight a few research opportunities for better support of FaaS in the Compute Continuum.
C3  - 2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)
DA  - 2023/03//
PY  - 2023
DO  - 10.1109/PDP59025.2023.00056
DP  - IEEE Xplore
SP  - 321
EP  - 328
SN  - 2377-5750
ST  - Serverless Functions in the Cloud-Edge Continuum
UR  - https://ieeexplore.ieee.org/document/10136957
Y2  - 2024/01/07/21:57:42
L2  - https://ieeexplore.ieee.org/document/10136957
KW  - Serverless
KW  - edge computing
KW  - Edge computing
KW  - Service platforms
KW  - Augmented reality
KW  - compute continuum
KW  - Compute continuum
KW  - Key Issues
KW  - Network-delay
KW  - Pricing models
KW  - Research communities
KW  - Sensitive application
KW  - Service paradigm
ER  - 

TY  - CONF
TI  - Serverless Web Application for The Life Cycle of Software Development Projects using Scrum in South America
AU  - Busto, Pablo Josué Francia Del
AU  - Tambra, Rodson Vladimir Ayme
AU  - Moroco, Juan Antonio Flores
T2  - 2023 2nd Asia-Pacific Computer Technologies Conference (APCT)
AB  - Serverless models are one of the latest architecture models provided by Cloud vendors such as AWS and Microsoft; we are exploring serverless applications to develop a progressive web application that will help future developers and project leaders to manage their projects better by having a tool that will help incorporate Scrum properly into their projects. As seen in the investigation done, we can see that South America is below the global average in completing projects on time or within the planned budget. In this work we explain the agile life cycle of the web application developed, focusing on the Scrum aspects of the tool design, the serverless architecture of the app and its following development, while also applying an analysis of the current projects to measure the level of effectiveness that a proper Scrum method can have on finishing a project correctly on time. With our initial surveys landing a score of 80 in the positive version of the System Usability Scale and getting good results in the completion of our main quality metrics, we can see that there is indeed an interest in finding a tool that helps use Scrum properly by adding video calls as a mean of direct communication between users.
C3  - 2023 2nd Asia-Pacific Computer Technologies Conference (APCT)
DA  - 2023/01//
PY  - 2023
DO  - 10.1109/APCT58752.2023.00008
DP  - IEEE Xplore
SP  - 1
EP  - 7
UR  - https://ieeexplore.ieee.org/document/10153878
Y2  - 2024/01/07/21:58:54
L2  - https://ieeexplore.ieee.org/document/10153878
ER  - 

TY  - CONF
TI  - Speech Recognition and Neural Networks based Talking Health Care Bot (THCB): Medibot
AU  - Bandopadhyay, Dwaipayan
AU  - Ghosh, Rajdeep
AU  - Chatterjee, Rajdeep
AU  - Das, Nabanita
AU  - Sadhukhan, Bikash
T2  - 2023 7th International Conference on Computing Methodologies and Communication (ICCMC)
AB  - The COVID-19 pandemic has affected healthcare in several ways. Some patients were unable to make it to appointments due to curfews, transportation restrictions, and stay-at-home directives, while less urgent procedures were postponed or cancelled. Others steered clear of hospitals out of fear of contracting an infection. With the use of a conversational artificial intelligence-based program, the Talking Health Care Bot (THCB) could be useful during the pandemic by allowing patients to receive supportive care without physically visiting a hospital. Therefore, the THCB will drastically and quickly change in-person care to patient consultation through the internet. To give patients free primary healthcare and to narrow the supply-demand gap for human healthcare professionals, this work created a conversational bot based on artificial intelligence and machine learning. The study proposes a revolutionary computer program that serves as a patient's personal virtual doctor. The program was carefully created and thoroughly trained to communicate with patients as if they were real people. Based on a serverless architecture, this application predicts the disease based on the symptoms of the patients. A Talking Healthcare chatbot confronts several challenges, but the user's accent is by far the most challenging. This study has then evaluated the proposed model by using one hundred different voices and symptoms, achieving an accuracy rate of 77%.
C3  - 2023 7th International Conference on Computing Methodologies and Communication (ICCMC)
DA  - 2023/02//
PY  - 2023
DO  - 10.1109/ICCMC56507.2023.10084191
DP  - IEEE Xplore
SP  - 399
EP  - 404
ST  - Speech Recognition and Neural Networks based Talking Health Care Bot (THCB)
UR  - https://ieeexplore.ieee.org/document/10084191
Y2  - 2024/01/07/21:59:16
L2  - https://ieeexplore.ieee.org/document/10084191
ER  - 

TY  - CONF
TI  - SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture
AU  - Barrak, Amine
AU  - Jaziri, Mayssa
AU  - Trabelsi, Ranim
AU  - Jaafar, Fehmi
AU  - Petrillo, Fabio
T2  - 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)
AB  - The advent of serverless computing has ushered in notable advancements in distributed machine learning, particularly within parameter server-based architectures. Yet, the integration of serverless features within peer-to-peer (P2P) distributed networks remains largely uncharted. In this paper, we introduce SPIRT, a fault-tolerant, reliable, scalable and secure serverless P2P ML training architecture. designed to bridge this existing gap. Capitalizing on the inherent robustness and reliability innate to P2P systems, we emphasized Intra-peer scalability for concurrent gradient to mitigate communication overhead from increased peer interactions. SPIRT, employs RedisAI for in-database operations, achieves an 82% reduction in model update times. This architecture showcases resilience against peer failures and adeptly manages the integration of new peers. Furthermore, SPIRT ensures secure communication between peers, enhancing the reliability of distributed machine learning tasks. Even in the face of Byzantine attacks, the system’s robust aggregation algorithms maintain high levels of accuracy. These findings illuminate the promising potential of serverless architectures in P2P distributed machine learning, offering a significant stride towards the development of more efficient, scalable, and resilient applications.
C3  - 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)
DA  - 2023/10//
PY  - 2023
DO  - 10.1109/QRS60937.2023.00069
DP  - IEEE Xplore
SP  - 650
EP  - 661
SN  - 2693-9177
ST  - SPIRT
UR  - https://ieeexplore.ieee.org/document/10366723
Y2  - 2024/01/07/22:00:32
L1  - https://arxiv.org/pdf/2309.14148
L2  - https://ieeexplore.ieee.org/document/10366723
ER  - 

TY  - CONF
TI  - The Cloud-Based Optimization for Automated Warehouse Design
AU  - Muliarevych, Oleksandr
T2  - 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)
AB  - The paper delves into the process of warehouse design and the methodologies applicable for creating an automated warehouse design system. Focusing on the calculation of entry and shipment zones, this study highlights the challenges faced in seeking optimal design solutions, given the numerous combinations generated from input data, parameters, and constraints. The presented a two-phase design approach, which forms the basis of demonstrating automated warehouse design system. This system incorporates cutting-edge cloud technology advancements, such as predictive modeling and serverless computing, to enhance performance. The article compares the performance of various cloud computing architectures, including monolithic, horizontally auto-scalable microservices, the suggested approach grounded in lightweight context-based functional processing.
C3  - 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IDAACS58523.2023.10348891
DP  - IEEE Xplore
VL  - 1
SP  - 380
EP  - 384
SN  - 2770-4254
UR  - https://ieeexplore.ieee.org/document/10348891
Y2  - 2024/01/07/22:01:23
L2  - https://ieeexplore.ieee.org/document/10348891
ER  - 

TY  - CONF
TI  - Storm-RTS: Stream Processing with Stable Performance for Multi-Cloud and Cloud-edge
AU  - Nguyen, Hai Duc
AU  - Chien, Andrew A.
T2  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
AB  - Stream Processing Engines (SPEs) traditionally de-ploy applications on a set of shared workers (e.g., threads, processes, or containers) requiring complex performance man-agement by SPEs and application developers. We explore a new approach that replaces workers with Rate-based Abstract Ma-chines (RBAMs). This allows SPEs to translate stream operations into FaaS invocations, and exploit guaranteed invocation rates to manage performance. This approach enables SPE applications to achieve transparent and predictable performance. We realize the approach in the Storm-RTS system. Exploring 36 stream processing scenarios over 5 different hardware config-urations, we demonstrate several key advantages. First, Storm-RTS provides stable application performance and can enable flexible reconfiguration across cloud resource configurations. Sec-ond, SPEs built on RBAM can be resource-efficient and scalable. Finally, Storm-RTS allows the stream-processing paradigm to be extended from the cloud to the edge, using its performance stability to hide edge heterogeneity and resource competition. An experiment with 4 cloud and edge sites over 300 cores shows how Storm-RTS can support flexible reconfiguration and simple high-level declarative policies that optimize resource cost or other criteria.
C3  - 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/CLOUD60044.2023.00015
DP  - IEEE Xplore
SP  - 45
EP  - 57
SN  - 2159-6190
ST  - Storm-RTS
UR  - https://ieeexplore.ieee.org/document/10254965
Y2  - 2024/01/07/22:01:54
L2  - https://ieeexplore.ieee.org/document/10254965
ER  - 

TY  - JOUR
TI  - The Jaseci Programming Paradigm and Runtime Stack: Building Scale-Out Production Applications Easy and Fast
AU  - Mars, Jason
AU  - Kang, Yiping
AU  - Daynauth, Roland
AU  - Li, Baichuan
AU  - Mahendra, Ashish
AU  - Flautner, Krisztian
AU  - Tang, Lingjia
T2  - IEEE Computer Architecture Letters
AB  - Today's production scale-out applications include many sub-application components, such as storage backends, logging infrastructure and AI models. These components have drastically different characteristics, are required to work in collaboration, and interface with each other as microservices. This leads to increasingly high complexity in developing, optimizing, configuring, and deploying scale-out applications, raising the barrier to entry for most individuals and small teams. We developed a novel co-designed runtime system, Jaseci, and programming language, Jac, which aims to reduce this complexity. The key design principle throughout Jaseci's design is to raise the level of abstraction by moving as much of the scale-out data management, microservice componentization, and live update complexity into the runtime stack to be automated and optimized automatically. We use real-world AI applications to demonstrate Jaseci's benefit for application performance and developer productivity.
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/LCA.2023.3274038
DP  - IEEE Xplore
VL  - 22
IS  - 2
SP  - 101
EP  - 104
J2  - IEEE Computer Architecture Letters
SN  - 1556-6064
ST  - The Jaseci Programming Paradigm and Runtime Stack
UR  - https://ieeexplore.ieee.org/document/10129141
Y2  - 2024/01/07/22:02:16
L1  - https://arxiv.org/pdf/2305.09864
L2  - https://ieeexplore.ieee.org/document/10129141
ER  - 

TY  - CONF
TI  - Time and Cost-Efficient Cloud Data Transmission based on Serverless Computing Compression
AU  - Gu, Rong
AU  - Chen, Xiaofei
AU  - Dai, Haipeng
AU  - Wang, Shulin
AU  - Wang, Zhaokang
AU  - Tu, Yaofeng
AU  - Huang, Yihua
AU  - Chen, Guihai
T2  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
AB  - Nowadays, there exists a lot of cross-region data transmission demand on cloud. It is promising to use serverless computing for compressing data to save the transmission data amount. However, it is challenging to estimate the data transmission time and monetary cost with serverless compression. In addition, minimizing the data transmission cost is non-trivial due to enormous parameter space and joint optimization. This paper focuses on this problem and makes the following contributions: (1) We propose empirical data transmission time and monetary cost models based on serverless compression. (2) For single-task cloud data transmission, we propose two efficient parameter search methods based on Sequential Quadratic Programming (SQP ) and Eliminate then Divide and Conquer (EDC), which are theoretically proven with error upper bounds. (3) Furthermore, for multi-task cloud data transmission, a parameter search method based on dynamic programming and numerical computation is proposed to reduce the algorithm complexity from exponential to linear complexity. We have implemented the entire actual system and evaluated it with various workloads and application cases on the real-world AWS serverless computing platform. Experimental results on cross-region public cloud show that the proposed approach can improve the parameter search efficiency by more than 3× compared with the state-of-art parameter search methods and achieves better parameter quality. Compared with other competing cloud data transmission approaches, our approach is able to achieve higher time efficiency and lower monetary cost.
C3  - IEEE INFOCOM 2023 - IEEE Conference on Computer Communications
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/INFOCOM53939.2023.10229090
DP  - IEEE Xplore
SP  - 1
EP  - 10
SN  - 2641-9874
UR  - https://ieeexplore.ieee.org/document/10229090
Y2  - 2024/01/07/22:02:38
L2  - https://ieeexplore.ieee.org/document/10229090
ER  - 

TY  - CONF
TI  - Towards a Holistic Cloud System with End-to-End Performance Guarantees
AU  - Andreoli, Remo
AU  - Cucinotta, Tommaso
T2  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Computing technologies are undergoing a relentless evolution from both the hardware and software sides, incorporating new mechanisms for low-latency networking, virtualization, operating systems, hardware acceleration, smart services orchestration, serverless computing, hybrid private-public Cloud solutions and others. Therefore, Cloud infrastructures are becoming increasingly attractive for deploying a wider and wider range of applications, including those with more and more stringent timing constraints, like the emerging use case of deploying time-critical applications. However, despite the availability of a number of public Cloud offerings, and of products (or open-source suites) for deploying in-house private Cloud infrastructures, still there are no solutions readily available for managing time-critical software components with predictable end-to-end timing requirements in the range of hundreds or even tens of milliseconds. The goal of this discussion is to present the multi-domain challenges associated with orchestrating a holistic Cloud system with end-to-end guarantees, which is the subject of my current PhD investigations.
C3  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IC2E59103.2023.00039
DP  - IEEE Xplore
SP  - 236
EP  - 238
SN  - 2694-0825
UR  - https://ieeexplore.ieee.org/document/10305824
Y2  - 2024/01/07/22:03:12
L2  - https://ieeexplore.ieee.org/document/10305824
ER  - 

TY  - JOUR
TI  - Toward Sustainable Smart City: Lessons From 20 Years of Korean Programs
AU  - Kwak, Young Hoon
AU  - Lee, Jaehyun
T2  - IEEE Transactions on Engineering Management
AB  - South Korea has a long history of the planning, development, and management of smart cities to integrate emerging technological advances into complex physical infrastructure. This article explores lessons learned from smart city programs in South Korea to better understand the challenges and opportunities of future sustainable smart city innovation and development. This article conducted a comprehensive review and analysis of South Korea's smart city programs and conceptualized a sustainable smart city framework that will assist policymakers, planners, citizens, and other key stakeholders. This research proposed Governance, Policy, and Services (GPS) as the three pillars of a successful smart city framework, in addition to integrating physical and cyber infrastructures. This article argues that a smart city should function as a service platform that incubates and delivers long-term services to citizens and society. This article also emphasizes that a strong groundwork of the GPS framework will lead to the successful adaptation of innovative technologies and ideas for future smart city programs.
DA  - 2023/02//
PY  - 2023
DO  - 10.1109/TEM.2021.3060956
DP  - IEEE Xplore
VL  - 70
IS  - 2
SP  - 740
EP  - 754
J2  - IEEE Transactions on Engineering Management
SN  - 1558-0040
ST  - Toward Sustainable Smart City
UR  - https://ieeexplore.ieee.org/document/9381515
Y2  - 2024/01/07/22:03:41
L2  - https://ieeexplore.ieee.org/document/9381515
ER  - 

TY  - CONF
TI  - Towards a Multi-objective Scheduling Policy for Serverless-based Edge-Cloud Continuum
AU  - Angelelli, Luc
AU  - da Silva, Anderson Andrei
AU  - Georgiou, Yiannis
AU  - Mercier, Michael
AU  - Mounié, Gregory
AU  - Trystram, Denis
T2  - 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
AB  - The cloud is extended towards the edge to form a computing continuum while managing resources' heterogeneity. The serverless technology simplified how to build cloud applications and use resources, becoming a driving force in consolidating the continuum with the deployment of small functions with short execution. However, the adaptation of serverless to the edge-cloud continuum brings new challenges mainly related to resource management and scheduling. Standard cloud scheduling policies are based on greedy algorithms that do not efficiently handle platforms' heterogeneity nor deal with problems such as cold start delays. This work introduces a new scheduling policy that tries to address these issues. It is based on multi-objective optimization for data transfers and makespan while considering heterogeneity. Using simulations that vary workloads, platforms, and heterogeneity levels, we study the system utilization, the trade-offs between the targets, and the impacts of considering platforms' heterogeneity. We perform comparisons with a baseline inspired by a Kubernetes-based policy, representing greedy algorithms. Our experiments show considerable gaps between the efficiency of a greedy-based scheduling policy and a multi-objective-based one. The last outperforms the baseline by reducing makespan, data transfers, and system utilization by up to two orders of magnitudes in relevant cases for the edge-cloud continuum.
C3  - 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/CCGrid57682.2023.00052
DP  - IEEE Xplore
SP  - 485
EP  - 497
UR  - https://ieeexplore.ieee.org/document/10171469
Y2  - 2024/01/07/22:04:03
L1  - https://hal.science/hal-04177085v2/file/main-ccgrid.pdf
L2  - https://ieeexplore.ieee.org/document/10171469
KW  - Serverless Computing
KW  - Serverless computing
KW  - Edge clouds
KW  - Data transfer
KW  - Economic and social effects
KW  - Edge-cloud continuum
KW  - Edge-Cloud Continuum
KW  - Greedy algorithms
KW  - Heterogeneous platforms
KW  - Heterogeneous Platforms
KW  - Makespan
KW  - Managing resources
KW  - Multi-objective scheduling
KW  - Multiobjective optimization
KW  - Scheduling policies
KW  - Scheduling Policies
KW  - Simulation platform
KW  - System utilization
ER  - 

TY  - CONF
TI  - Towards Serverless Sky Computing: An Investigation on Global Workload Distribution to Mitigate Carbon Intensity, Network Latency, and Cost
AU  - Cordingly, Robert
AU  - Kaur, Jasleen
AU  - Dwivedi, Divyansh
AU  - Lloyd, Wes
T2  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
AB  - The high demand for energy consumption and the resulting carbon footprint of the cloud pose significant sustainability challenges, as cloud data centers consume vast amounts of energy. The emergence of serverless cloud computing platforms has opened up new avenues for more sustainable cloud computing. Serverless Function-as-a-Service (FaaS) cloud computing platforms facilitate deploying applications as decoupled microservices to leverage automatic rapid scaling, high availability, fault tolerance, and on-demand pricing. The absence of always-on hosting costs associated with virtual machines enables serverless functions to be deployed with many different function configurations and cloud regions to achieve high performance, low network latency, and reduced costs. In this paper, we investigate the utility of a global sky computing platform where serverless resources are aggregated between up to 19 distinct cloud regions. We prototype a serverless load distribution system to distribute client requests across serverless aggregations to minimize performance objectives, including network latency, runtime, hosting costs, and carbon footprint. To evaluate our serverless distribution system's ability to meet performance objectives, we continuously executed large experiments across 19 regions around the world from November 2022 through March 2023. Our serverless load distribution approach using aggregated resources reduced the carbon intensity of a globally distributed serverless application by up to 99.8%, network latency by 65%, or hosting costs by 58% by optimizing function routing to deployments with optimal hardware configurations.
C3  - 2023 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2023/09//
PY  - 2023
DO  - 10.1109/IC2E59103.2023.00015
DP  - IEEE Xplore
SP  - 59
EP  - 69
SN  - 2694-0825
ST  - Towards Serverless Sky Computing
UR  - https://ieeexplore.ieee.org/document/10305816
Y2  - 2024/01/07/22:08:28
L2  - https://ieeexplore.ieee.org/document/10305816
ER  - 

TY  - CONF
TI  - Transformation of a Legacy Airport Meteorology Application into a Serverless Cloud Application
AU  - Hluchý, Ladislav
AU  - Habala, Ondrej
AU  - Bobák, Martin
AU  - Šeleng, Martin
T2  - 2023 IEEE 17th International Symposium on Applied Computational Intelligence and Informatics (SACI)
AB  - In this article we describe our continuing work on transforming a legacy application dealing with airport meteorology into a cloud application, using the concepts of serverless computing and Function-as-a-Service. In our previous work [1] we have described the architecture and concepts of our solution, and initial experiments with several FaaS tools. Here we continue the experiments, concentrating on the Apache OpenWhisk framework, which we have selected as our target serverless environment. We have further developed some of the application’s components and tried to execute them via OpenWhisk. During these experiments we have found multiple practical problems of transforming and executing legacy applications as serverless functions in OpenWhisk, and have devised solutions for these problems.
C3  - 2023 IEEE 17th International Symposium on Applied Computational Intelligence and Informatics (SACI)
DA  - 2023/05//
PY  - 2023
DO  - 10.1109/SACI58269.2023.10158660
DP  - IEEE Xplore
SP  - 000637
EP  - 000642
SN  - 2765-818X
UR  - https://ieeexplore.ieee.org/document/10158660
Y2  - 2024/01/07/22:09:08
L2  - https://ieeexplore.ieee.org/document/10158660
L2  - https://ieeexplore.ieee.org/document/10158660
KW  - Cloud computing
KW  - cloud computing
KW  - serverless computing
KW  - Serverless computing
KW  - Function-as-a-Service
KW  - Function-as-a-service
KW  - Cloud-computing
KW  - Cloud applications
KW  - Airports
KW  - Apache openwhisk
KW  - Apache OpenWhisk
KW  - Application components
KW  - legacy application
KW  - Legacy applications
KW  - Meteorology
KW  - Practical problems
ER  - 

TY  - JOUR
TI  - Using a Digital Twin as the Objective Function for Evolutionary Algorithm Applications in Large Scale Industrial Processes
AU  - Eklund, Miro
AU  - Sierla, Seppo A.
AU  - Niemistö, Hannu
AU  - Korvola, Timo
AU  - Savolainen, Jouni
AU  - Karhela, Tommi A.
T2  - IEEE Access
AB  - In this paper, we describe how the up-to-date state of a digital twin, and its corresponding simulation model, can be used as a fitness function of an evolutionary algorithm for optimizing a large-scale industrial process. An ICT architecture is presented for solving the computational challenges that arise when the fitness function evaluation takes considerable amount of time. Parallel computation of the fitness function in a cloud computing environment is proposed and the evolutionary algorithm is connected to the computational environment using the Function-as-a-Service approach. A case-study was conducted on the district heating network of Espoo, the second largest city in Finland. The study shows that the architecture is suited for optimizing the operating costs of the large district heating network, with over 800 km of water pipes and over 14 heat producers, reaching a cost-saving of an average of 2%, and up-to 4%, over the current industrial state-of-the-art method in use at the city of Espoo.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ACCESS.2023.3254896
DP  - IEEE Xplore
VL  - 11
SP  - 24185
EP  - 24202
J2  - IEEE Access
SN  - 2169-3536
UR  - https://ieeexplore.ieee.org/document/10064259
Y2  - 2024/01/07/22:10:40
L1  - https://research.aalto.fi/files/104263241/Using_a_Digital_Twin_as_the_Objective_Function_for_Evolutionary_Algorithm_Applications_in_Large_Scale_Industrial_Processes.pdf
L1  - https://ieeexplore.ieee.org/ielx7/6287639/6514899/10064259.pdf
L2  - https://ieeexplore.ieee.org/document/10064259
ER  - 

TY  - CONF
TI  - When Edge Meets FaaS: Opportunities and Challenges
AU  - Jin, Runyu
AU  - Yang, Qirui
AU  - Zhao, Ming
T2  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
AB  - The proliferation of edge devices and the rapid growth of IoT data have called forth the edge computing paradigm. Function-as-a-service (FaaS) is a promising computing paradigm to realize edge computing. This paper explores the feasibility and advantages of FaaS-based edge computing. It also studies the research challenges that should be addressed in the design of such systems, which are 1) the quick decomposing and recomposing of applications, 2) the trade-off between performance and isolation of sandbox mechanisms, and 3) distributed scheduling. The challenges are illustrated by evaluating existing FaaS-based edge platforms, AWS IoT Greengrass, and OpenFaaS.
C3  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/EDGE60047.2023.00016
DP  - IEEE Xplore
SP  - 23
EP  - 25
SN  - 2767-9918
ST  - When Edge Meets FaaS
UR  - https://ieeexplore.ieee.org/document/10234282
Y2  - 2024/01/07/22:12:19
L1  - https://arxiv.org/pdf/2307.06397
L2  - https://ieeexplore.ieee.org/document/10234282
ER  - 

TY  - JOUR
TI  - xAFCLxAFCL: Run Scalable Function Choreographies Across Multiple FaaS Systems
AU  - Ristov, Sasko
AU  - Pedratscher, Stefan
AU  - Fahringer, Thomas
T2  - IEEE Transactions on Services Computing
AB  - Most well-known cloud providers offer advanced support for serverless applications that goes beyond single function invocation by enabling developers to build entire workflows, which are known as serverless function choreographies (FCs). Current support for FCs by many FaaS systems uncovered important problems including maximum number of parallel function executions, unexpected considerable delays, and provider lock-in. These limitations can result in longer execution times or even failure to execute individual functions or entire FCs. To overcome some of these limitations, we introduce a scalable middleware service xAFCLxAFCL that can schedule and execute different functions of the same FC across multiple FaaS systems (currently supporting all top five providers). In order to support scheduling under xAFCLxAFCL, we introduce a novel FaaS model which estimates the completion time of functions by considering FaaS system limitations, submission delays, and overheads for executing functions. Experimental results demonstrate that xAFCLxAFCL’s FaaS model shows very low inaccuracy of up to 2.9%2.9% for AWS and 20%20% for IBM for real-life BWA data-bound FC that uses S3. Moreover, xAFCLxAFCL outperforms an earliest start time (EST) scheduler by up to 43%43% for makespan and 2.7\times2.7× for throughput.
DA  - 2023/01//
PY  - 2023
DO  - 10.1109/TSC.2021.3128137
DP  - IEEE Xplore
VL  - 16
IS  - 1
SP  - 711
EP  - 723
J2  - IEEE Transactions on Services Computing
SN  - 1939-1374
ST  - xAFCLxAFCL
UR  - https://ieeexplore.ieee.org/document/9616383/algorithms?tabFilter=dataset#algorithms
Y2  - 2024/01/07/22:12:40
L2  - https://ieeexplore.ieee.org/document/9616383/algorithms?tabFilter=dataset#algorithms
ER  - 

TY  - JOUR
TI  - Workflow Scheduling in Serverless Edge Computing for the Industrial Internet of Things: A Learning Approach
AU  - Xie, Renchao
AU  - Gu, Dier
AU  - Tang, Qinqin
AU  - Huang, Tao
AU  - Yu, Fei Richard
T2  - IEEE Transactions on Industrial Informatics
AB  - Serverless edge computing is seen as a promising enabler to execute differentiated Industrial Internet of Things (IIoT) applications without managing the underlying servers and clusters. In IIoT serverless edge computing, IIoT workflow scheduling for cloud-edge collaborative processing is closely related to the service quality of users. However, serverless functions decomposed by IIoT applications are limited in their deployment at the edge due to the resource-constrained nature of edge infrastructures. In addition, the scheduling of complex IIoT applications supported by serverless computing is more challenging. Therefore, considering the limited function deployment and the complex dependencies of serverless workflows, we model the workflow application as directed acyclic graph and formulate the scheduling problem as a multiobjective optimization problem. A dueling double deep Q-network-based solution is proposed to make scheduling decisions under dynamically changing systems. Extensive simulation experiments are conducted to validate the superiority of the proposed scheme.
DA  - 2023/07//
PY  - 2023
DO  - 10.1109/TII.2022.3217477
DP  - IEEE Xplore
VL  - 19
IS  - 7
SP  - 8242
EP  - 8252
J2  - IEEE Transactions on Industrial Informatics
SN  - 1941-0050
ST  - Workflow Scheduling in Serverless Edge Computing for the Industrial Internet of Things
UR  - https://ieeexplore.ieee.org/document/9931467
Y2  - 2024/01/07/22:13:15
L2  - https://ieeexplore.ieee.org/document/9931467
ER  - 

TY  - CONF
TI  - Firecracker: lightweight virtualization for serverless applications
AU  - Agache, Alexandru
AU  - Brooker, Marc
AU  - Florescu, Andreea
AU  - Iordache, Alexandra
AU  - Liguori, Anthony
AU  - Neugebauer, Rolf
AU  - Piwonka, Phil
AU  - Popa, Diana-Maria
T2  - 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)
T3  - NSDI'20
AB  - Serverless containers and functions are widely used for deploying and managing software in the cloud. Their popularity is due to reduced cost of operations, improved utilization of hardware, and faster scaling than traditional deployment methods. The economics and scale of serverless applications demand that workloads from multiple customers run on the same hardware with minimal overhead, while preserving strong security and performance isolation. The traditional view is that there is a choice between virtualization with strong security and high overhead, and container technologies with weaker security and minimal overhead. This tradeoff is unacceptable to public infrastructure providers, who need both strong security and minimal overhead. To meet this need, we developed Firecracker, a new open source Virtual Machine Monitor (VMM) specialized for serverless workloads, but generally useful for containers, functions and other compute workloads within a reasonable set of constraints. We have deployed Firecracker in two publically-available serverless compute services at Amazon Web Services (Lambda and Fargate), where it supports millions of production workloads, and trillions of requests per month. We describe how specializing for serverless informed the design of Firecracker, and what we learned from seamlessly migrating Lambda customers to Firecracker.
C1  - USA
C3  - Proceedings of the 17th Usenix Conference on Networked Systems Design and Implementation
DA  - 2020///
PY  - 2020
DO  - 10.5555/3388242.3388273
DP  - ACM Digital Library
SP  - 419
EP  - 434
LA  - en
PB  - USENIX Association
SN  - 978-1-939133-13-7
ST  - Firecracker
UR  - https://dl.acm.org/doi/10.5555/3388242.3388273
Y2  - 2024/01/14/
L1  - https://www.usenix.org/system/files/nsdi20-paper-agache.pdf
ER  - 

TY  - CONF
TI  - Serverless Edge Computing: Vision and Challenges
AU  - Aslanpour, Mohammad S.
AU  - Toosi, Adel N.
AU  - Cicconetti, Claudio
AU  - Javadi, Bahman
AU  - Sbarski, Peter
AU  - Taibi, Davide
AU  - Assuncao, Marcos
AU  - Gill, Sukhpal Singh
AU  - Gaire, Raj
AU  - Dustdar, Schahram
T3  - ACSW '21
AB  - Born from a need for a pure “pay-per-use” model and highly scalable platform, the “Serverless” paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things (IoT) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and IoT applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 Australasian Computer Science Week Multiconference
DA  - 2021/02/01/
PY  - 2021
DO  - 10.1145/3437378.3444367
DP  - ACM Digital Library
SP  - 1
EP  - 10
PB  - Association for Computing Machinery
SN  - 978-1-4503-8956-3
ST  - Serverless Edge Computing
UR  - https://doi.org/10.1145/3437378.3444367
Y2  - 2024/01/16/
L1  - https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/70411/2/Gill%20Serverless%20Edge%20Computing%202021%20Accepted.pdf
ER  - 

TY  - GEN
TI  - Serverless Platforms on the Edge: A Performance Analysis
AU  - Javed, Hamza
AU  - Toosi, Adel N.
AU  - Aslanpour, Mohammad S.
AB  - The exponential growth of Internet of Things (IoT) has given rise to a new wave of edge computing due to the need to process data on the edge, closer to where it is being produced and attempting to move away from a cloud-centric architecture. This provides its own opportunity to decrease latency and address data privacy concerns along with the ability to reduce public cloud costs. The serverless computing model provides a potential solution with its event-driven architecture to reduce the need for ever-running servers and convert the backend services to an as-used model. This model is an attractive prospect in edge computing environments with varying workloads and limited resources. Furthermore, its setup on the edge of the network promises reduced latency to the edge devices communicating with it and eliminates the need to manage the underlying infrastructure. In this book chapter, first, we introduce the novel concept of serverless edge computing, then, we analyze the performance of multiple serverless platforms, namely, OpenFaaS, AWS Greengrass, Apache OpenWhisk, when set up on the single-board computers (SBCs) on the edge and compare it with public cloud serverless offerings, namely, AWS Lambda and Azure Functions, to deduce the suitability of serverless architectures on the network edge. These serverless platforms are set up on a cluster of Raspberry Pis and we evaluate their performance by simulating different types of edge workloads. The evaluation results show that OpenFaaS achieves the lowest response time on the SBC edge computing infrastructure while serverless cloud offerings are the most reliable with the highest success rate.
DA  - 2021/11/11/
PY  - 2021
DO  - 10.48550/arXiv.2111.06563
DP  - arXiv.org
PB  - arXiv
ST  - Serverless Platforms on the Edge
UR  - http://arxiv.org/abs/2111.06563
Y2  - 2024/01/17/00:10:08
L1  - https://arxiv.org/pdf/2111.06563.pdf
L2  - https://arxiv.org/abs/2111.06563
KW  - Computer Science - Distributed, Parallel, and Cluster Computing
ER  - 

TY  - CONF
TI  - Real-time Serverless: Enabling Application Performance Guarantees
AU  - Nguyen, Hai Duc
AU  - Zhang, Chaojie
AU  - Xiao, Zhujun
AU  - Chien, Andrew A.
T3  - WOSC '19
AB  - Today's serverless provides "function-as-a-service" with dynamic scaling and fine-grained resource charging, enabling new cloud applications. Serverless functions are invoked as a best-effort service. We propose an extension to serverless, called real-time serverless that provides an invocation rate guarantee, a service-level objective (SLO) specified by the application, and delivered by the underlying implementation. Real-time serverless allows applications to guarantee real-time performance. We study real-time serverless behavior analytically and empirically to characterize its ability to support bursty, real-time cloud and edge applications efficiently. Finally, we use a case study, traffic monitoring, to illustrate the use and benefits of real-time serverless, on our prototype implementation.
C1  - New York, NY, USA
C3  - Proceedings of the 5th International Workshop on Serverless Computing
DA  - 2019/12/09/
PY  - 2019
DO  - 10.1145/3366623.3368133
DP  - ACM Digital Library
SP  - 1
EP  - 6
PB  - Association for Computing Machinery
SN  - 978-1-4503-7038-7
ST  - Real-time Serverless
UR  - https://dl.acm.org/doi/10.1145/3366623.3368133
Y2  - 2024/01/16/
L1  - https://dl.acm.org/doi/pdf/10.1145/3366623.3368133
KW  - Serverless
KW  - Bursty
KW  - Interface
KW  - Real-time
ER  - 

TY  - JOUR
TI  - AutoScaleSim: A simulation toolkit for auto-scaling Web applications in clouds
AU  - Aslanpour, Mohammad S.
AU  - Toosi, Adel N.
AU  - Taheri, Javid
AU  - Gaire, Raj
T2  - Simulation Modelling Practice and Theory
AB  - Auto-scaling of Web applications is an extensively investigated issue in cloud computing. To evaluate auto-scaling mechanisms, the cloud community is facing considerable challenges on either real cloud platforms or custom test-beds. Challenges include – but not limited to – deployment impediments, the complexity of setting parameters, and most importantly, the cost of hosting and testing Web applications on a massive scale. Hence, simulation is presently one of the most popular evaluation solutions to overcome these obstacles. Existing simulators, however, fail to provide support for hosting, deploying and subsequently auto-scaling of Web applications. In this paper, we introduce AutoScaleSim, which extends the existing CloudSim simulator, to support auto-scaling of Web applications in cloud environments in a customizable, extendable and scalable manner. Using AutoScaleSim, the cloud community can freely implement/evaluate policies for all four phases of auto-scaling mechanisms, that is, Monitoring, Analysis, Planning and Execution. AutoScaleSim can also be used for evaluating load balancing algorithms similarly. We conducted a set of experiments to validate and carefully evaluate the performance of AutoScaleSim in a real cloud platform, with a wide range of performance metrics.
DA  - 2021/04/01/
PY  - 2021
DO  - 10.1016/j.simpat.2020.102245
DP  - ScienceDirect
VL  - 108
SP  - 102245
J2  - Simulation Modelling Practice and Theory
SN  - 1569-190X
ST  - AutoScaleSim
UR  - https://www.sciencedirect.com/science/article/pii/S1569190X20301738
Y2  - 2024/01/17/00:12:06
L2  - https://www.sciencedirect.com/science/article/abs/pii/S1569190X20301738
KW  - Cloud computing
KW  - Simulation
KW  - Auto-scaling
KW  - Elasticity
KW  - Resource provisioning
KW  - Web application
ER  - 

TY  - JOUR
TI  - Survey on serverless computing
AU  - Hassan, Hassan B.
AU  - Barakat, Saman A.
AU  - Sarhan, Qusay I.
T2  - Journal of Cloud Computing
AB  - Serverless computing has gained importance over the last decade as an exciting new field, owing to its large influence in reducing costs, decreasing latency, improving scalability, and eliminating server-side management, to name a few. However, to date there is a lack of in-depth survey that would help developers and researchers better understand the significance of serverless computing in different contexts. Thus, it is essential to present research evidence that has been published in this area. In this systematic survey, 275 research papers that examined serverless computing from well-known literature databases were extensively reviewed to extract useful data. Then, the obtained data were analyzed to answer several research questions regarding state-of-the-art contributions of serverless computing, its concepts, its platforms, its usage, etc. We moreover discuss the challenges that serverless computing faces nowadays and how future research could enable its implementation and usage.
DA  - 2021/07/12/
PY  - 2021
DO  - 10.1186/s13677-021-00253-7
DP  - BioMed Central
VL  - 10
IS  - 1
SP  - 39
J2  - Journal of Cloud Computing
SN  - 2192-113X
UR  - https://doi.org/10.1186/s13677-021-00253-7
Y2  - 2024/01/17/00:13:09
L1  - https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-021-00253-7
L2  - https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-021-00253-7
KW  - Cloud computing
KW  - Serverless computing
KW  - Serverless benefits
KW  - Serverless challenges
KW  - Serverless platforms
KW  - Survey
ER  - 

TY  - JOUR
TI  - Rise of the Planet of Serverless Computing: A Systematic Review
AU  - Wen, Jinfeng
AU  - Chen, Zhenpeng
AU  - Jin, Xin
AU  - Liu, Xuanzhe
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.
DA  - 2023/07/21/
PY  - 2023
DO  - 10.1145/3579643
DP  - ACM Digital Library
VL  - 32
IS  - 5
SP  - 131:1
EP  - 131:61
J2  - ACM Trans. Softw. Eng. Methodol.
SN  - 1049-331X
ST  - Rise of the Planet of Serverless Computing
UR  - https://doi.org/10.1145/3579643
Y2  - 2024/01/17/01:52:21
L1  - https://arxiv.org/pdf/2206.12275
KW  - Serverless computing
KW  - literature view
ER  - 

TY  - CONF
TI  - An empirical study on challenges of application development in serverless computing
AU  - Wen, Jinfeng
AU  - Chen, Zhenpeng
AU  - Liu, Yi
AU  - Lou, Yiling
AU  - Ma, Yun
AU  - Huang, Gang
AU  - Jin, Xin
AU  - Liu, Xuanzhe
T3  - ESEC/FSE 2021
AB  - Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, and cloud providers can leverage these findings and implications to better understand and further explore the serverless computing paradigm.
C1  - New York, NY, USA
C3  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
DA  - 2021/08/18/
PY  - 2021
DO  - 10.1145/3468264.3468558
DP  - ACM Digital Library
SP  - 416
EP  - 428
PB  - Association for Computing Machinery
SN  - 978-1-4503-8562-6
UR  - https://doi.org/10.1145/3468264.3468558
Y2  - 2024/01/16/
KW  - Serverless Computing
KW  - Application Development
KW  - Empirical Study
KW  - Stack Overflow
ER  - 

TY  - CONF
TI  - BARISTA: Efficient and Scalable Serverless Serving System for Deep Learning Prediction Services
AU  - Bhattacharjee, Anirban
AU  - Chhokra, Ajay Dev
AU  - Kang, Zhuangwei
AU  - Sun, Hongyang
AU  - Gokhale, Aniruddha
AU  - Karsai, Gabor
T2  - 2019 IEEE International Conference on Cloud Engineering (IC2E)
AB  - Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.
C3  - 2019 IEEE International Conference on Cloud Engineering (IC2E)
DA  - 2019/06//
PY  - 2019
DO  - 10.1109/IC2E.2019.00-10
DP  - IEEE Xplore
SP  - 23
EP  - 33
ST  - BARISTA
UR  - https://ieeexplore.ieee.org/document/8790088/keywords#keywords
Y2  - 2024/02/05/13:18:36
L1  - https://arxiv.org/pdf/1904.01576
L2  - https://ieeexplore.ieee.org/document/8790088/keywords#keywords
KW  - Computational modeling
KW  - Containers
KW  - Deep learning
KW  - Analytical models
KW  - Forecasting
KW  - Load modeling
KW  - Predictive models
KW  - Resource Management, Machine Learning Models, Predictive Analytics, Serverless Computing, Containers
ER  - 

