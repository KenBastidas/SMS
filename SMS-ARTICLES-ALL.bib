
@article{barrak_serverless_2022,
	title = {Serverless on {Machine} {Learning}: {A} {Systematic} {Mapping} {Study}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Serverless on {Machine} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9888122},
	doi = {10.1109/ACCESS.2022.3206366},
	abstract = {Machine Learning Operations (MLOps) is an approach to managing the entire lifecycle of a machine learning model. It has evolved over the last years and has started attracting many people in research and businesses in the industry. It supports the development of machine learning (ML) pipelines typical in the phases of data collection, data pre-processing, building datasets, model training, hyper-parameters refinement, testing, and deployment to production. This complex pipeline workflow is a tedious process of iterative experimentation. Moreover, cloud computing services provide advanced features for managing ML stages and deploying them efficiently to production. Specifically, serverless computing has been applied in different stages of the machine learning pipeline. However, to the best of our knowledge, it is missing to know the serverless suitability and benefits it can provide to the ML pipeline. In this paper, we provide a systematic mapping study of machine learning systems applied on serverless architecture that include 53 relevant studies. During this study, we focused on (1) exploring the evolution trend and the main venues; (2) determining the researchers’ focus and interest in using serverless on machine learning; (3) discussing solutions that serverless computing provides to machine learning. Our results show that serverless usage is growing, and several venues are interested in the topic. In addition, we found that the most widely used serverless provider is AWS Lambda, where the primary application was used in the deployment of the ML model. Additionally, several challenges were explored, such as reducing cost, resource scalability, and reducing latency. We also discuss the potential challenges of adopting ML on serverless, such as respecting service level agreement, the cold start problem, security, and privacy. Finally, our contribution provides foundations for future research and applications that involve machine learning in serverless computing.},
	journal = {IEEE Access},
	author = {Barrak, Amine and Petrillo, Fabio and Jaafar, Fehmi},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Cloud computing, Computational modeling, Serverless computing, Computer architecture, Data models, FaaS, function as a service, machine learning, Machine learning, Serverless, SLR, SM, systematic literature review, systematic mapping},
	pages = {99337--99352},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HNXAN76J\\9888122.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4GE9IC3W\\9888122.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IJMM2GJ4\\9888122.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\EMZNB2KX\\Barrak et al. - 2022 - Serverless on Machine Learning A Systematic Mappi.pdf:application/pdf},
}

@article{kjorveziroski_iot_2021,
	title = {{IoT} {Serverless} {Computing} at the {Edge}: {A} {Systematic} {Mapping} {Review}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-431X},
	shorttitle = {{IoT} {Serverless} {Computing} at the {Edge}},
	url = {https://www.mdpi.com/2073-431X/10/10/130},
	doi = {10.3390/computers10100130},
	abstract = {Serverless computing is a new concept allowing developers to focus on the core functionality of their code, while abstracting away the underlying infrastructure. Even though there are existing commercial serverless cloud providers and open-source solutions, dealing with the explosive growth of new Internet of Things (IoT) devices requires more efficient bandwidth utilization, reduced latency, and data preprocessing closer to the source, thus reducing the overall data volume and meeting privacy regulations. Moving serverless computing to the edge of the network is a topic that is actively being researched with the aim of solving these issues. This study presents a systematic mapping review of current progress made to this effect, analyzing work published between 1 January 2015 and 1 September 2021. Using a document selection methodology which emphasizes the quality of the papers obtained through querying several popular databases with relevant search terms, we have included 64 entries, which we then further categorized into eight main categories. Results show that there is an increasing interest in this area with rapid progress being made to solve the remaining open issues, which have also been summarized in this paper. Special attention is paid to open-source efforts, as well as open-access contributions.},
	language = {en},
	number = {10},
	urldate = {2023-06-15},
	journal = {Computers},
	author = {Kjorveziroski, Vojdan and Filiposka, Sonja and Trajkovik, Vladimir},
	month = oct,
	year = {2021},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things, serverless computing, function as a service, edge computing, systematic review},
	pages = {130},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\ZJVCZSP3\\Kjorveziroski et al. - 2021 - IoT Serverless Computing at the Edge A Systematic.pdf:application/pdf},
}

@inproceedings{d_damkevala_behavior_2019,
	title = {Behavior {Analysis} using {Serverless} {Machine} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8991407},
	abstract = {This paper supplies a route for using the Watson Machine Learning API on IBM Cloud to carry out serverless data analytics using machine learning as a service. Transforming the large amount of data produced by an organization into intelligence can be done using advanced analytics methods such as using a modified Mahalanobis Distance algorithm for synthesis of correlation data under the purview of machine learning. Further refinement of correlation data is done using a Multivariate Reliability Classifier model. The consumption of this advanced analytics service can be done in a serverless manner where the developer only must be concerned with how the data is analyzed, i.e., scoring, batch or stream models with a continuous learning system without the outlay of hardware upon which to train those models. This paper examines the usage of such serverless AI systems in the scope of user behavior analysis over varied demographics.},
	booktitle = {2019 6th {International} {Conference} on {Computing} for {Sustainable} {Global} {Development} ({INDIACom})},
	author = {{D. Damkevala} and {R. Lunavara} and {M. Kosamkar} and {S. Jayachandran}},
	month = mar,
	year = {2019},
	note = {Journal Abbreviation: 2019 6th International Conference on Computing for Sustainable Global Development (INDIACom)},
	pages = {1068--1072},
}

@inproceedings{m_gramaglia_case_2020,
	title = {The case for serverless mobile networking},
	abstract = {The softwarization of communication networks provides notable benefits, such as flexibility, improved resource efficiency, and commoditization. In exchange, softwarization requires an increased management overhead and the need to re-design network operation. While the mobile networking ecosystem is currently adapting this new paradigm with other network-related aspects (e.g., network slicing), cloud computing already addressed such problems with the introduction of serverless architectures, also known as Function as a Service (FaaS). With this approach, the software is decomposed into its minimum building blocks, i.e., functions, maximizing scalability, resource efficiency, and flexibility. In this paper, we analyze the potential adoption of the FaaS paradigm by the mobile networking ecosystem, discussing the implicit advantages, the challenges to address, and some solutions to overcome them.},
	booktitle = {2020 {IFIP} {Networking} {Conference} ({Networking})},
	author = {{M. Gramaglia} and {P. Serrano} and {A. Banchs} and {G. Garcia-Aviles} and {A. Garcia-Saavedra} and {R. Perez}},
	month = jun,
	year = {2020},
	note = {Journal Abbreviation: 2020 IFIP Networking Conference (Networking)},
	pages = {779--784},
}

@inproceedings{r_chatley_nimbus_2020,
	title = {Nimbus: {Improving} the {Developer} {Experience} for {Serverless} {Applications}},
	isbn = {2574-1926},
	abstract = {We present Nimbus, a framework for writing and deploying Java applications on a Function-as-a-Service ("serverless") platform. Nimbus aims to soothe four main pain points experienced by developers working on serverless applications: that testing can be difficult, that deployment can be a slow and painful process, that it is challenging to avoid vendor lock-in, and that long cold start times can introduce unwelcome latency to function invocations. Nimbus provides a number of features that aim to overcome these challenges when working with serverless applications. It uses an annotation-based configuration to avoid having to work with large configuration files. It aims to allow the code written to be cloud-agnostic. It provides an environment for local testing where the complete application can be run locally before deployment. Lastly, Nimbus provides mechanisms for optimising the contents and size of the artifacts that are deployed to the cloud, which helps to reduce both deployment times and cold start times.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {{R. Chatley} and {T. Allerton}},
	month = oct,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
	pages = {85--88},
}

@inproceedings{i_pelle_fast_2021,
	title = {Fast {Edge}-to-{Edge} {Serverless} {Migration} in {5G} {Programmable} {Packet}-{Optical} {Networks}},
	abstract = {Ultra-low latency serverless applications are dynamically deployed and migrated between edge computing nodes in less than 10 ms, leveraging comprehensive telemetry data retrieved from programmable packet-optical 5G x-haul.},
	booktitle = {2021 {Optical} {Fiber} {Communications} {Conference} and {Exhibition} ({OFC})},
	author = {{I. Pelle} and {F. Paolucci} and {B. Sonkoly} and {F. Cugini}},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 Optical Fiber Communications Conference and Exhibition (OFC)},
	pages = {1--3},
}

@inproceedings{y_chen_devops_2022,
	title = {{DevOps} {Practices} in {Digital} {Library} {Development}},
	abstract = {In this demonstration, we present how we apply DevOps practices to our digital library project development. We demonstrate a digital library platform, which is a cloud-native, serverless, and microservice digital libraries management system. All the services are open-source and publicly available on GitHub1. We also share our experiences and lessons learned about adopting the DevOps process.CCS CONCEPTS •Software and its engineering \${\textbackslash}rightarrow\$ Software design engineering; •Information systems \${\textbackslash}rightarrow\$ Digital libraries and archives.},
	booktitle = {2022 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	author = {{Y. Chen}},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	pages = {1--4},
}

@article{rodrigues_quickfaas_2022,
	title = {{QuickFaaS}: {Providing} {Portability} and {Interoperability} between {FaaS} {Platforms}},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144595365&doi=10.3390%2ffi14120360&partnerID=40&md5=d97ae3c4aa636d08cbb7c8b08b9b30e9},
	doi = {10.3390/fi14120360},
	abstract = {Serverless computing hides infrastructure management from developers and runs code on-demand automatically scaled and billed during the code’s execution time. One of the most popular serverless backend services is called Function-as-a-Service (FaaS), in which developers are often confronted with cloud-specific requirements. Function signature requirements, and the usage of custom libraries that are unique to cloud providers, were identified as the two main reasons for portability issues in FaaS applications, leading to various vendor lock-in problems. In this work, we define three cloud-agnostic models that compose FaaS platforms. Based on these models, we developed QuickFaaS, a multi-cloud interoperability desktop tool targeting cloud-agnostic functions and FaaS deployments. The proposed cloud-agnostic approach enables developers to reuse their serverless functions in different cloud providers with no need to change code or install extra software. We also provide an evaluation that validates the proposed solution by measuring the impact of a cloud-agnostic approach on the function’s performance, when compared to a cloud-non-agnostic one. The study shows that a cloud-agnostic approach does not significantly impact the function’s performance. © 2022 by the authors.},
	number = {12},
	journal = {Future Internet},
	author = {Rodrigues, P. and Freitas, F. and Simão, J.},
	year = {2022},
	keywords = {Cloud computing, cloud computing, serverless computing, Serverless computing, Function-as-a-Service, Function-as-a-service, cloud interoperability, Cloud interoperability, cloud orchestration, Cloud orchestration, cloud-agnostic, Cloud-agnostic, Cloud-computing, Codes (symbols), Computer software portability, Computer software reusability, FaaS portability, Function-as-a-service portability, Interoperability, Lock-in, Locks (fasteners), Service platforms, vendor lock-in, Vendor lock-in},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\V9JFFQNQ\\Rodrigues et al. - 2022 - QuickFaaS Providing Portability and Interoperabil.pdf:application/pdf},
}

@inproceedings{copik_sebs_2021,
	title = {{SeBS}: {A} serverless benchmark suite for function-as-a-service computing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117609008&doi=10.1145%2f3464298.3476133&partnerID=40&md5=2a8e7418f27a9e41e36fda10c0216425},
	doi = {10.1145/3464298.3476133},
	abstract = {Function-as-a-Service (FaaS) is one of the most promising directions for the future of cloud services, and serverless functions have immediately become a new middleware for building scalable and cost-efficient microservices and appli cations. However, the quickly moving technology hinders reproducibility, and the lack of a standardized benchmarking suite leads to ad-hoc solutions and microbenchmarks being used in serverless research, further complicating meta-analysis and comparison of research solutions. To address this challenge, we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.  © 2021 ACM.},
	author = {Copik, M. and Kwasniewski, G. and Besta, M. and Podstawski, M. and Hoefler, T.},
	year = {2021},
	keywords = {FaaS, Serverless, Function-as-a-service, serverless, benchmark, Benchmark, Benchmark suites, Benchmarking, Cloud services, Cost-efficient, Evaluation methodologies, Function evaluation, function-as-a-service, Middleware, Reproducibilities, Service computing},
	pages = {64--78},
}

@inproceedings{meladakis_transferring_2022,
	title = {Transferring transactional business processes to {FaaS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145555314&doi=10.1145%2f3565382.3565882&partnerID=40&md5=35bdfced9355072b7473caa78f982a17},
	doi = {10.1145/3565382.3565882},
	abstract = {Function-as-a-Service (FaaS) is a modern cloud service model that has gained significant attention from the research and industry communities in recent years for its many benefits such as dynamic scaling, cost efficiency, faster programming, flexibility to microservices and containers technology. However, the building and deployment of serverless applications come with many challenges that need to be tackled, like workflow design complexity and migration of other applications. When transactions between different parties are involved, the workflow becomes knotty and the communication between participants and all properties of transactions have to be properly resolved. Transactions have widely been discussed in Business processes, so same practices might be adopted by serverless workflows. In this work we provide guidelines and mapping mechanisms for transforming transactional Business Process Modeling Notation 2.0 (BPMN2) applications to a serverless platform. We shed light on the current inability of function orchestrators to express workflow definitions, and deal with various architectural dilemmas that stem from the dissimilar nature of stateful BPMN vs. stateless serverless applications. We overcome the unbalanced capabilities between well-established BPMN notations and function orchestration definitions and illustrate how to exploit and combine cloud native services that comes with FaaS to create serverless applications.  © 2022 ACM.},
	author = {Meladakis, K. and Zeginis, C. and Magoutis, K. and Plexousakis, D.},
	year = {2022},
	keywords = {FaaS, Function-as-a-service, transactions, BPMN2, Business Process, Business process modeling, Business process modeling notation 2.0, function orchestration, Function orchestration, Industrial research, Modeling notation, Openwhisk, OpenWhisk, serverless workflow, Serverless workflow, Transaction, Work-flows},
	pages = {25--30},
}

@inproceedings{cordingly_enhancing_2021,
	title = {Enhancing observability of serverless computing with the serverless application analytics framework},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104969464&doi=10.1145%2f3447545.3451173&partnerID=40&md5=e3b56b9331b79f7e3e75dd8fcfced694},
	doi = {10.1145/3447545.3451173},
	abstract = {To improve the observability of workload performance, resource utilization, and infrastructure underlying serverless Function-as-a-Service (FaaS) platforms, we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can leverage to inspect performance, resource utilization, scalability, and infrastructure metrics of function deployments to commercial and open-source FaaS platforms. To automate reproducible FaaS performance experiments, we provide the FaaS Runner as a multithreaded FaaS client. FaaS Runner provides a programmable client that can orchestrate over one thousand concurrent FaaS function calls. The ReportGenerator is then used to aggregate experiment output into CSV files for consumption by popular data analytics tools. SAAF and its supporting tools combined can assess forty-eight distinct metrics to enhance observability of serverless software deployments. In this tutorial paper, we describe SAAF and its supporting tools and provide examples of observability insights that can be derived. © 2021 Association for Computing Machinery.},
	author = {Cordingly, R. and Heydari, N. and Yu, H. and Hoang, V. and Sadeghi, Z. and Lloyd, W.},
	year = {2021},
	keywords = {Computer software reusability, Analytics tools, Data Analytics, Function calls, Infrastructure as a service (IaaS), Multithreaded, Observability, Open source software, Open sources, Performance experiment, Platform as a Service (PaaS), Resource utilizations, Software deployment, Supporting tool},
	pages = {161--164},
}

@article{dalla_palma_go_2023,
	title = {Go {Serverless} {With} {RADON}! {A} {Practical} {DevOps} {Experience} {Report}},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129594819&doi=10.1109%2fMS.2022.3170153&partnerID=40&md5=536c120a870c73f82695cdd30b73a058},
	doi = {10.1109/MS.2022.3170153},
	abstract = {We evaluate a novel DevOps methodology for serverless software delivery and evolution, called RADON. The framework harmonizes the abstraction and actuation of action-trigger rules avoiding function-as-a-service (FaaS) lock-in while optimizing decomposition and reuse through model-based FaaS-enabled development and orchestration.  © 1984-2012 IEEE.},
	number = {2},
	journal = {IEEE Software},
	author = {Dalla Palma, S. and Catolino, G. and Di Nucci, D. and Tamburri, D.A. and Van Den Heuvel, W.-J.},
	year = {2023},
	keywords = {Serverless Computing, Software, Serverless computing, DevOps, Industrial research, Code, Data structures, European union, Experience report, FAA, Horizon 2020, Industrial case study, Large-scales, Radon, Situational method engineerin, Situational Method Engineerin, Software testing},
	pages = {80--89},
}

@inproceedings{cordingly_faaset_2022,
	title = {Faaset: {A} jupyter notebook to streamline every facet of serverless development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135977102&doi=10.1145%2f3491204.3527464&partnerID=40&md5=4737b62f155f92ef4724200ec997e982},
	doi = {10.1145/3491204.3527464},
	abstract = {Function-as-a-Service platforms require developers to use many different tools and services for function development, packaging, deployment, debugging, testing, orchestration of experiments, and analysis of results. Diverse toolchains are necessary due to the differences in how each platform is designed, the technologies they support, and the APIs they provide, leading to usability challenges for developers. To combine support for all of the tasks and tools into a unified workspace, we created the FaaS Experiment Toolkit (FaaSET). At the core of FaaSET is a Jupyter notebook development environment that enables developers to write functions, deploy them across multiple platforms, invoke and test them, automate experiments, and perform data analysis all in a single environment.  © 2022 Owner/Author.},
	author = {Cordingly, R. and Lloyd, W.},
	year = {2022},
	keywords = {Serverless, Function-as-a-service, serverless, Service platforms, function-as-a-service, development, Development, Development environment, Experiment and analysis, jupyter, Jupyter, Multiple platforms, profiling, Profiling, Program debugging, tools, Write functions},
	pages = {49--52},
}

@inproceedings{arjona_scaling_2023,
	title = {Scaling a {Variant} {Calling} {Genomics} {Pipeline} with {FaaS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180123450&doi=10.1145%2f3631295.3631403&partnerID=40&md5=37a03387f7943bd1c810e64390c12d14},
	doi = {10.1145/3631295.3631403},
	abstract = {With the escalating complexity and volume of genomic data, the capacity of biology institutions' HPC faces limitations. While the Cloud presents a viable solution for short-term elasticity, its intricacies pose challenges for bioinformatics users. Alternatively, serverless computing allows for workload scalability with minimal developer burden. However, porting a scientific application to serverless is not a straightforward process. In this article, we present a Variant Calling genomics pipeline migrated from single-node HPC to a serverless architecture. We describe the inherent challenges of this approach and the engineering efforts required to achieve scalability. We contribute by open-sourcing the pipeline for future systems research and as a scalable user-friendly tool for the bioinformatics community. © 2023 ACM.},
	author = {Arjona, A. and Gabriel-Atienza, A. and Lanuza-Orna, S. and Roca-Canals, X. and Bourramouss, A. and Chafin, T.K. and Marcello, L. and Ribeca, P. and García-López, P.},
	year = {2023},
	keywords = {FaaS, Serverless, serverless, Work-flows, Bioinformatics, Faas, Genes, Genome, Genomic data, genomics, Genomics, Pipelines, Scalability, Scalings, Scientific applications, Serverless architecture, Systems research, Viable solutions, workflow},
	pages = {59--64},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\WK6GYANL\\Arjona et al. - 2023 - Scaling a Variant Calling Genomics Pipeline with F.pdf:application/pdf},
}

@inproceedings{ginzburg_serverless_2020,
	title = {Serverless {Isn}'t {Server}-{Less}: {Measuring} and {Exploiting} {Resource} {Variability} on {Cloud} {FaaS} {Platforms}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607225&doi=10.1145%2f3429880.3430099&partnerID=40&md5=9f92f24b55a5d8c444b53772261f0413},
	doi = {10.1145/3429880.3430099},
	abstract = {Serverless computing in the cloud, or functions as a service (FaaS), poses new and unique systems design challenges. Serverless offers improved programmability for customers, yet at the cost of increased design complexity for cloud providers. One such challenge is effective and consistent resource management for serverless platforms, the implications of which we explore in this paper. In this paper, we conduct one of the first detailed in situ measurement studies of performance variability in AWS Lambda. We show that the observed variations in performance are not only significant, but stable enough to exploit. We then design and evaluate an end-to-end system that takes advantage of this resource variability to exploit the FaaS consumption-based pricing model, in which functions are charged based on their fine-grain execution time rather than actual low-level resource consumption. By using both light-weight resource probing and function execution times to identify attractive servers in serverless platforms, customers of FaaS services can cause their functions to execute on better performing servers and realize a cost savings of up to 13\% in the same AWS region.  © 2020 ACM.},
	author = {Ginzburg, S. and Freedman, M.J.},
	year = {2020},
	keywords = {Costs, Performance, Resource management, Cloud providers, Design challenges, Design complexity, In-situ measurement, Lambda's, Measurement study, Performance variability, Programmability},
	pages = {43--48},
}

@inproceedings{casale_radon_2020,
	title = {{RADON}: rational decomposition and orchestration for serverless computing},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071490657&doi=10.1007%2fs00450-019-00413-w&partnerID=40&md5=05759d936c13ea2b982fd942efb8c3ad},
	doi = {10.1007/s00450-019-00413-w},
	abstract = {Emerging serverless computing technologies, such as function as a service (FaaS), enable developers to virtualize the internal logic of an application, simplifying the management of cloud-native services and allowing cost savings through billing and scaling at the level of individual functions. Serverless computing is therefore rapidly shifting the attention of software vendors to the challenge of developing cloud applications deployable on FaaS platforms. In this vision paper, we present the research agenda of the RADON project (http://radon-h2020.eu), which aims to develop a model-driven DevOps framework for creating and managing applications based on serverless computing. RADON applications will consist of fine-grained and independent microservices that can efficiently and optimally exploit FaaS and container technologies. Our methodology strives to tackle complexity in designing such applications, including the solution of optimal decomposition, the reuse of serverless functions as well as the abstraction and actuation of event processing chains, while avoiding cloud vendor lock-in through models. © 2019, The Author(s).},
	author = {Casale, G. and Artač, M. and van den Heuvel, W.-J. and van Hoorn, A. and Jakovits, P. and Leymann, F. and Long, M. and Papanikolaou, V. and Presenza, D. and Russo, A. and Srirama, S.N. and Tamburri, D.A. and Wurster, M. and Zhu, L.},
	year = {2020},
	note = {Issue: 1-2},
	keywords = {Serverless computing, DevOps, Function as a service, Radon, Cloud applications, Application programs, Computing technology, Event Processing, Optimal decomposition, Software model, Software models, Software vendors},
	pages = {77--87},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\GRCKIT5Y\\Casale et al. - 2020 - RADON rational decomposition and orchestration fo.pdf:application/pdf},
}

@inproceedings{jindal_courier_2021,
	title = {Courier: {Delivering} serverless functions within heterogeneous {FaaS} deployments},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119572896&doi=10.1145%2f3468737.3494097&partnerID=40&md5=8ce66e3fa02016a17262733eb1a2d8e4},
	doi = {10.1145/3468737.3494097},
	abstract = {With the advent of serverless computing in different domains, there is a growing need for dynamic adaption to handle diverse and heterogeneous functions. However, serverless computing is currently limited to homogeneous Function-as-a-Service (FaaS) deployments or simply FaaS Deployment (FaaSD) consisting of deployments of serverless functions using a FaaS platform in a region with certain memory configurations. Extending serverless computing to support Heterogeneous FaaS Deployments (HeteroFaaSDs) consisting of multiple FaaSDs with variable configurations (FaaS platform, region, and memory) and dynamically load balancing the invocations of the functions across these FaaSDs within a HeteroFaaSD can provide an optimal way for handling such serverless functions. In this paper, we present a software system called Courier that is responsible for optimally distributing the invocations of the functions (called delivering of serverless functions) within the HeteroFaaSDs based on the execution time of the functions on the FaaSDs comprising the HeteroFaaSDs. To this end, we developed two approaches: Auto Weighted Round-Robin (AWRR) and PerFunction Auto Weighted Round-Robin (PFAWRR) that use functions execution times for delivering serverless functions within a HeteroFaaSD to reduce the overall execution time. We demonstrate and evaluate the functioning of our developed tool on three HeteroFaaSDs using three FaaS platforms: 1) on-premise Open-Whisk, 2) AWS Lambda, and 3) Google Cloud Functions (GCF). We show that Courier can improve the overall performance of the invocations of the functions within a HeteroFaaSD as compared to traditional load balancing algorithms.  © 2021 ACM.},
	author = {Jindal, A. and Frielinghaus, J. and Chadha, M. and Gerndt, M.},
	year = {2021},
	keywords = {serverless computing, Serverless computing, Function-as-a-service, function-as-a-service, Configuration function, Different domains, Dynamic adaptions, Function delivery, functions delivery, Homogeneous functions, Memory configuration, Service deployment, Weighted round robins},
}

@inproceedings{song_when_2023,
	title = {When {Serverless} {Computing} {Meets} {Different} {Degrees} of {Customization} for {DNN} {Inference}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180127927&doi=10.1145%2f3631295.3631400&partnerID=40&md5=c29e150e426f8aa48295096ec460d883},
	doi = {10.1145/3631295.3631400},
	abstract = {Serverless computing provides a method to develop application services without the burden of run-time execution environment management overhead. Since the initial offerings of serverless computing using function-as-a-service (FaaS), other variants of execution environments have been proposed, such as a special-purpose FaaS (SPF) for deep neural network (DNN) inference and a serverless container service (SCS) for general web applications. This paper qualitatively summarizes the characteristics of a general-purpose FaaS (GPF), SPF, and SCS from the perspective of customizability when setting up execution environments. To judge whether various serverless computing environments can be feasible solutions for an interactive DNN model inference application, we conduct extensive experiments and conclude that there are rooms for performance improvement serverless DNN inference, and allowing a custom environment setup can make the serverless computing platform for an interactive DNN application. © 2023 ACM.},
	author = {Song, M. and Hur, Y. and Lee, K.},
	year = {2023},
	keywords = {serverless computing, Serverless computing, Application services, Customisation, Deep neural networks, dnn inference, Dnn inference, Environment management, Execution environments, Network inference, Run-time execution, WEB application, Web applications},
	pages = {42--47},
}

@inproceedings{bouizem_active-standby_2020,
	title = {Active-{Standby} for {High}-{Availability} in {FaaS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099596478&doi=10.1145%2f3429880.3430097&partnerID=40&md5=660f0261eb6c41f814b91156c87b6507},
	doi = {10.1145/3429880.3430097},
	abstract = {Serverless computing is becoming more and more attractive for cloud solution architects and developers. This new computing paradigm relies on Function-as-a-Service (FaaS) platforms that enable deploying functions without being concerned with the underlying infrastructure. An important challenge in designing FaaS platforms is ensuring the availability of deployed functions. Existing FaaS platforms address this challenge principally through retrying function executions. In this paper, we propose and implement an alternative fault-tolerance approach based on active-standby failover. Results from an experimental evaluation show that our approach increases availability and performance compared to the retry-based approach.  © 2020 ACM.},
	author = {Bouizem, Y. and Parlavantzas, N. and DIb, D. and Morin, C.},
	year = {2020},
	keywords = {FaaS, Performance, Function-as-a-service, availability, Availability, Computing paradigm, Experimental evaluation, Failover, fault tolerance, Fault tolerance, High availability, Tolerance approach},
	pages = {31--36},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\T7XU3EKR\\Bouizem et al. - 2020 - Active-Standby for High-Availability in FaaS.pdf:application/pdf},
}

@inproceedings{bezverbnyi_serverless_2023,
	title = {Serverless computing for data processing in open learning and research environments},
	volume = {3482},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173565001&partnerID=40&md5=adb2c81777f0353992372fe8493338c6},
	abstract = {Serverless computing is a paradigm that enables the execution of code without provisioning or managing servers. It offers benefits such as scalability, cost-efficiency, and ease of development for cloud-based applications. In this paper, we explore the potential of serverless computing for supporting data processing in open learning and research environments. We propose a concept of a hybrid serverless cloud, which combines different types of cloud services to provide access to various tools and resources for learners and researchers. We present a case study of wave files processing using a lambda function, which demonstrates the feasibility and effectiveness of our approach. We also discuss the challenges and opportunities of integrating serverless components within open systems of learning and research. Finally, we present a vision of a cloud-based open learning and research university environment that leverages serverless technologies to enhance the quality and accessibility of education and research. © 2023 Copyright for this paper by its authors.},
	author = {Bezverbnyi, I.A. and Shyshkina, M.P.},
	year = {2023},
	keywords = {cloud computing, serverless computing, Serverless computing, Cloud-computing, Cloud services, Open systems, Case-studies, Cloud-based applications, Cost-efficiency, Data handling, data processing, Learning environments, Learning systems, Open Data, open learning, Open learning, open research, Open research, Research environment},
	pages = {229--236},
}

@inproceedings{jindal_estimating_2021,
	title = {Estimating the capacities of function-as-a-service functions},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119575511&doi=10.1145%2f3492323.3495628&partnerID=40&md5=b042e769c76abd995be64aa93a116ee1},
	doi = {10.1145/3492323.3495628},
	abstract = {Serverless computing is a cloud computing paradigm that allows developers to focus exclusively on business logic as cloud service providers manage resource management tasks. Serverless applications follow this model, where the application is decomposed into a set of fine-grained Function-as-a-Service (FaaS) functions. However, the obscurities of the underlying system infrastructure and dependencies between FaaS functions within the application pose a challenge for estimating the performance of FaaS functions. To characterize the performance of a FaaS function that is relevant for the user, we define Function Capacity (FC) as the maximal number of concurrent invocations the function can serve in a time without violating the Service-Level Objective (SLO). The paper addresses the challenge of quantifying the FC individually for each FaaS function within a serverless application. This challenge is addressed by sandboxing a FaaS function and building its performance model. To this end, we develop FnCapacitor - an end-to-end automated Function Capacity estimation tool. We demonstrate the functioning of our tool on Google Cloud Functions (GCF) and AWS Lambda. FnCapacitor estimates the FCs on different deployment configurations (allocated memory \& maximum function instances) by conducting time-framed load tests and building various models using statistical: linear, ridge, and polynomial regression, and Deep Neural Network (DNN) methods on the acquired performance data. Our evaluation of different FaaS functions shows relatively accurate predictions with an accuracy greater than 75\% using DNN for both cloud providers.  © 2021 ACM.},
	author = {Jindal, A. and Chadha, M. and Benedict, S. and Gerndt, M.},
	year = {2021},
	keywords = {serverless computing, Serverless computing, Performance, Function-as-a-service, Resource management, Cloud-computing, function-as-a-service, Service functions, Deep neural networks, Computing paradigm, Business logic, Cloud service providers, Computation theory, function capacity, Function capacity, Load testing},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\5W6BAIZP\\Jindal et al. - 2021 - Estimating the capacities of function-as-a-service.pdf:application/pdf},
}

@inproceedings{cordingly_serverless_2020,
	title = {The {Serverless} {Application} {Analytics} {Framework}: {Enabling} {Design} {Trade}-off {Evaluation} for {Serverless} {Software}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099550918&doi=10.1145%2f3429880.3430103&partnerID=40&md5=ecd7e97af17c8305211d74f6b6643b41},
	doi = {10.1145/3429880.3430103},
	abstract = {To help better understand factors that impact performance on Function-as-a-Service (FaaS) platforms we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can integrate into a function's package for deployment to multiple commercial and open source FaaS platforms. SAAF improves the observability of FaaS function deployments by collecting forty-eight distinct metrics to enable developers to profile CPU and memory utilization, monitor infrastructure state, and observe platform scalability. In this paper, we describe SAAF in detail and introduce supporting tools highlighting important features and how to use them. Our client application, FaaS Runner, provides a tool to orchestrate workloads and automate the process of conducting experiments across FaaS platforms. We provide a case study demonstrating the integration of SAAF into an existing open source image processing pipeline built for AWS Lambda. Using FaaS Runner, we automate experiments and acquire metrics from SAAF to profile each function of the pipeline to evaluate performance implications. Finally, we summarize contributions using our tools to evaluate implications of different programming languages for serverless data processing, and to build performance models to predict runtime for serverless workloads.  © 2020 ACM.},
	author = {Cordingly, R. and Yu, H. and Hoang, V. and Sadeghi, Z. and Foster, D. and Perez, D. and Hatchett, R. and Lloyd, W.},
	year = {2020},
	keywords = {Serverless Computing, Serverless computing, Function-as-a-Service, Function-as-a-service, Computer software reusability, Function evaluation, Open source software, Pipelines, Application programs, Economic and social effects, Open-source, Data handling, Commercial sources, Design tradeoff, Framework, Frameworks, Image processing, Impact performance, Performance Evaluation, Performances evaluation, Pipeline processing systems, Programming language, Programming Languages, Source functions},
	pages = {67--72},
}

@inproceedings{serrano-gutierrez_continuous_2020,
	title = {Continuous integration of faas driven by quality attributes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099546446&partnerID=40&md5=f37982252f37dc4e48332b476fab60a7},
	abstract = {The expansion of the cloud has led to the development of serverless applications in which there is no need to be concerned about infrastructure, that can be considered as a service provider in the form of functions, which is known as FaaS (Function as a Service). The development of these applications consists of implementing independent functions that will communicate with each other. This work aims to explore how to combine FaaS and the continuous integration philosophy of DevOps so that applications operators can continuously re-deploy different implementations of functions in order to meet some quality of service requirements, such as latency or energy consumption. © Ibero-American WWW / Internet Conference 2020.},
	author = {Serrano-Gutierrez, P.},
	year = {2020},
	keywords = {FaaS, Serverless, Function as a Service, DevOps, Quality of service, Infrastructure as a service (IaaS), Quality of Service, Service provider, Continuous Integration, Continuous integrations, Energy utilization, Independent functions, Quality attributes},
	pages = {179--182},
}

@inproceedings{chadha_towards_2020,
	title = {Towards {Federated} {Learning} using {FaaS} {Fabric}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099604179&doi=10.1145%2f3429880.3430100&partnerID=40&md5=7835ee3ce9f26a34046d77f90d9ad459},
	doi = {10.1145/3429880.3430100},
	abstract = {Federated learning (FL) enables resource-constrained edge devices to learn a shared Machine Learning (ML) or Deep Neural Network (DNN) model, while keeping the training data local and providing privacy, security, and economic benefits. However, building a shared model for heterogeneous devices such as resource-constrained edge and cloud makes the efficient management of FL-clients challenging. Furthermore, with the rapid growth of FL-clients, the scaling of FL training process is also difficult. In this paper, we propose a possible solution to these challenges: federated learning over a combination of connected Function-as-a-Service platforms, i.e., FaaS fabric offering a seamless way of extending FL to heterogeneous devices. Towards this, we present FedKeeper, a tool for efficiently managing FL over FaaS fabric. We demonstrate the functionality of FedKeeper by using three FaaS platforms through an image classification task with a varying number of devices/clients, different stochastic optimizers, and local computations (local epochs).  © 2020 ACM.},
	author = {Chadha, M. and Jindal, A. and Gerndt, M.},
	year = {2020},
	keywords = {FaaS, Serverless, Function-as-a-service, Faas, Deep neural networks, Faas platform, FaaS platforms, Federated learning, Heterogeneous devices, Learn+, Machine-learning, Neural network model, Neural networks, Neural-networks, Optimization, Stochastic systems},
	pages = {49--54},
}

@article{shafiei_serverless_2022,
	title = {Serverless {Computing}: {A} {Survey} of {Opportunities}, {Challenges}, and {Applications}},
	volume = {54},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150456234&doi=10.1145%2f3510611&partnerID=40&md5=b92919c2dbfae611897b7baea5b56960},
	doi = {10.1145/3510611},
	abstract = {The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this article, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.  © 2022 Association for Computing Machinery.},
	number = {11s},
	journal = {ACM Computing Surveys},
	author = {Shafiei, H. and Khonsari, A. and Mousavi, P.},
	year = {2022},
	keywords = {serverless computing, Serverless computing, Function-as-a-service, Cloud services, Scalings, Pricing models, Cloud applications, Web services, Computing paradigm, Pay as you go, Application developers, function-as-a-service (FaaS), Operational complexity},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\MZ8CA2PT\\Shafiei et al. - 2022 - Serverless Computing A Survey of Opportunities, C.pdf:application/pdf},
}

@inproceedings{sreekanti_fault-tolerance_2020,
	title = {A fault-{Tolerance} shim for serverless computing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087109549&doi=10.1145%2f3342195.3387535&partnerID=40&md5=be7f879740c5a8fd536c529c6753f207},
	doi = {10.1145/3342195.3387535},
	abstract = {Serverless computing has grown in popularity in recent years, with an increasing number of applications being built on Functions-As-A-Service (FaaS) platforms. By default, FaaS platforms support retry-based fault tolerance, but this is insufficient for programs that modify shared state, as they can unwittingly persist partial sets of updates in case of failures. To address this challenge, we would like atomic visibility of the updates made by a FaaS application. In this paper, we present aft, an atomic fault tolerance shim for serverless applications. aft interposes between a commodity FaaS platform and storage engine and ensures atomic visibility of updates by enforcing the read atomic isolation guarantee. aft supports new protocols to guarantee read atomic isolation in the serverless setting. We demonstrate that aft introduces minimal overhead relative to existing storage engines and scales smoothly to thousands of requests per second, while preventing a significant number of consistency anomalies. © 2020 ACM.},
	author = {Sreekanti, V. and Wu, C. and Chhatrapati, S. and Gonzalez, J.E. and Hellerstein, J.M. and Faleiro, J.M.},
	year = {2020},
	keywords = {Fault tolerance, Atoms, Engines, Fault tolerant computer systems, New protocol, Shared state, Shims, Storage engines, Virtual storage, Visibility},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\UF8PRJJ9\\Sreekanti et al. - 2020 - A fault-Tolerance shim for serverless computing.pdf:application/pdf},
}

@article{suhendinata_serverless_2022,
	title = {{SERVERLESS} {MICROSERVICES} {ARCHITECTURE} {FOR} {INDOOR} {POSITIONING} {SYSTEM} {USING} {BLUETOOTH} {LOW} {ENERGY}},
	volume = {16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138779069&doi=10.24507%2ficicel.16.09.1001&partnerID=40&md5=a9c9c3344221ae3b62ca61da937a01d1},
	doi = {10.24507/icicel.16.09.1001},
	abstract = {The development of Indoor Positioning System (IPS) research currently focuses on improving accuracy of indoor positioning systems. The use of complex techniques to improve location accuracy has an impact on the use of resources on smartphones. So offloading techniques are used to overcome the limitations of resources on smartphones by moving computing on smartphones to cloud computing. Serverless is one of the cloud computing technologies that have scalable capabilities by performing auto provision on cloud computing resources. In this paper, we propose a design architecture of serverless microservice for indoor positioning system using Bluetooth low energy. This architecture uses serverless technology with a micro services architecture design that divides services into small parts. The test results of 360 concurrent users per second with multitenant showed availability of serverless microservices is 97.83\% with an average response time of 4.63 seconds, which is better compared to availability of serverless monolith of 89.68\% with an average response time of 4.82 seconds.  © 2022 ISSN.},
	number = {9},
	journal = {ICIC Express Letters},
	author = {Suhendinata, H. and Kusuma, G.P.},
	year = {2022},
	keywords = {Cloud computing, Bluetooth low energy, Computational offloading, Indoor positioning system, Serverless microservices},
	pages = {1001--1009},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\NVLBWNR2\\Suhendinata and Kusuma - 2022 - SERVERLESS MICROSERVICES ARCHITECTURE FOR INDOOR P.pdf:application/pdf},
}

@inproceedings{sethi_lcs_2023,
	title = {{LCS} : {Alleviating} {Total} {Cold} {Start} {Latency} in {Serverless} {Applications} with {LRU} {Warm} {Container} {Approach}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145875136&doi=10.1145%2f3571306.3571404&partnerID=40&md5=fc90549680a820546d1d13df60f53e1c},
	doi = {10.1145/3571306.3571404},
	abstract = {Serverless computing offers "Function-as-a-Service"(FaaS), which promotes an application in the form of independent granular components called functions. FaaS goes well as a widespread standard that facilitates the development of applications in cloud-based environments. Clients can solely focus on developing applications in a serverless ecosystem, passing the overburden of resource governance to the service providers. However, FaaS platforms have to bear the degradation in performance originating from the cold starts of executables i.e. serverless functions. The cold start reflects the delay in provisioning a runtime container that processes the functions. Each serverless platform is handling the problem of cold start with its own solution. In recent times, approaches to deal with cold starts have received the attention of many researchers. This paper comes up with an extensive solution to handle the cold start problem. We propose a scheduling approach to reduce the cold start occurrences by keeping the containers alive for a longer period of time using the Least Recently Used warm Container Selection (LCS ) approach on Affinity-based scheduling. Further, we carried out an evaluation and compared the obtained results with the MRU container selection approach. The proposed LCS approach outperforms by approximately 48\% compared to the MRU approach.  © 2023 ACM.},
	author = {Sethi, B. and Addya, S.K. and Ghosh, S.K.},
	year = {2023},
	keywords = {Serverless Computing, Containers, Serverless computing, FaaS, Performance, Service platforms, Cold Start, Cold-start, Service provider, Cloud-based, Executables, Function-as-a-service", Granular components, Latency},
	pages = {197--206},
}

@inproceedings{hamza_software_2023,
	title = {Software {Architecture} {Design} of a {Serverless} {System}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162214035&doi=10.1145%2f3593434.3593471&partnerID=40&md5=438b90314b99274251754cf0acf234ff},
	doi = {10.1145/3593434.3593471},
	abstract = {Context: Serverless computing allows developers to create and deploy applications without the need to manage any underlying infrastructure, making it a more efficient and effective way to bring products to market. Serverless technology is gaining widespread adoption among many companies, becoming increasingly popular. However, the adoption of serverless technology brings with it several new challenges. Objective: To this end, we plan to gain a deep understanding of challenges and strategies, architectural issues and their causes, architectural patterns, antipatterns, migration towards serverless architecture, and state-of-the-art practices for vendor lock-in problems. Methodology: The research objective will be met through the use of an industrial empirical approach, including interviews, a case study, and a questionnaire survey. Possible outcomes: The expected outcomes would be (i) a multivocal literature review on design areas of serverless architecture (ii) an evidence-based framework for synthesizing serverless architectural challenges/solutions (iii) a decision-making process for migrating to serverless architecture (iv) a decision-making framework for selecting vendor platform.  © 2023 Owner/Author.},
	author = {Hamza, M.},
	year = {2023},
	keywords = {Serverless Architecture, Lock-in, Industrial research, Serverless architecture, Anti-patterns, Architectural pattern, Art practice, Decision making, Decision Model, Decision modeling, Empirical investigation, Empirical Investigation, Serverless systems, Software architecture, Software architecture design, State of the art},
	pages = {304--306},
}

@inproceedings{mirabelli_bringing_2020,
	title = {Bringing scaling transparency to {Proteomics} applications with serverless computing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099557165&doi=10.1145%2f3429880.3430101&partnerID=40&md5=ef871c164cf4821636d078eee2a52a23},
	doi = {10.1145/3429880.3430101},
	abstract = {Scaling transparency means that applications can expand in scale without changes to the system structure or the application algorithms. Serverless Computing's inherent auto-scaling support and fast function launching is ideally suited to support scaling transparency in different domains. In particular, Proteomic applications could considerably benefit from scaling transparency and serverless technologies due to their high concurrency requirements. Therefore, the auto-provisioning nature of serverless platforms makes this computing model an alternative to satisfy dynamically the resources required by protein folding simulation processes. However, the transition to these architectures must face challenges: they should show comparable performance and cost to code running in Virtual Machines (VMs). In this article, we demonstrate that Proteomics applications implemented with the Replica Exchange algorithm can be moved to serverless settings guaranteeing scaling transparency. We also validate that we can reduce the total execution time by around forty percent with comparable cost to cluster technologies (Work Queue) over VMs.  © 2020 ACM.},
	author = {Mirabelli, M.E. and García-López, P. and Vernik, G.},
	year = {2020},
	keywords = {Serverless, Scalings, Different domains, Simulation platform, Computing model, High concurrencies, Molecular biology, Protein folding simulation, Proteomics, Replica exchange, Replica Exchange, Scaling transparency, Scaling Transparency, Systems Structure, Transparency},
	pages = {55--60},
}

@inproceedings{quinn_implications_2021,
	title = {Implications of {Alternative} {Serverless} {Application} {Control} {Flow} {Methods}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121471495&doi=10.1145%2f3493651.3493668&partnerID=40&md5=1d1d8964c6084c9783f3dbce0ddf2bab},
	doi = {10.1145/3493651.3493668},
	abstract = {Function-as-a-Service or FaaS is a popular delivery model of serverless computing where developers upload code to be executed in the cloud as short running stateless functions. Using smaller functions to decompose processing of larger tasks or workflows introduces the question of how to instrument application control flow to orchestrate an overall task or workflow. In this paper, we examine implications of using different methods to orchestrate the control flow of a serverless data processing pipeline composed as a set of independent FaaS functions. We performed experiments on the AWS Lambda FaaS platform and compared how four different patterns of control flow impact the cost and performance of the pipeline. We investigate control flow using client orchestration, microservice controllers, event-based triggers, and state-machines. Overall, we found that asynchronous methods led to lower orchestration costs, and that event-based orchestration incurred a performance penalty.  © 2021 ACM.},
	author = {Quinn, S. and Cordingly, R. and Lloyd, W.},
	year = {2021},
	keywords = {Serverless Computing, Serverless computing, Function-as-a-Service, Function-as-a-service, Work-flows, Pipelines, Data handling, Framework, Frameworks, Performance Evaluation, Performances evaluation, Pipeline processing systems, Programming language, Programming Languages, Application-control, Control-flow, Delivery models, Event-based},
	pages = {17--22},
}

@article{bouizem_integrating_2023,
	title = {Integrating request replication into {FaaS} platforms: an experimental evaluation},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162899726&doi=10.1186%2fs13677-023-00457-z&partnerID=40&md5=6a41457201d5194316802712f2dfa1dc},
	doi = {10.1186/s13677-023-00457-z},
	abstract = {Function-as-a-Service (FaaS) is a popular programming model for building serverless applications, supported by all major cloud providers and many open-source software frameworks. One of the main challenges for FaaS providers is providing fault tolerance for the deployed applications, that is, providing the ability to mask failures of function invocations from clients. The basic fault tolerance approach in current FaaS platforms is automatically retrying function invocations. Although the retry approach is well suited for transient failures, it incurs delays in recovering from other types of failures, such as node crashes. This paper proposes the integration of a Request Replication mechanism in FaaS platforms and describes how this integration was implemented in Fission, a well-known, open-source platform. It provides a detailed experimental comparison of the proposed approach with the retry approach and an Active-Standby approach in terms of performance, availability, and resource consumption under different failure scenarios. © 2023, The Author(s).},
	number = {1},
	journal = {Journal of Cloud Computing},
	author = {Bouizem, Y. and Dib, D. and Parlavantzas, N. and Morin, C.},
	year = {2023},
	keywords = {FaaS, Serverless, Cloud, Function-as-a-service, Service platforms, Open source software, Cloud providers, Application programs, Open systems, Service provider, Experimental evaluation, Fault tolerance, High availability, Programming models, Open-source softwares, Software frameworks},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\UUJ4DVLD\\Bouizem et al. - 2023 - Integrating request replication into FaaS platform.pdf:application/pdf},
}

@inproceedings{raith_faas-sim_2023,
	title = {faas-sim: {A} trace-driven simulation framework for serverless edge computing platforms},
	volume = {53},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173703464&doi=10.1002%2fspe.3277&partnerID=40&md5=d3553627369fc425c4823f3bb3ea62c0},
	doi = {10.1002/spe.3277},
	abstract = {This paper presents faas-sim, a simulation framework tailored to serverless edge computing platforms. In serverless computing, platform operators are tasked with efficiently managing distributed computing infrastructure completely abstracted from application developers. To that end, platform operators and researchers need tools to design, build, and evaluate resource management techniques that efficiently use of infrastructure while optimizing application performance. This challenge is exacerbated in edge computing scenarios, where, compared to cloud computing, there is a lack of reference architectures, design tools, or standardized benchmarks. faas-sim bridges this gap by providing (a) a generalized model of serverless systems that builds on the function-as-a-service abstraction, (b) a simulator that uses trace data from real-world edge computing testbeds and representative workloads, and (c) a network topology generator to model and simulate distributed and heterogeneous edge-cloud systems. We present the conceptual design, implementation, and a thorough evaluation of faas-sim. By running experiments on both real-world test beds and replicating them using faas-sim, we show that the simulator provides accurate results and reasonable simulation performance. We have profiled a wide range of edge computing infrastructure and workloads, focusing on typical edge computing scenarios such as edge AI inference or data processing. Moreover, we present several instances where we have successfully used faas-sim to either design, optimize, or evaluate serverless edge computing systems. © 2023 The Authors. Software: Practice and Experience published by John Wiley \& Sons Ltd.},
	author = {Raith, P. and Rausch, T. and Furutanpey, A. and Dustdar, S.},
	year = {2023},
	note = {Issue: 12},
	keywords = {Edge computing, Edge clouds, Edge-cloud continuum, Simulation platform, Data handling, Computing platform, Abstracting, co-simulation, Computing infrastructures, Conceptual design, Cosimulation, edge-cloud continuum, serverless edge computing, Serverless edge computing, simulation, Simulation, Simulation framework, Topology, Trace driven simulation},
	pages = {2327--2361},
}

@inproceedings{chaipunyathat_conceptual_2019,
	title = {A {Conceptual} {Model} of {Requirement} {Engineering} in {Cloud} {Project} {Delivery} for {Thai} {Government} {Organizations}},
	url = {https://ieeexplore.ieee.org/document/8999923},
	doi = {10.1109/RI2C48728.2019.8999923},
	abstract = {The shift to cloud computing has affected the future of software engineering in several ways i.e., multilateral software development, scalability, and new technology stack such as an open-source software, plus infrastructure as code such as container, serverless architecture and software defined network (SDN). In order to support the cloud project delivery for government organizations, requirements engineering (RE) is a crucial step in software engineering that determines whether a project will be successful or result in a failure. RE steps include requirement elicitation, requirement analysis, requirement specification and requirement validation. Data is collected by semi-structured interview from the providers and the users of cloud services in 11 Thai government organizations, and from cloud service provider for meeting enterprise requirements and user requirements. The results reveal nine key issues that affect cloud project delivery: (1) lack of trust with external cloud service provider by generation X and baby boomer (2) lack of transparency as regards the legal agreement about the cloud user's personal data protection responsibility by cloud service provider, (3) the issues of reliability, security and service level agreements (4) lack of knowledge and lack of understanding of cloud technology (5) required training and a learning by doing (6) the policy to use government cloud services instead of developing their own cloud, (7) older people at top-level tend to resist cloud technology, (8) people in general lack of knowledge and understanding of cloud technology (9) problems about pricing model, it's impact on 23 cloud requirements (functional and non-functional) as well as factors that involved in RE phases. Based on these results this paper presents a Conceptual Model of Requirement Engineering for Cloud Project Delivery in Thai Government Organizations.},
	urldate = {2023-12-29},
	booktitle = {2019 {Research}, {Invention}, and {Innovation} {Congress} ({RI2C})},
	author = {Chaipunyathat, Ajchareeya and Porrawatpreyakorn, Nalinpat and Nuchitprasitchai, Siranee and Viriyapant, Kanchana},
	month = dec,
	year = {2019},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R846DDPC\\8999923.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9UBAL7CL\\8999923.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S24YFYLX\\8999923.html:text/html},
}

@article{sanso_platform_2019,
	title = {A {Platform} for {Lightweight} {Deployment} of {IoTApplications} {Based} on a {Function}-as-a-{ServiceModel}},
	volume = {17},
	issn = {1548-0992},
	url = {https://ieeexplore.ieee.org/document/8931204},
	doi = {10.1109/TLA.2019.8931204},
	abstract = {The use of Cloud computing for the development of Internet of Things (IoT) applications has emerged during the last years. But there is a lack of a platform which facilitates the deployment and the interoperability of this type of applications. This paper presents a platform to facilitate the deployment of Cloud-based applications to devices in IoT domains. The platform allows the programmers to use a Function-as-a-Service programming paradigm which is managed and configured in a Platform-as-a-Service web tool. The tool also allows establishing interoperability between the functions of the applications. The platform is validated by developing a Cloud application that orchestrates two IoT devices, one with a movement sensor and another one with a camera. Finally, a performance study is also performed. The proposed platform obtains faster and easier deployments of the applications. The resource usages of the IoT devices are also lower with regard to a deployment process based on Docker containers.},
	number = {07},
	urldate = {2023-12-29},
	journal = {IEEE Latin America Transactions},
	author = {Sansó, Sebastià and Guerrero, Carlos and Lera, Isaac and Juiz, Carlos},
	month = jul,
	year = {2019},
	note = {Conference Name: IEEE Latin America Transactions},
	pages = {1155--1162},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UJVZVBE3\\8931204.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SIX2G45V\\8931204.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JFEUK7CW\\8931204.html:text/html},
}

@inproceedings{flores_ai_2019,
	title = {{AI} on the {Move}: {From} {On}-{Device} to {On}-{Multi}-{Device}},
	shorttitle = {{AI} on the {Move}},
	url = {https://ieeexplore.ieee.org/document/8730873},
	doi = {10.1109/PERCOMW.2019.8730873},
	abstract = {On-Device AI is an emerging paradigm that aims to make devices more intelligent, autonomous and proactive by equipping them with machine and deep learning routines for robust decision making and optimal execution in devices' operations. On-Device intelligence promises the possibility of computing huge amounts of data close to its source, e.g., sensor and multimedia data. By doing so, devices can complement their counterpart cloud services with more sophisticated functionality to provide better applications and services. However, increased computational capabilities of smart devices, wearables and IoT devices along with the emergence of services at the Edge of the network are driving the trend of migrating and distributing computation between devices. Indeed, devices can reduce the burden of executing resource intensive tasks via collaborations in the wild. While several work has shown the benefits of an opportunistic collaboration of a device with others, not much is known regarding how devices can be organized as a group as they move together. In this paper, we contribute by analyzing how dynamic group organization of devices can be utilized to distribute intelligence on the moving Edge. The key insight is that instead of On-Device solutions complementing with cloud, dynamic groups can be formed to complement each other in an On-Multi-Device manner. Thus, we highlight the challenges and opportunities from extending the scope of On-Device AI from an egocentric view to a collaborative, multi-device view.},
	urldate = {2023-12-29},
	booktitle = {2019 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} ({PerCom} {Workshops})},
	author = {Flores, Huber and Nurmi, Petteri and Hui, Pan},
	month = mar,
	year = {2019},
	pages = {310--315},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SREK5KRX\\8730873.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FISICGAA\\8730873.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YGGRAR4T\\8730873.html:text/html},
}

@article{alvarez_edge--cloud_2019,
	title = {An {Edge}-to-{Cloud} {Virtualized} {Multimedia} {Service} {Platform} for {5G} {Networks}},
	volume = {65},
	issn = {1557-9611},
	url = {https://ieeexplore.ieee.org/document/8667014},
	doi = {10.1109/TBC.2019.2901400},
	abstract = {The focus of research into 5G networks to date has been largely on the required advances in network architectures, technologies, and infrastructures. Less effort has been put on the applications and services that will make use of and exploit the flexibility of 5G networks built upon the concept of software-defined networking (SDN) and network function virtualization (NFV). Media-based applications are amongst the most demanding services, requiring large bandwidths for high audio-visual quality, low-latency for interactivity, and sufficient infrastructure resources to deliver the computational power for running the media applications in the networked cloud. This paper presents a novel service virtualization platform (SVP), called 5G-MEDIA SVP, which leverages the principles of NFV and SDN to facilitate the development, deployment, and operation of media services on 5G networks. The platform offers an advanced cognitive management environment for the provisioning of network services (NSs) and media-related applications, which directly link their lifecycle management with user experience as well as optimization of infrastructure resource utilization. Another innovation of 5G-MEDIA SVP is the integration of serverless computing with media intensive applications in 5G networks, increasing cost effectiveness of operation and simplifying development and deployment time. The proposed SVP is being validated against three media use cases: 1) immersive virtual reality 3-D gaming application; 2) remote production of broadcast content incorporating user generated contents; and 3) dynamically adaptive content distribution networks for the intelligent distribution of ultrahigh definition content. The preliminary results of the 5G-MEDIA SVP platform evaluation are compared against current practice and show that the proposed platform provides enhanced functionality for the operators and infrastructure owners, while ensuring better NS performance to service providers and end users.},
	number = {2},
	urldate = {2023-12-29},
	journal = {IEEE Transactions on Broadcasting},
	author = {Alvarez, Federico and Breitgand, David and Griffin, David and Andriani, Pasquale and Rizou, Stamatia and Zioulis, Nikolaos and Moscatelli, Francesca and Serrano, Javier and Keltsch, Madeleine and Trakadas, Panagiotis and Phan, T. Khoa and Weit, Avi and Acar, Ugur and Prieto, Oscar and Iadanza, Francesco and Carrozzo, Gino and Koumaras, Harilaos and Zarpalas, Dimitrios and Jimenez, David},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Broadcasting},
	pages = {369--380},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S2U6CV3K\\8667014.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4L2GRS2S\\8667014.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YLWHPY9B\\8667014.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\I2HVQ8FV\\Alvarez et al. - 2019 - An Edge-to-Cloud Virtualized Multimedia Service Pl.pdf:application/pdf},
}

@inproceedings{boubin_autonomic_2019,
	title = {Autonomic {Computing} {Challenges} in {Fully} {Autonomous} {Precision} {Agriculture}},
	isbn = {2474-0756},
	url = {https://ieeexplore.ieee.org/document/8831204},
	doi = {10.1109/ICAC.2019.00012},
	abstract = {Precision agriculture examines crop fields, gathers data, analyzes crop health and informs field management. This data driven approach can reduce fertilizer runoff, prevent crop disease and increase yield. Frequent data collection improves outcomes, but also increases operating costs. Fully autonomous aerial systems (FAAS) can capture detailed images of crop fields without human intervention. They can reduce operating costs significantly. However, FAAS software must embed agricultural expertise to decide where to fly, which images to capture and when to land. This paper explores fully autonomous precision agriculture where FAAS map crop fields frequently. We have designed hardware and software architecture. We use unmanned aerial systems, edge computing components and software driven by reinforcement learning and ensemble models. In early results, we have collected data from an Ohio cornfield. We use this data to simulate a FAAS modeling crop yield. Our results (1) show that our approach predicts yield well and (2) can quantify computational demand. Computational costs can be prohibitive. We discuss how research on adaptive systems can reduce costs and enable fully autonomous precision agriculture. We also provide our simulation tools and dataset as part of our open source FAAS middleware, SoftwarePilot.},
	urldate = {2023-12-29},
	booktitle = {2019 {IEEE} {International} {Conference} on {Autonomic} {Computing} ({ICAC})},
	author = {Boubin, Jayson and Chumley, John and Stewart, Christopher and Khanal, Sami},
	month = jun,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Conference on Autonomic Computing (ICAC)},
	pages = {11--17},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TNGFBZK7\\8831204.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K6GPDCT7\\8831204.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KDUNHNSL\\8831204.html:text/html},
}

@inproceedings{gadepalli_challenges_2019,
	title = {Challenges and {Opportunities} for {Efficient} {Serverless} {Computing} at the {Edge}},
	isbn = {2575-8462},
	url = {https://ieeexplore.ieee.org/document/9049531},
	doi = {10.1109/SRDS47363.2019.00036},
	abstract = {Serverless computing frameworks allow users to execute a small application (dedicated to a specific task) without handling operational issues such as server provisioning, resource management, and resource scaling for the increased load. Serverless computing originally emerged as a Cloud computing framework, but might be a perfect match for IoT data processing at the Edge. However, the existing serverless solutions, based on VMs and containers, are too heavy-weight (large memory footprint and high function invocation time) for operating efficiency and elastic scaling at the Edge. Moreover, many novel IoT applications require low-latency data processing and near real-time responses, which makes the current cloud-based serverless solutions unsuitable. Recently, WebAssembly (Wasm) has been proposed as an alternative method for running serverless applications at near-native speeds, while having a small memory footprint and optimized invocation time. In this paper, we discuss some existing serverless solutions, their design details, and unresolved performance challenges for an efficient serverless management at the Edge. We outline our serverless framework, called aWsm, based on the WebAssembly approach, and discuss the opportunities enabled by the aWsm design, including function profiling and SLO-driven performance management of users' functions. Finally, we present an initial assessment of aWsm performance featuring average startup time (12μs to 30μs) and an economical memory footprint (ranging from 10s to 100s of kB) for a subset of MiBench microbenchmarks used as functions.},
	urldate = {2023-12-30},
	booktitle = {2019 38th {Symposium} on {Reliable} {Distributed} {Systems} ({SRDS})},
	author = {Gadepalli, Phani Kishore and Peach, Gregor and Cherkasova, Ludmila and Aitken, Rob and Parmer, Gabriel},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 38th Symposium on Reliable Distributed Systems (SRDS)},
	pages = {261--2615},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CTJKKGX2\\9049531.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XPT7I7NE\\9049531.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CNQF8MZU\\9049531.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K6QAG57T\\9049531.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L7AK6AN2\\9049531.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\U895JDWA\\9049531.html:text/html},
}

@inproceedings{aminov_cloud_2019,
	title = {Cloud {Based} {Algorithm} for {Task} {Management}},
	url = {https://ieeexplore.ieee.org/document/8919599},
	doi = {10.1109/CSE/EUC.2019.00055},
	abstract = {Time management is challenging because not only does one want to ensure they are able to get work done on time, they also want to utilize time in the most efficient way. This paper proposes a solution developed using Cloud Computing and a scheduling algorithm based on Pomodoro technique.},
	urldate = {2023-12-30},
	booktitle = {2019 {IEEE} {International} {Conference} on {Computational} {Science} and {Engineering} ({CSE}) and {IEEE} {International} {Conference} on {Embedded} and {Ubiquitous} {Computing} ({EUC})},
	author = {Aminov, Parvizsho and Bola, Navjot and Shiralkar, Dipti and Yoganarasimha, Meghana},
	month = aug,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)},
	pages = {249--253},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X3F5VLKQ\\8919599.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6FAT435Y\\8919599.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FDBJLQUE\\8919599.html:text/html},
}

@inproceedings{singh_comparison_2019,
	title = {Comparison of {Different} {CI}/{CD} {Tools} {Integrated} with {Cloud} {Platform}},
	url = {https://ieeexplore.ieee.org/document/8776985},
	doi = {10.1109/CONFLUENCE.2019.8776985},
	abstract = {A microservice architecture is a routine of enhancing cloud applications which enable large-scale enterprises to scale their application as per the demand. These microservices can be deployed using virtual machines, Docker containers or as a serverless function such as AWS lambda. However, with an increase in the number of microservices, it becomes strenuous to manage and deploy these services. Therefore, to overcome the problem, continuous integration and continuous delivery tool are used to deploy the microservices with minimum possible downtime. In this paper, we will be comparing different continuous integration and delivery tools taking into consideration different parameters like performance monitoring post-deployment, pipeline integration, cloud compatibility, and server monitoring.},
	urldate = {2023-12-30},
	booktitle = {2019 9th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} \& {Engineering} ({Confluence})},
	author = {Singh, Charanjot and Gaba, Nikita Seth and Kaur, Manjot and Kaur, Bhavleen},
	month = jan,
	year = {2019},
	note = {Journal Abbreviation: 2019 9th International Conference on Cloud Computing, Data Science \& Engineering (Confluence)},
	pages = {7--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HCVPF7YC\\8776985.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HFNJRL9G\\8776985.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\M6XNZLYP\\8776985.html:text/html},
}

@inproceedings{bucur_data_2019,
	title = {Data {Loss} {Prevention} and {Data} {Protection} in {Cloud} {Environments} {Based} on {Authentication} {Tokens}},
	isbn = {2379-0482},
	url = {https://ieeexplore.ieee.org/document/8744776},
	doi = {10.1109/CSCS.2019.00128},
	abstract = {Data leek issues represents a very high or critical importance within the cloud developers. Given the tremendous growth of cloud computing in the past 3 years the authors of this paper believe that an analysis of the implementation of modern security solutions for data leaks and data protection is of the utmost importance. Authentication tokens, provided by several different cloud providers and third parties, are one of the most common and useful tools that developers have in order to secure their applications. The goal of this paper is to analyze how these tokens are currently used and to provide a solution in order to improve their implementation by reviewing their compatibility with novel security concepts or challenges such as data tagging, hybrid data security algorithms, microservices deployment using Docker or serverless applications.},
	urldate = {2023-12-30},
	booktitle = {2019 22nd {International} {Conference} on {Control} {Systems} and {Computer} {Science} ({CSCS})},
	author = {Bucur, Vlad and Stan, Ovidiu and Miclea, Liviu C.},
	month = may,
	year = {2019},
	note = {Journal Abbreviation: 2019 22nd International Conference on Control Systems and Computer Science (CSCS)},
	pages = {720--725},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Z2ASVBC9\\8744776.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YPUEYUBU\\8744776.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MLQZGY4S\\8744776.html:text/html},
}

@inproceedings{tovarnitchi_designing_2019,
	title = {Designing {Distributed}, {Scalable} and {Extensible} {System} {Using} {Reactive} {Architectures}},
	isbn = {2379-0482},
	url = {https://ieeexplore.ieee.org/document/8745176},
	doi = {10.1109/CSCS.2019.00088},
	abstract = {Digital technologies are reshaping our world and are becoming an integral part of our being. It became obvious that in modern days digitalization in each industry is an irreversible process and the speed of adopting of such kind of technologies in an appropriate manner will determine the evolution and survival of any business. Digital technologies play a crucial role in any domain of our life: from simple everyday activities, to such important domains like security, safety and health. We are the witnesses of so-called "Industry 4.0 Revolution" which is dominated by Artificial Intelligence and Cloud Architectures with related technologies. When we think about big change or (r) evolution, we are looking mostly for advantages only, but don't care about challenges and what is happening under the hood. The truth is that "reality is distributed" and the things are getting more complex with each passing day. The information systems are dealing with more and more (huge) amount of data, nevertheless, a quick response (in milliseconds) is expected and no downtime is accepted. In this context, the challenges consist first of all in designing and implementing a reliable (doing well what it was designed for), performant (quick reaction time), low-overhead (non-blocking and optimal use of resources) and flexible (adding new features with minimal effect upon production environment) systems. To implement a system with such kind of features a specific programming models need to be used. In this paper is proposed an architectural approach aimed to ensure scalability and extensibility of a distributed information systems using not fresh, but still very actual concept like Actor Model and REST. Modern Event-Driven, Asynchronous and non-blocking programming models are used.},
	urldate = {2023-12-30},
	booktitle = {2019 22nd {International} {Conference} on {Control} {Systems} and {Computer} {Science} ({CSCS})},
	author = {Tovarnitchi, Vasile M.},
	month = may,
	year = {2019},
	note = {Journal Abbreviation: 2019 22nd International Conference on Control Systems and Computer Science (CSCS)},
	pages = {484--488},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A239Y53T\\8745176.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WG8AIT53\\8745176.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NZ74RPIK\\8745176.html:text/html},
}

@inproceedings{cholissodin_enhancement_2019,
	title = {Enhancement {Full} {Open} {Source} {Hadoop} {Distribution} {Universal} {Big} {Data} {Up} {Projects} ({UBig}) {From} {Education} {To} {Enterprise}},
	url = {https://ieeexplore.ieee.org/document/8986040},
	doi = {10.1109/SIET48054.2019.8986040},
	abstract = {The development of hadoop distribution has been carried out by developers from the big data field starting from Cloudera (CDH), Horton Work (HDP), and MapR, which are more widely used for enterprise version. In addition, when viewed from the field of both formal and non-formal educations, many students and laymen who learn about big data find it difficult to carry out the learning process, especially if the learning process is done locally and completely without a server (serverless), for example like using IBM cognitiveclass, AWS educate, and indeed, this local learning is very cheap without always having to connect to the internet. This is in line with the specifications of netbooks, notebooks and personal computers (PCs) that are used by most students with limited specs, for example it will run slowly when a single-node hadoop is implemented, especially a multi-note hadoop. And also in the hadoop itself requires quite a lot of supporting software that runs on hadoop, such as spark, hive, pig, flume, oozie, sqoop, hue and others. In this study, hadoop distribution with the name `UBig' or `HiDUPs' which was made using Linux Ubuntu and Windows OS so easily to be made independently by each student both the student and the general public as a great development solution to many various fields.},
	urldate = {2023-12-31},
	booktitle = {2019 {International} {Conference} on {Sustainable} {Information} {Engineering} and {Technology} ({SIET})},
	author = {Cholissodin, Imam and Supianto, Ahmad Afif},
	month = sep,
	year = {2019},
	note = {Journal Abbreviation: 2019 International Conference on Sustainable Information Engineering and Technology (SIET)},
	pages = {90--93},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JJXW8DBG\\8986040.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\96ZT4IEI\\8986040.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3PQK6Y7X\\8986040.html:text/html},
}

@inproceedings{quevedo_evaluating_2019,
	title = {Evaluating {Apache} {OpenWhisk} - {FaaS}},
	url = {https://ieeexplore.ieee.org/document/9014867},
	doi = {10.1109/ETCM48019.2019.9014867},
	abstract = {Function-as-a-Service (FaaS) platforms enable users to execute user-defined functions without worrying about operational issues such as the management of infrastructure resources. In order to improve performance, different FaaS platforms are implementing optimizations and improvements, but it's not clear how good these implementations are. In this work, Apache OpenWhisk platform is evaluated from an approach that allows to determinate and characterize the performance under different configuration options; it was found that under certain premises an improvement of the performance in cold-booting latencies up to 38\% is obtain.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {Fourth} {Ecuador} {Technical} {Chapters} {Meeting} ({ETCM})},
	author = {Quevedo, Sebastián and Merchán, Freddy and Rivadeneira, Rafael and Dominguez, Federico X.},
	month = nov,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE Fourth Ecuador Technical Chapters Meeting (ETCM)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VHS7JF3J\\9014867.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VWBWI8D9\\9014867.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3W8S62HS\\9014867.html:text/html},
}

@inproceedings{dumkasem_eyemath_2019,
	title = {{EyeMath}: {Increasing} {Accessibility} of {Mathematics} to {Visually} {Impaired} {Readers}},
	shorttitle = {{EyeMath}},
	url = {https://ieeexplore.ieee.org/document/8974682},
	doi = {10.1109/ICSEC47112.2019.8974682},
	abstract = {Mathematics education for visually impaired students is challenging because their learning materials are generally limited to braille books, and audiobooks. In order to increase the chance of learning mathematical content for people with visual impairment, this paper presents the design and development of a cloud-based mobile application called EyeMath, using serverless microservices in Amazon AWS. Users can provide images of page snippets for the application to process and read their content to the users. EyeMath segments an input image into smaller pieces and separates pieces that have only plain text from pieces with mathematical symbols. The mathematical-related pieces are further processed into an Abstract Syntax Tree (AST) and then parsed into Thai sentences. For plain text pieces, EyeMath relies on Tesseract OCR to convert them into text. Finally, results for all pieces are combined together systematically for the device's screen reader program to read aloud. The performance evaluation of the application shows high correctness in reading math content within test images and our usability testing confirms the potential usefulness of the application to visually impaired readers.},
	urldate = {2023-12-31},
	booktitle = {2019 23rd {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	author = {Dumkasem, Kanlayanee and Srisingchai, Padchaya and Rattanatamrong, Prapaporn},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 23rd International Computer Science and Engineering Conference (ICSEC)},
	pages = {197--202},
}

@inproceedings{zheng_globalflow_2019,
	title = {{GlobalFlow}: {A} {Cross}-{Region} {Orchestration} {Service} for {Serverless} {Computing} {Services}},
	isbn = {2159-6190},
	shorttitle = {{GlobalFlow}},
	url = {https://ieeexplore.ieee.org/document/8814505},
	doi = {10.1109/CLOUD.2019.00093},
	abstract = {With the development of serverless computing, orchestration of multiple serverless computing services is highly desired by many cloud-based applications. In this paper, we present GlobalFlow, an orchestration service that can coordinate various geographically distributed but logically dependent serverless computing services through copy-based or connector-based strategy. Through preliminary evaluation, the proposed service has demonstrated its effectiveness in orchestrating various AWS Lambda functions in different regions without significant overhead.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} 12th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Zheng, Ge and Peng, Yang},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)},
	pages = {508--510},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GSUKHC8E\\8814505.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GL5FE6LR\\8814505.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IRL9QHI4\\8814505.html:text/html},
}

@inproceedings{kadhe_gradient_2019,
	title = {Gradient {Coding} {Based} on {Block} {Designs} for {Mitigating} {Adversarial} {Stragglers}},
	isbn = {2157-8117},
	url = {https://ieeexplore.ieee.org/document/8849690},
	doi = {10.1109/ISIT.2019.8849690},
	abstract = {Distributed implementations of gradient-based methods, wherein a server distributes gradient computations across worker machines, suffer from slow running machines, called stragglers. Gradient coding is a coding-theoretic framework to mitigate stragglers by enabling the server to recover the gradient sum in the presence of stragglers. Approximate gradient codes are variants of gradient codes that reduce computation and storage overhead per worker by allowing the server to approximately reconstruct the gradient sum.In this work, our goal is to construct approximate gradient codes that are resilient to stragglers selected by a computationally unbounded adversary. Our motivation for constructing codes to mitigate adversarial stragglers stems from the challenge of tackling stragglers in massive-scale elastic and serverless systems, wherein it is difficult to statistically model stragglers. Towards this end, we propose a class of approximate gradient codes based on balanced incomplete block designs (BIBDs). We show that the approximation error for these codes depends only on the number of stragglers, and thus, adversarial straggler selection has no advantage over random selection. In addition, the proposed codes admit computationally efficient decoding at the server. Next, to characterize fundamental limits of adversarial straggling, we consider the notion of adversarial threshold - the smallest number of workers that an adversary must straggle to inflict certain approximation error. We compute a lower bound on the adversarial threshold, and show that codes based on symmetric BIBDs maximize this lower bound among a wide class of codes, making them excellent candidates for mitigating adversarial stragglers.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Kadhe, Swanand and Koyluoglu, O. Ozan and Ramchandran, Kannan},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Symposium on Information Theory (ISIT)},
	pages = {2813--2817},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\K2UREN7A\\Kadhe et al. - 2019 - Gradient Coding Based on Block Designs for Mitigat.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8HRCUDA3\\8849690.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\W9RPN76U\\8849690.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R6BXF9BW\\8849690.html:text/html},
}

@inproceedings{niu_gtaa_2019,
	title = {{GTAA}: {A} {Geo}-{Aware} {Task} {Allocation} {Approach} in {Cloud} {Workflow}},
	shorttitle = {{GTAA}},
	url = {https://ieeexplore.ieee.org/document/8818391},
	doi = {10.1109/ICWS.2019.00082},
	abstract = {The cloud computing simplifies application development into the orchestration of virtual-services workflow. However, network latency between geographically distributed hosts would slow down the workflow's makespan time. This paper proposes a geo-aware task allocation approach (GTAA). GTAA partitions the workflow for geo-distributed data centers(DCs) and reduces sub-workflows across DCs. GTAA aims to optimize overall workflow makespan time and improves the efficiency of workflow.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Niu, Meng and Cheng, Bo and Chen, Junling},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Conference on Web Services (ICWS)},
	pages = {449--451},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A6XEFJN7\\8818391.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TVYNGHAI\\8818391.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X5A4ADDA\\8818391.html:text/html},
}

@inproceedings{kang_hsv_2019,
	title = {{HSV} {Color} {Space} {Based} {Robot} {Grasping} for {Personalized} {Manufacturing} {Services}},
	isbn = {2162-1233},
	url = {https://ieeexplore.ieee.org/document/8939796},
	doi = {10.1109/ICTC46691.2019.8939796},
	abstract = {Recently, with the development of artificial intelligence technology, robots have been utilized not only in manufacturing but also in many industrial fields. The needs of customers have been diversified and the trend of future manufacturing has been changing from the conventional mass production to the multiproduct small volume production system. In a smart factory environment for custom manufacturing, the robot must be manually programmed for each work instruction every time, especially when a production product is changed. Robot programming takes a lot of time and effort of people. In this paper, we propose a method to automatically measure the product information(color, size) automatically which are 3D printed out based on HSV (hue, saturation, value) Color Space and to control the auto grasping of robots. Also, we applied this control system to the FaaS(Factory As a Service) testbed for functional verification.},
	urldate = {2023-12-31},
	booktitle = {2019 {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {Kang, Hyunchul and Han, Hyonyoung and Bae, Heechul and Lee, Eunseo and Kim, Mingi and Son, Jiyeon and Kim, Hyun and Kim, Young-Kuk},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 International Conference on Information and Communication Technology Convergence (ICTC)},
	pages = {1010--1012},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7R4V3HJI\\8939796.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EWWFDAAW\\8939796.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9XXGYYCI\\8939796.html:text/html},
}

@inproceedings{tankov_kotless_2019,
	title = {Kotless: {A} {Serverless} {Framework} for {Kotlin}},
	isbn = {2643-1572},
	shorttitle = {Kotless},
	url = {https://ieeexplore.ieee.org/document/8952482},
	doi = {10.1109/ASE.2019.00114},
	abstract = {Recent trends in Web development demonstrate an increased interest in serverless applications, i.e. applications that utilize computational resources provided by cloud services on demand instead of requiring traditional server management. This approach enables better resource management while being scalable, reliable, and cost-effective. However, it comes with a number of organizational and technical difficulties which stem from the interaction between the application and the cloud infrastructure, for example, having to set up a recurring task of reuploading updated files. In this paper, we present Kotless - a Kotlin Serverless Framework. Kotless is a cloud-agnostic toolkit that solves these problems by interweaving the deployed application into the cloud infrastructure and automatically generating the necessary deployment code. This relieves developers from having to spend their time integrating and managing their applications instead of developing them. Kotless has proven its capabilities and has been used to develop several serverless applications already in production. Its source code is available at https://github.com/JetBrains/kotless, a tool demo can be found at https://www.youtube.com/watch?v=IMSakPNl3TY.},
	urldate = {2023-12-31},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Tankov, Vladislav and Golubev, Yaroslav and Bryksin, Timofey},
	month = nov,
	year = {2019},
	note = {Journal Abbreviation: 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	pages = {1110--1113},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4C4G47B5\\8952482.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Q6JIQWH5\\8952482.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WDZ3BM6U\\8952482.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\Y4AXW672\\Tankov et al. - 2019 - Kotless A Serverless Framework for Kotlin.pdf:application/pdf},
}

@inproceedings{rana_less_2019,
	title = {Less {Energy} {Consumption} {Framework} for {Fog} {Computing} {With} {IoT}},
	url = {https://ieeexplore.ieee.org/document/8976772},
	doi = {10.1109/PEEIC47157.2019.8976772},
	abstract = {IOT applications nowadays have quickly expanded and the basic standard centralized models of cloud computing have faced numerous challenging situations which has excessiveness in latency; have low capacity and the failure of network, less capacity of storing and excessive use of power. Fog computing has brought the cloud nearer to the devices of IOT, which deals with the challenges. The services being provided by the fog have quicker response moreover more better quality, in comparison to the cloud. The choices which are best, allow the IOT to offer services which are efficient and secured for most of the users of IOT, that is being measured by the fog computing. In this paper, we are focusing on the fog computing furthermore the incorporation of fog computing with the IOT by specially focusing on the implementation of the challenges is being presented. We have proposed architecture for the less consumption of energy and power.},
	urldate = {2023-12-31},
	booktitle = {2019 2nd {International} {Conference} on {Power} {Energy}, {Environment} and {Intelligent} {Control} ({PEEIC})},
	author = {Rana, Prateek and Sharma, Monika},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 2nd International Conference on Power Energy, Environment and Intelligent Control (PEEIC)},
	pages = {41--46},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XYDU487B\\8976772.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\M6CVH6W2\\8976772.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MXM2MKGA\\8976772.html:text/html},
}

@inproceedings{serckumecka_low-cost_2019,
	title = {Low-{Cost} {Serverless} {SIEM} in the {Cloud}},
	isbn = {2575-8462},
	url = {https://ieeexplore.ieee.org/document/9049574},
	doi = {10.1109/SRDS47363.2019.00057},
	abstract = {Security systems such as the Security Information and Event Management (SIEMs) have been used to monitor logs and correlate data to quickly detect and respond to incidents. Despite their advantages, SIEMs are expensive to deploy and maintain, requiring extra budget and specialized staff. Another concern is the event retention period, which events are stored for a short period of time, missing important information about how threats may have affected the company infrastructure in the past. This thesis aims to improve these issues by using low-cost cloud services to correlate and store security events. We will investigate techniques to index, compress and store events in the cloud in a cost-efficient and safe way for a long time. We will create a cloud correlation engine using a serverless platform, such as Amazon Lambda. This approach can minimize the complexity of managing SIEMs in place, charging the customer only for the time actually spent processing events. Finally, we will integrate the storage and correlation engine into a cloud SIEM, providing also a monitoring tool, building a complete and innovative low-cost cloud-based security monitoring solution.},
	urldate = {2023-12-31},
	booktitle = {2019 38th {Symposium} on {Reliable} {Distributed} {Systems} ({SRDS})},
	author = {Serckumecka, Adriano and Medeiros, Ibéria and Bessani, Alysson},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 38th Symposium on Reliable Distributed Systems (SRDS)},
	pages = {381--3811},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\48MPKNG7\\9049574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9A8TPZIY\\9049574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IR6C8DM6\\9049574.html:text/html},
}

@inproceedings{winzinger_model-based_2019,
	title = {Model-{Based} {Analysis} of {Serverless} {Applications}},
	isbn = {2575-4475},
	url = {https://ieeexplore.ieee.org/document/8877078},
	doi = {10.1109/MiSE.2019.00020},
	abstract = {Serverless computing is a relatively new execution model where the cloud platform provider manages the allocation of resources for containerized functions dynamically. This evolving paradigm is called Function as a Service (FaaS). The statelessness of these functions enables the application to be scaled up elastically in the case of peak loads. They can be tested easily in isolation, but the behavior arising by integrating them to an application is both hard to predict and test. The parallel execution of the functions and the shift of its state to data storages can cause several workflows accessing the same data. These workflows are hard to detect, particularly for complex applications. Therefore, we suggest an approach for modelling an existing serverless application based on a specialized graph holding all relevant features. Our serverless-specific model can be applied during the whole life cycle of a complex application and offers a good basis for this specific class of applications. It helps to optimize an existing system by identifying hot spots, supports the generation of test cases and can be used to monitor an existing system. Furthermore, we show how the generation of the model can be automated by realizing a tool supporting Amazon's AWS Lambda.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE}/{ACM} 11th {International} {Workshop} on {Modelling} in {Software} {Engineering} ({MiSE})},
	author = {Winzinger, Stefan and Wirtz, Guido},
	month = may,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering (MiSE)},
	pages = {82--88},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CZTM9ADS\\8877078.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\28J949ST\\8877078.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TDIFLXWP\\8877078.html:text/html},
}

@inproceedings{schmitt_online_2019,
	title = {Online {Power} {Consumption} {Estimation} for {Functions} in {Cloud} {Applications}},
	isbn = {2474-0756},
	url = {https://ieeexplore.ieee.org/document/8831214},
	doi = {10.1109/ICAC.2019.00018},
	abstract = {The growth of cloud services leads to more and more data centers that are increasingly larger and consume considerable amounts of power. To increase energy efficiency, informed decisions on workload placement and provisioning are essential. Micro-services and the upcoming serverless platforms with more granular deployment options exacerbate this problem. For this reason, knowing the power consumption of the deployed application becomes crucial, providing the necessary information for autonomous decision making. However, the actual power draw of a server running a specific application under load is not available without specialized measurement equipment or power consumption models. Yet, granularity is often only down to machine level and not application level. In this paper, we propose a monitoring and modeling approach to estimate power consumption on an application function level. The model uses performance counters that are allocated to specific functions to assess their impact on the total power consumption. Hence our model applies to a large variety of servers and for micro-service and serverless workloads. Our model uses an additional correction to minimize falsely allocated performance counters and increase accuracy. We validate the proposed approach on real hardware with a dedicated benchmarking application. The evaluation shows that our approach can be used to monitor application power consumption down to the function level with high accuracy for reliable workload provisioning and placement decisions.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {International} {Conference} on {Autonomic} {Computing} ({ICAC})},
	author = {Schmitt, Norbert and Iffländer, Lukas and Bauer, André and Kounev, Samuel},
	month = jun,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Conference on Autonomic Computing (ICAC)},
	pages = {63--72},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GBZDRMGX\\8831214.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XKXKWCPV\\8831214.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XZMGN47Z\\8831214.html:text/html},
}

@inproceedings{czentye_optimizing_2019,
	title = {Optimizing {Latency} {Sensitive} {Applications} for {Amazon}'s {Public} {Cloud} {Platform}},
	isbn = {2576-6813},
	url = {https://ieeexplore.ieee.org/document/9013988},
	doi = {10.1109/GLOBECOM38437.2019.9013988},
	abstract = {Recent cloud technologies enable a diverse set of novel applications with capabilities never seen before. Cloud native programming, microservices, serverless architectures are novel paradigms reducing the burden on both software developers and operators while enabling cloud-grade service deployments. Several types of applications fit in well with the new concepts, however, latency sensitive applications with strict delay constraints pose additional challenges on the platforms. Can we run these applications on today's public cloud platforms making use of the brand new tools and techniques? In this paper, we try to answer this question by addressing one of the most widely used and versatile public cloud platforms, namely Amazon's AWS, and we propose a novel mechanism to optimize the software "layout" based on dynamic performance measurements. Our contribution is threefold. First, we define a combined performance and cost model on CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS, based on a comprehensive performance analysis, and we also provide an application model capturing the performance requirements. Second, we formulate an optimization problem which minimizes the deployment costs on AWS while meeting the latency constraints. A polynomial algorithm finding the optimal solution is also given. Third, we evaluate the model and the algorithm for different scenarios and investigate the performance on today's system.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {Global} {Communications} {Conference} ({GLOBECOM})},
	author = {Czentye, Janos and Pelle, Istvan and Kern, Andras and Gero, Balazs Peter and Toka, Laszlo and Sonkoly, Balazs},
	month = dec,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE Global Communications Conference (GLOBECOM)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BY2HTEJ2\\9013988.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LV2HSVSF\\9013988.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8R795N4D\\9013988.html:text/html},
}

@inproceedings{ling_pigeon_2019,
	title = {Pigeon: {A} {Dynamic} and {Efficient} {Serverless} and {FaaS} {Framework} for {Private} {Cloud}},
	shorttitle = {Pigeon},
	url = {https://ieeexplore.ieee.org/document/9071414},
	doi = {10.1109/CSCI49370.2019.00265},
	abstract = {Recently, voice-triggered small cloud functions such as Alexa skills, and cloud mini programs for IoT and smartphone, grow exponentially. These new developments also attract organizations to host their own cloud functions or mini programs in private cloud environment and move from traditional Microservice architecture to Serverless Function-as-a-Service (FaaS) architecture. However, current Serverless FaaS frameworks cannot meet cold start latency, resource efficiency required by short-lived cloud functions and mini programs. In this paper, we build a new Framework - Pigeon that brings Serverless and FaaS programming paradigm into private cloud to enable enterprises to host these applications. Pigeon creates function-oriented Serverless framework by introducing an independent and finer-grained function-level resource scheduler on top of Kubernetes. A new oversubscription-based static pre-warmed container solution is also proposed to effectively reduce function startup latency and increase resource recycling speed for short-lived cloud functions. Empirical results show that Pigeon framework enhances function cold trigger rate by 26\% to 80\% comparing to AWS Lambda Serverless platform. Comparing to Kubernetes native scheduler based serverless platforms, throughput gets 3 times improvement while handling short-lived functions.},
	urldate = {2023-12-31},
	booktitle = {2019 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Ling, Wei and Ma, Lin and Tian, Chen and Hu, Ziang},
	month = dec,
	year = {2019},
	note = {Journal Abbreviation: 2019 International Conference on Computational Science and Computational Intelligence (CSCI)},
	pages = {1416--1421},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\V4HPPYRC\\9071414.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2P6M7EQM\\9071414.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DSPJIFTI\\9071414.html:text/html},
}

@inproceedings{uchibayashi_policy_2019,
	title = {Policy {Management} {Technique} {Using} {Blockchain} for {Cloud} {VM} {Migration}},
	url = {https://ieeexplore.ieee.org/document/8890425},
	doi = {10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00073},
	abstract = {In recent years, the use of the cloud has become one of the essential elements when constructing a system for deploying web services. By combining serverless cloud services, various web services can be performed without building servers. However, serverless environments have various limitations, and traditional methods of building servers on VM are indispensable. In addition, technology for migrating VM to physically different hosts was established, and it became possible to easily migrate VM between hosts under the same cloud management. However, depending on the server running on VM, there may be restrictions on the range that can be physically moved. We have solved the problem by focusing on the problem that migration can be performed without checking the information about the server running on the VM at the time of migration. Therefore, in this paper, we introduce a managing technique for VM migration policy in the cloud using blockchain, and show that it is more useful than the conventional policy management method.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Uchibayashi, Toshihiro and Apduhan, Bernady O. and Shiratori, Norio and Suganuma, Takuo and Hiji, Masahiro},
	month = aug,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
	pages = {360--362},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\N6WTF2QM\\8890425.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L9UZEXH8\\8890425.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XTSUUAWE\\8890425.html:text/html},
}

@inproceedings{souza_sapparchi_2019,
	title = {Sapparchi: {An} {Architecture} for {Smart} {City} {Applications} from {Edge}, {Fog} and {Cloud} {Computing}},
	isbn = {2687-8860},
	shorttitle = {Sapparchi},
	url = {https://ieeexplore.ieee.org/document/9071686},
	doi = {10.1109/ISC246665.2019.9071686},
	abstract = {In the Smart Cities context, a plethora of Middleware Platforms has been proposed to support applications execution and data processing. However, just a few of them have explored the overall Smart Cities computing environment. The vast majority focuses on specific domains, typically presenting a sensor-acquisition architecture for processing in Cloud Computing. Most recent initiatives try to include Cloud Computing and Edge Computing, while few of them use the three computing levels (Cloud, Fog, and Edge). Besides, many of these platforms do not define the services that should be deployed at each level, nor how the developer can better use each feature. This work fulfills this gap presenting an architecture for applications classifying services implemented by a typical Computing Environment of Smart Cities. Our architecture uses all the computational levels (Cloud, Fog, Edge) of a city infrastructure, and it defines how to deploy each type of service at each level. We also present an example of the proposed architecture that we are implementing in the city of Natal, where some evaluative tests have been carried out.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {International} {Smart} {Cities} {Conference} ({ISC2})},
	author = {Souza, Arthur and Izidio, Larysse and Rocha, Aluízio and Cacho, Nélio and Batista, Thais},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Smart Cities Conference (ISC2)},
	pages = {262--267},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XQG7VXWB\\9071686.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2P8NSUG2\\9071686.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QKVJV4WR\\9071686.html:text/html},
}

@inproceedings{kaplunovich_scalability_2019,
	title = {Scalability {Analysis} of {Blockchain} on a {Serverless} {Cloud}},
	url = {https://ieeexplore.ieee.org/document/9005529},
	doi = {10.1109/BigData47090.2019.9005529},
	abstract = {While adopting Blockchain technologies to automate their enterprise functionality, organizations are recognizing the challenges of scalability and manual configuration that the state of art present. Scalability of Hyperledger Fabric is an open challenge recognized by the research community. We have automated many of the configuration steps of installing Hyperledger Fabric Blockchain on AWS infrastructure and have benchmarked the scalability of that system. We have used the UCR (University of California Riverside) Time Series Archive with 128 timeseries datasets containing over 191,177 rows of data totaling 76,453,742 numbers. Using an automated Serverless approach, we have loaded this dataset, by chunks, into different AWS instances, triggering the load by SQS messaging. In this paper, we present the results of this benchmarking study and describe the approach we took to automate the Hyperledger Fabric processes using serverless Lambda functions and SQS triggering. We will also discuss what is needed to make the Blockchain technology more robust and scalable.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Kaplunovich, Alex and Joshi, Karuna P. and Yesha, Yelena},
	month = dec,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE International Conference on Big Data (Big Data)},
	pages = {4214--4222},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\3YBYU7VJ\\Kaplunovich et al. - 2019 - Scalability Analysis of Blockchain on a Serverless.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AA7AF52T\\9005529.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\58QK8GHK\\9005529.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JB2D5WHD\\9005529.html:text/html},
}

@inproceedings{rahman_serverless_2019,
	title = {Serverless {Architecture} for {Big} {Data} {Analytics}},
	url = {https://ieeexplore.ieee.org/document/8978443},
	doi = {10.1109/GCAT47503.2019.8978443},
	abstract = {Big data is a phrase that describes the large quantity of data, it would be structure, semi structure and unstructured. In the present industry data is indispensable for the business organization. The Big Data initiatives and technologies are used to analyze this massive amount of data for gaining insights which may help in making strategic decisions. For example, data size is increasing day by day like petabyte, Exabyte, zettabyte, yottabyte and more. That is why it is going to tough and complex to manage this large scale of data. In practical, there are many challenges to process and compute this data like server management, storage, clustering, algorithm deployment, etc. As everything is done by manually, so it is hard to design the exact architecture for data analysis in the Cloud. Serverless computing is a mechanism to provide pay-per-use backend services to clients. A serverless provider lets users create and deploy code without worrying about operating and managing servers. In this paper, we present serverless architecture for big data analytics, also we show how to implement, maintain, and governance of a serverless big data application on AWS (Amazon Web Service). In addition to it, we will demonstrate the difference between traditional data analytics and big data analytics in a serverless system.},
	urldate = {2023-12-31},
	booktitle = {2019 {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Rahman, Md Mijanur and Hasibul Hasan, Md},
	month = oct,
	year = {2019},
	note = {Journal Abbreviation: 2019 Global Conference for Advancement in Technology (GCAT)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FYMGXIUN\\8978443.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4BULXC2T\\8978443.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WCFQKN9U\\8978443.html:text/html},
}

@inproceedings{hussain_serverless_2019,
	title = {Serverless {Edge} {Computing} for {Green} {Oil} and {Gas} {Industry}},
	isbn = {2166-5478},
	url = {https://ieeexplore.ieee.org/document/8767119},
	doi = {10.1109/GreenTech.2019.8767119},
	abstract = {Escalating demand of petroleum led the Oil and Gas (O\&G) industry to extend oil extraction operation in the remote reservoirs. Oil extraction is a fault intolerant process where the maximum penalty is disaster impacting the environment seriously. Therefore, efficient and nature-friendly green oil extraction is a challenging operation, especially with location constrained in accessing the sites. To overcome these challenges and protect the environment from pollution, smart oil fields with numerous sensors (e.g., for pipeline pressure, gas leakage, air pollution) are established to achieve clean O\&G extraction. Conventionally, cloud datacenters are utilized to process the generated data. High-latency satellite communication are used for data transfer, which is not suitable for time-sensitive operations/tasks. To process such latency-sensitive tasks, edge computing can be a suitable candidate, however, their computational power goes downhill at disaster time due to surge demand of many coordinated activities. Therefore, we propose green smart oil fields that operate based on edge computing. To overcome shortage of resources and rapid deployment of the edge computing systems, we propose to use lightweight serverless computing on a federation of edge computing resources from nearby oil rigs. Our solution coordinates urgent coordinated operations/tasks to prevent disasters in oil fields and enable the idea of green smart oil fields. Evaluation results demonstrate the efficacy of our proposed solution in compare to conventional solutions for smart oil fields.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {Green} {Technologies} {Conference}({GreenTech})},
	author = {Hussain, Razin Farhan and Salehi, Mohsen Amini and Semiari, Omid},
	month = apr,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE Green Technologies Conference(GreenTech)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\33GDAM55\\8767119.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IPPRR3IT\\8767119.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QDTN7BA6\\8767119.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\4PGYE8MD\\Hussain et al. - 2019 - Serverless Edge Computing for Green Oil and Gas In.pdf:application/pdf},
}

@inproceedings{handrianto_short_2019,
	title = {Short {Paper}: {TigerAware} {Assistant}: {A} {New} {Serverless} {Implementation} of {Conversational} {Agents} for {Customizable} {Surveys} on {Smart} {Devices}},
	shorttitle = {Short {Paper}},
	url = {https://ieeexplore.ieee.org/document/8940418},
	doi = {10.1109/TransAI46475.2019.00023},
	abstract = {Applications of conversational agents, also called chatbots, have accelerated in recent years. However, it is difficult for people with limited programming skills to implement chatbots and analyze the data collected by chatbots. In this paper, we present a new serverless implementation of conversational agents for delivering customizable surveys on smart devices, including smartphones and smart speakers, and a web-based system for applying advanced analytics techniques to survey data using cloud services. The system is called TigerAware Assistant, as part of the TigerAware platform for customizable mobile survey and sensor data collection. TigerAware Assistant enables non-technical people to easily create and deploy chatbots on various mobile devices to conduct surveys through conversations in natural languages via auditory and textual methods. It also provides a web-based system for survey data visualization, statistical analysis, and advanced machine learning-based data analysis using cloud services on survey responses collected by chatbots.},
	urldate = {2023-12-31},
	booktitle = {2019 {First} {International} {Conference} on ​{Transdisciplinary} {AI} ({TransAI})},
	author = {Handrianto, Yohanes and Huang, Rui and Shang, Yi},
	month = sep,
	year = {2019},
	note = {Journal Abbreviation: 2019 First International Conference on ​Transdisciplinary AI (TransAI)},
	pages = {88--91},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MJPFGI9C\\8940418.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UKAI8YYD\\8940418.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HTTW6QDS\\8940418.html:text/html},
}

@inproceedings{kritikos_simulation-as--service_2019,
	title = {Simulation-as-a-{Service} with {Serverless} {Computing}},
	volume = {2642-939X},
	isbn = {2642-939X},
	url = {https://ieeexplore.ieee.org/document/8817117},
	doi = {10.1109/SERVICES.2019.00056},
	abstract = {Simulations are the greatest means for evaluating systems and producing knowledge related to their optimal configuration for production. Simulation systems support the execution of simulations. These can be installed and executed internally to an organisation or can be offered as a service in the cloud. Current simulation-as-a-service (SimaaS) offerings rely on VM or container-based deployments which lead to additional costs due to the charging in an hourly basis. Further, such offerings cannot be easily adapted at runtime to still be able to sustain their promised service level. To resolve these issues, this paper proposes a novel SimaaS architecture and solution which exploits the serverless computing paradigm for reducing the simulation cost based on the actual usage of resources as well as accelerating the simulation time through the limitless, parallelised invocation of functions. Further, this solution relies on the MELODIC/Functionizer multi-cloud platform which enables adapting the simulation execution at runtime in order to sustain the right service level according to the user requirements and preferences. For the validation of our solution, a real business application provided by AI Investments has been used. It aims to optimise investment portfolio using the most advanced AI-based methods and requires heavy computational power to accomplish the respective tasks.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} {World} {Congress} on {Services} ({SERVICES})},
	author = {Kritikos, Kyriakos and Skrzypek, Pawel},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE World Congress on Services (SERVICES)},
	pages = {200--205},
}

@inproceedings{gunasekaran_spock_2019,
	title = {Spock: {Exploiting} {Serverless} {Functions} for {SLO} and {Cost} {Aware} {Resource} {Procurement} in {Public} {Cloud}},
	isbn = {2159-6190},
	shorttitle = {Spock},
	url = {https://ieeexplore.ieee.org/document/8814535},
	doi = {10.1109/CLOUD.2019.00043},
	abstract = {We are witnessing the emergence of elastic web services which are hosted in public cloud infrastructures. For reasons of cost-effectiveness, it is crucial for the elasticity of these web services to match the dynamically-evolving user demand. Traditional approaches employ clusters of virtual machines (VMs) to dynamically scale resources based on application demand. However, they still face challenges such as higher cost due to over-provisioning or incur service level objective (SLO) violations due to under-provisioning. Motivated by this observation, we propose Spock, a new scalable and elastic control system that exploits both VMs and serverless functions to reduce cost and ensure SLO for elastic web services. We show that under two different scaling policies, Spock reduces SLO violations of queries by up to 74\% when compared to VM-based resource procurement schemes. Further, Spock yields significant cost savings, by up to 33\% compared to traditional approaches which use only VMs.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} 12th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Kandemir, Mahmut Taylan and Urgaonkar, Bhuvan and Kesidis, George and Das, Chita},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)},
	pages = {199--208},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9LPH4SZF\\8814535.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\W3UDZSZ9\\8814535.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9IKZJ8Z5\\8814535.html:text/html},
}

@inproceedings{bartan_straggler_2019,
	title = {Straggler {Resilient} {Serverless} {Computing} {Based} on {Polar} {Codes}},
	url = {https://ieeexplore.ieee.org/document/8919767},
	doi = {10.1109/ALLERTON.2019.8919767},
	abstract = {We propose a serverless computing mechanism for distributed computation based on polar codes. Serverless computing is an emerging cloud based computation model that lets users run their functions on the cloud without provisioning or managing servers. Our proposed approach is a hybrid computing framework that carries out computationally expensive tasks such as linear algebraic operations involving large-scale data using serverless computing and does the rest of the processing locally. We address the limitations and reliability issues of serverless platforms such as straggling workers using coding theory, drawing ideas from recent literature on coded computation. The proposed mechanism uses polar codes to ensure straggler-resilience in a computationally effective manner. We provide extensive evidence showing polar codes outperform other coding methods. We have designed a sequential decoder specifically for polar codes in erasure channels with full-precision input and outputs. In addition, we have extended the proposed method to the matrix multiplication case where both matrices being multiplied are coded. The proposed coded computation scheme is implemented for AWS Lambda. Experiment results are presented where the performance of the proposed coded computation technique is tested in optimization via gradient descent. Finally, we introduce the idea of partial polarization which reduces the computational burden of encoding and decoding at the expense of straggler-resilience.},
	urldate = {2023-12-31},
	booktitle = {2019 57th {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
	author = {Bartan, Burak and Pilanci, Mert},
	month = sep,
	year = {2019},
	note = {Journal Abbreviation: 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	pages = {276--283},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\488P3YYC\\8919767.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KCILDXR6\\8919767.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\879ZSY9L\\8919767.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\CUSLUNAI\\Bartan and Pilanci - 2019 - Straggler Resilient Serverless Computing Based on .pdf:application/pdf},
}

@inproceedings{kaplunovich_tolambdaautomatic_2019,
	title = {{ToLambda}–{Automatic} {Path} to {Serverless} {Architectures}},
	url = {https://ieeexplore.ieee.org/document/8844428},
	doi = {10.1109/IWoR.2019.00008},
	abstract = {Serverless architectures are becoming computing standard and best practice. It is inevitable that more and more software systems will embrace the trend. Our tool toLambda provides automatic conversion of Java monolith application code into AWS Lambda Node.js microservices. During the refactoring, we provide assorted useful transformations of the original code and generate all the necessary artifacts to deploy the generated functions to the Cloud. In this paper we will describe the challenges we have faced including parsing, transformation, performance and testing. We will also underline the advantages of serverless compared to other architectures. Our approach will help to migrate systems to serverless microservices easier and faster.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE}/{ACM} 3rd {International} {Workshop} on {Refactoring} ({IWoR})},
	author = {Kaplunovich, Alex},
	month = may,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE/ACM 3rd International Workshop on Refactoring (IWoR)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\789AQEGM\\8844428.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BEJFFUC4\\8844428.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SFAC7ZV5\\8844428.html:text/html},
}

@inproceedings{pelle_towards_2019,
	title = {Towards {Latency} {Sensitive} {Cloud} {Native} {Applications}: {A} {Performance} {Study} on {AWS}},
	isbn = {2159-6190},
	shorttitle = {Towards {Latency} {Sensitive} {Cloud} {Native} {Applications}},
	url = {https://ieeexplore.ieee.org/document/8814512},
	doi = {10.1109/CLOUD.2019.00054},
	abstract = {Microservices, serverless architectures, cloud native programming are novel paradigms and techniques which could significantly reduce the burden on both developers and operators of future services. Several types of applications fit in well with the new concepts easing the life of different stakeholders while enabling cloud-grade service deployments. However, latency sensitive applications with strict delay constraints between different components pose additional challenges on the platforms. In order to gain benefit from recent cloud technologies for latency sensitive applications as well, a comprehensive performance analysis of available platforms and relevant components is a crucial first step. In this paper, we address one of the most widely used and versatile cloud platforms, namely Amazon Web Services (AWS), and reveal the delay characteristics of key components and services which impact the overall performance of latency sensitive applications. Our contribution is threefold. First, we define a detailed measurement methodology for CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS. Second, we provide a comprehensive analysis of AWS components focusing on delay characteristics. Third, we attempt to adjust a drone control application to the platform and investigate the performance on today's system.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} 12th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Pelle, István and Czentye, János and Dóka, János and Sonkoly, Balázs},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE 12th International Conference on Cloud Computing (CLOUD)},
	pages = {272--280},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3LGNXQ6H\\8814512.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JPJVF9ME\\8814512.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZRMA223K\\8814512.html:text/html},
}

@inproceedings{scheuner_transpiling_2019,
	title = {Transpiling {Applications} into {Optimized} {Serverless} {Orchestrations}},
	url = {https://ieeexplore.ieee.org/document/8791968},
	doi = {10.1109/FAS-W.2019.00031},
	abstract = {The serverless computing paradigm promises increased development productivity by abstracting the underlying hardware infrastructure and software runtime when building distributed cloud applications. However, composing a serverless application consisting of many tiny functions is still a cumbersome and inflexible process due to the lack of a unified source code view and strong coupling to non-standardized function-level interfaces for code and configuration. In our vision, developers can focus on writing readable source code in a logical structure, which then gets transformed into an optimized multi-function serverless orchestration. Our idea involves transpilation (i.e., source-to-source transformation) based on an optimization model (e.g., cost optimization) by dynamically deciding which set of methods will be grouped into individual deployment units. A successful implementation of our vision would enable a broader range of serverless applications and allow for dynamic deployment optimization based on monitoring runtime metrics. Further, we would expect increased developer productivity by using more familiar abstractions and facilitating clean coding practices and code reuse.},
	urldate = {2023-12-31},
	booktitle = {2019 {IEEE} 4th {International} {Workshops} on {Foundations} and {Applications} of {Self}* {Systems} ({FAS}*{W})},
	author = {Scheuner, Joel and Leitner, Philipp},
	month = jun,
	year = {2019},
	note = {Journal Abbreviation: 2019 IEEE 4th International Workshops on Foundations and Applications of Self* Systems (FAS*W)},
	pages = {72--73},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4HKQYXL5\\8791968.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YZXSV9TY\\8791968.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Z5VL6AZ5\\8791968.html:text/html},
}

@inproceedings{conover_visage_2019,
	title = {Visage – {A} {Visualization} and {Exploration} {Framework} for {Environmental} {Data}},
	isbn = {2153-7003},
	url = {https://ieeexplore.ieee.org/document/8897954},
	doi = {10.1109/IGARSS.2019.8897954},
	abstract = {Diverse airborne and ground-based environmental observations are important technologies for disaster assessment and response, as well as for the validation of environmental satellite observations and atmospheric models which can improve forecasts. The VISAGE (Visualization for Integrated Satellite, Airborne and Ground-based data Exploration) project is working to provide three-dimensional visualization and basic analytics capabilities for such datasets in an interactive user interface. The use of cloud-native, serverless technologies and analysis optimized data storage will position VISAGE for integration with other technologies into a Data Analytic Center Framework.},
	urldate = {2023-12-31},
	booktitle = {{IGARSS} 2019 - 2019 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Conover, Helen and Berendes, Todd and Gatlin, Patrick and Maskey, Manil and Naeger, Aaron and Wingo, Stephanie and Kulkarni, Ajinkya and Marouane, Abdelhak and Wang, Lihua and Ellingson, Brian and Dahal, Bibek and Singhirunnusorn, Khomsun},
	month = jul,
	year = {2019},
	note = {Journal Abbreviation: IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
	pages = {5405--5408},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VRQF2P6C\\8897954.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3R4PIMPF\\8897954.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CHKNW7RX\\8897954.html:text/html},
}

@article{aditya_will_2019,
	title = {Will {Serverless} {Computing} {Revolutionize} {NFV}?},
	volume = {107},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/8653379},
	doi = {10.1109/JPROC.2019.2898101},
	abstract = {Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.},
	number = {4},
	urldate = {2023-12-31},
	journal = {Proceedings of the IEEE},
	author = {Aditya, Paarijaat and Akkus, Istemi Ekin and Beck, Andre and Chen, Ruichuan and Hilt, Volker and Rimac, Ivica and Satzke, Klaus and Stein, Manuel},
	month = apr,
	year = {2019},
	note = {Conference Name: Proceedings of the IEEE},
	pages = {667--678},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ES3Q5BMW\\8653379.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7DKITIXF\\8653379.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TCNFAT6E\\8653379.html:text/html},
}

@article{eismann_case_2022,
	title = {A case study on the stability of performance tests for serverless applications},
	volume = {189},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222000498},
	doi = {10.1016/j.jss.2022.111294},
	abstract = {Context:
While in serverless computing, application resource management and operational concerns are generally delegated to the cloud provider, ensuring that serverless applications meet their performance requirements is still a responsibility of the developers. Performance testing is a commonly used performance assessment practice; however, it traditionally requires visibility of the resource environment.
Objective:
In this study, we investigate whether performance tests of serverless applications are stable, that is, if their results are reproducible, and what implications the serverless paradigm has for performance tests.
Method:
We conduct a case study where we collect two datasets of performance test results: (a) repetitions of performance tests for varying memory size and load intensities and (b) three repetitions of the same performance test every day for ten months.
Results:
We find that performance tests of serverless applications are comparatively stable if conducted on the same day. However, we also observe short-term performance variations and frequent long-term performance changes.
Conclusion:
Performance tests for serverless applications can be stable; however, the serverless model impacts the planning, execution, and analysis of performance tests.},
	urldate = {2023-12-31},
	journal = {Journal of Systems and Software},
	author = {Eismann, Simon and Costa, Diego Elias and Liao, Lizhi and Bezemer, Cor-Paul and Shang, Weiyi and van Hoorn, André and Kounev, Samuel},
	month = jul,
	year = {2022},
	keywords = {FaaS, Serverless, Function-as-a-Service, Performance, Reproducibility, Stability},
	pages = {111294},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\TG7N6X7I\\S0164121222000498.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\LP57C6IA\\S0164121222000498.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\SQKKWWS3\\S0164121222000498.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\5V8CMX9V\\Eismann et al. - 2022 - A case study on the stability of performance tests.pdf:application/pdf},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218302735},
	doi = {10.1016/j.jss.2018.12.013},
	abstract = {Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.},
	urldate = {2023-12-31},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	keywords = {Cloud computing, Serverless, Function-as-a-Service, Empirical research},
	pages = {340--359},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\PEHUC2I8\\Leitner et al. - 2019 - A mixed-method empirical study of Function-as-a-Se.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\4HMQ6PKU\\S0164121218302735.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KLN2LI9B\\S0164121218302735.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\5YN7BJUH\\S0164121218302735.html:text/html},
}

@article{zheng_package-aware_2023,
	title = {A package-aware scheduling strategy for edge serverless functions based on multi-stage optimization},
	volume = {144},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000547},
	doi = {10.1016/j.future.2023.02.013},
	abstract = {Serverless computing offers a promising deployment model for edge IoT applications. However, serverless functions that rely on large libraries suffer from severe library loading latency when containerized, which is unfriendly to edge latency-sensitive applications. Most function offload strategies in edge environments ignore the impact of this latency. We also found that the measures taken by serverless platforms to reduce loading latency may not work in edge environments. To remedy that, this paper proposes a function offloading strategy to minimize loading latency, a new way to deeply integrate placement optimization with cache optimization. In this way, we first design a package caching policy suitable for edge environments based on the consistency of execution topology. Then a Double Layers Dynamic Programming algorithm (DLDP) is proposed to solve the problem of function offloading considering the dependent packages using a multi-stage progressive optimization approach. The caching policy is embedded in the scheduling algorithm through a phased optimization approach to achieve joint optimization. Extensive experiments on the cluster trace from Alibaba show that DLDP reduces the loading latency of packages by more than 97.84\% and significantly outperforms four baselines in the application completion time by more than 55.67\%.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Zheng, Senjiong and Liu, Bo and Lin, Weiwei and Ye, Xiaoying and Li, Keqin},
	month = jul,
	year = {2023},
	keywords = {Dependency package awareness, Package caching strategy, Serverless function offloading},
	pages = {105--116},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\2Z2WV5L9\\S0167739X23000547.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\JD9UJHBR\\S0167739X23000547.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\UWEL2GXZ\\S0167739X23000547.html:text/html},
}

@article{jefferson_resource-optimized_2022,
	series = {4th {International} {Conference} on {Innovative} {Data} {Communication} {Technology} and {Application}},
	title = {A {Resource}-optimized and {Accelerated} {Sentiment} {Analysis} {Method} using {Serverless} {Computing}},
	volume = {215},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050922020750},
	doi = {10.1016/j.procs.2022.12.004},
	abstract = {Serverless computing is being touted as one of the promising and potential computing paradigms for the future. It clearly represents a deeper automation in information technology (IT) operations. Application developers can get server resources easily and quickly to run their applications and services without bothering about setting up server infrastructure and its sustenance. Application scalability is being met through the infrastructure elasticity. The faster maturity and stability of the containerization aspect has led to the success of the serverless paradigm. With serverless flourishing, the adoption of function as a service (FaaS) is catching up fast. A user request or any noteworthy event can trigger a request to this function to be astutely serviced through the serverless phenomenon. In this paper, we have explained how we are able to optimally use the power of serverless computing to bring forth a new resource-constrained and accelerated sentiment analysis method.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Jefferson, Steve and Chelliah, Pethuru and Surianarayanan, Chellammal},
	month = jan,
	year = {2022},
	keywords = {serverless computing, machine learning techniques, sentiment analysis},
	pages = {33--43},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\AMZ8PQ5K\\S1877050922020750.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\K29NCGRF\\S1877050922020750.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\K94ZVCCE\\S1877050922020750.html:text/html},
}

@article{emami_khansari_scalable_2024,
	title = {A scalable modified deep reinforcement learning algorithm for serverless {IoT} microservice composition infrastructure in fog layer},
	volume = {153},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004284},
	doi = {10.1016/j.future.2023.11.022},
	abstract = {Nowadays many modern and Artificial Intelligence (AI) enabled Internet of Things (IoT) applications consist of chains connecting microservices distributed across the fog and cloud layers to achieve a certain functionality. These microservices should be assigned to the available processing instances placed on the things, fog and cloud layers while considering the resource utilization, Quality of Service (QoS) parameters and requirements of the user. Despite several existing optimization approaches, IoT microservice composition remains a challenge in serverless computing architecture due to the large-scale environment, its dynamic nature and the heterogeneous nodes with limited capacity. To address these challenges, a novel IoT microservice composition based on serverless architecture is proposed in the fog layer. Serverless computing is a new paradigm which offers the ability to build scalable applications without worrying about back-end management making this paradigm particularly effective in IoT environment. We specifically propose a modified Deep Reinforcement Learning (DRL)-based Microservice Chaining at Fog Layer (DRLMCF) algorithm to improve the distributed chained microservice placement in order to minimize resource utilization and delay in a fog based serverless architecture. Unlike existing methods, the proposed DRLMCF is built on realistic assumptions and does not require expert tuning or a large amount of labeled data to make optimal decisions. Furthermore, our method is not entirely dependent on a central controller and fog nodes take part in the decision making thus making the algorithm scalable in IoT environment. Compared to state-of-the art methods including load balance, shortest path and random schedule algorithms, evaluation by simulating several real-world scenarios demonstrates the proposed DRLMCF improves end to end delay up to 57.3\% and the number of successfully chained microservices by up to 84\%.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Emami Khansari, Mina and Sharifian, Saeed},
	month = apr,
	year = {2024},
	keywords = {Serverless computing, IoT, Function as a service, Fog computing, Microservice composition, Modified deep reinforcement learning},
	pages = {206--221},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\L7CVXP8G\\S0167739X23004284.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\FDLNFTYP\\S0167739X23004284.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\3RZ7FU2L\\S0167739X23004284.html:text/html},
}

@article{eizaguirre_seer_2024,
	title = {A {Seer} knows best: {Auto}-tuned object storage shuffling for serverless analytics},
	volume = {183},
	issn = {0743-7315},
	shorttitle = {A {Seer} knows best},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731523001338},
	doi = {10.1016/j.jpdc.2023.104763},
	abstract = {Serverless platforms offer high resource elasticity and pay-as-you-go billing, making them a compelling choice for data analytics. To craft a “pure” serverless solution, the common practice is to transfer intermediate data between serverless functions via serverless object storage (IBM COS; AWS S3). However, prior works have led to inconclusive results about the performance of object storage systems, since they have left large margin for optimization. To verify that object storage has been underrated, we devise a novel shuffle manager for serverless data analytics called Seer. Specifically, Seer dynamically chooses between two shuffle algorithms to maximize performance. The algorithm choice is made online based on some predictive models, and very importantly, without end users having to specify intermediate shuffle data sizes at the time of the job submission. We integrate Seer with PyWren-IBM [31], a well-known serverless analytics framework, and evaluate it against both serverful (e.g., Spark) and serverless systems (e.g., Google BigQuery, Caerus [46] and SONIC [22]). Our results certify that our new shuffle manager can deliver performance improvements over them.},
	urldate = {2023-12-31},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Eizaguirre, Germán T. and Sánchez-Artigas, Marc},
	month = jan,
	year = {2024},
	keywords = {Serverless computing, I/O optimization, Object storage, Shuffle},
	pages = {104763},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\BV72B5AL\\S0743731523001338.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\M29HKEQU\\S0743731523001338.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\EUYABBJ8\\S0743731523001338.html:text/html},
}

@article{ristov_afcl_2021,
	title = {{AFCL}: {An} {Abstract} {Function} {Choreography} {Language} for serverless workflow specification},
	volume = {114},
	issn = {0167-739X},
	shorttitle = {{AFCL}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X20302648},
	doi = {10.1016/j.future.2020.08.012},
	abstract = {Serverless workflow applications or function choreographies (FCs), which connect serverless functions by data- and control-flow, have gained considerable momentum recently to create more sophisticated applications as part of Function-as-a-Service (FaaS) platforms. Initial experimental analysis of the current support for FCs uncovered important weaknesses, including provider lock-in, and limited support for important data-flow and control-flow constructs. To overcome some of these weaknesses, we introduce the Abstract Function Choreography Language (AFCL) for describing FCs at a high-level of abstraction, which abstracts the function implementations from the developer. AFCL is a YAML-based language that supports a rich set of constructs to express advanced control-flow (e.g. parallelFor loops, parallel sections, dynamic loop iterations counts) and data-flow (e.g multiple input and output parameters of functions, DAG-based data-flow). We introduce data collections which can be distributed to loop iterations and parallel sections that may substantially reduce the delays for function invocations due to reduced data transfers between functions. We also support asynchronous functions to avoid delays due to blocking functions. AFCL supports properties (e.g. expected size of function input data) and constraints (e.g. minimize execution time) for the user to optionally provide hints about the behavior of functions and FCs and to control the optimization by the underlying execution environment. We implemented a prototype AFCL environment that supports AFCL as input language with multiple backends (AWS Lambda and IBM Cloud Functions) thus avoiding provider lock-in which is a common problem in serverless computing. We created two realistic FCs from two different domains and encoded them with AWS Step Functions, IBM Composer and AFCL. Experimental results demonstrate that our current implementation of the AFCL environment substantially outperforms AWS Step Functions and IBM Composer in terms of development effort, economic costs, and makespan.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Ristov, Sasko and Pedratscher, Stefan and Fahringer, Thomas},
	month = jan,
	year = {2021},
	keywords = {FaaS, Performance, AWS Step Functions, Cost, IBM Composer},
	pages = {368--382},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\TC73IHRS\\Ristov et al. - 2021 - AFCL An Abstract Function Choreography Language f.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\JAXX3MWC\\S0167739X20302648.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\CQ2AQXZI\\S0167739X20302648.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\5WTHIKJ8\\S0167739X20302648.html:text/html},
}

@article{tuli_ai_2023,
	title = {{AI} augmented {Edge} and {Fog} computing: {Trends} and challenges},
	volume = {216},
	issn = {1084-8045},
	shorttitle = {{AI} augmented {Edge} and {Fog} computing},
	url = {https://www.sciencedirect.com/science/article/pii/S108480452300067X},
	doi = {10.1016/j.jnca.2023.103648},
	abstract = {In recent years, the landscape of computing paradigms has witnessed a gradual yet remarkable shift from monolithic computing to distributed and decentralized paradigms such as Internet of Things (IoT), Edge, Fog, Cloud, and Serverless. The frontiers of these computing technologies have been boosted by shift from manually encoded algorithms to Artificial Intelligence (AI)-driven autonomous systems for optimum and reliable management of distributed computing resources. Prior work focuses on improving existing systems using AI across a wide range of domains, such as efficient resource provisioning, application deployment, task placement, and service management. This survey reviews the evolution of data-driven AI-augmented technologies and their impact on computing systems. We demystify new techniques and draw key insights in Edge, Fog and Cloud resource management-related uses of AI methods and also look at how AI can innovate traditional applications for enhanced Quality of Service (QoS) in the presence of a continuum of resources. We present the latest trends and impact areas such as optimizing AI models that are deployed on or for computing systems. We layout a roadmap for future research directions in areas such as resource management for QoS optimization and service reliability. Finally, we discuss blue-sky ideas and envision this work as an anchor point for future research on AI-driven computing systems.},
	urldate = {2023-12-31},
	journal = {Journal of Network and Computer Applications},
	author = {Tuli, Shreshth and Mirhakimi, Fatemeh and Pallewatta, Samodha and Zawad, Syed and Casale, Giuliano and Javadi, Bahman and Yan, Feng and Buyya, Rajkumar and Jennings, Nicholas R.},
	month = jul,
	year = {2023},
	keywords = {Cloud computing, AI, Fog computing, Edge computing, Deployment, Fault-tolerance, Scheduling},
	pages = {103648},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\RK5EEP2W\\Tuli et al. - 2023 - AI augmented Edge and Fog computing Trends and ch.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\LXZ6CZ9B\\S108480452300067X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\QJRXIP5L\\S108480452300067X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\N585AYZ4\\S108480452300067X.html:text/html},
}

@article{alladi_applications_2020,
	title = {Applications of blockchain in unmanned aerial vehicles: {A} review},
	volume = {23},
	issn = {2214-2096},
	shorttitle = {Applications of blockchain in unmanned aerial vehicles},
	url = {https://www.sciencedirect.com/science/article/pii/S2214209620300206},
	doi = {10.1016/j.vehcom.2020.100249},
	abstract = {The recent advancement in Unmanned Aerial Vehicles (UAVs) in terms of manufacturing processes, and communication and networking technology has led to a rise in their usage in civilian and commercial applications. The regulations of the Federal Aviation Administration (FAA) in the US had earlier limited the usage of UAVs to military applications. However more recently, the FAA has outlined new enforcement that will also expand the usage of UAVs in civilian and commercial applications. Due to being deployed in open atmosphere, UAVs are vulnerable to being lost, destroyed or physically hijacked. With the UAV technology becoming ubiquitous, various issues in UAV networks such as intra-UAV communication, UAV security, air data security, data storage and management, etc. need to be addressed. Blockchain being a distributed ledger protects the shared data using cryptography techniques such as hash functions and public key encryption. It can also be used for assuring the truthfulness of the information stored and for improving the security and transparency of the UAVs. In this paper, we review various applications of blockchain in UAV networks such as network security, decentralized storage, inventory management, surveillance, etc., and discuss some broader perspectives in this regard. We also discuss various challenges to be addressed in the integration of blockchain and UAVs and suggest some future research directions.},
	urldate = {2023-12-31},
	journal = {Vehicular Communications},
	author = {Alladi, Tejasvi and Chamola, Vinay and Sahu, Nishad and Guizani, Mohsen},
	month = jun,
	year = {2020},
	keywords = {Blockchain technology, Internet of Things (IoT), Security and privacy, Unmanned Aerial Vehicle (UAV) network},
	pages = {100249},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\CMXJ6Q5J\\S2214209620300206.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\SSXPJ7HE\\S2214209620300206.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\HVB3AVY3\\S2214209620300206.html:text/html},
}

@article{li_automatic_2023,
	title = {Automatic abdominal segmentation using novel {3D} self-adjustable organ aware deep network in {CT} images},
	volume = {84},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809423001246},
	doi = {10.1016/j.bspc.2023.104691},
	abstract = {CT scan is an important reference means of disease diagnosis in practice. Automatic segmentation of organ regions can save a lot of time and labor costs, and allow doctors to produce more intuitive observations of the organization of the human body. However, automatic multi-organ segmentation in CT images remains challenging due to the complicated anatomical structures and low tissue contrast in CT images. Traditional segmentation methods are relatively inefficient for organ segmentation with large abdominal deformation, small volume, and blurry tissue boundaries, and the traditional network architectures are rarely designed to meet the requirements of lightweight and efficient clinical practice. In this paper, we propose a novel segmentation network named Self-Adjustable Organ Attention U-Net (SOA-Net) to overcome these limitations. To be a pragmatic solution for effective segmentation method, the SOA-Net includes multi-branches feature attention (MBFA) module and the feature attention aggregation (FAA) module. These two modules have multiple branches with different kernel sizes to capture different scales feature information based on multiple scales of the target organs. An adjustable attention is used on these branches to generate different sizes of the receptive fields in the fusion layer. On the whole, SOA-Net is a 3D self-adjustable organ aware deep network which can adaptively adjust their attention and receptive field sizes based on multiple scales of the target organs to realize the efficient segmentation of multiple abdominal organs. We evaluate our method on AbdomenCT-1K and AMOS2022 datasets and the final experiments proved that our model achieves the best segmentation performance compared with the state-of-the-art segmentation networks. (Our code will be publicly available soon).},
	urldate = {2023-12-31},
	journal = {Biomedical Signal Processing and Control},
	author = {Li, Laquan and Zhao, Haiguo and Wang, Hong and Li, Weisheng and Zheng, Shenhai},
	month = jul,
	year = {2023},
	keywords = {Deep learning, Abdominal image, Attention mechanism, Multi-organ segmentation},
	pages = {104691},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\PA4BDT47\\S1746809423001246.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KMGJ5MFR\\S1746809423001246.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\E9Y8ZJ9V\\S1746809423001246.html:text/html},
}

@article{olariu_challenges_2023,
	series = {27th {International} {Conference} on {Knowledge} {Based} and {Intelligent} {Information} and {Engineering} {Sytems} ({KES} 2023)},
	title = {Challenges {In} {Optimizing} {Migration} {Costs} {From} {On}-{Premises} {To} {Microsoft} {Azure}},
	volume = {225},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050923015181},
	doi = {10.1016/j.procs.2023.10.360},
	abstract = {The current paper analyzes the feasibility of a modular web application's migration procedure from on-premises to the cloud. Our focus is on identifying cost savings and options for hosting the application in the cloud. The research specifically examines the impact of architectural decisions records (ADR) on a modular monolith use case, utilizing .NET Core for the backend and Angular for the front end, with Clean Architecture as the design pattern. We investigate different cloud models (IaaS, PaaS, Serverless) considering project management's triple constraints (time, cost, performance). The study demonstrates that a modular monolith can be migrated with varying effort and costs depending on the chosen cloud model and technology stack. We explore optimizations such as resource utilization, licensing fees, and cost reduction through infrastructure reservations. Our findings show that the migration cost can range from a 20\% increase with IaaS to approximately 70\% cost reduction with the Serverless strategy compared to the on-premises environment, using equivalent resources. We also explore methods to lower expenses for each model, including resource modifications, Linux operating systems, and longer resource reservations. Considering limitations, we propose a two-stage migration strategy: initially lifting and shifting the application to IaaS with cost optimization, and subsequently migrating to PaaS for scalability and simplified resource management in the long term.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Olariu, Florin and Alboaie, Lenuța},
	month = jan,
	year = {2023},
	keywords = {Azure Pricing, Clean Architecture, Cloud Migration, Lift and Shift, Modular monolith, Optimizing costs, Optimizing resources},
	pages = {3649--3659},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\TLT76VSR\\S1877050923015181.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KQU4R69E\\S1877050923015181.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\Q79U54YN\\S1877050923015181.html:text/html},
}

@article{huf_composition_2019,
	title = {Composition of heterogeneous web services: {A} systematic review},
	volume = {143},
	issn = {1084-8045},
	shorttitle = {Composition of heterogeneous web services},
	url = {https://www.sciencedirect.com/science/article/pii/S108480451930205X},
	doi = {10.1016/j.jnca.2019.06.008},
	abstract = {Initial developments in Service-Oriented Computing (SOC) led to the development of Web Services using the SOAP protocol and an extensive set of tools and methods for composing new services from those existing. Subsequently, other types of services also emerged, such as event-oriented services and RESTful services. Nevertheless, all mentioned service types expose data and functionality, and users can benefit from their composition, regardless of the service type chosen for their implementation. In the Internet of Things, it is relevant to employ event-oriented services for sensing and SOAP, RESTful or lightweight web APIs for control. In the emerging field of microservices, heterogeneity is embraced as a design principle and services that are part of a single system may be implemented using heterogeneous technologies and paradigms. The research question of this review is: How heterogeneous services can be composed? There are several surveys that cover service composition with each of the existing service types, but the composition of heterogeneous services is only marginally addressed. This systematic literature review focuses explicitly on the heterogeneity of the aforementioned service types. A total of 66 documents, published from 2005 to 2018, have been surveyed, targeting all possible combinations of the three service types. In addition to summarizing existing works, the specific methods employed for supporting service type heterogeneity are grouped into archetypes and have their limitations and capabilities analyzed. Despite the large number of documents found, there are several open issues on heterogeneous service composition. The results of this review are confronted with emerging fields in service computing, namely microservices, serverless and IoT, yielding additional research directions.},
	urldate = {2023-12-31},
	journal = {Journal of Network and Computer Applications},
	author = {Huf, Alexis and Siqueira, Frank},
	month = oct,
	year = {2019},
	keywords = {Microservices, Event-oriented services, Heterogeneous services, RESTful services, SOAP services, Web service composition},
	pages = {89--110},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\VKDFSZWY\\S108480451930205X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\NYU9BGK4\\S108480451930205X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\W2N8SZUY\\S108480451930205X.html:text/html},
}

@article{v_s_container_2023,
	title = {Container security: {Precaution} levels, mitigation strategies, and research perspectives},
	volume = {135},
	issn = {0167-4048},
	shorttitle = {Container security},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404823004005},
	doi = {10.1016/j.cose.2023.103490},
	abstract = {The enterprise technique for application deployment has undergone a major transformation during the past two decades. Using conventional techniques, software developers write code in a particular computing environment, frequently leading to mistakes and defects when moving it to a new computing environment. However, during the past few years, enterprises have begun to use containers \& microservices to segregate infrastructure in a particular perspective and develop new models of the technology stack. Software developers could construct and deploy apps more quickly and effectively now, thanks to containerization. Despite the fact that containers have their own namespace, it is still feasible for a containerized image to attack the host system by inserting malicious software into it. This necessitates threat modeling of the container life span. During the investigation, we were able to create the elemental systematic modelling that identifies threats pertaining to container application workflow and its preliminary mitigation techniques, where attack trees are defined alongside the model, which helps academics and enthusiasts better comprehend the significance of container security. We utilize the well-known threat modeling framework, DREAD, to further advance threat modeling across the infrastructure of containers that aids in prioritizing the risks. Additionally, tools for assessing container vulnerabilities and discrete real-world exploits were researched, and approaches for security analysis in container technology were compared to the existing literature. Finally, this study brings to a conclusion by outlining the state-of-the-art survey for future research and identifying potential research topics in server-based and serverless containers.},
	urldate = {2023-12-31},
	journal = {Computers \& Security},
	author = {V s, Devi Priya and Chakkaravarthy Sethuraman, Sibi and Khan, Muhammad Khurram},
	month = dec,
	year = {2023},
	keywords = {Microservices, Container security- root-based and rootless, DREAD, Software development, Threat modeling-attack trees},
	pages = {103490},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KQAMHSDQ\\S0167404823004005.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\G2E8HJ3K\\S0167404823004005.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\BKZRRZ7N\\S0167404823004005.html:text/html},
}

@article{aoshima_cost_2022,
	series = {Knowledge-{Based} and {Intelligent} {Information} \& {Engineering} {Systems}: {Proceedings} of the 26th {International} {Conference} {KES2022}},
	title = {Cost estimates for modern e-business systems},
	volume = {207},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S187705092201002X},
	doi = {10.1016/j.procs.2022.09.121},
	abstract = {Recently, the adoption of cloud computing for e-business has become standardized and began evolving into further execution models, such as serverless computing. One of the features of cloud computing is to provide computational resources on a pay-as-you-go model, where users pay only for what they use. These managed and serviced resources have characteristics that make them suitable for cost estimation in the early stages of business. This paper proposes a rapid cost estimation method for serverless computing using directed acyclic graph- (DAG) based formalization and matrix operations. Using a case study, we demonstrate that cost estimation under serverless computing can be effective in the early stages of business. Furthermore, we show that the cost estimation of serverless computing is more proportional to the required computational resources than the virtual machine- (VM) based cost estimation method.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Aoshima, Tomohisa and Yoshida, Kenichi},
	month = jan,
	year = {2022},
	keywords = {cloud computing, serverless computing, application topology, cost estimation, e-business},
	pages = {664--673},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\LJMCS2V8\\S187705092201002X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\MEQFMRA5\\S187705092201002X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\ZYAADFAX\\S187705092201002X.html:text/html},
}

@article{zhang_dcml_2022,
	title = {{DCML}: {Deep} contrastive mutual learning for {COVID}-19 recognition},
	volume = {77},
	issn = {1746-8094},
	shorttitle = {{DCML}},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809422002920},
	doi = {10.1016/j.bspc.2022.103770},
	abstract = {COVID-19 is a form of disease triggered by a new strain of coronavirus. Automatic COVID-19 recognition using computer-aided methods is beneficial for speeding up diagnosis efficiency. Current researches usually focus on a deeper or wider neural network for COVID-19 recognition. And the implicit contrastive relationship between different samples has not been fully explored. To address these problems, we propose a novel model, called deep contrastive mutual learning (DCML), to diagnose COVID-19 more effectively. A multi-way data augmentation strategy based on Fast AutoAugment (FAA) was employed to enrich the original training dataset, which helps reduce the risk of overfitting. Then, we incorporated the popular contrastive learning idea into the conventional deep mutual learning (DML) framework to mine the relationship between diverse samples and created more discriminative image features through a new adaptive model fusion method. Experimental results on three public datasets demonstrate that the DCML model outperforms other state-of-the-art baselines. More importantly, DCML is easier to reproduce and relatively efficient, strengthening its high practicality.},
	urldate = {2023-12-31},
	journal = {Biomedical Signal Processing and Control},
	author = {Zhang, Hongbin and Liang, Weinan and Li, Chuanxiu and Xiong, Qipeng and Shi, Haowei and Hu, Lang and Li, Guangli},
	month = aug,
	year = {2022},
	keywords = {Adaptive model fusion, Contrastive learning, COVID-19 recognition, Deep mutual learning, Fast AutoAugment},
	pages = {103770},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\R9YB4C84\\Zhang et al. - 2022 - DCML Deep contrastive mutual learning for COVID-1.pdf:application/pdf},
}

@article{jauro_deep_2020,
	title = {Deep learning architectures in emerging cloud computing architectures: {Recent} development, challenges and next research trend},
	volume = {96},
	issn = {1568-4946},
	shorttitle = {Deep learning architectures in emerging cloud computing architectures},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494620305202},
	doi = {10.1016/j.asoc.2020.106582},
	abstract = {The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures.},
	urldate = {2023-12-31},
	journal = {Applied Soft Computing},
	author = {Jauro, Fatsuma and Chiroma, Haruna and Gital, Abdulsalam Y. and Almutairi, Mubarak and Abdulhamid, Shafi’i M. and Abawajy, Jemal H.},
	month = nov,
	year = {2020},
	keywords = {Serverless computing, Fog computing, Convolutional neural network, Deep learning, Deep reinforcement learning, Edge computing, Emerging cloud computing},
	pages = {106582},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\72Z9DEUE\\S1568494620305202.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\LPRBR974\\S1568494620305202.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\HVXMZEWK\\S1568494620305202.html:text/html},
}

@article{mampage_deep_2023,
	title = {Deep reinforcement learning for application scheduling in resource-constrained, multi-tenant serverless computing environments},
	volume = {143},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300047X},
	doi = {10.1016/j.future.2023.02.006},
	abstract = {Serverless computing has sparked a massive interest in both the cloud service providers and their clientele in recent years. This model entails the shift of the entire matter of resource management of user applications to the service provider. In serverless systems, the provider is highly motivated to attain cost efficient usage of their infrastructure, given the granular billing modules involved. However, due to the dynamic and multi-tenant nature of the serverless workloads and systems, achieving efficient resource management while maintaining function performance is a challenging task. Rapid changes in demand levels for applications cause variations in actual resource usage patterns of function instances. This leads to performance variations in co-located functions which compete for similar resources, due to resource contentions. Most existing serverless scheduling works offer heuristic techniques for function scheduling, which are unable to capture the true dynamism in these systems caused by multi-tenancy and varying user request patterns. Further, they rarely consider the often contradicting dual objectives of achieving provider resource efficiency along with application performance. In this article, we propose a novel technique incorporating Deep Reinforcement Learning (DRL) to overcome the aforementioned challenges for function scheduling in a highly dynamic serverless computing environment with heterogeneous computing resources. We train and evaluate our model in a practical setting incorporating Kubeless, an open-source serverless framework, deployed on a 23-node Kubernetes cluster setup. Extensive experiments done on this testbed environment show promising results with improvements of up to 24\% and 34\% in terms of application response time and resource usage cost respectively, compared to baseline techniques.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
	month = jun,
	year = {2023},
	keywords = {Serverless computing, Function scheduling, Practical experiments, Reinforcement learning, Resource contention, Resource cost efficiency},
	pages = {277--292},
}

@article{wang_deep_2024,
	title = {Deep {Reinforcement} {Learning}-based scheduling for optimizing system load and response time in edge and fog computing environments},
	volume = {152},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003862},
	doi = {10.1016/j.future.2023.10.012},
	abstract = {Edge/fog computing, as a distributed computing paradigm, satisfies the low-latency requirements of ever-increasing number of IoT applications and has become the mainstream computing paradigm behind IoT applications. However, because large number of IoT applications require execution on the edge/fog resources, the servers may be overloaded. Hence, it may disrupt the edge/fog servers and also negatively affect IoT applications’ response time. Moreover, many IoT applications are composed of dependent components incurring extra constraints for their execution. Besides, edge/fog computing environments and IoT applications are inherently dynamic and stochastic. Thus, efficient and adaptive scheduling of IoT applications in heterogeneous edge/fog computing environments is of paramount importance. However, limited computational resources on edge/fog servers imposes an extra burden for applying optimal but computationally demanding techniques. To overcome these challenges, we propose a Deep Reinforcement Learning-based IoT application Scheduling algorithm, called DRLIS to adaptively and efficiently optimize the response time of heterogeneous IoT applications and balance the load of the edge/fog servers. We implemented DRLIS as a practical scheduler in the FogBus2 function-as-a-service framework for creating an edge–fog–cloud integrated serverless computing environment. Results obtained from extensive experiments show that DRLIS significantly reduces the execution cost of IoT applications by up to 55\%, 37\%, and 50\% in terms of load balancing, response time, and weighted cost, respectively, compared with metaheuristic algorithms and other reinforcement learning techniques.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Wang, Zhiyu and Goudarzi, Mohammad and Gong, Mingming and Buyya, Rajkumar},
	month = mar,
	year = {2024},
	keywords = {Internet of Things, Machine learning, Fog computing, Deep reinforcement learning, Edge computing},
	pages = {55--69},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\NBRB8DQ2\\Wang et al. - 2024 - Deep Reinforcement Learning-based scheduling for o.pdf:application/pdf},
}

@article{kelly_denial_2021,
	title = {Denial of wallet—{Defining} a looming threat to serverless computing},
	volume = {60},
	issn = {2214-2126},
	url = {https://www.sciencedirect.com/science/article/pii/S221421262100079X},
	doi = {10.1016/j.jisa.2021.102843},
	abstract = {Serverless computing is the latest paradigm in cloud computing, offering a framework for the development of event driven, pay-as-you-go functions in a highly scalable environment. While these traits offer a powerful new development paradigm, they have also given rise to a new form of cyber-attack known as Denial of Wallet (forced financial exhaustion). In this work, we define and identify the threat of Denial of Wallet and its potential attack patterns. Also, we demonstrate how this new form of attack can potentially circumvent existing mitigation systems developed for a similar style of attack, Denial of Service. Our goal is twofold. Firstly, we will provide a concise and informative overview of this emerging attack paradigm. Secondly, we propose this paper as a starting point to enable researchers and service providers to create effective mitigation strategies. We include some simulated experiments to highlight the potential financial damage that such attacks can cause and the creation of an isolated test bed for continued safe research on these attacks.},
	urldate = {2023-12-31},
	journal = {Journal of Information Security and Applications},
	author = {Kelly, Daniel and Glavin, Frank G. and Barrett, Enda},
	month = aug,
	year = {2021},
	keywords = {Cloud computing, Serverless computing, Cloud security, Denial-of-wallet, Function-as-a-service},
	pages = {102843},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\M2BRHKIV\\Kelly et al. - 2021 - Denial of wallet—Defining a looming threat to serv.pdf:application/pdf},
}

@article{sadek_design_2022,
	series = {International {Conference} on {ENTERprise} {Information} {Systems} / {ProjMAN} - {International} {Conference} on {Project} {MANagement} / {HCist} - {International} {Conference} on {Health} and {Social} {Care} {Information} {Systems} and {Technologies} 2021},
	title = {Design and {Implementation} of {Medical} {Searching} {System} {Based} on {Microservices} and {Serverless} {Architectures}},
	volume = {196},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050921022791},
	doi = {10.1016/j.procs.2021.12.056},
	abstract = {Microservices and Serverless computing are promising cloud-based architectural models in the software development industry that have many advantages over previous technology models. The benefit of adopting these more novel models, however, is dependent on the volume of the application workload and the execution behavior. ScanMedicine is a new searching system dedicated to providing health care professionals, patients and researchers with easy access to intelligence underpinning health technology innovations. We present the design and implementation of ScanMedicine’s framework using AWS lambda functions and microservices. We incorporated a layered architectural style where each layer run on separate hardware and adopt different architectural technique.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Sadek, Jawad and Craig, Dawn and Trenell, Michael},
	month = jan,
	year = {2022},
	keywords = {AWS, Cloud Computing, Serverless, Clinical Trials, Medical Devices, Microservices},
	pages = {615--622},
}

@article{he_dissecting_2022,
	title = {Dissecting lightning strike hazard impact patterns to {National} {Airspace} {System} facilities in the contiguous {United} {States}},
	volume = {91},
	issn = {0198-9715},
	url = {https://www.sciencedirect.com/science/article/pii/S0198971521001423},
	doi = {10.1016/j.compenvurbsys.2021.101735},
	abstract = {Lightning strikes pose a severe threat to the United States (US) National Airspace System (NAS). Although the US Federal Aviation Administration (FAA) implements lightning protection practices and procedures to protect personnel, electronic equipment, and structures within the NAS, many lightning-induced outages still occur. To date we found that most research on lightning-induced facility outages has focused on understanding the physical processes of lightning strike effects on aircraft and airport ramp operations. Very little research has been done on examining the overall patterns and characteristics of such hazards to aviation from a geo-spatial standpoint. To bridge this gap, we analyze nationwide lightning strike spatiotemporal data and FAA airport facility outage records from 2009 through 2020 and apply innovative pattern recognition methods to identify key characteristics of lightning strike hazards. Our results uncover the complexities of lightning strike hazard impact patterns to NAS facilities, identifying five distinct typologies with climatological signatures critical to creating better hazard mitigation strategies.},
	urldate = {2023-12-31},
	journal = {Computers, Environment and Urban Systems},
	author = {He, Yiyi and Yue, Xiangyu and Lindbergh, Sarah and Gao, Jianxi and Graves, Chuck and Rakas, Jasenka},
	month = jan,
	year = {2022},
	keywords = {Facility management, Lightning strike hazard, National Airspace System, Pattern recognition},
	pages = {101735},
}

@article{rezaei_fast_2023,
	title = {Fast asymptotic algorithm for real-time causal connectivity analysis of multivariate systems and signals},
	volume = {204},
	issn = {0165-1684},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168422003619},
	doi = {10.1016/j.sigpro.2022.108822},
	abstract = {The purpose of this work is to optimize the current state-of-the-art asymptotic algorithm of connectivity Granger-causality measures in the frequency-domain, such as the Directed Transfer Function (DTF), Partial Directed Coherence (PDC), and their variants. These measures stem from the modeling of multidimensional time series by multivariate autoregressive model. Surrogate and asymptotic analysis are the most frequently used methods to quantify the statistical significance of such derived interactions, a critical step for validation of the results. The current asymptotic algorithms run fairly fast on low-dimensional datasets but become impractical for high-dimensional datasets due to the involved computational time and memory demand. This is a huge limitation in the application of these connectivity measures to the fields dealing with numerous concurrently acquired signals from probing of complex systems such as the human brain. Here, we optimized the current algorithms for the fast asymptotic analysis of these connectivity measures and achieved a reduction of their computation time by at least three orders of magnitude. The optimizations were accomplished by decreasing the dimension of the involved matrices, eliminating the complicated functions (e.g., eigenvalue estimation and Cholesky factorization), and variable separation. The superior performance of the proposed optimized algorithm is shown with simulation examples.},
	urldate = {2023-12-31},
	journal = {Signal Processing},
	author = {Rezaei, Farnaz and Alamoudi, Omar Ali and Davani, Shayan and Hou, Songming},
	month = mar,
	year = {2023},
	keywords = {Causality measures, Directed transfer function, Fast asymptotic analysis, Multivariate autoregressive model, Partial directed coherence},
	pages = {108822},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\78KAMBQI\\S0165168422003619.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\7ICMVM4Q\\S0165168422003619.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\Q4SK4E4T\\S0165168422003619.html:text/html},
}

@article{calancea_iassistme_2019,
	series = {Knowledge-{Based} and {Intelligent} {Information} \& {Engineering} {Systems}: {Proceedings} of the 23rd {International} {Conference} {KES2019}},
	title = {{iAssistMe} - {Adaptable} {Assistant} for {Persons} with {Eye} {Disabilities}},
	volume = {159},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S187705091931347X},
	doi = {10.1016/j.procs.2019.09.169},
	abstract = {Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Calancea, Cristina Georgiana and Miluţ, Camelia-Maria and Alboaie, Lenuţa and Iftene, Adrian},
	month = jan,
	year = {2019},
	keywords = {Cloud Architectures, Personal Assistants, Visually Challenged},
	pages = {145--154},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\7JT4DAGT\\S187705091931347X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\QPNQR5MG\\S187705091931347X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\BY7GEZD7\\S187705091931347X.html:text/html},
}

@article{pedratscher_m2faas_2022,
	title = {{M2FaaS}: {Transparent} and fault tolerant {FaaSification} of {Node}.js monolith code blocks},
	volume = {135},
	issn = {0167-739X},
	shorttitle = {{M2FaaS}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001509},
	doi = {10.1016/j.future.2022.04.021},
	abstract = {Porting existing monoliths to the Function-as-a-Service (FaaS) (FaaSification) can be very challenging for software developers due to different architectural styles. For a successful porting, developers need to resolve various dependencies, such as method invocations of external packages or user-defined codes, as well as global and local variables used in and after the code block that should be faasified. To bridge the gap and automatize FaaSification, this paper introduces M2FaaS, a FaaSifier that automatically converts a Node.js monolith into a hybrid by faasifying annotated code blocks as serverless functions on multiple FaaS providers. M2FaaS is a novel FaaSifier that resolves many challenges for the resulting monolith to work properly after the FaaSification. Developers may annotate all dependencies that need to be resolved for the generated functions to run properly and specify variables that should be returned by the function to the monolith because they are used later in the monolith. Moreover, M2FaaS is the first FaaSifier that faasifies arbitrary code blocks. The current M2FaaS prototype supports FaaSification of individual functions on two FaaS providers, AWS Lambda and IBM Cloud Functions. Finally, M2FaaS introduces an optional annotation for alternative functions to be invoked in case the primary faasified function fails. The resulting hybrid application invokes the automatically deployed serverless functions, while the original code remains executable. Experiments with four complementary monoliths demonstrate that M2FaaS outperforms state-of-the-art FaaSifiers in terms of development effort by up to 73.3\%. Moreover, with the fault tolerance support, M2FaaS finishes all submitted functions, thereby achieving by 18.5\% higher throughput than the other FaaSifiers.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Pedratscher, Stefan and Ristov, Sasko and Fahringer, Thomas},
	month = oct,
	year = {2022},
	keywords = {FaaS, Serverless, Cloud, FaaSification, Functions, Portability},
	pages = {57--71},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\JNQWKAMN\\Pedratscher et al. - 2022 - M2FaaS Transparent and fault tolerant FaaSificati.pdf:application/pdf},
}

@article{gimeno_sarroca_mlless_2024,
	title = {{MLLess}: {Achieving} cost efficiency in serverless machine learning training},
	volume = {183},
	issn = {0743-7315},
	shorttitle = {{MLLess}},
	url = {https://www.sciencedirect.com/science/article/pii/S074373152300134X},
	doi = {10.1016/j.jpdc.2023.104764},
	abstract = {Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this endeavor, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100 ms). Our results certify that MLLess can be 15X faster than serverful ML systems [27] at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization. Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers.},
	urldate = {2023-12-31},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Gimeno Sarroca, Pablo and Sánchez-Artigas, Marc},
	month = jan,
	year = {2024},
	keywords = {Serverless computing, Machine learning, Function-as-a-Service},
	pages = {104764},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\RUKGSLH8\\S074373152300134X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\ACYLI453\\S074373152300134X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\WEYE58T3\\S074373152300134X.html:text/html},
}

@article{keshavarzian_modified_2019,
	title = {Modified deep residual network architecture deployed on serverless framework of {IoT} platform based on human activity recognition application},
	volume = {101},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X19304376},
	doi = {10.1016/j.future.2019.06.009},
	abstract = {In the last few years, human activity recognition (HAR) is a subject undergoing intense study in various contexts such as pattern recognition and human-device interaction. HAR applications come to an aid of Telecare system which is paving the way for doctors and nurses to measure the health status of their patients. Due to the ubiquitous influence of smartphones in an individual’s life, we take embedded smartphone sensors into account as our case study. The proposed method, Modified Deep Residual Network, outperforms the accuracy of Human activity recognition compared with state-of-the-art machine learning techniques which are using Raw signals as their input. we defined new pooling layer called smooth-pooling to leverage the model performance. The accuracy of proposed architecture is evaluated on three common dataset that comprises accelerometer and gyroscope raw data. The results demonstrated the proposed method outperforms accuracy of classification while requiring just raw data with lower parameters compared to other works. Furthermore, The proposed HAR method is deployed in our IoT cloud platform which enables users to create scenarios based on what they are doing at home. Using Function as a Service (FaaS) architecture in this platform solves the scalability issues by running each function in a separate container. The IoT platform prepares an infrastructure for developers who want to integrate their application into the platform and use its functionality along with other IoT platform options.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Keshavarzian, Alireza and Sharifian, Saeed and Seyedin, Sanaz},
	month = dec,
	year = {2019},
	keywords = {Cloud computing, Deep residual network, Function as a service, Human action recognition},
	pages = {14--28},
}

@article{li_nonlinear_2022,
	title = {Nonlinear eigenvalue topology optimization for structures with frequency-dependent material properties},
	volume = {170},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327022000322},
	doi = {10.1016/j.ymssp.2022.108835},
	abstract = {Eigenvalue topology optimization problem has been a hot topic in recent years for its wide applications in many engineering areas. In the previous studies, the applied materials are usually assumed as elastic, and the resulting structural eigenfrequencies are obtained by solving a linear eigenvalue problem. However, many engineering materials, such as viscoelastic materials, have frequency-dependent modulus, which results in a more complicated nonlinear eigenvalue problem. This paper presents a systematic study on the nonlinear eigenvalue topology optimization problem with frequency-dependent material properties. The nonlinear eigenvalue problem is solved by a continuous asymptotic numerical method based on the homotopy algorithm and perturbation expansion technique, which involves higher-order differentiation of the nonlinear term and shows a fast convergence. Several schemes are proposed to improve the computational accuracy, applicability, and robustness of the method for the application in topology optimization, including Faà di Bruno's theorem, bisection method, and inverse iteration based eigenvector modification method. Three optimization problems are solved to demonstrate the effectiveness of the developed methods, including the maximization of the fundamental frequency, the eigenfrequency separation interval between two adjacent eigenfrequencies of given orders, and the eigenfrequency separation interval at a given frequency. Numerical examples show the large influence of the frequency-dependent material properties on the optimized results and validate the effectiveness of the developed method.},
	urldate = {2023-12-31},
	journal = {Mechanical Systems and Signal Processing},
	author = {Li, Quhao and Wu, Qiangbo and Dou, Suguang and Wang, Jilai and Liu, Shutian and Chen, Wenjiong},
	month = may,
	year = {2022},
	keywords = {Asymptotic numerical method, Eigenvector modification, Frequency-dependent material properties, Nonlinear eigenvalue problem, Topology optimization},
	pages = {108835},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\X3638FJ7\\S0888327022000322.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\7LXMQJH5\\S0888327022000322.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\CZQJ37WF\\S0888327022000322.html:text/html},
}

@article{stefanac_noops_2022,
	series = {International {Conference} on {ENTERprise} {Information} {Systems} / {ProjMAN} - {International} {Conference} on {Project} {MANagement} / {HCist} - {International} {Conference} on {Health} and {Social} {Care} {Information} {Systems} and {Technologies} 2021},
	title = {{NoOps} – {A} {Multivocal} literature review},
	volume = {196},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050921022250},
	doi = {10.1016/j.procs.2021.12.002},
	abstract = {Traditionally, an organization had to have in-house servers and hardware to build a web application. This evolved into Cloud computing where the possibility for cost reduction and scalable data storage became a reality. With the introduction of cloud computing came a concept known as NoOps, or No Operations. This paper aims to take a closer look into what NoOps is and the benefits and challenges of NoOps. The authors identified three RQs that could help to give more insight into NoOps. Further we discussed the findings and RQs and lay out the way forward for future studies into NoOps. We also looked at artificial intelligence (AI) and how AI seems to be heavily linked with a true NoOps environment. With the lack of scientific studies into NoOps, a Multivocal literature review was selected as the method used to investigate the concept and its implications. We try to show voices both for and against NoOps. Further, we try to look at a misconception of what NoOps really is, what true NoOps could be. Finally we look at what requirements there are for companies wanting to go NoOps, and discuss the possibility that many companies unknowingly are moving towards a NoOps environment.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Stefanac, Tommy and Colomo-Palacios, Ricardo},
	month = jan,
	year = {2022},
	keywords = {Cloud computing, Serverless, AI, Artificial intelligence, Evolution IT, NoOps},
	pages = {167--174},
}

@article{jatoth_optimal_2019,
	title = {Optimal {Fitness} {Aware} {Cloud} {Service} {Composition} using an {Adaptive} {Genotypes} {Evolution} based {Genetic} {Algorithm}},
	volume = {94},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18308501},
	doi = {10.1016/j.future.2018.11.022},
	abstract = {With the seamless proliferation of cloud services, it becomes challenging to select and compose cloud services that satisfy the requirements of users. A service may be connected with another service(s) for satisfying a workflow/function in a service composition. Further, the service assessment based on one or two QoS parameters is not accurate enough to achieve the desired optimality in a cloud service composition. Most of the existing methods in the literature consider either a single QoS parameter or two QoS parameters for QoS-aware composition and do not consider the balancing of QoS parameters and/or the connectivity constraints between two compositions. In this paper, we present an Optimal Fitness Aware Cloud Service Composition (OFASC) using an Adaptive Genotype Evolution based Genetic Algorithm (AGEGA) dealing with multiple QoS parameters and providing the solutions that satisfy the balancing QoS parameters and connectivity constraints of service composition. Experimental results show that our approach enhances the efficiency of cloud service composition by converging quickly and obtains better composition when compared to other approaches.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Jatoth, Chandrashekar and Gangadharan, G. R. and Buyya, Rajkumar},
	month = may,
	year = {2019},
	keywords = {Adaptive Genotypes Evolution, Cloud service composition, Genetic algorithm, Quality of Service (QoS)},
	pages = {185--198},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\MT92RMEP\\S0167739X18308501.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\R487FLCJ\\S0167739X18308501.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\E8LMH8LR\\S0167739X18308501.html:text/html},
}

@article{rausch_optimized_2021,
	title = {Optimized container scheduling for data-intensive serverless edge computing},
	volume = {114},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030399X},
	doi = {10.1016/j.future.2020.07.017},
	abstract = {Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Rausch, Thomas and Rashed, Alexander and Dustdar, Schahram},
	month = jan,
	year = {2021},
	keywords = {Machine learning, Serverless, Edge computing, Container scheduling},
	pages = {259--271},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\VIF2I9AM\\S0167739X2030399X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\C28CQYGJ\\S0167739X2030399X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\UXHN7NDY\\S0167739X2030399X.html:text/html},
}

@article{tran_optimized_2024,
	title = {Optimized resource usage with hybrid auto-scaling system for knative serverless edge computing},
	volume = {152},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004156},
	doi = {10.1016/j.future.2023.11.010},
	abstract = {In the most popular serverless platform - Knative, dynamic resource allocation is implemented using horizontal auto-scaling algorithms to create or delete service instances based on different monitored metrics. However, the assigned resources for each instance are fixed. Vertical scaling up or down assigned resources per instance is required to avoid over-provisioning resources which are limited at the edge. Hybrid (horizontal and vertical) auto-scaling solutions proposed by existing works have several limitations. These solutions are optimized for separated services and get degraded performance when applied in a normal environment with multiple concurrent services. Further, most methods make significant changes to the original Knative platform, and have not been considered to be adopted since then. In this article, instead of Knative modification, we developed separated Kubernetes operators and custom resources (CRs) that can assist the Knative auto-scaler with optimal hybrid auto-scaling configurations based on traffic prediction. First, we characterize each service with a profile of different assigned resource levels pairing with their optimal target Knative’s horizontal scaling request concurrency. Then, based on these profiles, we calculate the best-assigned resources level, target concurrency level, and the number of required instances corresponding to each future time step’s predicted traffic. Finally, these configurations are applied to Knative’s default auto-scaler and services’ CR. Experiments done on our testbed compared our solution with a Knative hybrid auto-scaler solution that does not consider the service’s target request concurrency, and the default Knative horizontal auto-scaler. The results show our solution improvements up to 14\% and 20\% in terms of resource usage, respectively.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Tran, Minh-Ngoc and Kim, YoungHan},
	month = mar,
	year = {2024},
	keywords = {Serverless computing, Edge computing, Horizontal scaling, Quality of service, Resource management, Vertical scaling},
	pages = {304--316},
}

@article{pelle_p4-assisted_2023,
	title = {P4-assisted seamless migration of serverless applications towards the edge continuum},
	volume = {146},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001450},
	doi = {10.1016/j.future.2023.04.010},
	abstract = {Serverless computing has recently been presented as an effective technology for handling short-lived compute tasks in the cloud. It has the potential of becoming an attractive option also in the context of edge computing where resource-aware deployment, constrained by both limited edge computing resources and experienced latency, plays a vital role. In this paper, we present and experimentally validate a framework that oversees serverless applications in an edge computing scenario. It completely automates serverless application deployment and provides hitless dynamic migration of application compute tasks between a pair of edge nodes, paving the way for handling significantly more complex cases. The framework relies on an integrated deployment, monitoring and offloading infrastructure that enhances AWS IoT Greengrass features and performance. Our implementation provides two separate options for relocating compute tasks by steering application traffic towards the most suitable node. One builds on an on-the-fly application component reconfiguration, while the other selects the suitable node through P4 in-network processing of resource metrics emitted by the nodes. Our experimental demonstration evaluates the migration performance using a latency-sensitive application decomposed to serverless functions. Results reveal extremely fast dynamic reconfiguration and traffic rerouting operations. The used methods avoid congestion peaks at the edge and show no end-to-end latency increase upon migration between the nodes.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Pelle, István and Paolucci, Francesco and Sonkoly, Balázs and Cugini, Filippo},
	month = sep,
	year = {2023},
	keywords = {FaaS, Serverless, Edge, AWS Greengrass, AWS Lambda, P4},
	pages = {122--138},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\ZGLHZNVM\\Pelle et al. - 2023 - P4-assisted seamless migration of serverless appli.pdf:application/pdf},
}

@article{yao_performance_2023,
	title = {Performance optimization of serverless edge computing function offloading based on deep reinforcement learning},
	volume = {139},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200293X},
	doi = {10.1016/j.future.2022.09.009},
	abstract = {It is difficult for resource-constrained edge servers to simultaneously meet the performance requirements of all the latency-sensitive Internet of Things (IoT) applications in edge computing. Therefore, it is a significant challenge to efficiently generate a task offloading strategy. Recently, deep reinforcement learning (DRL)-based task offloading methods have been studied to ensure long-term performance optimization. However, there are challenges in existing DRL-based task offloading methods, such as insufficient sample diversity and high exploration cost. To optimize the performance of edge computing and facilitate the development and deployment of event-driven IoT applications, the serverless edge computing model has emerged. It combines serverless computing, also known as Function as a Service (FaaS), with edge computing and has been adopted in edge AI inference and prediction, stream processing, face recognition, etc. In this paper, an experience-sharing deep reinforcement learning-based distributed function offloading method called ES-DRL is proposed in the setting of a combined stateful and stateless execution model for serverless edge computing. ES-DRL adopts a distributed learning architecture, where each edge FaaS (EFaaS) obtains the current state of the local environment and inputs them to the local DRL agent, which outputs the function offloading strategy. Then, each EFaaS uploads the experience data interacting with the environment to a global shared replay buffer located in the cloud and randomly draws a batch of data from it to optimize the parameters of the local network. A population-guided policy search method is introduced to speed up the convergence of the DRL agent and avoid falling into the local optimum. The experimental results demonstrate that ES-DRL can reduce the average latency by up to approximately 17 percent compared to the existing DRL-based task offloading method.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Yao, Xuyi and Chen, Ningjiang and Yuan, Xuemei and Ou, Pingjie},
	month = feb,
	year = {2023},
	keywords = {Serverless computing, Deep reinforcement learning, Edge computing, Function offloading, Task offloading},
	pages = {74--86},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\V74HR2D4\\S0167739X2200293X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KYDCSH5X\\S0167739X2200293X.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\43NAEH8E\\S0167739X2200293X.html:text/html},
}

@article{nawaz_psac-pdb_2023,
	title = {{PSAC}-{PDB}: {Analysis} and classification of protein structures},
	volume = {158},
	issn = {0010-4825},
	shorttitle = {{PSAC}-{PDB}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523002792},
	doi = {10.1016/j.compbiomed.2023.106814},
	abstract = {This paper presents a novel framework, called PSAC-PDB, for analyzing and classifying protein structures from the Protein Data Bank (PDB). PSAC-PDB first finds, analyze and identifies protein structures in PDB that are similar to a protein structure of interest using a protein structure comparison tool. Second, the amino acids (AA) sequences of identified protein structures (obtained from PDB), their aligned amino acids (AAA) and aligned secondary structure elements (ASSE) (obtained by structural alignment), and frequent AA (FAA) patterns (discovered by sequential pattern mining), are used for the reliable detection/classification of protein structures. Eleven classifiers are used and their performance is compared using six evaluation metrics. Results show that three classifiers perform well on overall, and that FAA patterns can be used to efficiently classify protein structures in place of providing the whole AA sequences, AAA or ASSE. Furthermore, better classification results are obtained using AAA of protein structures rather than AA sequences. PSAC-PDB also performed better than state-of-the-art approaches for SARS-CoV-2 genome sequences classification.},
	urldate = {2023-12-31},
	journal = {Computers in Biology and Medicine},
	author = {Nawaz, M. Saqib and Fournier-Viger, Philippe and He, Yulin and Zhang, Qin},
	month = may,
	year = {2023},
	keywords = {Classification, DALI, PDB, Protein structures, SARS-CoV-2, Spike, SPM},
	pages = {106814},
}

@article{vaquero_research_2019,
	title = {Research challenges in nextgen service orchestration},
	volume = {90},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18303157},
	doi = {10.1016/j.future.2018.07.039},
	abstract = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Vaquero, Luis M. and Cuadrado, Felix and Elkhatib, Yehia and Bernal-Bernabe, Jorge and Srirama, Satish N. and Zhani, Mohamed Faten},
	month = jan,
	year = {2019},
	keywords = {FaaS, Serverless, Edge, Fog, Churn, Large scale, NFV, NVM, Orchestration, SDN},
	pages = {20--38},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\B5BU2A8P\\Vaquero et al. - 2019 - Research challenges in nextgen service orchestrati.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\B2ACJYIY\\S0167739X18303157.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\CGHJW3HU\\S0167739X18303157.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\S2TGQRE8\\S0167739X18303157.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\3GIR856Z\\Vaquero et al. - 2019 - Research challenges in nextgen service orchestrati.pdf:application/pdf},
}

@article{chit_scalable_2023,
	series = {27th {International} {Conference} on {Knowledge} {Based} and {Intelligent} {Information} and {Engineering} {Sytems} ({KES} 2023)},
	title = {Scalable {Remote} {Cloud} {Data} {Center} for {Vessel} {Equipment} {Predictive} {Maintenance} {Service}-as-a-{Product} ({SaaP})},
	volume = {225},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050923014333},
	doi = {10.1016/j.procs.2023.10.275},
	abstract = {Predictive maintenance is gradually replacing conventional preventive maintenance, through an informed decision-making process for a fleet operator to proactively monitor the health status of the sea-based equipment and machinery. As a result, the fleet operator will only need to ask for on-time repair and maintenance services instead of periodic maintenance, which saves both time and cost significantly. While moving into this new maritime business model, which is also known as Service-as-a-Product (SaaP), both fleet operators and maintenance, repair, and overhaul (MRO) companies could also build a stronger collaboration in the maritime industries. With the advent of digital transformation towards Industry 4.0 (I4.0), the Industrial Internet-of-Things (IIoT) renders the massive collection of operational and machinery process data from the vessel equipment through sensorization, where these collected real-time information are useful for advanced analytics to predict equipment failure and to avoid unplanned downtime. Unlike other industries, there are a few challenges in the maritime industries when developing a centralized smart vessel equipment monitoring platform. One of the key challenges, is the lack of a feasible data management system, requires the centralized host must be able to handle and manage the telemetry IoT data that is transmitted over the satellite communication, by complying with all cybersecurity considerations and regulations in maritime sector. In this paper, we present a scalable, hybrid cloud-based data management framework that can connect to multiple edge systems where each system is deployed on a physical vessel, to establish a scalable SaaP business model. Hence, the ship-to-shore sensorized data can be processed and monitored via a real-time visualization dashboard through the centralized cloud-based platform. We argue that our approach, would be a turn-key solution that can be implemented for mostly all types of marine equipment and machinery, and thus to further improve the prediction tools to support advanced decision-making techniques, such as optimal time repair of vessel equipment.},
	urldate = {2023-12-31},
	journal = {Procedia Computer Science},
	author = {Chit, Tan Wei and Toro, Carlos and Lim, Ho Choon and Muthu, Raguram},
	month = jan,
	year = {2023},
	keywords = {Serverless Computing, Data Management System, Industry 4.0 applications, Service-as-a-Product (SaaP)},
	pages = {2826--2834},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\KPSI49HP\\S1877050923014333.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\W3VN3YIW\\S1877050923014333.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\UVUZ28AH\\S1877050923014333.html:text/html},
}

@article{wang_sd-srf_2024,
	title = {{SD}-{SRF}: {An} {Intelligent} {Service} {Deployment} {Scheme} for {Serverless}-operated {Cloud}-{Edge} {Computing} in {6G} {Networks}},
	volume = {151},
	issn = {0167-739X},
	shorttitle = {{SD}-{SRF}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X2300359X},
	doi = {10.1016/j.future.2023.09.027},
	abstract = {With the development of serverless computing, developers can implement and deploy business applications as a combination of stateless functions. Although originally proposed for cloud computing, serverless computing is gradually applied to cloud-edge systems for service deployment to provide users with high-quality, low-latency services. However, optimized service deployment in 6G networks is a very challenging issue because of the vast number of deployable devices in the network, and its permutations are highly exponential. In this paper, we propose optimized service deployment schemes for online and offline, respectively, to minimize the overall latency at a lower cost. (1) First, a SD-SRF algorithm based on the greedy algorithm is proposed to optimize the service deployment for a multi-layer edge network, which consists of two phases: SR and SF. (a) Services are deployed in the nearest ancestor devices in the routing tree of all such service requests with the least cost of deployment. (b) When the deployment cost of moving some service replicas to devices in the lower layer is less than the benefit, the service will fall. (2) However, the offline algorithm relies on the availability of prior information heavily, such as request arrival pattern and number, which is difficult to obtain. Therefore, this paper proposes a PPO-MSD algorithm for optimal deployment online, where a Markov decision process (MDP) is modeled. Extensive simulation results show that PPO-MSD outperforms existing algorithms in terms of overall delay and utility, and its performance is close to the optimal ones obtained by SD-SRF, with the SD-SRF and PPO-MSD algorithms reducing the delay on average by 32.40\% and 9.91\%.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Wang, Luying and Liu, Anfeng and Xiong, Neal N. and Zhang, Shaobo and Wang, Tian and Dong, Mianxiong},
	month = feb,
	year = {2024},
	keywords = {Serverless computing, Reinforcement learning, 6G networks, Cloud-Edge computing, Intelligent service deployment},
	pages = {242--259},
}

@article{zhang_security_2024,
	title = {Security computing resource allocation based on deep reinforcement learning in serverless multi-cloud edge computing},
	volume = {151},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003461},
	doi = {10.1016/j.future.2023.09.016},
	abstract = {Handling computationally intensive tasks is challenging for user devices (UDs) with limited computing resources. Serverless cloud edge computing solves this problem and reduces maintenance and management. Its crucial function is to allocate computing resources reasonably. However, linking multiple computing resource nodes to perform computing resource allocation and ensure data security is a significant challenge. This study proposes an approach based on action-constrained deep reinforcement learning (DRL) to allocate computing resources securely. First, we consider a model of a serverless multi-cloud edge computing network with multiple computing resource nodes that possess various attribute characteristics. Then, we design a security mechanism to guarantee data security. Afterward, we formalize the network model and objectives and further transform them into a modeling process known as the Markov decision process. Finally, we propose DRL based on action constraints to provide an optimal resource allocation scheduling policy. Simulation results demonstrate that our approach can reduce system costs and improve working performance compared with the comparison schemes.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Zhang, Hang and Wang, Jinsong and Zhang, Hongwei and Bu, Chao},
	month = feb,
	year = {2024},
	keywords = {Deep reinforcement learning, Resource allocation, Serverless cloud edge computing},
	pages = {152--161},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\9XVTNXSX\\S0167739X23003461.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\Z8IHW956\\S0167739X23003461.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\CY6D23SN\\S0167739X23003461.html:text/html},
}

@article{czentye_serverless_2024,
	title = {Serverless application composition leveraging function fusion: {Theory} and algorithms},
	volume = {153},
	issn = {0167-739X},
	shorttitle = {Serverless application composition leveraging function fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004648},
	doi = {10.1016/j.future.2023.12.010},
	abstract = {Serverless computing is a novel cloud computing paradigm enabling flexible, cost-efficient and fine-granular development of cloud-native applications, although without any guarantees on scheduling or execution times. Thus, various high-level solutions have been proposed in recent years to find proper configurations of individual FaaS functions, while considering user-given QoS requirements. However, the ever-increasing complexity of invocation patterns among stateless functions, externalized management of intermediate states, and diverse public cloud resources pose new challenges to the composition of highly data-intensive serverless applications. In this paper, we fill this gap by proposing novel algorithms based on the emerging function fusion technique, along with the related cost/performance models of composite functions supporting implicit instance parallelization and internal state propagation. We prove the NP-completeness of the underlying latency-constrained tree partitioning problem, and design a bicriteria approximation scheme and a greedy heuristic to derive cost-efficient deployment configurations in polynomial time. With the help of extensive simulations using synthetic call graphs generated from public cloud traces, we demonstrate the applicability and superior runtime performance of our proposed methods compared to state-of-the-art solutions. In addition, we showcase that further cost-reduction of up to 3–6 \% can be achieved compared to the optimal partitioning with the allowance of tolerable latency violations.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Czentye, János and Sonkoly, Balázs},
	month = apr,
	year = {2024},
	keywords = {FaaS, Serverless, Cloud native, Function fusion, Tree partitioning},
	pages = {403--418},
}

@article{cassel_serverless_2022,
	title = {Serverless computing for {Internet} of {Things}: {A} systematic literature review},
	volume = {128},
	issn = {0167-739X},
	shorttitle = {Serverless computing for {Internet} of {Things}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004167},
	doi = {10.1016/j.future.2021.10.020},
	abstract = {Serverless computing, or Function as a Service (FaaS), represents a research trend where applications are built and deployed as a group of stateless functions. Although initially proposed for the cloud, serverless computing has also found its place on Internet of Things (IoT) while bringing functions closer to the devices, in order to reduce latency and avoid unnecessary energy and resource consumption. It is interesting that solutions can work in an integrated manner on edge, fog, and cloud layers. Mission-critical functions can be executed on edge and fog in order to benefit from low-latency responses, while heavy functions can be executed on the cloud to process huge amount of data produced by IoT sensors, as long as Internet connection is available. Existing surveys focus on serverless computing for specific layers and do not address a broad, integrated, and systematic vision regarding how IoT benefits from serverless on edge, fog, and cloud. With this in mind, this paper provides a comprehensive Systematic Literature Review that, after the selection process, covers 60 papers on the field of serverless computing for IoT on the three layers. This gives us insights about how functions are offloaded to different devices and how they interact with each other. We bring main components employed to incubate and execute functions, as well as the main challenges and open questions for this subject. Protocols, programming languages, and storage services related to the solutions are also presented. Finally, we show a rich taxonomy summarizing all characteristics in a single figure, along with a discussion about the overall architecture of serverless applications for IoT. We conclude that serverless computing is a promising technology for IoT applications, but several improvements still need to be made to popularize this concept and make it easier to use.},
	language = {en},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Cassel, Gustavo André Setti and Rodrigues, Vinicius Facco and da Rosa Righi, Rodrigo and Bez, Marta Rosecler and Nepomuceno, Andressa Cruz and André da Costa, Cristiano},
	month = mar,
	year = {2022},
	keywords = {Internet of Things, Serverless, Cloud, Edge, Fog, Function as a Service},
	pages = {299--316},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\C6AUCSWM\\S0167739X21004167.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\MLNCFIH7\\S0167739X21004167.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\H7MHFWAR\\S0167739X21004167.html:text/html},
}

@article{poojara_serverless_2022,
	title = {Serverless data pipeline approaches for {IoT} data in fog and cloud computing},
	volume = {130},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004933},
	doi = {10.1016/j.future.2021.12.012},
	abstract = {With the increasing number of Internet of Things (IoT) devices, massive amounts of raw data is being generated. The latency, cost, and other challenges in cloud-based IoT data processing have driven the adoption of Edge and Fog computing models, where some data processing tasks are moved closer to data sources. Properly dealing with the flow of such data requires building data pipelines, to control the complete life cycle of data streams from data acquisition at the data source, edge and fog processing, to Cloud side storage and analytics. Data analytics tasks need to be executed dynamically at different distances from the data sources and often on very heterogeneous hardware devices. This can be streamlined by the use of a Serverless (or FaaS) cloud computing model, where tasks are defined as virtual functions, which can be migrated from edge to cloud (and vice versa) and executed in an event-driven manner on data streams. In this work, we investigate the benefits of building Serverless data pipelines (SDP) for IoT data analytics and evaluate three different approaches for designing SDPs: (1) Off-the-shelf data flow tool (DFT) based, (2) Object storage service (OSS) based and (3) MQTT based. Further, we applied these strategies on three fog applications (Aeneas, PocketSphinx, and custom Video processing application) and evaluated the performance by comparing their processing time (computation time, network communication and disk access time), and resource utilization. Results show that DFT is unsuitable for compute-intensive applications such as video or image processing, whereas OSS is best suitable for this task. However, DFT is nicely fit for bandwidth-intensive applications due to the minimum use of network resources. On the other hand, MQTT-based SDP is observed with increase in CPU and Memory usage as the number of users rose, and experienced a drop in data units in the pipeline for PocketSphinx and custom video processing applications, however it performed well for Aeneas which had low size data units.},
	urldate = {2023-12-31},
	journal = {Future Generation Computer Systems},
	author = {Poojara, Shivananda R. and Dehury, Chinmaya Kumar and Jakovits, Pelle and Srirama, Satish Narayana},
	month = may,
	year = {2022},
	keywords = {Cloud computing, Serverless computing, Fog computing, Edge computing, Data pipelines, Internet of things},
	pages = {91--105},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\GTY7AQAN\\S0167739X21004933.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\YGTLU4RJ\\S0167739X21004933.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\QX3VNLML\\S0167739X21004933.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\WP2YZCZ7\\Poojara et al. - 2022 - Serverless data pipeline approaches for IoT data i.pdf:application/pdf},
}

@article{eskandani_uphill_2023,
	title = {The uphill journey of {FaaS} in the open-source community},
	volume = {198},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222002655},
	doi = {10.1016/j.jss.2022.111589},
	abstract = {Since its introduction in 2014 by Amazon, the Function as a Service (FaaS) model of serverless computing has set the expectation to fulfill the promise of on-demand, pay-as-you-go, infrastructure-independent processing, originally formulated by cloud computing. Yet, serverless applications are fundamentally different than traditional service-oriented software in that they pose specific performance (e.g., cold start), design (e.g., stateless), and development challenges (e.g., debugging). A growing number of cloud solutions have been continuously attempting to address each of these challenges as a result of the increasing popularity of FaaS. Yet, the characteristics of this model have been poorly understood; therefore, the challenges are poorly tackled. In this paper, we assess the state of FaaS in open-source community with a study on almost 2K real-world serverless applications. Our results show a jeopardized ecosystem, where, despite the hype of serverless solutions in the last years, a number of challenges remain untackled, especially concerning component reuse, support for software development, and flexibility among different platforms — resulting in arguably slow adoption of the FaaS model. We believe that addressing the issues discussed in this paper may help researchers shaping the next generation of cloud computing models.},
	urldate = {2023-12-31},
	journal = {Journal of Systems and Software},
	author = {Eskandani, Nafise and Salvaneschi, Guido},
	month = apr,
	year = {2023},
	keywords = {Cloud computing, FaaS, Serverless, Function as a service, Performance, Cloud-computing, Computer software reusability, Open source software, Program debugging, Application programs, Open systems, On demands, Open source communities, Pay as you go, Service modeling, Service-oriented softwares, Software design, Traditional services},
	pages = {111589},
}

@article{dehury_toscadata_2022,
	title = {{TOSCAdata}: {Modeling} data pipeline applications in {TOSCA}},
	volume = {186},
	issn = {0164-1212},
	shorttitle = {{TOSCAdata}},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221002508},
	doi = {10.1016/j.jss.2021.111164},
	abstract = {The serverless platform allows a customer to effectively use cloud resources and pay for the exact amount of used resources. A number of dedicated open source and commercial cloud data management tools are available to handle the massive amount of data. Such modern cloud data management tools are not enough matured to integrate the generic cloud application with the serverless platform due to the lack of mature and stable standards. One of the most popular and mature standards, TOSCA (Topology and Orchestration Specification for Cloud Applications), mainly focuses on application and service portability and automated management of the generic cloud application components. This paper proposes the extension of the TOSCA standard, TOSCAdata, that focuses on the modeling of data pipeline-based cloud applications. Keeping the requirements of modern data pipeline cloud applications, TOSCAdata provides a number of TOSCA models that are independently deployable, schedulable, scalable, and re-usable, while effectively handling the flow and transformation of data in a pipeline manner. We also demonstrate the applicability of proposed TOSCAdata models by taking a web-based cloud application in the context of tourism promotion as a use case scenario.},
	urldate = {2023-12-31},
	journal = {Journal of Systems and Software},
	author = {Dehury, Chinmaya Kumar and Jakovits, Pelle and Srirama, Satish Narayana and Giotis, Giorgos and Garg, Gaurav},
	month = apr,
	year = {2022},
	keywords = {Serverless computing, Data flow management, Data migration, Data pipeline, DevOps, TOSCA},
	pages = {111164},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\I9S7XWNV\\Dehury et al. - 2022 - TOSCAdata Modeling data pipeline applications in .pdf:application/pdf},
}

@article{cicconetti_uncoordinated_2020,
	title = {Uncoordinated access to serverless computing in {MEC} systems for {IoT}},
	volume = {172},
	issn = {1389-1286},
	url = {https://www.sciencedirect.com/science/article/pii/S1389128619313684},
	doi = {10.1016/j.comnet.2020.107184},
	abstract = {Edge computing is a promising solution to enable low-latency Internet of Things (IoT) applications, by shifting computation from remote data centers to local devices, less powerful but closer to the end user devices. However, this creates the challenge on how to best assign clients to edge nodes offering compute capabilities. So far, two antithetical architectures are proposed: centralized resource orchestration or distributed overlay. In this work we explore a third way, called uncoordinated access, which consists in letting every device exploring multiple opportunities, to opportunistically embrace the heterogeneity of network and load conditions towards diverse edge nodes. In particular, our contribution is intended for emerging serverless IoT applications, which do not have a state on the edge nodes executing tasks. We model the proposed system as a set of M/M/1 queues and show that it achieves a smaller delay than single edge node allocation. Furthermore, we compare uncoordinated access with state-of-the-art centralized and distributed alternatives in testbed experiments under more realistic conditions. Based on the results, our proposed approach, which requires a tiny fraction of the complexity of the alternatives in both the device and network components, is very effective in using the network resources, while incurring only a small penalty in terms of increased compute load and high percentiles of delay.},
	urldate = {2023-12-31},
	journal = {Computer Networks},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	month = may,
	year = {2020},
	keywords = {Internet of Things, Serverless computing, Computation offloading, Distributed cloud, Mobile Edge Computing, Online job dispatching, Performance evaluation},
	pages = {107184},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\VTXX4AQN\\S1389128619313684.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\VMVFR73P\\S1389128619313684.html:text/html;ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\5I2UWHFN\\S1389128619313684.html:text/html},
}

@article{lopez_garcia_cloud-based_2020,
	title = {A {Cloud}-{Based} {Framework} for {Machine} {Learning} {Workloads} and {Applications}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8950411},
	doi = {10.1109/ACCESS.2020.2964386},
	abstract = {In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models.},
	urldate = {2023-12-31},
	journal = {IEEE Access},
	author = {López García, Álvaro and De Lucas, Jesús Marco and Antonacci, Marica and Zu Castell, Wolfgang and David, Mario and Hardt, Marcus and Lloret Iglesias, Lara and Moltó, Germán and Plociennik, Marcin and Tran, Viet and Alic, Andy S. and Caballer, Miguel and Plasencia, Isabel Campos and Costantini, Alessandro and Dlugolinsky, Stefan and Duma, Doina Cristina and Donvito, Giacinto and Gomes, Jorge and Heredia Cacha, Ignacio and Ito, Keiichi and Kozlov, Valentin Y. and Nguyen, Giang and Orviz Fernández, Pablo and Šustr, Zděnek and Wolniewicz, Pawel},
	year = {2020},
	note = {Conference Name: IEEE Access},
	pages = {18681--18692},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\34LM7NJ6\\López García et al. - 2020 - A Cloud-Based Framework for Machine Learning Workl.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\STJ5CAW3\\8950411.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BB5S94GN\\8950411.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4ZITULEB\\8950411.html:text/html},
}

@inproceedings{gadea_control_2020,
	title = {A {Control} {Loop}-based {Algorithm} for {Operational} {Transformation}},
	url = {https://ieeexplore.ieee.org/document/9118822},
	doi = {10.1109/SACI49304.2020.9118822},
	abstract = {Operational Transformation (OT) has emerged as a viable theoretical principle for the implementation of real-time collaboration applications. In such systems, the collaboration consists of operations generated by members of a group who are performing concurrent actions on the same document or content. This powerful multi-user co-editing has been researched ever since the seminal works of the late 1980s. As the web evolved into a dominant platform for content consumption and creation, classes of algorithms like OT and Conflict-free Replicated Data Types (CRDT) have enabled flexible content synchronization for applications such as online word processors. Despite their long history in academia, OT and CRDT continue to have unsolved issues due to the centralized approach required for scalable and reliable web-based document editing. This paper proposes a Control Loop-based OT approach based on a serverless architecture and on Finite State Automata (FSA). A control loop principle is used to design a series of algorithms for distributed conflict resolution. The proposed architecture consists of a series of blocks which internally contain a number of multi-level Finite State Machines. The architecture of the new serverless approach for OT is introduced and the basic FSAs that model the co-editing processes are described. Cases encountered in the dynamics of the co-editing processes were modeled to prove that the essential OT properties of causality preservation, convergence, and intention preservation are all satisfied. Simulation results are given at the end of the paper.},
	urldate = {2023-12-31},
	booktitle = {2020 {IEEE} 14th {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics} ({SACI})},
	author = {Gadea, Cristian and Ionescu, Bogdan and Ionescu, Dan},
	month = may,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 14th International Symposium on Applied Computational Intelligence and Informatics (SACI)},
	pages = {000247--000254},
}

@inproceedings{tricomi_nodered-based_2020,
	title = {A {NodeRED}-based dashboard to deploy pipelines on top of {IoT} infrastructure},
	url = {https://ieeexplore.ieee.org/document/9239699},
	doi = {10.1109/SMARTCOMP50058.2020.00036},
	abstract = {With the widespread emergence of the Internet of Things (IoT), our environment and locations are turning progressively into smart environments ranging from individual houses/offices to schools, factories, and hospitals. Even more interesting, with the rise of Fog/Edge paradigms, the IoT application scope has been extended to provide critical services. By pushing resources such as compute and storage to the network edge, IoT-based services are taking benefits from their proximity to provide better performances. However, albeit an exciting development in and by itself, Edge/Fog computing platforms currently do not provide a convenient level of flexibility and efficiency to support the dynamic composition of services with a data-oriented approach. In this context, the Function-as-a-Service computing paradigm rises as a convenient/suitable paradigm to be adopted in the IoT landscape. For the sake of providing flexible IoT Edge/Fog deployments, this paper introduces a system providing FaaS services based on a distributed IoT infrastructure. Besides, we provide a dashboard based on Node-RED that exploits, in the backend, the FaaS system to make the users able to conceive customized applications using the resources (i.e., sensors and actuators) that the IoT devices can host.},
	urldate = {2023-12-31},
	booktitle = {2020 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Tricomi, Giuseppe and Benomar, Zakaria and Aragona, Francesco and Merlino, Giovanni and Longo, Francesco and Puliafito, Antonio},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Smart Computing (SMARTCOMP)},
	pages = {122--129},
}

@inproceedings{wu_survey_2020,
	title = {A {Survey} on {Serverless} {Computing} and {Its} {Implications} for {JointCloud} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9183650},
	doi = {10.1109/JCC49151.2020.00023},
	abstract = {Serverless computing is known as an appealing alternative cloud computing paradigm with its auto-scaling nature and pay-as-you-go charging model. Mainstream cloud vendors have proposed their own serverless platforms, while various kinds of applications have been refactored in a serverless manner for execution. However, the serverless computing model still entails refinement as it introduces performance and security issues. In this paper, we conduct a comprehensive survey on the serverless computing, mainly in three aspects: the type of applications suitable for serverless, the performance issues, and the security issues. We specifically elaborate previous efforts on resolving issues in serverless and shed light on the unresolved issues. We also discuss the opportunities and challenges in integrating serverless computing in the jointcloud infrastructure.},
	urldate = {2023-12-31},
	booktitle = {2020 {IEEE} {International} {Conference} on {Joint} {Cloud} {Computing}},
	author = {Wu, Mingyu and Mi, Zeyu and Xia, Yubin},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Joint Cloud Computing},
	pages = {94--101},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R4NTPDTC\\9183650.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HQC5ETR3\\9183650.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MBRMGW4J\\9183650.html:text/html},
}

@article{stoyanova_survey_2020,
	title = {A {Survey} on the {Internet} of {Things} ({IoT}) {Forensics}: {Challenges}, {Approaches}, and {Open} {Issues}},
	volume = {22},
	issn = {1553-877X},
	shorttitle = {A {Survey} on the {Internet} of {Things} ({IoT}) {Forensics}},
	url = {https://ieeexplore.ieee.org/document/8950109},
	doi = {10.1109/COMST.2019.2962586},
	abstract = {Today is the era of the Internet of Things (IoT). The recent advances in hardware and information technology have accelerated the deployment of billions of interconnected, smart and adaptive devices in critical infrastructures like health, transportation, environmental control, and home automation. Transferring data over a network without requiring any kind of human-to-computer or human-to-human interaction, brings reliability and convenience to consumers, but also opens a new world of opportunity for intruders, and introduces a whole set of unique and complicated questions to the field of Digital Forensics. Although IoT data could be a rich source of evidence, forensics professionals cope with diverse problems, starting from the huge variety of IoT devices and non-standard formats, to the multi-tenant cloud infrastructure and the resulting multi-jurisdictional litigations. A further challenge is the end-to-end encryption which represents a trade-off between users' right to privacy and the success of the forensics investigation. Due to its volatile nature, digital evidence has to be acquired and analyzed using validated tools and techniques that ensure the maintenance of the Chain of Custody. Therefore, the purpose of this paper is to identify and discuss the main issues involved in the complex process of IoT-based investigations, particularly all legal, privacy and cloud security challenges. Furthermore, this work provides an overview of the past and current theoretical models in the digital forensics science. Special attention is paid to frameworks that aim to extract data in a privacy-preserving manner or secure the evidence integrity using decentralized blockchain-based solutions. In addition, the present paper addresses the ongoing Forensics-as-a-Service (FaaS) paradigm, as well as some promising cross-cutting data reduction and forensics intelligence techniques. Finally, several other research trends and open issues are presented, with emphasis on the need for proactive Forensics Readiness strategies and generally agreed-upon standards.},
	number = {2},
	urldate = {2023-12-31},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Stoyanova, Maria and Nikoloudakis, Yannis and Panagiotakis, Spyridon and Pallis, Evangelos and Markakis, Evangelos K.},
	year = {2020},
	note = {Conference Name: IEEE Communications Surveys \& Tutorials},
	pages = {1191--1221},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\THHGD76X\\8950109.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QBHW4EAT\\8950109.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\V29FSW6D\\8950109.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\EW2JSPSI\\Stoyanova et al. - 2020 - A Survey on the Internet of Things (IoT) Forensics.pdf:application/pdf},
}

@inproceedings{thong_tran_autonomous_2020,
	title = {An autonomous {Mobile} {Robot} {System} based on {Serverless} {Computing} and {Edge} {Computing}},
	isbn = {2576-8565},
	url = {https://ieeexplore.ieee.org/document/9236976},
	doi = {10.23919/APNOMS50412.2020.9236976},
	abstract = {Strengthen by Artificial Intelligence (AI) and complexly integrated sensors, an autonomous mobile robot (AMR) is extensively applied in coping with various human resources tasks in indoor office environments. However, implementing an AMR system from scratch needs a strong Electric Engineer background due to the complexity of robot-controlling. Besides, Communication between robot-server, sensor, and client-service also increases the difficulty and time-cost in AMR developing. In this paper, the AMR system we proposed aims to be implemented by people without a EE background, and we will achieve this by employing Robot Operating System. Besides, the AMR system should work independently but still capable of responding to human requests. We will demonstrate a serverless could structure that includes the client-side service and integrate the edge-computing, which in charge of the immediacy-demanding job.},
	urldate = {2023-12-31},
	booktitle = {2020 21st {Asia}-{Pacific} {Network} {Operations} and {Management} {Symposium} ({APNOMS})},
	author = {Thong Tran, Tri and Zhang, Yu-Chen and Liao, Wei-Tung and Lin, Yu-Jen and Li, Ming-Chia and Huang, Huai-Sheng},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)},
	pages = {334--337},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HILIAGRW\\9236976.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TH8E69VR\\9236976.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\P9KXMC89\\9236976.html:text/html},
}

@inproceedings{kaplunovich_automatic_2020,
	title = {Automatic {Tuning} of {Hyperparameters} for {Neural} {Networks} in {Serverless} {Cloud}},
	url = {https://ieeexplore.ieee.org/document/9378280},
	doi = {10.1109/BigData50022.2020.9378280},
	abstract = {Deep Neural Networks are used to solve the most challenging world problems. In spite of the numerous advancements in the field, most of the models are being tuned manually. Experienced Data Scientists have to manually optimize hyperparameters, such as dropout rate, learning rate or number of neurons for Big Data applications. We have implemented a flexible automatic real-time hyperparameter tuning methodology. It works for arbitrary models written in Python and Keras. We also utilized state of the art Cloud services such as trigger based serverless computing (Lambda), and advanced GPU instances to implement automation, reliability and scalability.The existing tuning libraries, such as hyperopt, Scikit-Optimize or SageMaker, require developers to provide a list of hyperparameters and the range of their values manually. Our novel approach detects potential hyperparameters automatically from the source code, updates the original model to tune the parameters, runs the evaluation in the Cloud on spot instances, finds the optimal hyperparameters, and saves the results in the No-SQL database. The methodology can be applied to numerous Big Data Machine Learning systems.},
	urldate = {2023-12-31},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Kaplunovich, Alex and Yesha, Yelena},
	month = dec,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Big Data (Big Data)},
	pages = {2751--2756},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XJRSXKB9\\9378280.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IVK3JXDH\\9378280.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H4DL9VC9\\9378280.html:text/html},
}

@inproceedings{ali_batch_2020,
	title = {{BATCH}: {Machine} {Learning} {Inference} {Serving} on {Serverless} {Platforms} with {Adaptive} {Batching}},
	shorttitle = {{BATCH}},
	url = {https://ieeexplore.ieee.org/document/9355312},
	doi = {10.1109/SC41405.2020.00073},
	abstract = {Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker.},
	urldate = {2023-12-31},
	booktitle = {{SC20}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
	month = nov,
	year = {2020},
	note = {Journal Abbreviation: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {1--15},
}

@inproceedings{ghosh_caching_2020,
	title = {Caching {Techniques} to {Improve} {Latency} in {Serverless} {Architectures}},
	isbn = {2155-2509},
	url = {https://ieeexplore.ieee.org/abstract/document/9027427},
	doi = {10.1109/COMSNETS48256.2020.9027427},
	abstract = {Serverless computing has gained a significant traction in recent times because of its simplicity of development, deployment and fine-grained billing. However, while implementing complex services comprising databases, file stores, or more than one serverless function, the performance in terms of latency of serving requests often degrades severely. In this work, we analyze different serverless architectures with AWS Lambda services and compare their performance in terms of latency with a traditional virtual machine (VM) based approach. We observe that database access latency in serverless architecture is almost 14 times than that in VM based setup. Further, we introduce some caching strategies which can improve the response time significantly, and compare their performance.},
	urldate = {2023-12-31},
	booktitle = {2020 {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Ghosh, Bishakh Chandra and Addya, Sourav Kanti and Somy, Nishant Baranwal and Nath, Shubha Brata and Chakraborty, Sandip and Ghosh, Soumya K},
	month = jan,
	year = {2020},
	note = {Journal Abbreviation: 2020 International Conference on COMmunication Systems \& NETworkS (COMSNETS)},
	pages = {666--669},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YVWBKBMV\\9027427.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8RE8CKZB\\9027427.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\N537G5PZ\\9027427.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\R3CFKPS8\\Ghosh et al. - 2020 - Caching Techniques to Improve Latency in Serverles.pdf:application/pdf},
}

@inproceedings{vahidinia_cold_2020,
	title = {Cold {Start} in {Serverless} {Computing}: {Current} {Trends} and {Mitigation} {Strategies}},
	shorttitle = {Cold {Start} in {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9191377},
	doi = {10.1109/COINS49042.2020.9191377},
	abstract = {Serverless Computing is the latest cloud computing model, which facilitates application development. By adopting and leveraging the modern paradigm of Serverless Computing, developers do not need to manage the servers. In this computational model, the executables are independent functions that are individually deployed on a Serverless platform offering instant per-request elasticity. Such elasticity typically comes at the cost of the “Cold Starts” problem. This phenomenon is associated with a delay occurring due to provision a runtime container to execute the functions. Shortly after Amazon introduced this computing model with the AWS Lambda platform in 2014, several open source and commercial platforms also started embracing and offering this technology. Each platform has its own solution to deal with Cold Starts. The evaluation of the performance of each platform under the load and factors influencing the cold start problem has received much attention over the past few years. This paper provides a comprehensive overview on the recent advancements and state-of-the-art works in mitigating the cold start delay. Moreover, several sets of experiments have been performed to study the behavior of the AWS Lambda as the base platform with respect to the cold start delay.},
	urldate = {2024-01-01},
	booktitle = {2020 {International} {Conference} on {Omni}-layer {Intelligent} {Systems} ({COINS})},
	author = {Vahidinia, Parichehr and Farahani, Bahar and Aliee, Fereidoon Shams},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 International Conference on Omni-layer Intelligent Systems (COINS)},
	pages = {1--7},
}

@inproceedings{mcclenaghan_computational_2020,
	title = {Computational {Edge} for {Multiple} {Autonomous} {Objects}},
	url = {https://ieeexplore.ieee.org/document/9209547},
	doi = {10.1109/ICHMS49158.2020.9209547},
	abstract = {This research explores possibilities of creating software architectures for managing multiple autonomous objects in computational environments, which move away from clouds and use computational power of objects, at the edge of computing and communication networks. The emphasis is on shaping the behaviour of autonomous objects through human involvement in order to manipulate functions and change the purpose and levels of autonomy of these objects. The proposed computational model, generated from the software architectures, which gives rise to serverless and edge computing, should work across problem domain. By collecting relevant data and allowing a variable level of human input, the solution will enable us to choose, merge and combine multiple objects for a variety of tasks and according to environments in which autonomous objects reside.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Human}-{Machine} {Systems} ({ICHMS})},
	author = {McClenaghan, Karoline and Moholth, Ole Christian and Bjerknes, Jan Dyre},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Human-Machine Systems (ICHMS)},
	pages = {1--6},
}

@inproceedings{quaresma_controlling_2020,
	title = {Controlling {Garbage} {Collection} and {Request} {Admission} to {Improve} {Performance} of {FaaS} {Applications}},
	isbn = {2643-3001},
	url = {https://ieeexplore.ieee.org/document/9235063},
	doi = {10.1109/SBAC-PAD49847.2020.00033},
	abstract = {Runtime environments like Java's JRE, .NET's CLR, and Ruby's MRI, are popular choices for cloud-based applications and particularly in the Function as a Service (FaaS) serverless computing context. A critical component of runtime environments of these languages is the garbage collector (GC). The GC frees developers from manual memory management, which could potentially ease development and avoid bugs. The benefits of using the GC come with a negative impact on performance; that impact happens because either the GC needs to pause the runtime execution or competes with the running program for computational resources. In this work, we evaluated the usage of a technique - Garbage Collector Control Interceptor (GCI) - that eliminates the negative impact of GC on performance by controlling GC executions and transparently shedding requests while the collections are happening. We executed experiments simulating AWS Lambda's behavior and found that GCI is a viable solution. It benefited the user by improving the response time up to 10.86\% at 99.9th percentile and reducing cost by 7.22\%, but it also helped the platform provider by improving resource utilization by 14.52\%.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 32nd {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing} ({SBAC}-{PAD})},
	author = {Quaresma, David and Fireman, Daniel and Pereira, Thiago Emmanuel},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
	pages = {175--182},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QCKM9SEV\\9235063.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IG3YFWR9\\9235063.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WQETFDXK\\9235063.html:text/html},
}

@inproceedings{reuter_cost_2020,
	title = {Cost efficiency under mixed serverless and serverful deployments},
	url = {https://ieeexplore.ieee.org/document/9226321},
	doi = {10.1109/SEAA51224.2020.00049},
	abstract = {Function as a Service (FaaS) is an integral part of the serverless computing paradigm. It offers a true pay-per-use billing model and releases developers from the burden of managing the application stack. A discussion on whether and when this model is more appropriate for cloud computing users in terms of accruing costs compared to the more "traditional" delivery models has already been started by existing works. However, by treating this subject as a regular service selection problem, these approaches fail to exploit the space created by distributing the load between simultaneous FaaS and non-FaaS deployments of an application in a hybrid deployment model. This work aims to provide the means for application owners to decide which deployment scenario is cost optimal for their needs. In case this scenario is a hybrid deployment, the proposed approach also determines the optimal number of virtual machines that will need to be provisioned. An extensible and configurable FaasSimulator open source tool is presented for these purposes.},
	urldate = {2024-01-01},
	booktitle = {2020 46th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Reuter, Anja and Back, Timon and Andrikopoulos, Vasilios},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
	pages = {242--245},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\I9AWEV34\\Reuter et al. - 2020 - Cost efficiency under mixed serverless and serverf.pdf:application/pdf},
}

@inproceedings{birman_cost-effective_2020,
	title = {Cost-{Effective} {Malware} {Detection} as a {Service} {Over} {Serverless} {Cloud} {Using} {Deep} {Reinforcement} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9139646},
	doi = {10.1109/CCGrid49817.2020.00-51},
	abstract = {The current trends of cloud computing in general, and serverless computing in particular, affect multiple aspects of organizational activity. Organizations of all sizes are transitioning parts of their operations off-premise in order to reduce costs and scale their operations more efficiently. The field of network security is no exception, with many organizations taking advantage of the distributed and scalable cloud environment. Since the charging model for serverless computing is "pay as you go" (i.e., payment per action), a reduction in the number of required computations translates into significant cost savings. This understanding is also relevant to the field of malware detection, where organizations often deploy multiple types of detectors to increase detection accuracy. In this study, we utilize deep reinforcement learning to reduce computational costs in the cloud by selectively querying only a subset of available detectors. We demonstrate that our approach is not only effective both for on-premise and cloud-based computing architectures, but that applying it to serverless computing can reduce costs by an order of magnitude while maintaining near-optimal performance.},
	urldate = {2024-01-01},
	booktitle = {2020 20th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGRID})},
	author = {Birman, Yoni and Hindi, Shaked and Katz, Gilad and Shabtai, Asaf},
	month = may,
	year = {2020},
	note = {Journal Abbreviation: 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)},
	pages = {420--429},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JCTYHSMY\\9139646.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NJIP37GB\\9139646.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WHE55WYR\\9139646.html:text/html},
}

@inproceedings{mistry_demonstrating_2020,
	title = {Demonstrating the {Practicality} of {Unikernels} to {Build} a {Serverless} {Platform} at the {Edge}},
	isbn = {2330-2186},
	url = {https://ieeexplore.ieee.org/document/9407311},
	doi = {10.1109/CloudCom49646.2020.00001},
	abstract = {The rise of IoT has led to large volumes of personal data being produced at the network's edge. Most IoT applications process data in the cloud raising concerns over privacy and security. As many IoT applications are event-based and are implemented on cloud-based, serverless platforms, we've seen a number of proposals to deploy serverless solutions at the edge to address concerns over data transfer. However, conventional serverless platforms use container technology to run user-defined functions. Containers introduce their own issues regarding security - due to a large trusted computing base -, and performance issues including long initialisation times. Additionally, OpenWhisk a popular and widely used container-based serverless platform available for edge devices perform relatively poorly as we demonstrate in our evaluation. In this paper, we propose to investigate unikernel as a solution to build serverless platform at the edge, addressing in particular performance and security concerns. We present UniFaaS, a prototype edge-serverless platform which leverages unikernels - tiny library single-address-space operating systems that only contain the parts of the OS needed to run a given application - to execute functions. The result is a serverless platform with extremely low memory and CPU footprints, and excellent performance. UniFaaS has been designed to be deployed on low-powered single-board computer devices, such as Raspberry Pi or Arduino, without compromising on performance.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Cloud} {Computing} {Technology} and {Science} ({CloudCom})},
	author = {Mistry, Chetankumar and Stelea, Bogdan and Kumar, Vijay and Pasquier, Thomas},
	month = dec,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
	pages = {25--32},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BR8JLSQG\\9407311.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7UN3CJU3\\9407311.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3IWENSP2\\9407311.html:text/html},
}

@inproceedings{ihejimba_detectsignal_2020,
	title = {{DetectSignal}: {A} {Cloud}-{Based} {Traffic} {Signal} {Notification} {System} for the {Blind} and {Visually} {Impaired}},
	isbn = {2687-8860},
	shorttitle = {{DetectSignal}},
	url = {https://ieeexplore.ieee.org/document/9239004},
	doi = {10.1109/ISC251055.2020.9239004},
	abstract = {The ability to know when to cross the road at an intersection has always been a problem for the Visually Impaired or Blind person (VIB). In this paper, we present DetectSignal, a cloud-based Internet of Things (IoT) edge computing solution that provides a highly available, highly scalable, high-speed, and low latency traffic signal notification for the VIB. DetectSignal provides an interconnected traffic signal by reusing existing infrastructure and attaching a low-cost IoT device that connects with a traffic light and the public cloud. Our experimental results involve testing a notification system using a regular compute virtual machine and IoT edge-based serverless compute. The experimental results show that DetectSignal provides a more reliable solution that alleviates the current issues facing the VIB.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Smart} {Cities} {Conference} ({ISC2})},
	author = {Ihejimba, Chikadibia and Wenkstern, Rym Z.},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Smart Cities Conference (ISC2)},
	pages = {1--6},
}

@inproceedings{elsakhawy_faas2f_2020,
	title = {{FaaS2F}: {A} {Framework} for {Defining} {Execution}-{SLA} in {Serverless} {Computing}},
	shorttitle = {{FaaS2F}},
	url = {https://ieeexplore.ieee.org/document/9283723},
	doi = {10.1109/IEEECloudSummit48914.2020.00015},
	abstract = {The emergence of serverless computing has brought significant advancements to the delivery of computing resources to cloud users. With the abstraction of infrastructure, platform, and execution environments, the administration overhead of these layers is shifted to the cloud provider. Thus, cloud users can focus on the application layer while relying on the cloud provider to provision and operate the underlying layers. Furthermore, desirable features such as autoscaling and high-availability are implemented by the cloud provider and adopted by the user's code at no extra overhead. Despite such advancements, as applications transition from monolithic stand-alone deployments to the ephemeral and stateless microservice model of serverless computing, significant challenges must be overcome. These challenges pertain to the uniqueness of the conceptual and implementation models of serverless computing. In this paper, we investigate the modeling of the Service Level Agreement (SLA) of serverless functions' executions. We highlight how serverless SLA fundamentally differs from earlier cloud delivery models. We then propose an approach to define SLA for serverless functions in terms of resource utilization fingerprints for functions' executions, and a method to assess if executions adhere to that SLA. We conclude by presenting the empirical validation of our proposed approach, which was able to detect Execution-SLA violations with accuracy exceeding 76\% and up to 100\%.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {Cloud} {Summit}},
	author = {Elsakhawy, Mohamed and Bauer, Michael},
	month = oct,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE Cloud Summit},
	pages = {58--65},
}

@article{cornetta_fabrication-as--service_2020,
	title = {Fabrication-as-a-{Service}: {A} {Web}-{Based} {Solution} for {STEM} {Education} {Using} {Internet} of {Things}},
	volume = {7},
	issn = {2327-4662},
	shorttitle = {Fabrication-as-a-{Service}},
	url = {https://ieeexplore.ieee.org/document/8915690},
	doi = {10.1109/JIOT.2019.2956401},
	abstract = {Recently, fabrication laboratories (Fab Labs) have been shown to have a great impact on learners' academic and personal progress. As a result, an increasing effort is being put to integrate Fab Labs into schools' curricula. Yet, owing to the high cost of setting up and maintaining Fab Labs as well as the lack of sufficient funding for most schools and universities, only a limited number of institutions can afford them. In this article, we propose a new concept called Fabrication-as-a-Service (FaaS) that uses Internet of Things to democratize access to Fab Labs via enabling a wide learning community to remotely access these computer-controlled tools and equipment over the Internet. It employs a two-tier architecture consisting of a hub, deployed in the cloud, and a network of distributed Fab Labs. Each Fab Lab interacts with the hub and other digital labs via a Fab Lab Gateway. This is to support scalability and high availability of fabrication services as well as ensure the system's security. FaaS also adopts an innovative master-slave approach that uses inexpensive external hardware to monitor and control the activity of expensive fabrication equipment. This article also describes the FaaS deployment in the context of the European Union Horizon 2020 NEWTON project. Multiple scenarios have been deployed to fully illustrate the benefits of the FaaS architecture and to assess the performance of its communication protocol stack.},
	number = {2},
	urldate = {2024-01-01},
	journal = {IEEE Internet of Things Journal},
	author = {Cornetta, Gianluca and Touhafi, Abdellah and Togou, Mohammed Amine and Muntean, Gabriel-Miro},
	month = feb,
	year = {2020},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {1519--1530},
}

@article{savazzi_federated_2020,
	title = {Federated {Learning} {With} {Cooperating} {Devices}: {A} {Consensus} {Approach} for {Massive} {IoT} {Networks}},
	volume = {7},
	issn = {2327-4662},
	shorttitle = {Federated {Learning} {With} {Cooperating} {Devices}},
	url = {https://ieeexplore.ieee.org/document/8950073},
	doi = {10.1109/JIOT.2020.2964162},
	abstract = {Federated learning (FL) is emerging as a new paradigm to train machine learning (ML) models in distributed systems. Rather than sharing and disclosing the training data set with the server, the model parameters (e.g., neural networks' weights and biases) are optimized collectively by large populations of interconnected devices, acting as local learners. FL can be applied to power-constrained Internet of Things (IoT) devices with slow and sporadic connections. In addition, it does not need data to be exported to third parties, preserving privacy. Despite these benefits, a main limit of existing approaches is the centralized optimization which relies on a server for aggregation and fusion of local parameters; this has the drawback of a single point of failure and scaling issues for increasing network size. This article proposes a fully distributed (or serverless) learning approach: the proposed FL algorithms leverage the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. The approach lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing, with intelligence distributed over the end devices. The proposed methodology is verified by the experimental data sets collected inside an Industrial IoT (IIoT) environment.},
	number = {5},
	urldate = {2024-01-01},
	journal = {IEEE Internet of Things Journal},
	author = {Savazzi, Stefano and Nicoli, Monica and Rampa, Vittorio},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {4641--4654},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\DCT7SGCG\\Savazzi et al. - 2020 - Federated Learning With Cooperating Devices A Con.pdf:application/pdf},
}

@inproceedings{savazzi_federated_2020-1,
	title = {Federated {Learning} with {Mutually} {Cooperating} {Devices}: {A} {Consensus} {Approach} {Towards} {Server}-{Less} {Model} {Optimization}},
	isbn = {2379-190X},
	shorttitle = {Federated {Learning} with {Mutually} {Cooperating} {Devices}},
	url = {https://ieeexplore.ieee.org/document/9054055},
	doi = {10.1109/ICASSP40776.2020.9054055},
	abstract = {Federated learning (FL) is emerging as a new paradigm for training a machine learning model in cooperative networks. The model parameters are optimized collectively by large populations of interconnected devices, acting as cooperative learners that exchange local model updates with the server, rather than user data. The FL framework is however centralized, as it relies on the server for fusion of the model updates and as such it is limited by a single point of failure. In this paper we propose a distributed FL approach that performs a decentralized fusion of local model parameters by leveraging mutual cooperation between the devices and local (in-network) data operations via consensus-based methods. Communication with the server can be partially, or fully, replaced by in-network operations, scaling down the traffic load on the server as well as paving the way towards a fully serverless FL approach. This proposal also lays the groundwork for integration of FL methods within future (beyond 5G) wireless networks characterized by distributed and decentralized connectivity. The proposed algorithms are implemented and published as open source. They are also designed and verified by experimental data.},
	urldate = {2024-01-01},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Savazzi, Stefano and Nicoli, Monica and Rampa, Vittorio and Kianoush, Sanaz},
	month = may,
	year = {2020},
	note = {Journal Abbreviation: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages = {3937--3941},
}

@inproceedings{cordingly_implications_2020,
	title = {Implications of {Programming} {Language} {Selection} for {Serverless} {Data} {Processing} {Pipelines}},
	url = {https://ieeexplore.ieee.org/document/9251194},
	doi = {10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00120},
	abstract = {Serverless computing platforms have emerged offering software engineers an option for application hosting without the need to configure servers or manage scaling while guaranteeing high availability and fault tolerance. In the ideal scenario, a developer should be able to create a microservice, deploy it to a serverless platform, and never have to manage or configure anything; a truly serverless platform. The current implementation of serverless computing platforms is known as Function-as-a-Service or FaaS. Adoption of FaaS platforms, however, requires developers to address a major question- what programming language should functions be written in? To investigate this question, we implemented identical multi-function data processing pipelines in Java, Python, Go, and Node.js. Using these pipelines as a case study, we ran experiments tailored to investigate FaaS data processing performance. Specifically, we investigate data processing performance implications: for data payloads of varying size, with cold and warm serverless infrastructure, over scaling workloads, and when varying the available function memory. We found that Node.js had up to 94\% slower runtime vs. Java for the same workload. In other scenarios, Java had 20\% slower runtime than Go resulting from differences in how the cloud provider orchestrates infrastructure for each language with respect to the serverless freeze/thaw lifecycle. We found that no single language provided the best performance for every stage of a data processing pipeline and the fastest pipeline could be derived by combining a hybrid mix of languages to optimize performance.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Cordingly, Robert and Yu, Hanfei and Hoang, Varik and Perez, David and Foster, David and Sadeghi, Zohreh and Hatchett, Rashad and Lloyd, Wes J.},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
	pages = {704--711},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XUVH5H9Z\\9251194.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\U9WBCE3I\\9251194.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GBJQJCYR\\9251194.html:text/html},
}

@inproceedings{fan_knative_2020,
	title = {Knative {Autoscaler} {Optimize} {Based} on {Double} {Exponential} {Smoothing}},
	url = {https://ieeexplore.ieee.org/document/9141858},
	doi = {10.1109/ITOEC49072.2020.9141858},
	abstract = {Knative is a popular Kubernetes-based platform for managing serverless workloads with two main components Eventing and Serving. The Serving primitive helps to deploy serverless apps as Knative services and automatically scale them, even down to zero. However, the serving module uses a moving average method to calculate the number of pods, that calculated based on past data and not good at accounting for future changes. In this paper, we use double exponential smoothing to optimize the calculation of the number of pods. Preliminary experiments show that the results are better than the moving average.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	author = {Fan, Dayong and He, Dongzhi},
	month = jun,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)},
	pages = {614--617},
}

@inproceedings{chahal_migrating_2020,
	title = {Migrating {Large} {Deep} {Learning} {Models} to {Serverless} {Architecture}},
	url = {https://ieeexplore.ieee.org/document/9307673},
	doi = {10.1109/ISSREW51248.2020.00047},
	abstract = {Serverless computing platform is emerging as a solution for event-driven artificial intelligence applications. Function-as-a-Service (FaaS) using serverless computing paradigm provides high performance and low cost solutions for deploying such applications on cloud while minimizing the operational logic. Using FaaS for efficient deployment of complex applications, such as natural language processing (NLP) and image processing, containing large deep learning models will be an advantage. However, constrained resources and stateless nature of FaaS offers numerous challenges while deploying such applications. In this work, we discuss the methodological suggestions and their implementation for deploying pre-trained large size machine learning and deep learning models on FaaS. We also evaluate the performance and deployment cost of an enterprise application, consisting of suite of deep vision preprocessing algorithms and models, on VM and FaaS platform. Our evaluation shows that migration from monolithic to FaaS platform significantly improves the performance of the application at a reduced cost.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Chahal, Dheeraj and Ojha, Ravi and Ramesh, Manju and Singhal, Rekha},
	month = oct,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
	keywords = {FaaS, Serverless, AI, Function-as-a-service, Deep learning, serverless, Serverless architecture, Computing paradigm, Computation theory, Image processing, Software as a service (SaaS), cloud, Computing platform, Event-driven, High-low, Learning algorithms, Learning models, Low-cost solution, Natural language processing systems, Performance costs},
	pages = {111--116},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y3JRURBN\\9307673.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2Z5M24YA\\9307673.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6CZILCUK\\9307673.html:text/html},
}

@inproceedings{george_nanolambda_2020,
	title = {{NanoLambda}: {Implementing} {Functions} as a {Service} at {All} {Resource} {Scales} for the {Internet} of {Things}},
	shorttitle = {{NanoLambda}},
	url = {https://ieeexplore.ieee.org/document/9355717},
	doi = {10.1109/SEC50012.2020.00035},
	abstract = {Internet of Things (IoT) devices are becoming increasingly prevalent in our environment, yet the process of programming these devices and processing the data they produce remains difficult. Typically, data is processed on device, involving arduous work in low level languages, or data is moved to the cloud, where abundant resources are available for Functions as a Service (FaaS) or other handlers. FaaS is an emerging category of flexible computing services, where developers deploy self-contained functions to be run in portable and secure containerized environments; however, at the moment, these functions are limited to running in the cloud or in some cases at the “edge” of the network using resource rich, Linux-based systems.In this paper, we present NanoLambda, a portable platform that brings FaaS, high-level language programming, and familiar cloud service APIs to non-Linux and microcontroller-based IoT devices. To enable this, NanoLambda couples a new, minimal Python runtime system that we have designed for the least capable end of the IoT device spectrum, with API compatibility for AWS Lambda and S3. NanoLambda transfers functions between IoT devices (sensors, edge, cloud), providing power and latency savings while retaining the programmer productivity benefits of high-level languages and FaaS. A key feature of NanoLambda is a scheduler that intelligently places function executions across multi-scale IoT deployments according to resource availability and power constraints. We evaluate a range of applications that use NanoLambda to run on devices as small as the ESP8266 with 64KB of ram and 512KB flash storage.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE}/{ACM} {Symposium} on {Edge} {Computing} ({SEC})},
	author = {George, Gareth and Bakir, Fatih and Wolski, Rich and Krintz, Chandra},
	month = nov,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE/ACM Symposium on Edge Computing (SEC)},
	pages = {220--231},
}

@inproceedings{pascale_nanoservice_2020,
	title = {Nanoservice {Infrastructure} {Notation} ({NINo}) and the {ASPIRE} {Interns}},
	isbn = {2330-331X},
	url = {https://ieeexplore.ieee.org/document/9397808},
	doi = {10.1109/ISEC49744.2020.9397808},
	abstract = {NINo is a future DevOps / Data Science pipeline tool that is being developed by JHU APL and two ASPIRE interns. The goal of this capability is to expose function-level capabilities, via either a simple application or configuration file, for use in Docker [1], Serverless Architectures [2], or data science/analytic pipelines. The goal is similar to efforts such as Metaparticle [3] and Source-to-Image[4] that aim to lower the barrier to horizontal scaling of data processing and analysis capabilities. In previous years ASPIRE interns have developed tools to ease the acceptance of DevOps principles in JHU APL. They have created a web application, Harmonia, that asked users a few simple questions and supplied the scaffolding for a software project with artifacts to support sound software engineering processes. The lack of user interest has driven us to a more focused objective. NINo will focus on easing deployment to cloud environments. Ideally, any person could develop cloud-based data science services. The team and its work has been organized in an asynchronous and agile manner. There have been three members working on three subsystems: configuration, framework/integration, and artifact generation. An incremental and prototype-driven approach has allowed for creation of increasingly more functional software as internship has proceeded. Interns have been given extensive control over their development processes and have investigated the programming frameworks used. While the initial stages have not resulted in a complete system, the interns have improved their programming skills and complete common coding challenges. The team is close to integration testing and initial demonstration. As the academic year closes, team members will work on design improvement, refactoring, and generation of future feature requests from prospective users. One summer intern will focus on developing a user interface for configuring and observing results.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {Integrated} {STEM} {Education} {Conference} ({ISEC})},
	author = {Pascale, Chancellor T. and Rice, Maria and Sharma, Shiva},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE Integrated STEM Education Conference (ISEC)},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IYK8IPKC\\9397808.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\V26294YW\\9397808.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VK5EQB4V\\9397808.html:text/html},
}

@inproceedings{bhagat_natural_2020,
	title = {Natural {Language} {Processing} on {Diverse} {Data} {Layers} {Through} {Microservice} {Architecture}},
	url = {https://ieeexplore.ieee.org/document/9298027},
	doi = {10.1109/INOCON50539.2020.9298027},
	abstract = {With the rapid growth in Natural Language Processing (NLP), all types of industries find a need for analyzing a massive amount of data. Sentiment analysis is becoming a more exciting area for the businessmen and researchers in Text mining \& NLP. This process includes the calculation of various sentiments with the help of text mining. Supplementary to this, the world is connected through Information Technology and, businesses are moving toward the next step of the development to make their system more intelligent. Microservices have fulfilled the need for development platforms which help the developers to use various development tools (Languages and applications) efficiently. With the consideration of data analysis for business growth, data security becomes a major concern in front of developers. This paper gives a solution to keep the data secured by providing required access to data scientists without disturbing the base system software. This paper has discussed data storage and exchange policies of microservices through common JavaScript Object Notation (JSON) response which performs the sentiment analysis of customer's data fetched from various microservices through secured APIs.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} for {Innovation} in {Technology} ({INOCON})},
	author = {Bhagat, Vandana and J, Bastin Robins},
	month = nov,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference for Innovation in Technology (INOCON)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\68L8GKZM\\9298027.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PFNIX2RD\\9298027.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8ZMLEYP9\\9298027.html:text/html},
}

@inproceedings{luthra_operator_2020,
	title = {Operator as a {Service}: {Stateful} {Serverless} {Complex} {Event} {Processing}},
	shorttitle = {Operator as a {Service}},
	url = {https://ieeexplore.ieee.org/document/9378142},
	doi = {10.1109/BigData50022.2020.9378142},
	abstract = {Complex Event Processing (CEP) is a powerful paradigm for scalable data management that is employed in many real-world scenarios such as detecting credit card fraud in banks. The so-called complex events are expressed using a specification language that is typically implemented and executed on a specific runtime system. While the tight coupling of these two components has been regarded as the key for supporting CEP at high performance, such dependencies pose several inherent challenges as follows. (1) Application development atop a CEP system requires extensive knowledge of how the runtime system operates, which is typically highly complex in nature. (2) The specification language dependence requires the need of domain experts and further restricts and steepens the learning curve for application developers.In this paper, we propose CEPless, a scalable data management system that decouples the specification from the runtime system by building on the principles of serverless computing. CEPless provides "operator as a service" and offers flexibility by enabling the development of CEP application in any specification language while abstracting away the complexity of the CEP runtime system. As part of CEPless, we designed and evaluated novel mechanisms for in-memory processing and batching that enable the stateful processing of CEP operators even under high rates of ingested events. Our evaluation demonstrates that CEPless can be easily integrated into existing CEP systems like Apache Flink while attaining similar throughput under high scale of events (up to 100K events per second) and dynamic operator update in 238 ms.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Luthra, Manisha and Hennig, Sebastian and Razavi, Kamran and Wang, Lin and Koldehofe, Boris},
	month = dec,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Big Data (Big Data)},
	pages = {1964--1973},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\E9PME76D\\Luthra et al. - 2020 - Operator as a Service Stateful Serverless Complex.pdf:application/pdf},
}

@inproceedings{gupta_oversketched_2020,
	title = {{OverSketched} {Newton}: {Fast} {Convex} {Optimization} for {Serverless} {Systems}},
	shorttitle = {{OverSketched} {Newton}},
	url = {https://ieeexplore.ieee.org/document/9378289},
	doi = {10.1109/BigData50022.2020.9378289},
	abstract = {Motivated by recent developments in serverless systems for large-scale computation as well as improvements in scalable randomized matrix algorithms, we develop OverSketched Newton, a randomized Hessian-based optimization algorithm to solve large-scale convex optimization problems in serverless systems. OverSketched Newton leverages matrix sketching ideas from Randomized Numerical Linear Algebra to compute the Hessian approximately. These sketching methods lead to inbuilt resiliency against stragglers that are a characteristic of serverless architectures. Depending on whether or not the problem is strongly convex, we propose different iteration updates using the approximate Hessian. For both cases, we establish convergence guarantees for OverSketched Newton, and we empirically validate our results by solving large-scale supervised learning problems on real-world datasets. Experiments demonstrate a reduction of 50\% in total running time on AWS Lambda, compared to state-of-the-art distributed optimization schemes.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Gupta, Vipul and Kadhe, Swanand and Courtade, Thomas and Mahoney, Michael W. and Ramchandran, Kannan},
	month = dec,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Big Data (Big Data)},
	pages = {288--297},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\YMRFYTZY\\Gupta et al. - 2020 - OverSketched Newton Fast Convex Optimization for .pdf:application/pdf},
}

@inproceedings{somu_panopticon_2020,
	title = {{PanOpticon}: {A} {Comprehensive} {Benchmarking} {Tool} for {Serverless} {Applications}},
	isbn = {2155-2509},
	shorttitle = {{PanOpticon}},
	url = {https://ieeexplore.ieee.org/document/9027346},
	doi = {10.1109/COMSNETS48256.2020.9027346},
	abstract = {Serverless computing manifests as the FaaS offering where clients submit code to be managed and run by the service provider in the place of hiring and managing VMs for this purpose. Multiple Cloud Service providers have come up with their own implementations of FaaS infrastructure providing end-users with a multitude of choices. Each such platform provides a non-overlapping set of features which satisfies a subset of users. Further the design of the platform dictates the performance overheads of triggering the function. A tool that automates capturing how a function behaves under different configurations of a platform and across platforms will, therefore, be useful for end-users intending to deploy applications as a collection of FaaS units. In spite of the presence of a few benchmarking tools for FaaS offerings, they lack the comprehensive breadth required to understand the performance aspects of the design choices made by the end-users. Most tools focus on tuning resource parameters like memory, CPU requirements and measure metrics like execution time. They lack the option to measure the effects of features such as function chaining and choice of function triggers. We present PanOpticon - a tool that automates the deployment of FaaS applications on different platforms under a set of tunable configuration choices and presents the users with performance measurements for each configuration and platform.},
	urldate = {2024-01-01},
	booktitle = {2020 {International} {Conference} on {COMmunication} {Systems} \& {NETworkS} ({COMSNETS})},
	author = {Somu, Nikhila and Daw, Nilanjan and Bellur, Umesh and Kulkarni, Purushottam},
	month = jan,
	year = {2020},
	note = {Journal Abbreviation: 2020 International Conference on COMmunication Systems \& NETworkS (COMSNETS)},
	pages = {144--151},
}

@inproceedings{khatri_potential_2020,
	title = {Potential {Bottleneck} and {Measuring} {Performance} of {Serverless} {Computing}: {A} {Literature} {Study}},
	shorttitle = {Potential {Bottleneck} and {Measuring} {Performance} of {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9197837},
	doi = {10.1109/ICRITO48877.2020.9197837},
	abstract = {Trending form of cloud computing is Serverless computing, where developer just needs to focus on his code rather than worrying about server management. In serverless computing, application is nothing but collection of one or more functions, written for specific business functionality, which triggers on an event. There are various cloud service providers, i.e. Amazon, Microsoft, Google, IBM, etc. who provide serverless services, on pay as you use and auto scalable solution to execute the application code as a function. The developer just needs to upload the code for execution. The performance of the serverless computing may vary due to dynamic configuration of the solution, technologies and different technology used by the service provider.This paper reviews various past and recent work in the serverless computing to identify possible bottlenecks and the scope of measuring performance of serverless computing. It will also put some light to leverage machine learning in various possible ways to do performance engineering for future research.},
	urldate = {2024-01-01},
	booktitle = {2020 8th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Khatri, Deepak and Khatri, Sunil Kumar and Mishra, Deepti},
	month = jun,
	year = {2020},
	note = {Journal Abbreviation: 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)},
	pages = {161--164},
}

@inproceedings{cordingly_predicting_2020,
	title = {Predicting {Performance} and {Cost} of {Serverless} {Computing} {Functions} with {SAAF}},
	url = {https://ieeexplore.ieee.org/document/9251165},
	doi = {10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00111},
	abstract = {Next generation software built for the cloud recently has embraced serverless computing platforms that use temporary infrastructure to host microservices offering building blocks for resilient, loosely coupled systems that are scalable, easy to manage, and extend. Serverless architectures enable decomposing software into independent components packaged and run using isolated containers or microVMs. This decomposition approach enables application hosting using very fine-grained cloud infrastructure enabling cost savings as deployments are billed granularly for resource use. Adoption of serverless platforms promise reduced hosting costs while achieving high availability, fault tolerance, and dynamic elasticity. These benefits are offset by pricing obfuscation, as performance variance from CPU heterogeneity, multitenancy, and provisioning variation obscure the true cost of hosting applications with serverless platforms. Where determining hosting costs for traditional VM-based application deployments simply involves accounting for the number of VMs and their uptime, predicting hosting costs for serverless applications can be far more complex. To address these challenges, we introduce the Serverless Application Analytics Framework (SAAF), a tool that allows profiling FaaS workload performance, resource utilization, and infrastructure to enable accurate performance predictions. We apply Linux CPU time accounting principles and multiple regression to estimate FaaS function runtime. We predict runtime using a series of increasingly variant compute bound workloads that execute across heterogeneous CPUs, different memory settings, and to alternate FaaS platforms evaluating our approach for 77 different scenarios. We found that the mean absolute percentage error of our runtime predictions for these scenarios was just 3.49\% resulting in an average cost error of 6.46 for 1-million FaaS function workloads averaging 150.45 in price.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Cordingly, Robert and Shu, Wen and Lloyd, Wes J.},
	month = aug,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
	pages = {640--649},
}

@article{cho_qos-aware_2020,
	title = {{QoS}-{Aware} {Workload} {Distribution} in {Hierarchical} {Edge} {Clouds}: {A} {Reinforcement} {Learning} {Approach}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{QoS}-{Aware} {Workload} {Distribution} in {Hierarchical} {Edge} {Clouds}},
	url = {https://ieeexplore.ieee.org/document/9237967},
	doi = {10.1109/ACCESS.2020.3033421},
	abstract = {Recently, edge computing is getting attention as a new computing paradigm that is expected to achieve short-delay and high-throughput task offloading for large scale Internet-of-Things (IoT) applications. In edge computing, workload distribution is one of the most critical issues that largely influences the delay and throughput performance of edge clouds, especially in distributed Function-as-a-Service (FaaS) over networked edge nodes. In this paper, we propose the Resource Allocation Control Engine with Reinforcement learning (RACER), which provides an efficient workload distribution strategy to reduce the task response slowdown with per-task response time Quality-of-Service (QoS). First, we present a novel problem formulation with the per-task QoS constraint derived from the well-known token bucket mechanism. Second, we employ a problem relaxation to reduce the overall computation complexity by compromising just a bit of optimality. Lastly, we take the deep reinforcement learning approach as an alternative solution to the workload distribution problem to cope with the uncertainty and dynamicity of underlying environments. Evaluation results show that RACER achieves a significant improvement in terms of per-task QoS violation ratio, average slowdown, and control efficiency, compared to AREA, a state-of-the-art workload distribution method.},
	urldate = {2024-01-01},
	journal = {IEEE Access},
	author = {Cho, Chunglae and Shin, Seungjae and Jeon, Hongseok and Yoon, Seunghyun},
	year = {2020},
	note = {Conference Name: IEEE Access},
	pages = {193297--193313},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ENIULN73\\9237967.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F3XGEFJ7\\9237967.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IF3TKGXW\\9237967.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\NHPU5V4B\\Cho et al. - 2020 - QoS-Aware Workload Distribution in Hierarchical Ed.pdf:application/pdf},
}

@inproceedings{spillner_rule-based_2020,
	title = {Rule-{Based} {Resource} {Matchmaking} for {Composite} {Application} {Deployments} across {IoT}-{Fog}-{Cloud} {Continuums}},
	url = {https://ieeexplore.ieee.org/document/9302804},
	doi = {10.1109/UCC48980.2020.00053},
	abstract = {Where shall my new shiny application run? Hundreds of such questions are asked by software engineers who have many cloud services at their disposition, but increasingly also many other hosting options around managed edge devices and fog spectrums, including for functions and container hosting (FaaS/CaaS). Especially for composite applications prevalent in this field, the combinatorial deployment space is exploding. We claim that a systematic and automated approach is unavoidable in order to scale functional decomposition applications further so that each hosting facility is fully exploited. To support engineers while they transition from cloud-native to continuum-native, we provide a rule-based matchmaker called RBMM that combines several decision factors typically present in software description formats and applies rules to them. Using the MaestroNG orchestrator and OsmoticToolkit, we also contribute an integration of the matchmaker into an actual deployment environment.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE}/{ACM} 13th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Spillner, Josef and Gkikopoulos, Panagiotis and Buzachis, Alina and Villari, Massimo},
	month = dec,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)},
	pages = {336--341},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\6RWJM4X4\\Spillner et al. - 2020 - Rule-Based Resource Matchmaking for Composite Appl.pdf:application/pdf},
}

@inproceedings{zuk_scheduling_2020,
	title = {Scheduling {Methods} to {Reduce} {Response} {Latency} of {Function} as a {Service}},
	isbn = {2643-3001},
	url = {https://ieeexplore.ieee.org/document/9235070},
	doi = {10.1109/SBAC-PAD49847.2020.00028},
	abstract = {Function as a Service (FaaS) permits cloud customers to deploy to cloud individual functions, in contrast to complete virtual machines or Linux containers. All major cloud providers offer FaaS products (Amazon Lambda, Google Cloud Functions, Azure Serverless); there are also popular open-source implementations (Apache OpenWhisk) with commercial offerings (Adobe I/O Runtime, IBM Cloud Functions). A new feature of FaaS is function composition: a function may (sequentially) call another function, which, in turn, may call yet another function - forming a chain of invocations. From the perspective of the infrastructure, a composed FaaS is less opaque than a virtual machine or a container. We show that this additional information enables the infrastructure to reduce the response latency. In particular, knowing the sequence of future invocations, the infrastructure can schedule these invocations along with environment preparation. We model resource management in FaaS as a scheduling problem combining (1) sequencing of invocations, (2) deploying execution environments on machines, and (3) allocating invocations to deployed environments. For each aspect, we propose heuristics. We explore their performance by simulation on a range of synthetic workloads. Our results show that if the setup times are long compared to invocation times, algorithms that use information about the composition of functions consistently outperform greedy, myopic algorithms, leading to significant decrease in response latency.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 32nd {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing} ({SBAC}-{PAD})},
	author = {Zuk, Pawel and Rzadca, Krzysztof},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
	pages = {132--140},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\H2AHI8RE\\Zuk and Rzadca - 2020 - Scheduling Methods to Reduce Response Latency of F.pdf:application/pdf},
}

@inproceedings{shaw_scalable_2020,
	title = {Scalable {IoT} {Solution} using {Cloud} {Services} – {An} {Automobile} {Industry} {Use} {Case}},
	url = {https://ieeexplore.ieee.org/document/9243544},
	doi = {10.1109/I-SMAC49090.2020.9243544},
	abstract = {The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.},
	urldate = {2024-01-01},
	booktitle = {2020 {Fourth} {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC})},
	author = {Shaw, Ankit Kumar and Chakraborty, Amit and Mohapatra, Debaniranjan and Dutta, Subrata},
	month = oct,
	year = {2020},
	note = {Journal Abbreviation: 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)},
	pages = {326--331},
}

@article{nithya_sdcf_2020,
	title = {{SDCF}: {A} {Software}-{Defined} {Cyber} {Foraging} {Framework} for {Cloudlet} {Environment}},
	volume = {17},
	issn = {1932-4537},
	shorttitle = {{SDCF}},
	url = {https://ieeexplore.ieee.org/document/9163409},
	doi = {10.1109/TNSM.2020.3015657},
	abstract = {The cloudlets can be deployed over mobile devices or even fixed state powerful servers that can provide services to its users in physical proximity. Executing workloads on cloudlets involves challenges centering on limited computing resources. Executing Virtual Machine (VM) based workloads for cloudlets does not scale due to the high computational demands of a VM. Another approach is to execute container-based workloads on cloudlets. However, container-based methods suffer from the cold-start problem, making it unfit for mobile edge computing scenarios. In this work, we introduce executing serverless functions on Web-assembly as workloads for both mobile and fixed state cloudlets. To execute the serverless workload on mobile cloudlets, we built a lightweight Web-assembly runtime. The orchestration of workloads and management of cloudlets or serverless runtime is done by introducing software-defined Cyber Foraging (SDCF) framework, which is a hybrid controller including a control plane for local networks and cloudlets. The SDCF framework integrates the management of cloudlets by utilizing the control plane traffic of the underlying network and thus avoids the extra overhead of cloudlet control plane traffic management. We evaluate SDCF using three use cases: (1) Price aware resource allocation (2) Energy aware resource scheduling for mobile cloudlets (3) Mobility pattern aware resource scheduling in mobile cloudlets. Through the virtualization of cloudlet resources, SDCF preserves minimal maintenance property by providing a centralized approach for configuring and management of cloudlets.},
	number = {4},
	urldate = {2024-01-01},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Nithya, S. and Sangeetha, M. and Prethi, K. N. Apinaya and Sahoo, Kshira Sagar and Panda, Sanjaya Kumar and Gandomi, Amir H.},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	pages = {2423--2435},
}

@inproceedings{mejia_serverless_2020,
	title = {Serverless based control and monitoring for search and rescue robots},
	isbn = {2166-0727},
	url = {https://ieeexplore.ieee.org/document/9140444},
	doi = {10.23919/CISTI49556.2020.9140444},
	abstract = {The number and scale of disasters, being natural or human made, has influenced the creation of search and rescue missions and the use of modern technologies in them. Robots have become an attractive and more common tool in these operations. Even though there still are challenges to overcome with them. These challenges arise predominantly in the way they are controlled and monitored. Challenges like complex and costly infrastructure are the main obstacle to have a widespread use of these robots. This article presents an architecture for the control and monitoring of search and rescue robots based in serverless technologies to reduce infrastructure complexity, cost and setup. A custom methodology was used based on investigation, design, implementation and validation to develop this architecture. Design considerations, usefulness, limitations are analyzed and discussed. Overall effectiveness of the architecture in search and rescue missions was found a future work for improvement has been established.},
	urldate = {2024-01-01},
	booktitle = {2020 15th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Mejía, Alexander and Marcillo, Diego and Guaño, Miguel and Gualotuña, Tatiana},
	month = jun,
	year = {2020},
	note = {Journal Abbreviation: 2020 15th Iberian Conference on Information Systems and Technologies (CISTI)},
	keywords = {cloud services, Control and monitoring, Design considerations, Firebase, Information systems, Information use, Modern technologies, Overall effectiveness, robots, Robots, search and rescue, Search and rescue, Search and rescue robot, serverless technologies},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YK7D2YTY\\9140444.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\27BANS7K\\9140444.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CGDCI9M5\\9140444.html:text/html},
}

@inproceedings{kelly_serverless_2020,
	title = {Serverless {Computing}: {Behind} the {Scenes} of {Major} {Platforms}},
	isbn = {2159-6190},
	shorttitle = {Serverless {Computing}},
	url = {https://ieeexplore.ieee.org/abstract/document/9284261},
	doi = {10.1109/CLOUD49709.2020.00050},
	abstract = {Serverless computing offers an event driven pay-as-you-go framework for application development. A key selling point is the concept of no back-end server management, allowing developers to focus on application functionality. This is achieved through severe abstraction of the underlying architecture the functions run on. We examine the underlying architecture and report on the performance of serverless functions and how they are effected by certain factors such as memory allocation and interference caused by load induced by other users on the platform. Specifically, we focus on the serverless offerings of the four largest platforms; AWS Lambda, Google Cloud Functions, Microsoft Azure Functions and IBM Cloud Functions. In this paper, we observe and contrast between these platforms in their approach to the common issue of “cold starts”, we devise a means to unveil the underlying architecture serverless functions execute on and we investigate the effects of interference from load on the platform over the time span of one month.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Kelly, Daniel and Glavin, Frank and Barrett, Enda},
	month = oct,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 13th International Conference on Cloud Computing (CLOUD)},
	pages = {304--312},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TT5PWCAK\\9284261.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D74M3HF4\\9284261.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IBSUAVMA\\9284261.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\U56EBEYL\\Kelly et al. - 2020 - Serverless Computing Behind the Scenes of Major P.pdf:application/pdf},
}

@inproceedings{gupta_serverless_2020,
	title = {Serverless {Straggler} {Mitigation} using {Error}-{Correcting} {Codes}},
	isbn = {2575-8411},
	url = {https://ieeexplore.ieee.org/document/9355638},
	doi = {10.1109/ICDCS47774.2020.00019},
	abstract = {Inexpensive cloud services, such as serverless computing, are often vulnerable to straggling nodes that increase the end-to-end latency for distributed computation. We propose and implement simple yet principled approaches for straggler mitigation in serverless systems for matrix multiplication and evaluate them on several common applications from machine learning and high-performance computing. The proposed schemes are inspired by error-correcting codes and employ parallel encoding and decoding over the data stored in the cloud using serverless workers. This creates a fully distributed computing framework without using a master node to conduct encoding or decoding, which removes the computation, communication and storage bottleneck at the master. On the theory side, we establish that our proposed scheme is asymptotically optimal in terms of decoding time and provide a lower bound on the number of stragglers it can tolerate with high probability. Through extensive experiments, we show that our scheme outperforms existing schemes such as speculative execution and other coding theoretic methods by at least 25\%.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 40th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Gupta, Vipul and Carrano, Dominic and Yang, Yaoqing and Shankar, Vaishaal and Courtade, Thomas and Ramchandran, Kannan},
	month = nov,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)},
	pages = {135--145},
}

@inproceedings{nupponen_serverless_2020,
	title = {Serverless: {What} it {Is}, {What} to {Do} and {What} {Not} to {Do}},
	shorttitle = {Serverless},
	url = {https://ieeexplore.ieee.org/document/9095731},
	doi = {10.1109/ICSA-C50368.2020.00016},
	abstract = {Serverless, the new buzzword, has been gaining a lot of attention from the developers and industry. Cloud vendors such as AWS and Microsoft have hyped the architecture almost everywhere, from practitioners' conferences to local events, to blog posts. In this work, we introduce serverless functions (also known as Function-as-a-Service or FaaS), together with on bad practices experienced by practitioners, members of the Tampere Serverless Meetup group.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Nupponen, Jussi and Taibi, Davide},
	month = mar,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Software Architecture Companion (ICSA-C)},
	pages = {49--50},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I2B8K9NV\\9095731.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KFKBT6IE\\9095731.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9W7DQJ9C\\9095731.html:text/html},
}

@inproceedings{zhang_stoic_2020,
	title = {{STOIC}: {Serverless} {Teleoperable} {Hybrid} {Cloud} for {Machine} {Learning} {Applications} on {Edge} {Device}},
	shorttitle = {{STOIC}},
	url = {https://ieeexplore.ieee.org/document/9156239},
	doi = {10.1109/PerComWorkshops48775.2020.9156239},
	abstract = {Serverless computing is a promising new event-driven programming model that was designed by cloud vendors to expedite the development and deployment of scalable web services on cloud computing systems. Using the model, developers write applications that consist of simple, independent, stateless functions that the cloud invokes on-demand (i.e. elastically), in response to system-wide events (data arrival, messages, web requests, etc.). In this work, we present STOIC (Serverless TeleOperable HybrId Cloud), an application scheduling and deployment system that extends the serverless model in two ways. First, it uses the model in a distributed setting and schedules application functions across multiple cloud systems. Second, STOIC supports serverless function execution using hardware acceleration (e.g. GPU resources) when available from the underlying cloud system. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multi-tier (e.g. edge-cloud) deployments. We find that STOIC's combined use of edge and cloud resources is able to outperform using either cloud in isolation for the applications and datasets that we consider.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} ({PerCom} {Workshops})},
	author = {Zhang, Michael and Krintz, Chandra and Wolski, Rich},
	month = mar,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R66LGU9G\\9156239.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2DD5BA8D\\9156239.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WSFDIHJC\\9156239.html:text/html},
}

@inproceedings{kuhlenkamp_ifs_2020,
	title = {The {Ifs} and {Buts} of {Less} is {More}: {A} {Serverless} {Computing} {Reality} {Check}},
	shorttitle = {The {Ifs} and {Buts} of {Less} is {More}},
	url = {https://ieeexplore.ieee.org/document/9096486},
	doi = {10.1109/IC2E48712.2020.00023},
	abstract = {Serverless computing defines a pay-as-you-go cloud execution model, where the unit of computation is a function that a cloud provider executes and auto-scales on behalf of a cloud consumer. Serverless suggests not (or less) caring about servers but focusing (more) on business logic expressed in functions. Server'less' may be `more' when getting developer expectations and platform propositions right and when engineering solutions that take specific behavior and constraints of (current) Function-as-a-Service platforms into account. To this end, in this invited paper, we present a summary of findings and lessons learned from a series of research experiments conducted over the past two years. We argue that careful attention must be placed on the promises associated with the serverless model, provide a reality-check for five common assumptions, and suggest ways to mitigate unwanted effects. Our findings focus on application workload distribution and computational processing complexity, the specific auto-scaling mechanisms in place, the behavior and strategies implemented with operational tasks, the constraints and limitations existing when composing functions, and the costs of executing functions.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Kuhlenkamp, Jörn and Werner, Sebastian and Tai, Stefan},
	month = apr,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {154--161},
}

@inproceedings{pfandzelter_tinyfaas_2020,
	title = {{tinyFaaS}: {A} {Lightweight} {FaaS} {Platform} for {Edge} {Environments}},
	shorttitle = {{tinyFaaS}},
	url = {https://ieeexplore.ieee.org/document/9103476},
	doi = {10.1109/ICFC49376.2020.00011},
	abstract = {The Function-as-a-Service (FaaS) model is a great fit for data and event processing in the Internet of Things (IoT). Sending all data to a cloud-based FaaS platform, however, may cause performance and privacy issues. While these issues could be mitigated using edge computing, existing FaaS approaches, designed for the cloud, are too heavyweight to run on small, constrained edge nodes. In this paper, we propose tinyFaaS, a new FaaS system that is specifically designed for edge environments and their unique challenges. Our platform is lightweight enough to run on low-performance single machine edge nodes, provides a CoAP endpoint to support communication with low-power devices, and uses Docker containers to isolate tenants. We evaluate tinyFaaS through a proof-of-concept implementation that we benchmark and compare to state-of-the-art FaaS platforms. For IoT processing scenarios, we find that tinyFaaS outperforms existing systems by at least an order of magnitude.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Fog} {Computing} ({ICFC})},
	author = {Pfandzelter, Tobias and Bermbach, David},
	month = apr,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Fog Computing (ICFC)},
	pages = {17--24},
}

@inproceedings{tricomi_toward_2020,
	title = {Toward a {Function}-as-a-{Service} {Framework} for {Genomic} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9239679},
	doi = {10.1109/SMARTCOMP50058.2020.00070},
	abstract = {Nowadays, the study of nucleic acids (DNA/RNA) has become a digital science thanks to the advent of modern massive parallel sequencing technologies, better known with the acronym NGS standing for next-generation sequencing, and to the availability of a vast amount of genetic data easily accessible from publicly available databases. Due to the quantity and complexity of such data, its processing requires strong computer science knowledge and skills. This background includes topics such as programming and scripting languages, command-line interfaces, low-level data management tools, which are not always part of the toolbox of molecular biologists and geneticists. The need to adapt to entirely new IT tools and workflows slow down even the more experienced researchers, thus dedicated and customizable GUIs would be much more preferable and conducive. In this paper, we tackle this issue by proposing a preliminary architecture for a framework providing the following benefits: i) it supports the post-NGS analysis process definition phase (commonly called pipeline definition) via a graphical dashboard designed with NodeRED; ii) it automatically deploys the workflows on top of a cluster of computational resources, according to the Function-as-a-Service paradigm, i.e., treating each step of the pipeline as a function to be executed within Linux-based containers, pre-configured with all the necessary dependencies; iii) it runs such containers taking care automatically of resource load balancing. Finally, the framework is thought to include human feedback in the loop, thanks to the availability of a smart notification system, allowing the researcher to monitor the workflows and make any decision needed for its continuation.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Tricomi, Giuseppe and Giosa, Domenico and Merlino, Giovanni and Romeo, Orazio and Longo, Francesco},
	month = sep,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Smart Computing (SMARTCOMP)},
	pages = {314--319},
}

@inproceedings{bermbach_towards_2020,
	title = {Towards {Auction}-{Based} {Function} {Placement} in {Serverless} {Fog} {Platforms}},
	url = {https://ieeexplore.ieee.org/document/9103477},
	doi = {10.1109/ICFC49376.2020.00012},
	abstract = {The Function-as-a-Service (FaaS) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud.In this position paper, we propose an auction-based approach in which application developers bid on resources. This allows fog nodes to make a local decision about which functions to offload while maximizing revenue. For a first evaluation of our approach, we use simulation.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} {International} {Conference} on {Fog} {Computing} ({ICFC})},
	author = {Bermbach, David and Maghsudi, Setareh and Hasenburg, Jonathan and Pfandzelter, Tobias},
	month = apr,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE International Conference on Fog Computing (ICFC)},
	pages = {25--31},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\PMZV9GLG\\Bermbach et al. - 2020 - Towards Auction-Based Function Placement in Server.pdf:application/pdf},
}

@inproceedings{choi_-nic_2020,
	title = {λ-{NIC}: {Interactive} {Serverless} {Compute} on {Programmable} {SmartNICs}},
	isbn = {2575-8411},
	shorttitle = {λ-{NIC}},
	url = {https://ieeexplore.ieee.org/document/9355790},
	doi = {10.1109/ICDCS47774.2020.00029},
	abstract = {There is a growing interest in serverless compute, a cloud computing model that automates infrastructure resource- allocation and management while billing customers only for the resources they use. Workloads like stream processing benefit from high elasticity and fine-grain pricing of these serverless frameworks. However, so far, limited concurrency and high latency of server CPUs prohibit many interactive workloads (e.g., web servers and database clients) from taking advantage of serverless compute to achieve high performance.In this paper, we argue that server CPUs are ill-suited to run serverless workloads (i.e., lambdas) and present λ-NIC, an open- source framework, that runs interactive workloads directly on a SmartNIC; more specifically an ASIC-based NIC that consists of a dense grid of Network Processing Unit (NPU) cores. λ- NIC leverages SmartNIC's proximity to the network and a vast array of NPU cores to simultaneously run thousands of lambdas on a single NIC with strict tail-latency guarantees. To ease the development and deployment of lambdas, λ-NIC exposes an event-based programming abstraction, Match+Lambda, and a machine model that allows developers to compose and execute lambdas on SmartNICs easily. Our evaluation shows that λ- NIC achieves up to 880x and 736x improvements in workloads' response latency and throughput, respectively, while significantly reducing host CPU and memory usage.},
	urldate = {2024-01-01},
	booktitle = {2020 {IEEE} 40th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Choi, Sean and Shahbaz, Muhammad and Prabhakar, Balaji and Rosenblum, Mendel},
	month = nov,
	year = {2020},
	note = {Journal Abbreviation: 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)},
	pages = {67--77},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DNGTSD4J\\9355790.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7WYGS5P6\\9355790.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ENZMQUJ6\\9355790.html:text/html},
}

@inproceedings{cho_design_2021,
	title = {A {Design} of {Serverless} {Computing} {Service} for {Edge} {Clouds}},
	isbn = {2162-1233},
	url = {https://ieeexplore.ieee.org/document/9621162},
	doi = {10.1109/ICTC52510.2021.9621162},
	abstract = {The method of Serverless Computing at the Edge is drawing attention because of the efficiency of using resources. This paper presents a design of Serverless Computing for Edge Clouds, based on an open-source project. One of the methods of Serverless at the Edge is that centralized architecture with a Serverless Platform in the control plane cluster. However, in a centralized architecture, if the resources or the status of a particular Edge is not good, the control plane cluster must detect anomalies and redeploy the Functions considering the environment of other Edges. It will take time to select and deploy to the appropriate Edge. The proposed architecture introduces cross-monitoring which imports monitoring metrics directly from a nearby Edge. Grouping Edge sites as a Zone and cross-monitoring each other in the Zone. So that, Serverless computing-based Functions can operate quickly when adjacent Edges are in poor condition. As a result, the neighboring Edges of the Zone can back up with each other.},
	urldate = {2024-01-01},
	booktitle = {2021 {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {Cho, Jaeeun and Kim, Younghan},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 International Conference on Information and Communication Technology Convergence (ICTC)},
	pages = {1889--1891},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EXWG8MBM\\9621162.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7JKLWX7A\\9621162.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EYRV5FKI\\9621162.html:text/html},
}

@inproceedings{wen_measurement_2021,
	title = {A {Measurement} {Study} on {Serverless} {Workflow} {Services}},
	url = {https://ieeexplore.ieee.org/document/9590280},
	doi = {10.1109/ICWS53863.2021.00102},
	abstract = {Major cloud providers increasingly roll out their serverless workflow services to orchestrate serverless functions, making it possible to construct complex applications effectively. A comprehensive study is necessary to help developers understand the pros and cons, and make better choices among these serverless workflow services. However, the characteristics of these serverless workflow services have not been systematically analyzed. To fill the knowledge gap, we conduct a comprehensive measurement study on four mainstream serverless workflow services, focusing on both features and the performance. First, we review their official documentation and extract their features from six dimensions, including programming model, state management, etc. Then, we compare their performance (i.e., the execution time of functions, execution time of workflows, orchestration overhead time of workflows) under various settings considering activity complexity and data-flow complexity of workflows, as well as function complexity of serverless functions. Our findings and implications could help developers and cloud providers improve their development efficiency and user experience.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Wen, Jinfeng and Liu, Yi},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Web Services (ICWS)},
	pages = {741--750},
}

@inproceedings{cicconetti_preliminary_2021,
	title = {A {Preliminary} {Evaluation} of {QUIC} for {Mobile} {Serverless} {Edge} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9469496},
	doi = {10.1109/WoWMoM51794.2021.00050},
	abstract = {Deployment of computing infrastructures at the edge of the network will drive a revolution in integrated solutions for smart mobility in the cities of the future, thanks to the promises of reduced latency and outbound traffic. The adoption of serverless computing will help realising this vision since it simplifies management while at the same time providing the application developers with a neat and clean Function-as-a-Service (FaaS) programming model. Today FaaS relies on HTTP over TCP, but QUIC is emerging fast as a replacement because it is more robust to packet losses and it allows connection roaming: both these advantages are especially important for mobile scenarios. In this paper we report the results of a preliminary evaluation of QUIC+HTTP/3 when used instead of TCP+HTTP within a framework for decentralized dispatching of FaaS function invocations, which shows that this direction is promising and deserves to be delved further in the future.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 22nd {International} {Symposium} on a {World} of {Wireless}, {Mobile} and {Multimedia} {Networks} ({WoWMoM})},
	author = {Cicconetti, Claudio and Lossi, Leonardo and Mingozzi, Enzo and Passarella, Andrea},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)},
	pages = {268--273},
}

@inproceedings{sedghani_randomized_2021,
	title = {A {Randomized} {Greedy} {Method} for {AI} {Applications} {Component} {Placement} and {Resource} {Selection} in {Computing} {Continua}},
	url = {https://ieeexplore.ieee.org/document/9566164},
	doi = {10.1109/JCC53141.2021.00022},
	abstract = {Artificial Intelligence (AI) and Deep Learning (DL) are pervasive today, with applications spanning from personal assistants to healthcare. Nowadays, the accelerated migration towards mobile computing and Internet of Things, where a huge amount of data is generated by widespread end devices, is determining the rise of the edge computing paradigm, where computing resources are distributed among devices with highly heterogeneous capacities. In this fragmented scenario, efficient component placement and resource allocation algorithms are crucial to orchestrate at best the computing continuum resources. In this paper, we propose a tool to effectively address the component placement problem for AI applications at design time. Through a randomized greedy algorithm, it identifies the placement of minimum cost providing performance guarantees across heterogeneous resources including edge devices, cloud GPU-based Virtual Machines and Function as a Service solutions.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Joint} {Cloud} {Computing} ({JCC})},
	author = {Sedghani, Hamta and Filippini, Federica and Ardagna, Danilo},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Joint Cloud Computing (JCC)},
	pages = {65--70},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\3BXQMCVP\\Sedghani et al. - 2021 - A Randomized Greedy Method for AI Applications Com.pdf:application/pdf},
}

@inproceedings{sedghani_random_2021,
	title = {A {Random} {Greedy} based {Design} {Time} {Tool} for {AI} {Applications} {Component} {Placement} and {Resource} {Selection} in {Computing} {Continua}},
	isbn = {2767-9918},
	url = {https://ieeexplore.ieee.org/document/9712094},
	doi = {10.1109/EDGE53862.2021.00014},
	abstract = {Artificial Intelligence (AI) and Deep Learning (DL) are pervasive today, with applications spanning from personal assistants to healthcare. Nowadays, the accelerated migration towards mobile computing and Internet of Things, where a huge amount of data is generated by widespread end devices, is determining the rise of the edge computing paradigm, where computing resources are distributed among devices with highly heterogeneous capacities. In this fragmented scenario, efficient component placement and resource allocation algorithms are crucial to orchestrate at best the computing continuum resources. In this paper, we propose a tool to effectively address the component placement problem for AI applications at design time. Through a randomized greedy algorithm, our approach identifies the placement of minimum cost providing performance guar-antees across heterogeneous resources including edge devices, cloud GPU-based Virtual Machines and Function as a Service solutions. Finally, we compare the random greedy method with the HyperOpt framework and demonstrate that our proposed approach converges to a near-optimal solution much faster, especially in large scale systems.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Edge} {Computing} ({EDGE})},
	author = {Sedghani, Hamta and Filippini, Federica and Ardagna, Danilo},
	month = sep,
	year = {2021},
	note = {ISSN: 2767-9918},
	pages = {32--40},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LEDDTHRF\\9712094.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CGZ7HV6L\\9712094.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F5GI6QW8\\9712094.html:text/html},
}

@inproceedings{agarwal_reinforcement_2021,
	title = {A {Reinforcement} {Learning} {Approach} to {Reduce} {Serverless} {Function} {Cold} {Start} {Frequency}},
	url = {https://ieeexplore.ieee.org/document/9499423},
	doi = {10.1109/CCGrid51090.2021.00097},
	abstract = {Serverless computing is an event-driven cloud computing architecture for processing requests on-demand, using light weight function containers and a micro-services model. A variety of applications like Internet of Things (IoT) services, edge computing, and stream processing have been introduced to the serverless paradigm. These applications are characterized by their stringent response time requirements, therefore expecting a quick and fault tolerant feedback from the application. The serverless, or Function-as-a-Service (FaaS), paradigm suffers from function ‘cold start’ challenges, where the serverless platform takes time to set up the dependencies, prepare the runtime environment and code for execution before serving the incoming workload. Most of the current works address the problem of cold start by (1) reducing the start-up or preparation time of function containers, or (2) reducing the frequency of function cold starts on the platform. Recent industrial research has identified that factors such as runtime environment, CPU and memory settings, invocation concurrency, and networking requirements, affect the cold start of a function. Therefore, we propose a Reinforcement Learning (Q-Learning) agent setting, to analyze the identified factors such as function CPU utilization, to ascertain the function-invocation patterns and reduce the function cold start frequency by preparing the function instances in advance. The proposed Q-Learning agent interacts with the Kubeless serverless platform by discretizing the environment states, actions and rewards with the use of per-instance CPU utilization, available function instances and success or failure rate of response, respectively. The workload is replicated using the Apache JMeter non-GUI toolkit and our agent is evaluated against the baseline default auto-scale feature of Kubeless. The agent demonstrates the capability of learning the invocation pattern, make informed decisions by preparing the optimal number of function instances over the period of learning, under controlled environment settings.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Agarwal, Siddharth and Rodriguez, Maria A. and Buyya, Rajkumar},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {797--803},
}

@article{ravaglia_tinyml_2021,
	title = {A {TinyML} {Platform} for {On}-{Device} {Continual} {Learning} {With} {Quantized} {Latent} {Replays}},
	volume = {11},
	issn = {2156-3365},
	url = {https://ieeexplore.ieee.org/document/9580920},
	doi = {10.1109/JETCAS.2021.3121554},
	abstract = {In the last few years, research and development on Deep Learning models \& techniques for ultra-low-power devices– in a word, TinyML – has mainly focused on a train-then-deploy assumption, with static models that cannot be adapted to newly collected data without cloud-based data collection and fine-tuning. Latent Replay-based Continual Learning (CL) techniques (Pellegrini et al., 2020) enable online, serverless adaptation in principle, but so far they have still been too computation- and memory-hungry for ultra-low-power TinyML devices, which are typically based on microcontrollers. In this work, we introduce a HW/SW platform for end-to-end CL based on a 10-core FP32 -enabled parallel ultra-low-power (PULP) processor. We rethink the baseline Latent Replay CL algorithm, leveraging quantization of the frozen stage of the model and Latent Replays (LRs) to reduce their memory cost with minimal impact on accuracy. In particular, 8-bit compression of the LR memory proves to be almost lossless (−0.26\% with 3000LR) compared to the full-precision baseline implementation, but requires 4{\textbackslash}times less memory, while 7-bit can also be used with an additional minimal accuracy degradation (up to 5\%). We also introduce optimized primitives for forward and backward propagation on the PULP processor, together with data tiling strategies to fully exploit its memory hierarchy, while maximizing efficiency. Our results show that by combining these techniques, continual learning can be achieved in practice using less than 64MB of memory – an amount compatible with embedding in TinyML devices. On an advanced 22nm prototype of our platform, called VEGA, the proposed solution performs on average 65 {\textbackslash}times faster than a low-power STM32 L4 microcontroller, being 37{\textbackslash}times more energy efficient – enough for a lifetime of 535h when learning a new mini-batch of data once every minute.},
	number = {4},
	urldate = {2024-01-01},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Ravaglia, Leonardo and Rusci, Manuele and Nadalini, Davide and Capotondi, Alessandro and Conti, Francesco and Benini, Luca},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	pages = {789--802},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\9Y7A6E8T\\Ravaglia et al. - 2021 - A TinyML Platform for On-Device Continual Learning.pdf:application/pdf},
}

@inproceedings{schuler_ai-based_2021,
	title = {{AI}-based {Resource} {Allocation}: {Reinforcement} {Learning} for {Adaptive} {Auto}-scaling in {Serverless} {Environments}},
	shorttitle = {{AI}-based {Resource} {Allocation}},
	url = {https://ieeexplore.ieee.org/document/9499535},
	doi = {10.1109/CCGrid51090.2021.00098},
	abstract = {Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach to request-based auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Schuler, Lucia and Jamil, Somaya and Kühl, Niklas},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {804--811},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BEF9RSXA\\9499535.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3QS3MB24\\9499535.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MIKVSDIM\\9499535.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\TZDRHR3B\\Schuler et al. - 2021 - AI-based Resource Allocation Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{riggio_aiedge_2021,
	title = {{AI}@{EDGE}: {A} {Secure} and {Reusable} {Artificial} {Intelligence} {Platform} for {Edge} {Computing}},
	isbn = {2575-4912},
	shorttitle = {{AI}@{EDGE}},
	url = {https://ieeexplore.ieee.org/document/9482440},
	doi = {10.1109/EuCNC/6GSummit51104.2021.9482440},
	abstract = {Artificial Intelligence (AI) has become a major innovative force and a major pillar in the fourth industrial revolution. This trend has been acknowledged by the European Commission, who has pointed out how high-performance, intelligent, and secure networks are fundamental for the evolution of the multiservice Next Generation Internet (NGI). While great progress has been done in the accuracy and performance of AI-enabled platforms, their integration in autonomous decision-making and critical systems requires end-to-end quality assurance. AI@EDGE addresses these challenges harnessing the concept of “reusable, secure, and trustworthy AI for network automation”. To this end, AI@EDGE targets significant breakthroughs in two fields: (i) general-purpose frameworks for closed-loop network automation capable of supporting flexible and programmable pipelines for the creation, utilization, and adaptation of the secure, reusable, and trustworthy AI/ML models; and (ii) converged connect-compute platform for creating and managing resilient, elastic, and secure end-to-end slices supporting a diverse range of AI-enabled network applications. Cooperative perception for vehicular networks, secure, multi-stakeholder AI for Industrial Internet of Things, aerial infrastructure inspections, and in-flight entertainment are the uses cases targeted by AI@EDGE to maximise its commercial, societal, and environmental impact.},
	urldate = {2024-01-01},
	booktitle = {2021 {Joint} {European} {Conference} on {Networks} and {Communications} \& {6G} {Summit} ({EuCNC}/{6G} {Summit})},
	author = {Riggio, Roberto and Coronado, Estefanía and Linder, Neiva and Jovanka, Adzic and Mastinu, Gianpiero and Goratti, Leonardo and Rosa, Miguel and Schotten, Hans and Pistore, Marco},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 Joint European Conference on Networks and Communications \& 6G Summit (EuCNC/6G Summit)},
	pages = {610--615},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\JRLL8TEI\\Riggio et al. - 2021 - AI@EDGE A Secure and Reusable Artificial Intellig.pdf:application/pdf},
}

@inproceedings{majewski_algorithms_2021,
	title = {Algorithms for scheduling scientific workflows on serverless architecture},
	url = {https://ieeexplore.ieee.org/document/9499676},
	doi = {10.1109/CCGrid51090.2021.00095},
	abstract = {Serverless computing is a novel cloud computing paradigm where the cloud provider manages the underlying infrastructure, while users are only required to upload the code of the application. Function as a Service (FaaS) is a serverless computing model where short-lived methods are executed in the cloud. One of the promising use cases for FaaS is running scientific workflow applications, which represent a scientific process composed of related tasks. Due to the distinctive features of FaaS, which include rapid resource provisioning, indirect infrastructure management, and fine-grained billing model a need arises to create dedicated scheduling methods to effectively use the novel infrastructures as an environment for workflow applications. In this paper we propose two novel scheduling algorithms SMOHEFT and SML, which are designed to create a schedule for executing scientific workflows on serverless infrastructures concerning time and cost constraints. We evaluated proposed algorithms by performing experiments, where we planned the execution of three applications: Ellipsoids, Vina and Montage. SDBWS and SDBCS algorithms were used as a baseline. SML achieved the best results when executing Ellipsoids workflow, with a success rate above 80\%, while other algorithms were below 60\%. In the case of Vina, all the algorithms, except SDBWS, had a success rate above 87.5\% and in the case of Montage, the success rate of all algorithms was similar, over 87.5\%. The proposed algorithms' success rate is comparable or better than offered by other studied solutions.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Majewski, Marcin and Pawlik, Maciej and Malawski, Maciej},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {782--789},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X6C9MN37\\9499676.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\33D5SFP9\\9499676.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FSF4S6ES\\9499676.html:text/html},
}

@inproceedings{aknesil_fpga_2021,
	title = {An {FPGA} {Implementation} of 4×4 {Arbiter} {PUF}},
	isbn = {2378-2226},
	url = {https://ieeexplore.ieee.org/document/9459657},
	doi = {10.1109/ISMVL51352.2021.00035},
	abstract = {The need of protecting data and bitstreams increases in computation environments such as FPGA as a Service (FaaS). Physically Unclonable Functions (PUFs) have been proposed as a solution to this problem. In this paper, we present an implementation of Arbiter PUF with 4 × 4 switch blocks in Xilinx Series 7 FPGA, perform its statistical analysis, and compare it to other Arbiter PUF variants. We show that the presented implementation utilizes five times less area than 2 × 2 Arbiter PUF-based implementations. It is suitable for many real-world applications, including identification, authentication, key provisioning, and random number generation.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 51st {International} {Symposium} on {Multiple}-{Valued} {Logic} ({ISMVL})},
	author = {Aknesil, Can and Dubrova, Elena},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 51st International Symposium on Multiple-Valued Logic (ISMVL)},
	pages = {160--165},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UWA8WQZH\\9459657.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F57ZTUGA\\9459657.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KZFWSTPK\\9459657.html:text/html},
}

@inproceedings{choudhari_architectural_2021,
	title = {Architectural {Vision} of {Cloud} {Computing} in the {Indian} {Government}},
	url = {https://ieeexplore.ieee.org/document/9399598},
	doi = {10.1109/ICITIIT51526.2021.9399598},
	abstract = {The GI (Govt. of India) cloud started in 2014 is built on the state of art technologies and rich architecture with the nationwide network infrastructure and Data Centres located across the country on National and State data centres. This paper investigates, study and analyze the cloud architecture of Govt. of India and suggests modifications that need to be adapted for sustainable development as per the global changing scenario and fulfill the future needs with improved service delivery, increased throughput, and increased efficiency to provide secured cloud services and to minimize the gap between the cloud service providers and end-users. The cloud services are designed for centralized storage and processing. The cloud data centers are generally located thousands of miles away from the end-users where the data is actually generated. The physical distance between the cloud infrastructure and the data source at edge level end-users produces latency for the real-time processing of the huge amount of data generated at the source level. In recent years the automation scenario is changing globally with various emerging technologies such as the Internet of Things (IoT), Wireless Fidelity 6 (Wi-Fi 6), Fifth Generation Mobile Network connectivity (5G), Artificial Intelligence (AI), and Machine Learning, etc. Emerging technologies like IoT, Wi-Fi 6, 5G gives large scope for boundary level computing and generates a very huge amount of data at the data source level produced by the end-users. These technologies require agile real-time processing and analysis of the data at the source level. Edge computing and Fog computing are the distributed architectures that work together, for reduced latency and speedy real-time processing where the data is actually generated by the end-user. According to the new implementation demands, various emerging cloud technologies such as Mobile Cloud Apps, Containers, Serverless, Microservices, Development and Information Technology Operations (DevOps), BlockChain, Fog computing, Edge Computing, and Software-Defined Infrastructure (SDI), etc are proposed for implementation.},
	urldate = {2024-01-01},
	booktitle = {2021 {International} {Conference} on {Innovative} {Trends} in {Information} {Technology} ({ICITIIT})},
	author = {Choudhari, Nitin Vishnu and Sasankar, Ashish B.},
	month = feb,
	year = {2021},
	note = {Journal Abbreviation: 2021 International Conference on Innovative Trends in Information Technology (ICITIIT)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PQRJ4WIH\\9399598.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PMHV8Z7Q\\9399598.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BYGXQ38H\\9399598.html:text/html},
}

@inproceedings{jayaraman_asset_2021,
	title = {Asset {Modeling} using {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9671711},
	doi = {10.1109/BigData52589.2021.9671711},
	abstract = {Assets in the domain of Internet of Things (IoT) generate time-series data such as sensor readings and alerts. In addition, the assets have associated static data such as the make, model and other manufacturing information. The sensors in the asset components may have implicit relationships with each other, which are not interpretable without domain knowledge. Many problems exist which involve computation of relationships between sensors or subsystems in the asset components. Typically, the number of sensors in a real world asset may range anywhere from tens to thousands of sensors - and in this case, finding relationships between them becomes a highly computationally intensive task. In this paper, we study one such problem of anomaly detection in industrial data based on the functioning of the sensors and their interrelationships in both normal and abnormal conditions. We further demonstrate the issue of run-time and performance complexity in this problem, and present a speed-up strategy using Serverless Computing for parallelization, and demonstrate the usefulness of this method by comparing the speed-up achieved.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Jayaraman, Srideepika and Reddy, Chandra and Khabiri, Elham and Patel, Dhaval and Bhamidipaty, Anuradha and Kalagnanam, Jayant},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Big Data (Big Data)},
	pages = {4084--4090},
}

@inproceedings{jarachanthan_astra_2021,
	title = {Astra: {Autonomous} {Serverless} {Analytics} with {Cost}-{Efficiency} and {QoS}-{Awareness}},
	isbn = {1530-2075},
	shorttitle = {Astra},
	url = {https://ieeexplore.ieee.org/document/9460548},
	doi = {10.1109/IPDPS49936.2021.00085},
	abstract = {With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter with the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astra, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astra relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy Astra in the AWS Lambda platform and conduct real-world experiments over three representative benchmarks with different scales. Results demonstrate that Astra can achieve the optimal execution decision for serverless analytics, by improving the performance of 21\% to 60\% under a given budget constraint, and resulting in a cost reduction of 20\% to 80\% without violating performance requirement, when compared with three baseline configuration algorithms.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {756--765},
}

@inproceedings{grambow_befaas_2021,
	title = {{BeFaaS}: {An} {Application}-{Centric} {Benchmarking} {Framework} for {FaaS} {Platforms}},
	shorttitle = {{BeFaaS}},
	url = {https://ieeexplore.ieee.org/document/9610428},
	doi = {10.1109/IC2E52221.2021.00014},
	abstract = {Following the increasing interest and adoption of FaaS systems, benchmarking frameworks for determining nonfunctional properties have also emerged. While existing (microbenchmark) frameworks only evaluate single aspects of FaaS platforms, a more holistic, application-driven approach is still missing. In this paper, we design and present BeFaaS, an extensible application-centric benchmarking framework for FaaS environments that focuses on the evaluation of FaaS platforms through realistic and typical examples of FaaS applications. BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload profiles and new platforms, supports federated benchmark runs in which the benchmark application is distributed over multiple providers, and supports a fine-grained result analysis. Our evaluation compares three major FaaS providers in single cloud provider setups and shows that BeFaaS is capable of running each benchmark automatically with minimal configuration effort and providing detailed insights for each interaction.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Grambow, Martin and Pfandzelter, Tobias and Burchard, Luk and Schubert, Carsten and Zhao, Max and Bermbach, David},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5EUQ2WAF\\9610428.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WUYF7ZFL\\9610428.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ICEJUYLW\\9610428.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\P9698TL7\\Grambow et al. - 2021 - BeFaaS An Application-Centric Benchmarking Framew.pdf:application/pdf},
}

@inproceedings{roy_characterizing_2021,
	title = {Characterizing and {Mitigating} the {I}/{O} {Scalability} {Challenges} for {Serverless} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9668299},
	doi = {10.1109/IISWC53511.2021.00018},
	abstract = {As serverless computing paradigm becomes widespread, it is important to understand the I/O performance characteristics on serverless computing platforms. To the best of our knowledge, we provide the first study that analyzes the observed I/O performance characteristics - some expected and some unexpected findings that reveal the hidden, complex interactions between the application I/O characteristics, the serverless computing platform, and the storage engines. The goal of this analysis is to provide data-driven guidelines to serverless programmers and system designers about the performance trade-offs and pitfalls of serverless I/O.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Roy, Rohan Basu and Patel, Tirthak and Tiwari, Devesh},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Symposium on Workload Characterization (IISWC)},
	pages = {74--86},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B8LACTXW\\9668299.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JGZH7DSA\\9668299.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y6NPYXNP\\9668299.html:text/html},
}

@inproceedings{koschel_cloud_2021,
	title = {Cloud {Computing}: {Serverless}},
	shorttitle = {Cloud {Computing}},
	url = {https://ieeexplore.ieee.org/document/9555534},
	doi = {10.1109/IISA52424.2021.9555534},
	abstract = {A serverless architecture is a new approach to offering services over the Internet. It combines BaaS (Backend-as-a-service) and FaaS (Function-as-a-service). With the serverless architecture no own or rented infrastructures are needed anymore. In addition, the company does not have to worry about scaling any longer, as this happens automatically and immediately. Furthermore, there is no need any longer for maintenance work on the servers, as this is completely taken over by the provider. Administrators are also no longer needed for the same reason. Finally, many ready-made functions are offered, with which the development effort can be reduced. As a result, the serverless architecture is very well suited to many application scenarios, and it can save considerable costs (server costs, maintenance costs, personnel costs, electricity costs, etc.). The company only must subdivide the source code of the application and upload it to the provider’s server. The rest is done by the provider.},
	urldate = {2024-01-01},
	booktitle = {2021 12th {International} {Conference} on {Information}, {Intelligence}, {Systems} \& {Applications} ({IISA})},
	author = {Koschel, Arne and Klassen, Samuel and Jdiya, Kerim and Schaaf, Marc and Astrova, Irina},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 12th International Conference on Information, Intelligence, Systems \& Applications (IISA)},
	keywords = {Cloud computing, cloud computing, Costs, Computer architecture, Function-as-a-service, Cloud-computing, Scalings, Serverless architecture, Serverless function, Architecture, Service modeling, Application scenario, BaaS (Backend-as-a-service), Backend-as-a-service, FaaS (Function-as-a-service), Maintenance work, New approaches, scaling, serverless architecture, serverless functions, service models},
	pages = {1--7},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\UKEF2RL5\\Koschel et al. - 2021 - Cloud Computing Serverless.pdf:application/pdf},
}

@inproceedings{kumar_coding_2021,
	title = {Coding the {Computing} {Continuum}: {Fluid} {Function} {Execution} in {Heterogeneous} {Computing} {Environments}},
	shorttitle = {Coding the {Computing} {Continuum}},
	url = {https://ieeexplore.ieee.org/document/9460607},
	doi = {10.1109/IPDPSW52791.2021.00018},
	abstract = {Advances in network technologies have greatly decreased barriers to accessing physically distributed computers. This newfound accessibility coincides with increasing hardware specialization, creating exciting new opportunities to dispatch workloads to the best resource for a specific purpose, rather than those that are closest or most easily accessible. We present Delta, a service designed to intelligently schedule function-based workloads across a distributed set of heterogeneous computing resources. Delta implements an extensible architecture in which different predictors and scheduling algorithms can be integrated to provide dynamically evolving estimates of function execution times on different resources-estimates that can be used to determine the most appropriate location for execution. We describe predictors for function runtime, data transfer time, and cold-start resource provisioning and configuration delay; dynamic learning methods that update predictor models over time; and scheduling strategies that take into account both function and endpoint information. We show that these methods can halve workload makespan when compared with a strategy that selects the fastest resource, and decrease makespan by a factor of five when compared to a round robin strategy, when deployed on a heterogeneous testbed with resources ranging from a Raspberry Pi to a GPU node in an academic cloud.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Kumar, Rohan and Baughman, Matt and Chard, Ryan and Li, Zhuozhao and Babuji, Yadu and Foster, Ian and Chard, Kyle},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	pages = {66--75},
}

@inproceedings{li_cofunc_2021,
	title = {{CoFunc}: {A} unified development framework for heterogeneous {FaaS} computing platforms},
	shorttitle = {{CoFunc}},
	url = {https://ieeexplore.ieee.org/document/9445991},
	doi = {10.1109/CISCE52179.2021.9445991},
	abstract = {Function as a Service (FaaS) is a new popular cloud computing model in recent years, which has the characteristics of automatic scaling, on-demand billing and easy maintenance. However, the SDKs provided by different public FaaS platforms are inconsistent, increasing the cost of learning and application migration for developers. This paper proposes to build a unified development framework by encapsulating the SDKs of each FaaS platform, which uses the factory pattern of object-oriented design mode to define a set of unified abstract classes. This framework can help developers to operate across different FaaS platforms with a unified set of SDK, and has good scalability.},
	urldate = {2024-01-01},
	booktitle = {2021 {International} {Conference} on {Communications}, {Information} {System} and {Computer} {Engineering} ({CISCE})},
	author = {Li, Bao and Li, Zhe and Tan, Yusong and Yu, Jie},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 International Conference on Communications, Information System and Computer Engineering (CISCE)},
	pages = {726--730},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R7PLXHV5\\9445991.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\93FSZPW2\\9445991.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YULU4RKY\\9445991.html:text/html},
}

@inproceedings{li_confidential_2021,
	title = {Confidential {Serverless} {Made} {Efficient} with {Plug}-{In} {Enclaves}},
	isbn = {2575-713X},
	url = {https://ieeexplore.ieee.org/document/9499793},
	doi = {10.1109/ISCA52012.2021.00032},
	abstract = {Serverless computing has become a fact of life on modern clouds. A serverless function may process sensitive data from clients. Protecting such a function against untrusted clouds using hardware enclave is attractive for user privacy. In this work, we run existing serverless applications in SGX enclave, and observe that the performance degradation can be as high as 5.6× to even 422.6×. Our investigation identifies these slowdowns are related to architectural features, mainly from page-wise enclave initialization. Leveraging insights from our overhead analysis, we revisit SGX hardware design and make minimal modification to its enclave model. We extend SGX with a new primitive—region-wise plugin enclaves that can be mapped into existing enclaves to reuse attested common states amongst functions. By remapping plugin enclaves, an enclave allows in-situ processing to avoid expensive data movement in a function chain. Experiments show that our design reduces the enclave function latency by 94.74-99.57\%, and boosts the autoscaling throughput by 19-179×.},
	urldate = {2024-01-01},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Li, Mingyu and Xia, Yubin and Chen, Haibo},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
	pages = {306--318},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LRR7UVNR\\9499793.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KASGRGBM\\9499793.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KST6ASMQ\\9499793.html:text/html},
}

@inproceedings{shahidi_cross-platform_2021,
	title = {Cross-{Platform} {Performance} {Evaluation} of {Stateful} {Serverless} {Workflows}},
	url = {https://ieeexplore.ieee.org/document/9668304},
	doi = {10.1109/IISWC53511.2021.00017},
	abstract = {Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and AWS Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that AWS is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, AWS is 89\% more expensive than Azure for machine learning training application while Azure is 2× faster than AWS for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms,},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Shahidi, Narges and Gunasekaran, Jashwant Raj and Kandemir, Mahmut Taylan},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Symposium on Workload Characterization (IISWC)},
	pages = {63--73},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H5V34BBG\\9668304.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KY3WB33L\\9668304.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VKDVJAD2\\9668304.html:text/html},
}

@inproceedings{reategui_customized_2021,
	title = {Customized {IoT} devices for the architectural education future in connectivity for smart cities},
	url = {https://ieeexplore.ieee.org/document/9540124},
	doi = {10.1109/SCLA53004.2021.9540124},
	abstract = {Architecture, and urbanism, in synergy with electronic engineering, computing, and mechatronics, promote the development of applications and services for smart cities in the context of the Fourth Industrial revolution. We link this knowledge from the implementation of IoT in undergraduate education in the field of architecture, in a model to analyze behavior patterns of users and environment variables, monitoring thousands of daily data, to optimize the efficiency and performance of the problems of buildings, promoting people-centered design. Our academic ecosystem is inspired by the traditional use of the PBL of design studios but powered by emerging technologies selected for this research (IoT, Cloud Computing, Microservices, and Parametric Design), to customize GPRS/GSM devices, according to students needs and design problems. Our validation included four knowledge integration mechanisms and the evaluation of the effectiveness of student work to build a low-cost implementation ecosystem: Socialization (Microcontroller Software and Circuit Design), exchange (Telecommunication software), direction (Backend Serverless software), and internalization (Parametric Design). As a whole, it promotes teamwork, collaboration, interaction, constant feedback, and student adaptability, providing an academic background for the construction of sustainable cities from a Latin American perspective.},
	urldate = {2024-01-01},
	booktitle = {2021 2nd {Sustainable} {Cities} {Latin} {America} {Conference} ({SCLA})},
	author = {Reátegui, José L. and Herrera, Pablo C.},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 2nd Sustainable Cities Latin America Conference (SCLA)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FYM5UWDM\\9540124.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2VEBC2BH\\9540124.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y5GUVBKT\\9540124.html:text/html},
}

@inproceedings{bhole_data_2021,
	title = {Data {Transfer} between {Cloud} {Document} {Management} {Systems} in {Serverless} {Paradigm}},
	url = {https://ieeexplore.ieee.org/document/9711817},
	doi = {10.1109/ICCCT53315.2021.9711817},
	abstract = {Data and communication are at the heart of business and interactions. With the onset of economic and reliable Document Management System (DMS) providers, there has been a large-scale migration to these cloud-based services. When it comes to Data Transfer we find ourselves in the realm of mailing files, link sharing, FTP and the list goes on, however, all of these methods involve some level of local computing and storage resource consumption and do not support transfers across different DMS providers. This paper proposes a system that aims to facilitate cloud-agnostic data transfers built over a cloud system with Serverless computing paradigm.},
	urldate = {2024-01-01},
	booktitle = {2021 4th {International} {Conference} on {Computing} and {Communications} {Technologies} ({ICCCT})},
	author = {Bhole, Ritika and Dalvi, Sanket and Acharya, Manas and Nirkhe, Sahil and Rane, Pradnya and Jain, Sheetal},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 4th International Conference on Computing and Communications Technologies (ICCCT)},
	pages = {280--284},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QMRAMYQ5\\9711817.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HMPS39RR\\9711817.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7N9PC59K\\9711817.html:text/html},
}

@inproceedings{hoseinyfarahabady_data-intensive_2021,
	title = {Data-{Intensive} {Workload} {Consolidation} in {Serverless} ({Lambda}/{FaaS}) {Platforms}},
	isbn = {2643-7929},
	url = {https://ieeexplore.ieee.org/document/9685244},
	doi = {10.1109/NCA53618.2021.9685244},
	abstract = {A significant amount of research studies in the past years has been devoted on developing efficient mechanisms to control the level of degradation among consolidate workloads in a shared platform. Workload consolidation is a promising feature that is employed by most service providers to reduce the total operating costs in traditional computing systems [1]–[3]. Serverless paradigm - also known as Function as a Service, FaaS, and Lambda - recently emerged as a new virtualization run-time model that disentangles the traditional state of applications' users from the burden of provisioning physical computing resources, leaving the difficulty of providing the adequate resource capacity on the service provider's side. This paper focuses on a number of challenges associated with workload consolidation when a serverless platform is expected to execute several data-intensive functional units. Each functional unit is considered to be the atomic component that reacts to a stream of input data. A serverless application in the proposed model is composed of a series of functional units. Through a systematic approach, we highlight the main challenges for devising an efficient workload consolidation process in a data-intensive serverless platform. To this end, we first study the performance interference among multiple workloads to obtain the capacity of last level cache (LLC). We show how such contention among workloads can lead to a significant throughput degradation on a single physical server. We expand our investigation into a general case with the aim to prevent the total throughput never falling below a predefined utilization level. Based on the empirical results, we develop a consolidation model and then design a computationally efficient controller to optimize the throughput degradation among a platform consists fs multiple machines. The performance evaluation is conducted using modern workloads inspired by data management services, and data analytic benchmark tools in our in-house four node platform showing the efficiency of the proposed solution to mitigate the QoS violation rate for high priority applications by 90\% while can enhance the normalized throughput usage of disk devices by 39 \%.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 20th {International} {Symposium} on {Network} {Computing} and {Applications} ({NCA})},
	author = {HoseinyFarahabady, M.Reza and Taheri, Javid and Zomaya, Albert Y. and Tari, Zahir},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 20th International Symposium on Network Computing and Applications (NCA)},
	pages = {1--8},
}

@inproceedings{mampage_deadline-aware_2021,
	title = {Deadline-aware {Dynamic} {Resource} {Management} in {Serverless} {Computing} {Environments}},
	url = {https://ieeexplore.ieee.org/document/9499407},
	doi = {10.1109/CCGrid51090.2021.00058},
	abstract = {Serverless computing enables rapid application development and deployment by composing loosely coupled microservices at a scale. This emerging paradigm greatly unburdens the users of cloud environments, from the need to provision and manage the underlying cloud resources. With this shift in responsibility, the cloud provider faces the challenge of providing acceptable performance to the user without compromising on reliability, while having minimal knowledge of the application requirements. Sub-optimal resource allocations, specifically the CPU resources, could result in the violation of performance requirements of applications. Further, the fine-grained serverless billing model only charges for resource usage in terms of function execution time. At the same time, the provider has to maintain the underlying infrastructure in always-on mode to facilitate asynchronous function calls. Thus, achieving optimum utilization of cloud resources without compromising on application requirements is of high importance to the provider. Most of the current works only focus on minimizing function execution times caused by delays in infrastructure set up and reducing resource costs for the end-user. However, in this paper, we focus on both the provider and user’s perspective and propose a function placement policy and a dynamic resource management policy for applications deployed in serverless computing environments. The policies minimize the resource consumption cost for the service provider while meeting the user’s application requirement, i.e., deadline. The proposed solutions are sensitive to deadline and efficiently increase the resource utilization for the provider, while dynamically managing resources to improve function response times. We implement and evaluate our approach through simulation using ContainerCloudSim toolkit. The proposed function placement policy when compared with baseline scheduling techniques can reduce resource consumption by up to three times. The dynamic resource allocation policy when evaluated with a fixed resource allocation policy and a proportional CPU-shares policy shows improvements of up to 25\% in meeting the required function deadlines.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {483--492},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L2LFJTZN\\9499407.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7U8UZ9JN\\9499407.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\37LSJ2IL\\9499407.html:text/html},
}

@inproceedings{shen_defuse_2021,
	title = {Defuse: {A} {Dependency}-{Guided} {Function} {Scheduler} to {Mitigate} {Cold} {Starts} on {FaaS} {Platforms}},
	volume = {2021-July},
	isbn = {2575-8411},
	shorttitle = {Defuse},
	url = {https://ieeexplore.ieee.org/document/9546470},
	doi = {10.1109/ICDCS51616.2021.00027},
	abstract = {Function-as-a-Service (FaaS) is becoming a prevalent paradigm in developing cloud applications. With FaaS, clients can develop applications as serverless functions, leaving the burden of resource management to cloud providers. However, FaaS platforms suffer from the performance degradation caused by the cold starts of serverless functions. Cold starts happen when serverless functions are invoked before they have been loaded into the memory. The problem is unavoidable because the memory in datacenters is typically too limited to hold all serverless functions simultaneously. The latency of cold function invocations will greatly degenerate the performance of FaaS platforms. Currently, FaaS platforms employ various scheduling methods to reduce the occurrences of cold starts. However, they do not consider the ubiquitous dependencies between serverless functions. Observing the potential of using dependencies to mitigate cold starts, we propose Defuse, a Dependency-guided Function Scheduler on FaaS platforms. Specifically, Defuse identifies two types of dependencies between serverless functions, i.e., strong dependencies and weak ones. It uses frequent pattern mining and positive point-wise mutual information to mine such dependencies respectively from function invocation histories. In this way, Defuse constructs a function dependency graph. The connected components (i.e., dependent functions) on the graph can be scheduled to diminish the occurrences of cold starts. We evaluate the effectiveness of Defuse by applying it to an industrial serverless dataset. The experimental results show that Defuse can reduce 22\% of memory usage while having a 35\% decrease in function cold-start rates compared with the state-of-the-art method.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 41st {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Shen, Jiacheng and Yang, Tianyi and Su, Yuxin and Zhou, Yangfan and Lyu, Michael R.},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
	keywords = {Cloud Computing, FaaS, Serverless, Function-as-a-service, Resource management, Scheduling, Cloud-computing, Cloud providers, Cloud applications, Cold Start, Cold-start, Datacenter, Performance degradation, Service dependency, Service Dependency},
	pages = {194--204},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4CA2UDY9\\9546470.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KTTIRRCT\\9546470.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XRIYR24H\\9546470.html:text/html},
}

@inproceedings{odonncha_deployment_2021,
	title = {Deployment and {Management} of {Time} {Series} {Forecasts} in {Ocean} {Industry}},
	url = {https://ieeexplore.ieee.org/document/9671877},
	doi = {10.1109/BigData52589.2021.9671877},
	abstract = {Machine learning has not achieved the same degree of success in environmental applications as in other industries. Challenges around data sparsity, quality, and consistency have limited the impact of deep neural network approaches and restricted the focus to research applications. An alternative approach – that is more amenable to the characteristics of data coming from disparate IoT devices deployed at different times and locations in the ocean – is to develop many lightweight models that can be readily scaled up or down based on the number of devices available at any time. This paper presents a serverless framework that naturally marries a single IoT sensor device with a forecasting model. Aspects related to data ingestion, data processing, model training and deployment are described. The framework is applied to a fish farm site in Atlantic Canada.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {O’Donncha, Fearghal and Akhriev, Albert and Eck, Bradley and Burke, Meredith and Filgueira, Ramon and Grant, Jon},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Big Data (Big Data)},
	pages = {4091--4096},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\T2QTM3PC\\O’Donncha et al. - 2021 - Deployment and Management of Time Series Forecasts.pdf:application/pdf},
}

@inproceedings{guo_design_2021,
	title = {Design and {Implementation} of a {New} {Serverless} {Conversational} {Survey} {System}},
	url = {https://ieeexplore.ieee.org/document/9650203},
	doi = {10.1109/ICDSCA53499.2021.9650203},
	abstract = {Conversational agents or chatbots have great potentials in improving survey responses and accessibility. However, it is still a challenge for researchers without programming skills to create voice-enabled chatbots to conduct customizable surveys. This paper presents a new easy-to-use serverless survey chatbot system based on the existing TigerAware mobile survey platform, called TigerAware chatbot. This chatbot system enables non-technical persons to build and deploy customized surveys on mobile devices as text or voice-based conversations. The system is based on Dialogflow and Firebase Realtime database, supports voice input and social dialog, and provides visual responses for users to select answers among several options. The chatbots can be deployed on any iOS and Android mobile device and platform that support Google Assistant. Survey question types that have been implemented include yes/no, multiple-choice, free response, numeric entry, date/time picker, and scale. This system is more efficient and effective than existing survey chatbot systems.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Data} {Science} and {Computer} {Application} ({ICDSCA})},
	author = {Guo, Wenbin and Zong, Shaoyi and Chen, Songxi and Zhao, Fengxiang and Shang, Yi},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)},
	pages = {358--363},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HP2IYAU4\\9650203.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MJ7ESJ5U\\9650203.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9AJIJ7BA\\9650203.html:text/html},
}

@inproceedings{nagano_design_2021,
	title = {Design and {Implementation} of {Device} {Monitoring} {SaaS} for {DIY}-{IoT} {Systems}},
	isbn = {2158-4001},
	url = {https://ieeexplore.ieee.org/document/9427662},
	doi = {10.1109/ICCE50685.2021.9427662},
	abstract = {We propose a novel SaaS platform, namely, motch to support the operation of the IoT systems developed by the end-users. The proposed motch consists of three modules to monitor the availability of various IoT devices to enable the end-users to easily operate their own IoT system: 1) survival report realization in various IoT devices, 2) Web front-end, and 3) serverless back-end. Performance evaluation shows that the proposed motch SDK and motch daemon can realize availability monitoring for various IoT devices. In addition, the serverless back-end achieves adequate running costs according to the required computational resource for the motch.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Nagano, Motoki and Arai, Yusuke and Fujihashi, Takuya and Watanabe, Takashi and Saruwatari, Shunsuke},
	month = jan,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Consumer Electronics (ICCE)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HHY8FQIM\\9427662.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2YTZIKGG\\9427662.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7XLXAZ87\\9427662.html:text/html},
}

@inproceedings{shah_design_2021,
	title = {Design of a {Reference} {Architecture} for {Serverless} {IoT} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9524180},
	doi = {10.1109/COINS51742.2021.9524180},
	abstract = {The Internet of Things (IoT) proposes the concept of a global and pervasive network of heterogeneous devices that interact with the users and their surroundings improving work efficiency and quality of life. From the conventional software systems development perspective, a good system design and architecture are critical. Its importance increases many-fold when the domain’s objective is to connect an infinite set of heterogeneous devices, i.e., smaller systems should integrate and interoperate with each other with no or low modifications. Therefore, a consensus from a design perspective must be achieved in the community and the corporations working in the IoT domain to achieve this integrability and interoperability. One way to document this consensus is a reference architecture. This paper proposes a reference architecture that will act as a blueprint in developing Serverless IoT systems that integrate and interoperate with each other easily. The motivation behind selecting the serverless computing paradigm for IoT systems is that the system engineers can incorporate more decentralization. Thus, a significant amount of tasks will be handled at the Edge and the Fog layer. Only the heavy functions will be delegated to the Cloud, resulting in more efficient and inexpensive systems. Also, the event-driven programming model at the core of serverless computing, its advantages in scalability, focus on the core business logic, and high cohesion becomes necessary and valuable tools in the toolbox for efficiently developing IoT systems.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Omni}-{Layer} {Intelligent} {Systems} ({COINS})},
	author = {Shah, Neel Pradip},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FFC4Q2LB\\9524180.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S29T5VRF\\9524180.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LXHVITNS\\9524180.html:text/html},
}

@inproceedings{benomar_deviceless_2021,
	title = {Deviceless: {A} {Serverless} {Approach} for the {Internet} of {Things}},
	shorttitle = {Deviceless},
	url = {https://ieeexplore.ieee.org/document/9662096},
	doi = {10.23919/ITUK53220.2021.9662096},
	abstract = {Developers of cloud-native applications have rapidly adopted the Serverless/Function-as-a-Service (FaaS) paradigm as it exempts them from provisioning and operating the infrastructure. Within this context, an interesting approach that can foster IoT applications’ development is extending the serverless paradigm towards the network edge to cover IoT environments: a paradigm that we refer to as ‘deviceless’. In this approach, an IoT infrastructure composed of devices deployed at the network edge can seamlessly be integrated as an application execution infrastructure to enable interactions with hosted sensors/actuators. This paper discusses several perspectives available in the literature on the (IoT) edge-based serverless paradigm and related use cases in an IoT/edge computing context. Besides, we present our preliminary prototype for implementing the deviceless approach to show its viability. We exploit the deviceless paradigm to conceive data pipelines under a flow-based development environment leveraging a geographically distributed IoT infrastructure.},
	urldate = {2024-01-01},
	booktitle = {2021 {ITU} {Kaleidoscope}: {Connecting} {Physical} and {Virtual} {Worlds} ({ITU} {K})},
	author = {Benomar, Zakaria and Longo, Francesco and Merlino, Giovanni and Puliafito, Antonio},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 ITU Kaleidoscope: Connecting Physical and Virtual Worlds (ITU K)},
	keywords = {Cloud computing, Virtualization, FaaS, Serverless, edge computing, IoT, Function-as-a-service, Edge computing, Internet of things, virtualization, serverless, Application development, Cloud-computing, Service paradigm, Application execution, deviceless, Deviceless, Network edges, Virtualizations},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2GRXGD52\\9662096.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EMDPJNNB\\9662096.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FHRIJHFQ\\9662096.html:text/html},
}

@article{yang_ebi-pai_2021,
	title = {{EBI}-{PAI}: {Toward} an {Efficient} {Edge}-{Based} {IoT} {Platform} for {Artificial} {Intelligence}},
	volume = {8},
	issn = {2327-4662},
	shorttitle = {{EBI}-{PAI}},
	url = {https://ieeexplore.ieee.org/document/9174943},
	doi = {10.1109/JIOT.2020.3019008},
	abstract = {Edge computing, especially multiaccess edge computing, is seen as a promising technology to improve the Quality of user Experience (QoE) of many artificial intelligence (AI) applications in the evolution toward Internet-of-Things (IoT) infrastructure. However, the management and deployment of massive edge data centers bring new challenges for the current network. In this article, we propose a new edge-based IoT platform for AI (EBI-PAI), based on software-defined network (SDN) and serverless technology. EBI-PAI provides a unified service calling interface and schedules the resources automatically to satisfy the QoE requirements of users. To optimize performances during incremental deployment, we formulate the deployment problem, prove its complexity, and design heuristic algorithms to solve it. We implement EBI-PAI based on an opensource serverless project and deploy it in real networks. To evaluate EBI-PAI, we conduct comprehensive simulations based on the generated and real-world network topology, and real-world base station data set. The simulation results show that EBI-PAI can greatly improve QoE with the same budget and save the budget to achieve similar QoE. We finally carry out a case study with real user demands, and it further validates the simulation results.},
	number = {12},
	urldate = {2024-01-01},
	journal = {IEEE Internet of Things Journal},
	author = {Yang, Shu and Xu, Kunkun and Cui, Laizhong and Ming, Zhongxing and Chen, Ziteng and Ming, Zhong},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {9580--9593},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\37D4BNFD\\9174943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F9DYL5NX\\9174943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\J3VURPWH\\9174943.html:text/html},
}

@inproceedings{daneshmand_edge--cloud_2021,
	title = {Edge-to-cloud {Virtualized} {Cyberinfrastructure} for {Near} {Real}-time {Water} {Quality} {Forecasting} in {Lakes} and {Reservoirs}},
	url = {https://ieeexplore.ieee.org/document/9582387},
	doi = {10.1109/eScience51609.2021.00024},
	abstract = {The management of drinking water quality is critical to public health and can benefit from techniques and technologies that support near real-time forecasting of lake and reservoir conditions. The cyberinfrastructure (CI) needed to support forecasting has to overcome multiple challenges, which include: 1) deploying sensors at the reservoir requires the CI to extend to the network’s edge and accommodate devices with constrained network and power; 2) different lakes need different sensor modalities, deployments, and calibrations; hence, the CI needs to be flexible and customizable to accommodate various deployments; and 3) the CI requires to be accessible and usable to various stakeholders (water managers, reservoir operators, and researchers) without barriers to entry. This paper describes the CI underlying FLARE (Forecasting Lake And Reservoir Ecosystems), a novel system co-designed in an interdisciplinary manner between CI and domain scientists to address the above challenges. FLARE integrates R packages that implement the core numerical forecasting (including lake process modeling and data assimilation) with containers, overlay virtual networks, object storage, versioned storage, and event-driven Function-as-a-Service (FaaS) serverless execution. It is a flexible forecasting system that can be deployed in different modalities, including the Manual Mode suitable for end-users’ personal computers and the Workflow Mode ideal for cloud deployment. The paper reports on experimental data and lessons learned from the operational deployment of FLARE in a drinking water supply (Falling Creek Reservoir in Vinton, Virginia, USA). Experiments with a FLARE deployment quantify its edge-to-cloud virtual network performance and serverless execution in OpenWhisk deployments on both XSEDE-Jetstream and the IBM Cloud Functions FaaS system.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 17th {International} {Conference} on {eScience} ({eScience})},
	author = {Daneshmand, Vahid and Breef-Pilz, Adrienne and Carey, Cayelan C. and Jin, Yuqi and Ku, Yun-Jung and Subratie, Kensworth C. and Thomas, R. Quinn and Figueiredo, Renato J.},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 17th International Conference on eScience (eScience)},
	pages = {138--148},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WZPD9ARL\\9582387.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YCMGGEUT\\9582387.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GPIDR5LB\\9582387.html:text/html},
}

@article{gu_enclavisor_2021,
	title = {Enclavisor: {A} {Hardware}-{Software} {Co}-{Design} for {Enclaves} on {Untrusted} {Cloud}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {Enclavisor},
	url = {https://ieeexplore.ieee.org/document/9178442},
	doi = {10.1109/TC.2020.3019704},
	abstract = {The releases of Intel SGX and AMD SEV mark the transition of hardware-based enclaves from research prototypes to mainstream products. These two paradigms of secure enclaves are attractive to both the cloud providers and tenants, since security is one of the key pillars of cloud computing. However, it is found that current hardware-defined enclaves are not flexible and efficient enough for the cloud. For example, although SGX can provide strong memory protection with both confidentiality and integrity, the size of secure memory is tightly restricted. On the contrary, SEV enables enclaves to use more memory but has critical security flaws due to no memory integrity protection. Meanwhile, both types of enclaves have relatively long booting latency, which makes them not suitable for short-term tasks like serverless workloads. After an in-depth analysis, we find that there are some intrinsic tradeoffs between security and performance due to the limitation of architectural designs. In this article, we investigate a novel hardware-software co-design of enclaves to meet the requirements of cloud by placing a part of the logic of the enclave mechanism into a lightweight software layer, named Enclavisor, to achieve a balance between security, performance, and flexibility. Specifically, our implementation is based on AMD's SEV and, Enclavisor is placed in the guest kernel mode of SEV's secure virtual machines. Enclavisor inherently supports memory encryption with no memory limitation and also achieves efficient booting, multiple enclave granularities, and post-launch remote attestation. Meanwhile, we also propose hardware/software solutions to mitigate the security flaws caused by the lack of memory integrity. We implement a prototype of Enclavisor on an AMD SEV server. The experiments on both micro-benchmarks and application benchmarks show that enclaves on Enclavisor can have close-to-native performance.},
	number = {10},
	urldate = {2024-01-01},
	journal = {IEEE Transactions on Computers},
	author = {Gu, Jinyu and Wu, Xinyue and Zhu, Bojun and Xia, Yubin and Zang, Binyu and Guan, Haibing and Chen, Haibo},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {1598--1611},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XZM4J9MV\\9178442.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XYP8ZXQV\\9178442.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XDI7WPTR\\9178442.html:text/html},
}

@inproceedings{wang_evolving_2021,
	title = {Evolving the {Edge} and the {Cloud}: {A} hybrid computing paradigm},
	isbn = {2693-9371},
	shorttitle = {Evolving the {Edge} and the {Cloud}},
	url = {https://ieeexplore.ieee.org/document/9741997},
	doi = {10.1109/QRS-C55045.2021.00108},
	abstract = {The edge-to-cloud ecosystem includes cloud management service providers in addition to cloud service providers. The system is also an isolated disorder state, and there are both edge computation and fog computation structures. To realize the effective flow of resources, a hybrid computing paradigm HEC is analyzed with respect to its typical characteristics and applications in various industry scenarios. This paper analyzes the challenges in edge serverless computing, edge big data processing and edge AI, and expounds our views on the technologies adopted, such as offloading, resource allocation, resource provisioning, and deep learning. In the end, it looks forward to the flexible work of HEC integrating resources, which will lead to dynamic, flexible and intelligent changes in computing from edge to cloud.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Wang, Jin},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
	pages = {718--721},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HDRZWF2W\\9741997.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WYPLVVRR\\9741997.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\35KQENG9\\9741997.html:text/html},
}

@inproceedings{borges_faaster_2021,
	title = {{FaaSter} {Troubleshooting} - {Evaluating} {Distributed} {Tracing} {Approaches} for {Serverless} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9610265},
	doi = {10.1109/IC2E52221.2021.00022},
	abstract = {Serverless applications can be particularly difficult to troubleshoot, as these applications are often composed of various managed and partly managed services. Faults are often unpredictable and can occur at multiple points, even in simple compositions. Each additional function or service in a serverless composition introduces a new possible fault source and a new layer to obfuscate faults. Currently, serverless platforms offer only limited support for identifying runtime faults. Developers looking to observe their serverless compositions often have to rely on scattered logs and ambiguous error messages to pinpoint root causes. In this paper, we investigate the use of distributed tracing for improving the observability of faults in serverless applications. To this end, we first introduce a model for characterizing fault observability, then provide a prototypical tracing implementation-specifically, a developer-driven and a platform-supported tracing approach. We compare both approaches with our model, measure associated trade-offs (execution latency, resource utilization), and contribute new insights for troubleshooting serverless compositions.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Borges, Maria C. and Werner, Sebastian and Kilic, Ahmet},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {83--90},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\IYTRW2PW\\Borges et al. - 2021 - FaaSter Troubleshooting - Evaluating Distributed T.pdf:application/pdf},
}

@inproceedings{nasrin_feature_2021,
	title = {Feature and {Performance} {Based} {Comparative} {Study} on {Serverless} {Frameworks}},
	url = {https://ieeexplore.ieee.org/document/9689779},
	doi = {10.1109/ICCIT54785.2021.9689779},
	abstract = {We conduct experiments on the different public clouds provided in this paper to bring out a comparative study, to help the developers understand the diversity of the platforms. This study will help them choose a suitable platform for their desired application. The contemporary and future usage of this diverse nature of cloud computing is hugely demanding. Thus, we elaborate our study on serverless cloud computing to suffice the demand. Serverless prevents a great deal of unessential consumption of power and is a pay-as-you-go service. This technology has added a great impact on software and application development. Although the major obstacle to this development field is that there is not enough documentation on how the big companies provide this facility and how their architecture is built. The comparative study on this diverse platform is missing in the literature. Therefore, our research is based on the on-demand serverless use cases and comparative study with necessary measures. This can be effective and efficient to use for further serverless implementation. Hence, we and others can follow our research for understanding the technical complexity.},
	urldate = {2024-01-01},
	booktitle = {2021 24th {International} {Conference} on {Computer} and {Information} {Technology} ({ICCIT})},
	author = {Nasrin, Sabiha and Sahryer, T.I.M. Fahim and Al Islam, A. B. M. Alim and Noor, Jannatun},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 24th International Conference on Computer and Information Technology (ICCIT)},
	pages = {1--6},
}

@inproceedings{gharibi_federatedtree_2021,
	title = {{FederatedTree}: {A} {Secure} {Serverless} {Algorithm} for {Federated} {Learning} to {Reduce} {Data} {Leakage}},
	shorttitle = {{FederatedTree}},
	url = {https://ieeexplore.ieee.org/document/9672039},
	doi = {10.1109/BigData52589.2021.9672039},
	abstract = {In Federated Learning there have been many op-timization methods that allow flexible local updating such as FedAvg that has become the de facto mechanism for averaging local stochastic gradient descent without sharing the data. Classic FL methods such as FedAvg struggle with trust and data leakage issues. In FedAvg and similar techniques, clients assume the aggregator server is a trusted but curious server. However, even if the server is trusted, the models still leak a lot of data through the weights. Several techniques have been proposed to reduce data leakage. One mechanism involves sharing pieces of the data with the server, but it violates the key privacy assumption of federated learning. Other solutions such as Federated Learning with Differential Privacy aim to reduce data leakage by adding noise to the weights/gradients. However, there is a trade-off between accuracy and the amount of noise added.In this paper, we propose a practical Federated Learning algorithm of deep neural networks on iterative model averaging we called FederatedTree. While FedAvg with differential privacy adds noise to the weights to provide a level of privacy, our algorithm applies a secure sequential averaging without adding noise to the models. FederatedTree solves the trust issue between client-to-client, client-to-server (if exists) and reduces the amount of data leakage without adding noise that lowers the model accuracy. The results show that the FederatedTree algorithm provides a high privacy rate with higher accuracy on popular datasets: MNIST, Fashion MNIST, CIFAR-10. Furthermore, FederatedTree utilizes a binary tree structure to reduce the sequential averaging time and remove the overhead of the excessive communication between the server and the clients.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Gharibi, Mohamed and Bhagavan, Srini and Rao, Praveen},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Big Data (Big Data)},
	pages = {4078--4083},
}

@inproceedings{grafberger_fedless_2021,
	title = {{FedLess}: {Secure} and {Scalable} {Federated} {Learning} {Using} {Serverless} {Computing}},
	shorttitle = {{FedLess}},
	url = {https://ieeexplore.ieee.org/document/9672067},
	doi = {10.1109/BigData52589.2021.9672067},
	abstract = {The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables clients to learn a shared ML model while keeping the data local. However, conventional FL systems face challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Grafberger, Andreas and Chadha, Mohak and Jindal, Anshul and Gu, Jianfeng and Gerndt, Michael},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Big Data (Big Data)},
	keywords = {serverless computing, Serverless computing, Function-as-a-service, Deep learning, Scalability, Service provider, Federated learning, Central servers, Data privacy, deep learning, federated learning, Federated learning system, Function-as-a-service (FaaS), Infrastructure managements, Learning paradigms, Training data},
	pages = {164--173},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\WIMPJDJG\\Grafberger et al. - 2021 - FedLess Secure and Scalable Federated Learning Us.pdf:application/pdf},
}

@inproceedings{bhagavan_fedsmarteum_2021,
	title = {{FedSmarteum}: {Secure} {Federated} {Matrix} {Factorization} {Using} {Smart} {Contracts} for {Multi}-{Cloud} {Supply} {Chain}},
	shorttitle = {{FedSmarteum}},
	url = {https://ieeexplore.ieee.org/document/9671789},
	doi = {10.1109/BigData52589.2021.9671789},
	abstract = {With increased awareness comes unprecedented expectations. We live in a digital, cloud era wherein the underlying information architectures are expected to be elastic, secure, resilient, and handle petabyte scaling. The expectation of epic proportions from the next generation of the data frameworks is to not only do all of the above but also build it on a foundation of trust and explainability across multi-organization business networks. From cloud providers to automobile industries or even vaccine manufacturers, components are often sourced by a complex, not full digitized thread of disjoint suppliers. Building Machine Learning and AI-based order fulfillment and predictive models, remediating issues, is a challenge for multi-organization supply chain automation. We posit that Federated Learning in conjunction with blockchain and smart contracts are technologies primed to tackle data privacy and centralization challenges. In this paper, motivated by challenges in the industry, we propose a decentralized distributed system in conjunction with a recommendation system model (Matrix Factorization) that is trained using Federated Learning on an Ethereum blockchain network. We leverage smart contracts that allow decentralized serverless aggregation to update local-ized items vectors. Furthermore, we utilize Homomorphic Encryption (HE) to allow sharing the encrypted gradients over the network while maintaining their privacy. Based on our results, we argue that training a model over a serverless Blockchain network using smart contracts will provide the same accuracy as in a centralized model while maintaining our serverless model privacy and reducing the overhead communication to a central server. Finally, we assert such a system that provides transparency, audit-ready and deep insights into supply chain operations for enterprise cloud customers resulting in cost savings and higher Quality of Service (QoS).},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Bhagavan, Srini and Gharibi, Mohamed and Rao, Praveen},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Big Data (Big Data)},
	pages = {4054--4063},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\SYDJ5HF6\\Bhagavan et al. - 2021 - FedSmarteum Secure Federated Matrix Factorization.pdf:application/pdf},
}

@inproceedings{roy_gaffer_2021,
	title = {Gaffer: {Cloud} {Computing} based {Serverless} {Orchestration} {Framework} for {Unprecedented} {Workflow}},
	shorttitle = {Gaffer},
	url = {https://ieeexplore.ieee.org/document/9544528},
	doi = {10.1109/ICIRCA51532.2021.9544528},
	abstract = {Modern serverless orchestrations are based on either simple step chaining or function invocations. This raises the challenges of implementing high-performance systems. In the context of this, the paper explores how a system can be structured to scale across cross-platform infrastructures by design. This research work has proposed a framework that can be used under circumstances where heavy computation with data consistent results are demanded from the serverless networks, in which the instances depend upon multiple nested servers and the flow of the data is unprecedented without worrying about concurrency and function invocation limitation resulting in zero throttling of requests on the service. This means that the function that needs to be triggered next will be decided by the present function which can demand multiple arguments to run that are to be provided by some other functions. The algorithm is based on the Gaffer Framework which is designed to take care of resource allocation, maintaining the function(servers) level order dependency also sub-level dependency, and resource integration using event-driven semaphore signals. The framework proposed in this paper demonstrates high coordination-free consistency along with the performance that exceeds the current orchestrations. Gaffer Framework run-time system also takes care of distributed data partitioning and efficient handling of machine failures. This makes it simple for programmers with little or no expertise with serverless and distributed systems to create and maintain large interconnected serverless containers.},
	urldate = {2024-01-01},
	booktitle = {2021 {Third} {International} {Conference} on {Inventive} {Research} in {Computing} {Applications} ({ICIRCA})},
	author = {Roy, Saurav and Kolanu, Shreekar and S, Krishnaveni},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)},
	pages = {1054--1060},
}

@inproceedings{ghasemshirazi_gitcbot_2021,
	title = {{GITCBot}: {A} {Novel} {Approach} for the {Next} {Generation} of {C}\&{C} {Malware}},
	shorttitle = {{GITCBot}},
	url = {https://ieeexplore.ieee.org/document/9420590},
	doi = {10.1109/CSICC52343.2021.9420590},
	abstract = {Online Social Networks (OSNs) attracted millions of users in the world. OSNs made adversaries more passionate to create malware variants to subvert the cyber defence of OSNs. Through various threat vectors, adversaries persuasively lure OSN users into installing malware on their devices at an enormous scale. One of the most horrendous forms of named malware is OSNs' botnets that conceal C\&C information using OSNs' accounts of unaware users. In this paper, we present GITC (Ghost In The Cloud), which uses Telegram as a C\&C server to communicate with threat actors and access targets' information in an undetectable way. Furthermore, we present our implementation of GITC. We show how GITC uses the encrypted telegram Application Programming Interface (API) to cover up records of the adversary connections to the target, and we discuss why current intrusion detection systems cannot detect GITC. In the end, we run some sets of experiments that confirm the feasibility of GITC.},
	urldate = {2024-01-01},
	booktitle = {2021 26th {International} {Computer} {Conference}, {Computer} {Society} of {Iran} ({CSICC})},
	author = {Ghasemshirazi, Saeid and Shirvani, Ghazaleh},
	month = mar,
	year = {2021},
	note = {Journal Abbreviation: 2021 26th International Computer Conference, Computer Society of Iran (CSICC)},
	pages = {1--6},
}

@inproceedings{chahal_high_2021,
	title = {High {Performance} {Serverless} {Architecture} for {Deep} {Learning} {Workflows}},
	url = {https://ieeexplore.ieee.org/document/9499397},
	doi = {10.1109/CCGrid51090.2021.00096},
	abstract = {Serverless architecture is a rapidly growing paradigm for deploying deep learning applications performing ephemeral computing and serving bursty workloads. Serverless architecture promises automatic scaling and cost efficiency for inferencing deep learning models while minimizing the operational logic. However, serverless computing is stateless with constraints on local resources. Hence, deploying complex deep learning applications containing large size models, frameworks, and libraries is a challenge.In this work, we discuss a methodology and architecture for migrating deep vision algorithms and model based applications to a serverless computing platform. We have tested our methodology using AWS infrastructure (AWS Lambda, Provisioned Concurrency, VPC endpoint, S3 and EFS) to mitigate the challenges in deploying composition of APIs containing large deep learning models and frameworks. We evaluate the performance and cost of our architecture for a real-life enterprise application used for document processing.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Chahal, Dheeraj and Ramesh, Manju and Ojha, Ravi and Singhal, Rekha},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {790--796},
}

@article{chiang_hysteretic_2021,
	title = {Hysteretic {Optimality} of {Container} {Warming} {Control} in {Serverless} {Computing} {Systems}},
	volume = {3},
	issn = {2576-3156},
	url = {https://ieeexplore.ieee.org/document/9447030},
	doi = {10.1109/LNET.2021.3086489},
	abstract = {Keeping containers warm for prompt service responses and reducing warm containers for light-weight system management exhibit a fundamental tradeoff in serverless computing systems. In this letter, we investigate the problem of container warming control for serverless computing, and we formulate it as a Markov decision process (MDP). By observing that the value functions corresponding to the MDP are partially submodular, we show that the derived optimal policy is hysteretic and partially non-decreasing. Our numerical results show that the derived optimal policy exhibits a hysteretic structure, which can be realized via switching-up/-down thresholds in practice.},
	number = {3},
	urldate = {2024-01-01},
	journal = {IEEE Networking Letters},
	author = {Chiang, Yi-Han and Zhu, Chao and Lin, Hai and Ji, Yusheng},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Networking Letters},
	pages = {138--141},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D32N5PWK\\9447030.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UXBJLQFW\\9447030.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T8R3LRTR\\9447030.html:text/html},
}

@inproceedings{apostolou_fog_2021,
	title = {In the {Fog}: {Application} {Deployment} for the {Cloud} {Continuum}},
	shorttitle = {In the {Fog}},
	url = {https://ieeexplore.ieee.org/document/9555532},
	doi = {10.1109/IISA52424.2021.9555532},
	abstract = {Serverless and the Function-as-a-Service (FaaS) paradigms are seen as two enabling technologies for next-generation computing on the cloud continuum. This article discusses prominent frameworks to deploy and monitor applications that span the cloud continuum. It discusses associated challenges and proposes a novel architecture for a framework that manages intelligently multi-cloud, fog and edge resources in order to cope with the requirements of FaaS-enabled applications and services.},
	urldate = {2024-01-01},
	booktitle = {2021 12th {International} {Conference} on {Information}, {Intelligence}, {Systems} \& {Applications} ({IISA})},
	author = {Apostolou, Dimitris and Verginadis, Yiannis and Mentzas, Gregoris},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 12th International Conference on Information, Intelligence, Systems \& Applications (IISA)},
	keywords = {Serverless, Fog, Fog computing, serverless, Service paradigm, Multi-clouds, Application deployment, cloud continuum, Cloud continuum, Edge resources, Enabling technologies, fog computing, Novel architecture},
	pages = {1--7},
}

@inproceedings{yadav_-browser_2021,
	title = {In-{Browser} {Attendance} {System} using {Face} {Recognition} and {Serverless} {Edge} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9580042},
	doi = {10.1109/ICCCNT51525.2021.9580042},
	abstract = {Because of Covid-19, schools, colleges, and institutions have moved to online learning. The education system has encountered and continues to encounter various challenges in this online format in managing the attendance of the students. The teacher used to call out the students' roll numbers or names when they were in the physical education mode. Nowadays as the world is developing towards a digital era, numerous techniques of collecting attendance such as attendance via biometric technologies like eye recognition, face scanning, voice recognition, fingerprint analysis have earned a lot of fame. Face recognition is the most efficient of these approaches as the face can be captured using a camera and compared using a trained model, but the others are more complex to implement at the user end, and some even need hardware. A lot of research work has been already done related to face recognition using models such as YOLO, MTCNN, FaceNet, HOG, LBPH, C2D-CNN. Models are usually loaded in the backend which causes latency issues and makes the system inefficient to use. Our proposed system aims to perform face recognition within the browser itself with the help of serverless edge computing. For the students, a simple web portal is developed, from which they can navigate to our plugin extension, where the model will capture attendance and dynamically update it in a Google Sheet. Face detection was done with Tiny Face Detector, while face recognition was done with Face Recognition Net. A few more models operate in conjunction with these two, recognizing the student from his or her livestream, checking the student's authenticity using logged in credentials, and updating the attendance in real-time across the browser.},
	urldate = {2024-01-01},
	booktitle = {2021 12th {International} {Conference} on {Computing} {Communication} and {Networking} {Technologies} ({ICCCNT})},
	author = {Yadav, Deepak and Maniar, Sarthak and Sukhani, Krish and Devadkar, Kailas},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
	pages = {01--06},
}

@inproceedings{tankov_infrastructure_2021,
	title = {Infrastructure in {Code}: {Towards} {Developer}-{Friendly} {Cloud} {Applications}},
	isbn = {2643-1572},
	shorttitle = {Infrastructure in {Code}},
	url = {https://ieeexplore.ieee.org/document/9678943},
	doi = {10.1109/ASE51524.2021.9678943},
	abstract = {The popularity of cloud technologies has led to the development of a new type of applications that specifically target cloud environments. Such applications require a lot of cloud infrastructure to run, which brought about the Infrastructure as Code approach, where the infrastructure is also coded using a separate language in parallel to the main application. In this paper, we propose a new concept of Infrastructure in Code, where the infrastructure is deduced from the application code itself, without the need for separate specifications. We describe this concept, discuss existing solutions that can be classified as Infrastructure in Code and their limitations, and then present our own framework called Kotless — an extendable cloud-agnostic serverless framework for Kotlin that supports two cloud providers, three DSLs, and two runtimes. Finally, we showcase the usefulness of Kotless by demonstrating its efficiency in migrating an existing application to a serverless environment.},
	urldate = {2024-01-01},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Tankov, Vladislav and Valchuk, Dmitriy and Golubev, Yaroslav and Bryksin, Timofey},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	pages = {1166--1170},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L9XXV7DQ\\9678943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\P4LJ774V\\9678943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4BLYU34S\\9678943.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\AUMSRF6D\\Tankov et al. - 2021 - Infrastructure in Code Towards Developer-Friendly.pdf:application/pdf},
}

@inproceedings{fox_introduction_2021,
	title = {Introduction to {Digital} {Libraries}},
	url = {https://ieeexplore.ieee.org/document/9651898},
	doi = {10.1109/JCDL52503.2021.00074},
	abstract = {This tutorial is a thorough and deep introduction to the Digital Libraries (DL) field, providing a firm foundation: covering key concepts and terminology, as well as services, systems, technologies, methods, standards, projects, issues, and practices. It introduces and builds upon a firm theoretical foundation (starting with the ‘5S’ set of intuitive aspects: Streams, Structures, Spaces, Scenarios, Societies), giving careful definitions and explanations of all the key parts of a ‘minimal digital library’, and expanding from that basis to cover key DL issues. Illustrations come from a set of case studies, including from multiple current projects, including with the application of natural language processing and machine learning to webpages, tweets, and long documents. Attendees will be exposed to four Morgan and Claypool books that elaborate on 5S. Further, new material will be added on building digital libraries using container and cloud services, on developing a digital library for electronic theses and dissertations, and methods to integrate UX and DL design approaches.},
	urldate = {2024-01-01},
	booktitle = {2021 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	author = {Fox, Edward A. and Chen, Yinlin},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
	pages = {354--355},
}

@inproceedings{chen_low-latency_2021,
	title = {Low-latency {Serverless} {Computing}: {Characterization}, {Optimization} and {Outlooking}: {JCC} 2021 {Invited} {Keynote}},
	shorttitle = {Low-latency {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9566166},
	doi = {10.1109/JCC53141.2021.00008},
	abstract = {Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Serverless computing promises cost-efficiency and elasticity for high-productive software development. To achieve this, the serverless computing platform must address two challenges: strong isolation between function instances, and low startup latency to ensure user experience. In this talk, I will first present a characterization of state-of-the-art serverless platform and derive several key metrics, which collectly forms a systematic methodology and a benchmark called severlessbench. Then, I will show how severless platform can be optimized for (sub-)millisecond startup latency for both normal and condential severless computing. Finally, I will present an outlook on challenges and opportunities of serverless comptuting, including how to make it secure and efficient for joint cloud computing.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Joint} {Cloud} {Computing} ({JCC})},
	author = {Chen, Haibo},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Joint Cloud Computing (JCC)},
	pages = {xii--xii},
}

@inproceedings{mastenbroek_opendc_2021,
	title = {{OpenDC} 2.0: {Convenient} {Modeling} and {Simulation} of {Emerging} {Technologies} in {Cloud} {Datacenters}},
	shorttitle = {{OpenDC} 2.0},
	url = {https://ieeexplore.ieee.org/abstract/document/9499454},
	doi = {10.1109/CCGrid51090.2021.00055},
	abstract = {Cloud datacenters are important for the digital society, serving stakeholders across industry, government, and academia. Simulation is a critical part of exploring datacenter technologies, enabling scalable experimentation with millions of jobs and hundreds of thousands of machines, and what-if analysis in a matter of minutes to hours. Although the community has already developed powerful simulators, emerging technologies and applications in modern datacenters require new approaches. Addressing this requirement, in this work we propose OpenDC, a new platform for datacenter simulation. OpenDC includes novel models for emerging cloud-datacenter technologies and applications, such as serverless computing with FaaS deployment and TensorFlow-based machine learning. Our design also focuses on convenience, with a web-based interface for interactive experimentation, support for experiment automation, a library of prefabs for constructing and sharing datacenter designs, and support for diverse input formats and output metrics. We implement, validate, and open-source OpenDC 2.0, a significant redesign and release after a multi-year research and development process. We demonstrate the benefits of OpenDC for the field through a set of representative use-cases: serverless, machine learning, procurement of HPC-as-a-Service infrastructure, educational practices, and reproducibility studies. Overall, OpenDC helps understand how datacenters work, design datacenter infrastructure, and train the next generation of experts.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Mastenbroek, Fabian and Andreadis, Georgios and Jounaid, Soufiane and Lai, Wenchen and Burley, Jacob and Bosch, Jaro and van Eyk, Erwin and Versluis, Laurens and van Beek, Vincent and Iosup, Alexandru},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {455--464},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EFLFYLSD\\9499454.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BWRGAYUU\\9499454.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BSLNQ4WN\\9499454.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\3NTTCZ56\\Mastenbroek et al. - 2021 - OpenDC 2.0 Convenient Modeling and Simulation of .pdf:application/pdf},
}

@article{pelle_operating_2021,
	title = {Operating {Latency} {Sensitive} {Applications} on {Public} {Serverless} {Edge} {Cloud} {Platforms}},
	volume = {8},
	issn = {2327-4662},
	url = {https://ieeexplore.ieee.org/document/9279315},
	doi = {10.1109/JIOT.2020.3042428},
	abstract = {Cloud native programming and serverless architectures provide a novel way of software development and operation. A new generation of applications can be realized with features never seen before while the burden on developers and operators will be reduced significantly. However, latency sensitive applications, such as various distributed IoT services, generally do not fit in well with the new concepts and today's platforms. In this article, we adapt the cloud native approach and related operating techniques for latency sensitive IoT applications operated on public serverless platforms. We argue that solely adding cloud resources to the edge is not enough and other mechanisms and operation layers are required to achieve the desired level of quality. Our contribution is threefold. First, we propose a novel system on top of a public serverless edge cloud platform, which can dynamically optimize and deploy the microservice-based software layout based on live performance measurements. We add two control loops and the corresponding mechanisms which are responsible for the online reoptimization at different timescales. The first one addresses the steady-state operation, while the second one provides fast latency control by directly reconfiguring the serverless runtime environments. Second, we apply our general concepts to one of today's most widely used and versatile public cloud platforms, namely, Amazon's AWS, and its edge extension for IoT applications, called Greengrass. Third, we characterize the main operation phases and evaluate the overall performance of the system. We analyze the performance characteristics of the two control loops and investigate different implementation options.},
	number = {10},
	urldate = {2024-01-01},
	journal = {IEEE Internet of Things Journal},
	author = {Pelle, István and Czentye, János and Dóka, János and Kern, András and Gerő, Balázs P. and Sonkoly, Balázs},
	month = may,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {7954--7972},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D2PIRLE8\\9279315.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9GYEXQ5A\\9279315.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WICS8TAP\\9279315.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\T8GFJZZG\\Pelle et al. - 2021 - Operating Latency Sensitive Applications on Public.pdf:application/pdf},
}

@inproceedings{nazari_optimizing_2021,
	title = {Optimizing and {Extending} {Serverless} {Platforms}: {A} {Survey}},
	shorttitle = {Optimizing and {Extending} {Serverless} {Platforms}},
	url = {https://ieeexplore.ieee.org/document/9732138},
	doi = {10.1109/SDS54264.2021.9732138},
	abstract = {Serverless Computing is a new cloud computing paradigm wherein people in academia and industry are actively proposing either interesting improvements or building excellent applications on top of it. AWS, Google Cloud, Microsoft Azure, and IBM are popular samples of public clouds that offer Function-as-a-Service on top of their Serverless Computing platforms. Although this paradigm has had numerous advantages for software developers and programmers, it has introduced new challenges to cloud providers. Factors like fine-grained pricing and pay-as-you-go manner, eliminating the responsibility of resource management on the developer side, promises of elasticity and highly-available service, fault tolerance, auto-scaling, and being able to run embarrassingly parallel jobs make it a suitable platform for developers. On the other hand, efficient resource management, offering low-latency service, and providing proper security/isolation have been partly the main challenges introduced on the cloud provider side. This paper presents a literature review on today's Serverless platform optimizations and extensions that people have proposed and implemented to further capitalize the Serverless infras-tructure. In the end, we will provide the current Serverless paradigm's limitations and a few future directions and research opportunities regarding Serverless Computing.},
	urldate = {2024-01-01},
	booktitle = {2021 {Eighth} {International} {Conference} on {Software} {Defined} {Systems} ({SDS})},
	author = {Nazari, Maziyar and Goodarzy, Sepideh and Keller, Eric and Rozner, Eric and Mishra, Shivakant},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 Eighth International Conference on Software Defined Systems (SDS)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4YVIIHUQ\\9732138.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D8MIM45L\\9732138.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T3N9679P\\9732138.html:text/html},
}

@inproceedings{manner_optimizing_2021,
	title = {Optimizing {Cloud} {Function} {Configuration} via {Local} {Simulations}},
	isbn = {2159-6190},
	url = {https://ieeexplore.ieee.org/document/9582177},
	doi = {10.1109/CLOUD53861.2021.00030},
	abstract = {Function as a Service (FaaS) - the reason why so many practitioners and researchers talk about Serverless Computing - claims to hide all operational concerns. The promise when using FaaS is that users only have to focus on the core business functionality in form of cloud functions. However, a few configuration options remain within the developer's responsibility. Most of the currently available cloud function offerings force the user to choose a memory or other resource setting and a timeout value. CPU is scaled based on the chosen options. At a first glance, this seems like an easy task, but the tradeoff between performance and cost has implications on the quality of service of a cloud function. Therefore, in this paper we present a local simulation approach for cloud functions and support developers in choosing a suitable configuration. The methodology we propose simulates the execution behavior of cloud functions locally, makes the cloud and local environment comparable and maps the local profiling data to a cloud platform. This reduces time during the development and enables developers to work with their familiar tools. This is especially helpful when implementing multi-threaded cloud functions.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 14th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Manner, Johannes and Endreβ, Martin and Böhm, Sebastian and Wirtz, Guido},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)},
	pages = {168--178},
}

@inproceedings{elsakhawy_performance_2021,
	title = {Performance {Analysis} of {Serverless} {Execution} {Environments}},
	url = {https://ieeexplore.ieee.org/document/9514081},
	doi = {10.1109/ICECCE52056.2021.9514081},
	abstract = {Serverless computing has introduced a novel Cloud delivery model that abstracts the infrastructure, platform, and execution layers, allowing users to focus on code development. With such abstraction, users lose insight into the factors that affect their serverless functions' execution performance. In this paper, we investigate the factors affecting serverless functions' execution performance. We examine the impact of container choices, language interpreter versions, and compilation options on a function's execution performance and execution consistency. The results of our investigation illustrate that seemingly trivial decisions can result in significant performance degradations for serverless functions' executions},
	urldate = {2024-01-01},
	booktitle = {2021 {International} {Conference} on {Electrical}, {Communication}, and {Computer} {Engineering} ({ICECCE})},
	author = {Elsakhawy, Mohamed and Bauer, Michael},
	month = jun,
	year = {2021},
	note = {Journal Abbreviation: 2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)},
	pages = {1--6},
}

@inproceedings{jindal_poster_2021,
	title = {Poster: {Function} {Delivery} {Network}: {Extending} {Serverless} to {Heterogeneous} {Computing}},
	isbn = {2575-8411},
	shorttitle = {Poster},
	url = {https://ieeexplore.ieee.org/document/9546403},
	doi = {10.1109/ICDCS51616.2021.00120},
	abstract = {Several of today's cloud applications are spread over heterogeneous connected computing resources and are highly dynamic in their structure and resource requirements. However, serverless computing and Function-as-a-Service (FaaS) platforms are limited to homogeneous clusters and homogeneous functions. We introduce an extension of FaaS to heterogeneous computing and to support heterogeneous functions through a network of distributed heterogeneous target platforms called Function Delivery Network (FDN). A target platform is a combination of a cluster of a homogeneous computing system and a FaaS platform on top of it. FDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function invocations to the right target platform. We showcase the opportunities such as collaborative execution between multiple target platforms and varied target platform's characteristics that the FDN offers in fulfilling two objectives: Service Level Objective (SLO) requirements and energy efficiency when scheduling functions invocations by evaluating over five distributed target platforms.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 41st {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Jindal, Anshul and Chadha, Mohak and Gerndt, Michael and Frielinghaus, Julian and Podolskiy, Vladimir and Chen, Pengfei},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)},
	pages = {1128--1129},
}

@inproceedings{vassiliou-gioles_quality_2021,
	title = {Quality {Assurance} of {Micro}-{Services} - {When} to trust your micro-service test results?},
	isbn = {2693-9371},
	url = {https://ieeexplore.ieee.org/document/9742182},
	doi = {10.1109/QRS-C55045.2021.00024},
	abstract = {Micro-service architecture has become a standard software architecture style, with loosely coupled, specified, and implemented services owned by small teams and independently deployable. In particular, with the emergence of managed services, deployment aspects have to be addressed explicitly. While tools and frameworks support micro-service developers in developing and unit-testing their services, less attention has been given to higher testing levels, particularly to the integration testing phase. This paper identifies aspects that limit the expressiveness and therefore the trust of integration testing and test results in the context of managed micros-services and function as a service. We propose the introduction of instance identification to overcome these limitations and illustrate how instance identification can be used to enhance integration testing's expressiveness and trust into integration test results.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Vassiliou-Gioles, Theofanis},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
	pages = {01--06},
}

@inproceedings{szalay_real-time_2021,
	title = {Real-time task scheduling in a {FaaS} cloud},
	isbn = {2159-6190},
	url = {https://ieeexplore.ieee.org/document/9582186},
	doi = {10.1109/CLOUD53861.2021.00065},
	abstract = {Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgrad-ability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multinode systems make real-time scheduling significantly complex since guaranteeing real-time task execution is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm for a partitioned scheduling system suitable for real-time FaaS platforms of multi-node clusters. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 14th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Szalay, Márk and Mátray, Péter and Toka, László},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 14th International Conference on Cloud Computing (CLOUD)},
	pages = {497--507},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PTMEI9RF\\9582186.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HZ8TJ6Q8\\9582186.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NGC5BYWQ\\9582186.html:text/html},
}

@inproceedings{ozdemir_sat_2021,
	title = {{SAT} {Solving} in the {Serverless} {Cloud}},
	isbn = {2708-7824},
	url = {https://ieeexplore.ieee.org/document/9617671},
	doi = {10.34727/2021/isbn.978-3-85448-046-4_33},
	abstract = {In recent years, cloud service providers have sold computation in increasingly granular units. Most recently, “serverless” executors run a single executable with restricted network access and for a limited time. The benefit of these restrictions is scale: thousand-way parallelism can be allocated in seconds, and CPU time is billed with sub-second granularity. To exploit these executors, we introduce gg-SAT: an implementation of divide-and-conquer SAT solving. Infrastructurally, gg-SAT departs substantially from previous implementations: rather than handling process or server management itself, gg-SAT builds on the gg framework, allowing computations to be executed on a configurable backend, including serverless offerings such as AWS Lambda. Our experiments suggest that when run on the same hardware, gg-SAT performs competitively with other D\&C solvers, and that the 1000-way parallelism it offers (through AWS Lambda) is useful for some challenging SAT instances.},
	urldate = {2024-01-01},
	booktitle = {2021 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	author = {Ozdemir, Alex and Wu, Haoze and Barrett, Clark},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 Formal Methods in Computer Aided Design (FMCAD)},
	pages = {241--245},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\SLBIE26G\\A. Ozdemir et al. - 2021 - SAT Solving in the Serverless Cloud.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PSDF9SPW\\9617671.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UTSN33BS\\9617671.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\44ELRJE6\\9617671.html:text/html},
}

@inproceedings{gusev_serverless_2021,
	title = {Serverless and {Deviceless} {Dew} {Computing}: {Founding} an {Infrastructureless} {Computing}},
	isbn = {0730-3157},
	shorttitle = {Serverless and {Deviceless} {Dew} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9529887},
	doi = {10.1109/COMPSAC51774.2021.00273},
	abstract = {Cloud computing provides computing resources on a subscription basis, targeting infrastructure (hardware), platform (hardware plus system software), and software. Although post-cloud computing models bring the computing closer to the user, they still use the same principles to provide computing resources to the requestor. While building applications, designers face challenges to specify optimal computing resources. Serverless computing is the answer from cloud providers to take care about the availability of computing resources to relieve programmers suggesting to concentrate on programming functions to be executed as a service. Deviceless approach goes even further allowing functions to be executed on nearby devices instead of servers. In this sense, we define infrastructureless computing as the architectural approach where the programming is isolated from specifying the infrastructure requirements, as a general platform of serverless and deviceless computing. Moreover, the generalization of this approach initiates a new computing model where the functions are executed on a lower architectural layer instead of the higher one. For example, an edge server or device, can activate smart devices on the dew computing level and distribute computing to devices (embedded systems) on a lower architectural level. An example may be activating smartphones to perform computing tasks while being charged overnight, or using smart devices installed in cars, while parked in a parking lot. This concept enhances the dew computing architectural model making it a sophisticated platform for future architectural models.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Gusev, Marjan},
	month = jul,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)},
	pages = {1814--1818},
}

@inproceedings{nishimiya_serverless_2021,
	title = {Serverless {Architecture} for {Service} {Robot} {Management} {System}},
	isbn = {2577-087X},
	url = {https://ieeexplore.ieee.org/document/9561824},
	doi = {10.1109/ICRA48506.2021.9561824},
	abstract = {We have developed service robot management system to facilitate effective collaboration between multiple units and types of robots in operation. This system is implemented by serverless architecture on cloud and using cellular based IoT communication. So it has not only usual cloud system advantage that it is not necessary to prepare dedicated server and network equipment, but it reduces management efforts of servers. We have tested the system with robots in a public facility, and successfully confirmed its performance and functionality.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Nishimiya, Kenji and Imai, Yuta},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Robotics and Automation (ICRA)},
	pages = {11379--11385},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LZ879SQF\\9561824.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NX6C4UBD\\9561824.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\95FX4KND\\9561824.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\MQD5VSCA\\Nishimiya and Imai - 2021 - Serverless Architecture for Service Robot Manageme.pdf:application/pdf},
}

@inproceedings{patil_serverless_2021,
	title = {Serverless {Computing} and the {Emergence} of {Function}-as-a-{Service}},
	url = {https://ieeexplore.ieee.org/document/9573962},
	doi = {10.1109/RTEICT52294.2021.9573962},
	abstract = {Serverless computing is one of the most recent additions to the long list of services provided by cloud computing. Improving upon the attributes of scalability, affordability, and granularity present in earlier services offered by cloud computing, serverless computing is the next major development. Function as a Service has further modularized applications and enabled the individual execution of functions using triggers which has led to reduced costs, improved scaling of applications and almost no configuration expenses. Several cloud computing providers have developed their serverless computing services, each having its advantages and disadvantages. This paper aims to describe the need for serverless computing, its working, its economics, applications, its pros and cons in the current state, and the developments over existing technologies. Various serverless computing providers and their services have also been compared and studied.},
	urldate = {2024-01-01},
	booktitle = {2021 {International} {Conference} on {Recent} {Trends} on {Electronics}, {Information}, {Communication} \& {Technology} ({RTEICT})},
	author = {Patil, Rishabh and Chaudhery, Tanveesh Singh and Qureshi, Muhammad Ali and Sawant, Vinaya and Dalvi, Harshal},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 International Conference on Recent Trends on Electronics, Information, Communication \& Technology (RTEICT)},
	pages = {764--769},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8BIUA47Q\\9573962.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BIIPNRFX\\9573962.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UGZMGRUZ\\9573962.html:text/html},
}

@inproceedings{anand_serverless_2021,
	title = {Serverless {Multi}-{Query} {Motion} {Planning} for {Fog} {Robotics}},
	isbn = {2577-087X},
	url = {https://ieeexplore.ieee.org/document/9561571},
	doi = {10.1109/ICRA48506.2021.9561571},
	abstract = {Robots in semi-structured environments such as homes and warehouses sporadically require computation of high-dimensional motion plans. Cloud and fog-based parallelization of motion planning can speed up planning. This can be further made efficient by the use of "serverless" on-demand computing as opposed to always-on high end computers. This paper explores parallelizing the computation of a sampling-based multi-query motion planner based on asymptotically-optimal Probabilistic Road Maps (PRM*) using the simultaneous execution of 100s of cloud-based serverless functions. We propose an algorithm to overcome the communication and bandwidth limitations of serverless computing and use different work-sharing techniques to further optimize the cost and run time. Additionally, we provide proofs of probabilistic completeness and asymptotic optimality. In experiments on synthetic benchmarks and on a physical Fetch robot performing a sequence of decluttering motions, we observe up to a 50x speedup relative to a 4 core edge computer with only a marginally higher cost.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Anand, Raghav and Ichnowski, Jeffrey and Wu, Chenggang and Hellerstein, Joseph M. and Gonzalez, Joseph E. and Goldberg, Ken},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Robotics and Automation (ICRA)},
	pages = {7457--7463},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AYNVYTNB\\9561571.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ISTHS4QM\\9561571.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TKKFQ5YW\\9561571.html:text/html},
}

@inproceedings{suo_tackling_2021,
	title = {Tackling {Cold} {Start} of {Serverless} {Applications} by {Efficient} and {Adaptive} {Container} {Runtime} {Reusing}},
	isbn = {2168-9253},
	url = {https://ieeexplore.ieee.org/document/9556063},
	doi = {10.1109/Cluster48925.2021.00018},
	abstract = {During the past few years, serverless computing has changed the paradigm of application development and deployment in the cloud and edge due to its unique advantages, including easy administration, automatic scaling, built-in fault tolerance, etc. Nevertheless, serverless computing is also facing challenges such as long latency due to the cold start. In this paper, we present an in-depth performance analysis of cold start in the serverless framework and propose HotC, a container-based runtime management framework that leverages the lightweight containers to mitigate the cold start and improve the network performance of serverless applications. HotC maintains a live container runtime pool, analyzes the user input or configuration file, and provides available runtime for immediate reuse. To precisely predict the request and efficiently manage the hot containers, we design an adaptive live container control algorithm combining the exponential smoothing model and Markov chain method. Our evaluation results show that HotC introduces negligible overhead and can efficiently improve the performance of various applications with different network traffic patterns in both cloud servers and edge devices.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Suo, Kun and Son, Junggab and Cheng, Dazhao and Chen, Wei and Baidya, Sabur},
	month = sep,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Cluster Computing (CLUSTER)},
	pages = {433--443},
}

@article{gimenez-alventosa_tascaas_2021,
	title = {{TaScaaS}: {A} {Multi}-{Tenant} {Serverless} {Task} {Scheduler} and {Load} {Balancer} as a {Service}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{TaScaaS}},
	url = {https://ieeexplore.ieee.org/document/9528423},
	doi = {10.1109/ACCESS.2021.3109972},
	abstract = {A combination of distributed multi-tenant infrastructures, such as public Clouds and on-premises installations belonging to different organisations, are frequently used for scientific research because of the high computational requirements involved. Although resource sharing maximises their usage, it typically causes undesirable effects such as the noisy neighbour, producing unpredictable variations of the infrastructure computing capabilities. These fluctuations affect execution efficiency, even of loosely coupled applications, such as many Monte Carlo based simulation programs. This highlights the need of a service capable to handle workload distribution across multiple infrastructures to mitigate these unpredictable performance fluctuations. With this aim, this work introduces TaScaaS, a highly scalable and completely serverless service deployed on AWS to distribute loosely coupled jobs among several computing infrastructures, and load balance them using a completely asynchronous approach to cope with the performance fluctuations with minimum impact in the execution time. We demonstrate how TaScaaS is not only capable of handling these fluctuations efficiently, achieving reduction in execution times up to 45\% in our experiments, but also split the jobs to be computed to meet the user-defined execution time.},
	urldate = {2024-01-01},
	journal = {IEEE Access},
	author = {Giménez-Alventosa, Vicent and Moltó, Germán and Segrelles, J. Damian},
	year = {2021},
	note = {Conference Name: IEEE Access},
	pages = {125215--125228},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D8FEXMA5\\9528423.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XT5GKZBR\\9528423.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A8TNTHLF\\9528423.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\PGW4V2BX\\Giménez-Alventosa et al. - 2021 - TaScaaS A Multi-Tenant Serverless Task Scheduler .pdf:application/pdf},
}

@inproceedings{wang_towards_2021,
	title = {Towards a {Proactive} {Lightweight} {Serverless} {Edge} {Cloud} for {Internet}-of-{Things} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9605384},
	doi = {10.1109/NAS51552.2021.9605384},
	abstract = {Edge cloud solutions that bring the cloud closer to the sensors can be very useful to meet the low latency requirements of many Internet-of-Things (IoT) applications. However, IoT traffic can also be intermittent, so running applications constantly can be wasteful. Therefore, having a serverless edge cloud that is responsive and provides low-latency features is a very attractive option for a resource and cost-efficient IoT application environment.In this paper, we discuss the key components needed to support IoT traffic in the serverless edge cloud and identify the critical challenges that make it difficult to directly use existing serverless solutions such as Knative, for IoT applications. These include overhead from heavyweight components for managing the overall system and software adaptors for communication protocol translation used in off-the-shelf serverless platforms that are designed for large-scale centralized clouds. The latency imposed by ‘cold start’ is a further deterrent.To address these challenges we redesign several components of the Knative serverless framework. We use a streamlined protocol adaptor to leverage the MQTT IoT protocol in our serverless framework for IoT event processing. We also create a novel, event-driven proxy based on the extended Berkeley Packet Filter (eBPF), to replace the regular heavyweight Knative queue proxy. Our preliminary experimental results show that the event-driven proxy is a suitable replacement for the queue proxy in an IoT serverless environment and results in lower CPU usage and a higher request throughput.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Networking}, {Architecture} and {Storage} ({NAS})},
	author = {Wang, Ian-Chin and Qi, Shixiong and Liri, Elizabeth and Ramakrishnan, K. K.},
	month = oct,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE International Conference on Networking, Architecture and Storage (NAS)},
	pages = {1--4},
}

@inproceedings{zhang_towards_2021,
	title = {Towards a {Serverless} {Java} {Runtime}},
	isbn = {2643-1572},
	url = {https://ieeexplore.ieee.org/document/9678709},
	doi = {10.1109/ASE51524.2021.9678709},
	abstract = {Java virtual machine (JVM) has the well-known slow startup and warmup issues. This is because the JVM needs to dynamically create many runtime data before reaching peak performance, including class metadata, method profile data, and just-in-time (JIT) compiled native code, for each run of even the same application. Many techniques are then proposed to reuse and share these runtime data across different runs. For example, Class Data Sharing (CDS) and Ahead-of-time (AOT) compilation aim to save and share class metadata and compiled native code, respectively. Unfortunately, these techniques are developed independently and cannot leverage the ability of each other well. This paper presents an approach that systematically reuses JVM runtime data to accelerate application startup and warmup. We first propose and implement JWarmup, a technique that can record and reuse JIT compilation data (e.g., compiled methods and their profile data). Then, we feed JIT compilation data to the AOT compiler to perform profile-guided optimization (PGO). We also integrate existing CDS and AOT techniques to further optimize application startup. Evaluation on real-world applications shows that our approach can bring a 41.35\% improvement to the application startup. Moreover, our approach can trigger JIT compilation in advance and reduce CPU load at peak time.},
	urldate = {2024-01-01},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Zhang, Yifei and Gu, Tianxiao and Zheng, Xiaolin and Yu, Lei and Kuai, Wei and Li, Sanhong},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	pages = {1156--1160},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UHTSN5VL\\9678709.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BXVQGYDE\\9678709.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H7KYT76N\\9678709.html:text/html},
}

@inproceedings{zhao_understanding_2021,
	title = {Understanding, {Predicting} and {Scheduling} {Serverless} {Workloads} under {Partial} {Interference}},
	isbn = {2167-4337},
	url = {https://ieeexplore.ieee.org/document/9910093},
	doi = {10.1145/3458817.3476215},
	abstract = {Interference among distributed cloud applications can be classified into three types: full, partial and zero. While prior research merely focused on full interference, the partial interference that occurs at parts of applications is far more common yet still lacks in-depth study. Serverless computing that structures applications into small-sized, short-lived functions further exacerbate partial interference. We characterize the features of partial interference in serverless as exhibiting high volatility, spatial-temporal variation, and propagation. Given these observations, we propose an incremental learning predictor, named Gsight, which can achieve high precision by harnessing the spatial-temporal overlap codes and profiles of functions via an end-to-end call path. Experimental results show that Gsight can achieve an average error of 1.71\%. Its convergence speed is at least 3x faster than that in a serverful system. A scheduling case study shows that the proposed method can improve function density by ≥18.79\% while guaranteeing the quality of service (QoS).},
	urldate = {2024-01-01},
	booktitle = {{SC21}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Zhao, Laiping and Yang, Yanan and Li, Yiming and Zhou, Xian and Li, Keqiu},
	month = nov,
	year = {2021},
	note = {Journal Abbreviation: SC21: International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {1--14},
}

@inproceedings{yaqot_unmanned_2021,
	title = {Unmanned {Aerial} {Vehicle} ({UAV}) in {Precision} {Agriculture}: {Business} {Information} {Technology} {Towards} {Farming} as a {Service}},
	shorttitle = {Unmanned {Aerial} {Vehicle} ({UAV}) in {Precision} {Agriculture}},
	url = {https://ieeexplore.ieee.org/document/9515736},
	doi = {10.1109/eSmarTA52612.2021.9515736},
	abstract = {Humanity has facing emerging global issues as new virus diseases, extremes in weather conditions, increasing climatic changes, depletion of the environment and natural resources, sharply rising demand for food, to just name a few. Therefore, the agriculture industry has been challenged in its processes and products, resulting in a surge of application of novel technologies and practices to maintain itself sustainable. Despite that, this industry is still responsible for 37\% of the worldwide workforce, consumes 34\% of the global arable land, utilizes 70\% of the total water, and emits up to 30\% of greenhouse gases (GHG). Progressively widespread in the sector, smart farming is a high-tech, efficient, and sustainable approach achieved by applying integrated technologies within the agricultural value chain processes. It includes increasing feed and food production and decreasing of their waste, prediction of diseases, better estimation of product yields ahead of time, determination of the best harvest time, monitoring of plants-growth cycles, etc. The results are going to yield a sustainable use of soil and water resources while maintaining the green landscape and biodiversity of nature. Emerging remote-sensing technologies and artificial intelligence applications have become essential tools to address these challenges. Drones, also known as Unmanned aerial vehicles (UAVs) are among the most promising industry 4.0 (I4) applications for the next generation of agriculture. This paper is a forehead into applications of drones from the innovation economy's standpoint as a viable tool and an effective manpower replacement in the agro-industry. In such a field, artificial intelligence (AI) has the potential to be the engine for automation of processes to be integrated into cyber-physical systems and enhanced modeling towards improved agriculture, more efficiently than the previous stages of the applications of technologies in this sector. Agricultural communities and businesses must take a strategic approach for continuous improvement production processes by implementing quicker, safer, and cheaper plans through data analytics and farming as a service (FaaS).},
	urldate = {2024-01-01},
	booktitle = {2021 1st {International} {Conference} on {Emerging} {Smart} {Technologies} and {Applications} ({eSmarTA})},
	author = {Yaqot, Mohammed and Menezes, Brenno C.},
	month = aug,
	year = {2021},
	note = {Journal Abbreviation: 2021 1st International Conference on Emerging Smart Technologies and Applications (eSmarTA)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RERYLMRY\\9515736.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HS8TM9K4\\9515736.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZZAP4B4H\\9515736.html:text/html},
}

@inproceedings{martella_virtual_2021,
	title = {Virtual {Device} {Model} extending {NGSI}-{LD} for {FaaS} at the {Edge}},
	url = {https://ieeexplore.ieee.org/abstract/document/9499631},
	doi = {10.1109/CCGrid51090.2021.00079},
	abstract = {Smart environments are composed of an ever-increasing number of heterogeneous resources and devices for collecting and processing of a large amount of context data. These activities can be performed at the edge of the smart area, over a distributed and heterogeneous infrastructure, so to be close to the end-user and optimize response time. However, it is hard to define a data model able to support data exchange among different systems and between systems and users. This paper presents the key features of a smart environment and introduces the concept of virtual device, that is an abstracted component characterized by specific high-level functionalities. Then, the paper proposes a data model useful to represent and optimize the adoption of virtual device in smart environments. To better explain the data model features and benefits, we refer to a video surveillance use case, where a smart camera is able to provide the solid angle detection as a service.},
	urldate = {2024-01-01},
	booktitle = {2021 {IEEE}/{ACM} 21st {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Martella, Francesco and Parrino, Giovanni and Ciulla, Giuseppe and Bernardo, Roberto Di and Celesti, Antonio and Fazio, Maria and Villari, Massimo},
	month = may,
	year = {2021},
	note = {Journal Abbreviation: 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {660--667},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\W9N5TM5C\\9499631.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X2HPHWZJ\\9499631.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XERXSVQB\\9499631.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\DJ9L9RX7\\Martella et al. - 2021 - Virtual Device Model extending NGSI-LD for FaaS at.pdf:application/pdf},
}

@inproceedings{gedeon_microservice_2019,
	title = {A {Microservice} {Store} for {Efficient} {Edge} {Offloading}},
	url = {https://ieeexplore.ieee.org/document/9014114},
	doi = {10.1109/GLOBECOM38437.2019.9014114},
	abstract = {Current edge computing frameworks require tight coupling between mobile clients and surrogates, i.e., the offloaded code has been preconfigured with its required execution environment. In many cases, this includes prior transfers of code blocks or execution environments from mobile devices to the offloading infrastructure. This approach incurs additional latency and is detrimental for the energy consumption of the mobile devices. In this paper, we propose the concept of a microservice store. Using the microservice abstraction common in software development and following the serverless paradigm, we envision a repository through which said services are made accessible to developers and can be re-used across applications. We implement a proof-of-concept edge computing system based on a microservice repository and demonstrate its benefits with real-world applications on mobile devices. Our results show that we were able to reduce latencies by up to 14x and save up to 94\% of battery life.},
	urldate = {2024-01-01},
	booktitle = {2019 {IEEE} {Global} {Communications} {Conference} ({GLOBECOM})},
	author = {Gedeon, Julien and Wagner, Martin and Heuschkel, Jens and Wang, Lin and Muhlhauser, Max},
	month = dec,
	year = {2019},
	note = {ISSN: 2576-6813},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DB6PNKZR\\9014114.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TXVBLMFL\\9014114.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H78RCBEH\\9014114.html:text/html},
}

@article{cherif_lightweight_2019,
	title = {A {Lightweight} and {Secure} {Data} {Collection} {Serverless} {Protocol} {Demonstrated} in an {Active} {RFIDs} {Scenario}},
	volume = {18},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3274667},
	doi = {10.1145/3274667},
	abstract = {In the growinggrowing Internet of ThingsThings context, thousands of computingcomputing devices with variousvarious functionalities areare producingproducing data (from environmental sensors or other sourcessources). However, they areare also collectingcollecting, storingstoring, processingprocessing and transmittingtransmitting data to eventually communicatecommunicate them securelysecurely to third partiesparties (e.g., owners of devices or cloud data storagestorage). The deployeddeployed devices areare oftenoften battery-poweredbattery-powered mobilemobile or static nodesnodes equippedequipped with sensors and/or actuators, and they communicatecommunicate usingusing wirelesswireless technologiestechnologies. ExamplesExamples includeinclude unmannedunmanned aerialaerial vehiclesvehicles, wirelesswireless sensor nodesnodes, smart beacons, and wearablewearable healthhealth objects. Such resource-constrainedresource-constrained devices includeinclude ActiveActive Radio Frequency IDentification (RFID) nodesnodes, and thesethese areare usedused to illustrateillustrate our proposal. In most scenariosscenarios, thesethese nodesnodes areare unattended in an adverseadverse environment, so data confidentiality must be ensuredensured from the sensingsensing phasephase throughthrough to delivery to authorizedauthorized entitiesentities: in other words, data must be securelysecurely storedstored and transmitted to prevent attack by activeactive adversariesadversaries even if the nodesnodes areare capturedcaptured. However, duedue to the scarcescarce resourcesresources availableavailable to nodesnodes in terms of energy, storagestorage, and/or computation, the proposedproposed security solution has to be lightweightlightweight. In this articlearticle, we proposepropose a serverless protocol to enableenable MobileMobile Data Collectors (MDCs), such as dronesdrones, to securelysecurely collect data from mobilemobile and static ActiveActive RFID nodesnodes and then deliver them later to an authorizedauthorized third party. The wholewhole solution ensuresensures data confidentiality at each step (from the sensingsensing phasephase, beforebefore data collection by the MDC, onceonce data havehave been collected by MDC, and duringduring final delivery), whilewhile fulfillingfulfilling the lightweightlightweight requirementsrequirements for the resource-limitedresource-limited entitiesentities involvedinvolved. To assess the suitabilitysuitability of the protocol againstagainst the performanceperformance requirementsrequirements, it was implemented on the most resource-constrainedresource-constrained devices to get the worst possiblepossible results. In addition, to proveprove the protocol fulfills the security requirementsrequirements, it was analyzedanalyzed usingusing security gamesgames and also formally verifiedverified usingusing the AVISPA and ProVerif tools.},
	number = {3},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Embedded ComputingComputing Systems},
	author = {Cherif, Amina and Belkadi, Malika and Sauveron, Damien},
	month = apr,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {active RFID nodes, Data collection protocol, data confidentiality, lightweight cryptography, serverless protocol, activeactive RFID nodesnodes, lightweightlightweight cryptography},
	pages = {27:1--27:27},
}

@article{chun_cloud_2019,
	title = {Cloud computing and running code on {Google} cloud},
	volume = {35},
	issn = {1937-4771},
	url = {https://dl.acm.org/doi/10.5555/3381569.3381571},
	abstract = {Cloud computing skills are critical for student career readiness, and more cloud should be integrated into the curriculum. This lecture+hands-on workshop shows you how! The lecture begins with a vendor-agnostic tour of cloud computing to ensure common vocabulary and pedagogy focus. An overview of Google Cloud Platform (GCP) and G Suite developer tools follows. Products covered can be applied to many courses. Lecture also covers Google education grants (in qualifying countries) which include both teaching and research grants. In the hands-on component, attendees learn how to use our REST APIs and run code on our serverless platforms via hands-on tutorials},
	number = {3},
	journal = {Journal of Computing Sciences in Colleges},
	author = {Chun, Wesley},
	month = oct,
	year = {2019},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {19--20},
}

@article{li_cloud-native_2019,
	title = {Cloud-native database systems at {Alibaba}: opportunities and challenges},
	volume = {12},
	issn = {2150-8097},
	shorttitle = {Cloud-native database systems at {Alibaba}},
	url = {https://doi.org/10.14778/3352063.3352141},
	doi = {10.14778/3352063.3352141},
	abstract = {Cloud-native databases become increasingly important for the era of cloud computing, due to the needs for elasticity and on-demand usage by various applications. These challenges from cloud applications present new opportunities for cloud-native databases that cannot be fully addressed by traditional on-premise enterprise database systems. A cloud-native database leverages software-hardware co-design to explore accelerations offered by new hardware such as RDMA, NVM, kernel bypassing protocols such as DPDK. Meanwhile, new design architectures, such as shared storage, enable a cloud-native database to decouple computation from storage and provide excellent elasticity. For highly concurrent workloads that require horizontal scalability, a cloud-native database can leverage a shared-nothing layer to provide distributed query and transaction processing. Applications also require cloud-native databases to offer high availability through distributed consensus protocols. At Alibaba, we have explored a suite of technologies to design cloud-native database systems. Our storage engine, X-Engine and PolarFS, improves both write and read throughputs by using a LSM-tree design and self-adapted separation of hot and cold data records. Based on these efforts, we have designed and implemented POLARDB and its distributed version POLARDB-X, which has successfully supported the extreme transaction workloads during the 2018 Global Shopping Festival on November 11, 2018, and achieved commercial success on Alibaba Cloud. We have also designed an OLAP system called AnalyticDB (ADB in short) for enabling real-time interactive data analytics for big data. We have explored a self-driving database platform to achieve autoscaling and intelligent database management. We will report key technologies and lessons learned to highlight the technical challenges and opportunities for cloud-native database systems at Alibaba.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Feifei},
	month = aug,
	year = {2019},
	note = {Publisher: VLDB Endowment},
	pages = {2263--2272},
}

@article{jangda_formal_2019,
	title = {Formal foundations of serverless computing},
	volume = {3},
	url = {https://dl.acm.org/doi/10.1145/3360575},
	doi = {10.1145/3360575},
	abstract = {Serverless computing (also known as functions as a service) is a new cloud computing abstraction that makes it easier to write robust, large-scale web services. In serverless computing, programmers write what are called serverless functions, which are programs that respond to external events. When demand for the serverless function spikes, the platform automatically allocates additional hardware and manages load-balancing; when demand falls, the platform silently deallocates idle resources; and when the platform detects a failure, it transparently retries affected requests. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major cloud computing platforms. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting λλ, an operational semantics of the essence of serverless computing. Despite being a small (half a page) core calculus, λλ models all the low-level details that serverless functions can observe. To show that λλ is useful, we present three applications. First, to ease reasoning about code, we present a simplified naive semantics of serverless execution and precisely characterize when the naive semantics and λλ coincide. Second, we augment λλ with a key-value store to allow reasoning about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend λλ with a composition language and show that our implementation can outperform prior work.},
	number = {OOPSLA},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Jangda, Abhinav and Pinckney, Donald and Brun, Yuriy and Guha, Arjun},
	month = oct,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {serverless computing, distributed computing, formal language semantics},
	pages = {149:1--149:26},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\9WZ5RD8Z\\Jangda et al. - 2019 - Formal foundations of serverless computing.pdf:application/pdf;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\PI76GP8V\\Jangda et al. - 2019 - Formal Foundations of Serverless Computing.pdf:application/pdf},
}

@article{wimmer_initialize_2019,
	title = {Initialize once, start fast: application initialization at build time},
	volume = {3},
	shorttitle = {Initialize once, start fast},
	url = {https://dl.acm.org/doi/10.1145/3360610},
	doi = {10.1145/3360610},
	abstract = {Arbitrary program extension at run time in language-based VMs, e.g., Java's dynamic class loading, comes at a startup cost: high memory footprint and slow warmup. Cloud computing amplifies the startup overhead. Microservices and serverless cloud functions lead to small, self-contained applications that are started often. Slow startup and high memory footprint directly affect the cloud hosting costs, and slow startup can also break service-level agreements. Many applications are limited to a prescribed set of pre-tested classes, i.e., use a closed-world assumption at deployment time. For such Java applications, GraalVM Native Image offers fast startup and stable performance. GraalVM Native Image uses a novel iterative application of points-to analysis and heap snapshotting, followed by ahead-of-time compilation with an optimizing compiler. Initialization code can run at build time, i.e., executables can be tailored to a particular application configuration. Execution at run time starts with a pre-populated heap, leveraging copy-on-write memory sharing. We show that this approach improves the startup performance by up to two orders of magnitude compared to the Java HotSpot VM, while preserving peak performance. This allows Java applications to have a better startup performance than Go applications and the V8 JavaScript VM.},
	number = {OOPSLA},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Wimmer, Christian and Stancu, Codrut and Hofer, Peter and Jovanovic, Vojin and Wögerer, Paul and Kessler, Peter B. and Pliss, Oleg and Würthinger, Thomas},
	month = oct,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {ahead-of-time compilation, compiler, Graal, GraalVM, Java, optimization, virtual machine},
	pages = {184:1--184:29},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\WURPKX62\\Wimmer et al. - 2019 - Initialize once, start fast application initializ.pdf:application/pdf},
}

@article{faas_jam_2019,
	title = {Jam {Today}, {Jam} {Tomorrow}: {Learning} in {Online} {Game} {Jams}},
	volume = {3},
	shorttitle = {Jam {Today}, {Jam} {Tomorrow}},
	url = {https://doi.org/10.1145/3361121},
	doi = {10.1145/3361121},
	abstract = {Game jams, which are game creation events in which developers design and build a game over a short period of time, have been shown to support participatory, active STEM learning. Game jams have expanded from their origins as physically co-located experiences and many are now conducted exclusively online. Though the co-located game jam has been noted as educational, little is known about how learning happens within online game jams. To better understand the ways online game jams support self-development, we interviewed fifteen online jam participants about their learning experiences during their jams. Additionally, we observed and analyzed several jams using activity theory. We found that online game jams support participants' learning through extensive feedback from others during and after the jam. Such feedback sessions were a key social and participatory learning elements for online jams, providing much-need social support. In contrast to the group-focused development in offline jams many participants in our study chose to develop games alone. Many individuals participated in online game jams regularly, treating them as a broader experience than singular events. We use these findings to discuss how we might better design for self-directed learning online and offer suggestions on how to better attune online game jams to the needs of participants.},
	number = {GROUP},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Faas, Travis and Liu, I-ching and Dombrowski, Lynn and Miller, Andrew D.},
	month = dec,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {connectivism, critique, feedback, game jam, indie, self-directed learning},
	pages = {240:1--240:27},
}

@article{akhter_stateful_2019,
	title = {Stateful functions as a service in action},
	volume = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3352063.3352092},
	doi = {10.14778/3352063.3352092},
	abstract = {In the serverless model, users upload application code to a cloud platform and the cloud provider undertakes the deployment, execution and scaling of the application, relieving users from all operational aspects. Although very popular, current serverless offerings offer poor support for the management of local application state, the main reason being that managing state and keeping it consistent at large scale is very challenging. As a result, the serverless model is inadequate for executing stateful, latency-sensitive applications. In this paper we present a high-level programming model for developing stateful functions and deploying them in the cloud. Our programming model allows functions to retain state as well as call other functions. In order to deploy stateful functions in a cloud infrastructure, we translate functions and their data exchanges into a stateful dataflow graph. With this paper we aim at demonstrating that using a modified version of an open-source dataflow engine as a runtime for stateful functions, we can deploy scalable and stateful services in the cloud with surprisingly low latency and high throughput.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Akhter, Adil and Fragkoulis, Marios and Katsifodimos, Asterios},
	month = aug,
	year = {2019},
	note = {Publisher: VLDB Endowment},
	pages = {1890--1893},
}

@article{tsigkanos_cloud_2020,
	title = {Cloud {Deployment} {Tradeoffs} for the {Analysis} of {Spatially} {Distributed} {Internet} of {Things} {Systems}},
	volume = {20},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3381452},
	doi = {10.1145/3381452},
	abstract = {Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.},
	number = {2},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Tsigkanos, Christos and Garriga, Martin and Baresi, Luciano and Ghezzi, Carlo},
	month = may,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {17:1--17:23},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\UKZAVZVM\\Tsigkanos et al. - 2020 - Cloud Deployment Tradeoffs for the Analysis of Spa.pdf:application/pdf},
}

@article{sreekanti_cloudburst_2020,
	title = {Cloudburst: stateful functions-as-a-service},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {Cloudburst},
	url = {https://doi.org/10.14778/3407790.3407836},
	doi = {10.14778/3407790.3407836},
	abstract = {Function-as-a-Service (FaaS) platforms and "serverless" cloud computing are becoming increasingly popular due to ease-of-use and operational simplicity. Current FaaS offerings are targeted at stateless functions that do minimal I/O and communication. We argue that the benefits of serverless computing can be extended to a broader range of applications and algorithms while maintaining the key benefits of existing FaaS offerings. We present the design and implementation of Cloudburst, a stateful FaaS platform that provides familiar Python programming with low-latency mutable state and communication, while maintaining the autoscaling benefits of serverless computing. Cloudburst accomplishes this by leveraging Anna, an autoscaling key-value store, for state sharing and overlay routing combined with mutable caches co-located with function executors for data locality. Performant cache consistency emerges as a key challenge in this architecture. To this end, Cloudburst provides a combination of lattice-encapsulated state and new definitions and protocols for distributed session consistency. Empirical results on benchmarks and diverse applications show that Cloudburst makes stateful functions practical, reducing the state-management overheads of current FaaS platforms by orders of magnitude while also improving the state of the art in serverless consistency.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sreekanti, Vikram and Wu, Chenggang and Lin, Xiayue Charles and Schleier-Smith, Johann and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Tumanov, Alexey},
	month = jul,
	year = {2020},
	note = {Publisher: VLDB Endowment},
	pages = {2438--2452},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\QTLCGETV\\Sreekanti et al. - 2020 - Cloudburst stateful functions-as-a-service.pdf:application/pdf},
}

@article{sheng_virtual_2020,
	title = {From {Virtual} {Strangers} to {IRL} {Friends}: {Relationship} {Development} in {Livestreaming} {Communities} on {Twitch}},
	volume = {4},
	shorttitle = {From {Virtual} {Strangers} to {IRL} {Friends}},
	url = {https://doi.org/10.1145/3415165},
	doi = {10.1145/3415165},
	abstract = {Accounts of the social experience within livestreaming channels vary widely, from the frenetic "crowdroar" offered in some channels to the close-knit, "participatory communities" within others. What kinds of livestreaming communities enable the types of meaningful conversation and connection that support relationship development, and how? In this paper, we explore how personal relationships develop within Twitch, a popular livestreaming service. Interviews with 21 pairs who met initially within Twitch channels illustrate how interactions originating in Twitch's text-based, pseudonymous chat environment can evolve into close relationships, marked by substantial trust and support. Consistent with Walther's hyperpersonal model, these environments facilitate self-disclosure and conversation by reducing physical cues and emphasizing common ground, while frequent, low-stakes interaction allow relationships to deepen over time. Our findings also highlight boundaries of the hyperpersonal model. As group size increases, participants leverage affordances for elevated visibility to spark interactions; as relationships deepen, they incorporate complementary media channels to increase intimacy. Often, relationships become so deep through purely computer-mediated channels that face-to-face meetings become yet another step in a continuum of relationship development. Findings from a survey of 1,367 members of Twitch communities demonstrate how the suitability of these spaces as venues for relational interaction decreases as communities increase in size. Together, these findings illustrate vividly how hyperpersonal interaction functions in the context of real online communities. We consider implications for the design and management of online communities, including their potential for supporting "strong bridges," relationships which combine the benefits of strong ties and network bridges.},
	number = {CSCW2},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Sheng, Jeff T. and Kairam, Sanjay R.},
	month = oct,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {livestreaming, networks, online communities, relationships, strong bridges, tie strength, twitch},
	pages = {94:1--94:34},
}

@article{kuo_set_2020,
	title = {Set the {Configuration} for the {Heart} of the {OS}: {On} the {Practicality} of {Operating} {System} {Kernel} {Debloating}},
	volume = {4},
	shorttitle = {Set the {Configuration} for the {Heart} of the {OS}},
	url = {https://dl.acm.org/doi/10.1145/3379469},
	doi = {10.1145/3379469},
	abstract = {This paper presents a study on the practicality of operating system (OS) kernel debloating---reducing kernel code that is not needed by the target applications---in real-world systems. Despite their significant benefits regarding security (attack surface reduction) and performance (fast boot times and reduced memory footprints), the state-of-the-art OS kernel debloating techniques are seldom adopted in practice, especially in production systems. We identify the limitations of existing kernel debloating techniques that hinder their practical adoption, including both accidental and essential limitations. To understand these limitations, we build an advanced debloating framework named {\textbackslash}tool which enables us to conduct a number of experiments on different types of OS kernels (including Linux and the L4 microkernel) with a wide variety of applications (including HTTPD, Memcached, MySQL, NGINX, PHP and Redis). Our experimental results reveal the challenges and opportunities towards making kernel debloating techniques practical for real-world systems. The main goal of this paper is to share these insights and our experiences to shed light on addressing the limitations of kernel debloating in future research and development efforts.},
	number = {1},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
	author = {Kuo, Hsuan-Chi and Chen, Jianyan and Mohan, Sibin and Xu, Tianyin},
	month = may,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {configuration, debloating, kernel, operating system, os},
	pages = {03:1--03:27},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\SZBH35DL\\Kuo et al. - 2020 - Set the Configuration for the Heart of the OS On .pdf:application/pdf},
}

@article{freeman_tale_2021,
	title = {A {Tale} of {Creativity} and {Struggles}: {Team} {Practices} for {Bottom}-{Up} {Innovation} in {Virtual} {Game} {Jams}},
	volume = {5},
	shorttitle = {A {Tale} of {Creativity} and {Struggles}},
	url = {https://dl.acm.org/doi/10.1145/3449150},
	doi = {10.1145/3449150},
	abstract = {Game jams are intense and time-sensitive online or face-to-face game creation events where a digital game is developed in a relatively short time frame (typically 48 to 72 hours) exploring given design constraints and end results are shared publicly. They have increasingly become emerging sites where non-professional game developers, amateurs, and hobbyists engage in bottom-up technological innovation by collaboratively designing and developing more creative and novel digital products. Drawing on 28 interviews, in this paper we focus on how game developers collaborate as small teams to innovate game design and development from the bottom up in virtual game jams (i.e., exclusively online) and the unique role of virtual game jams in their technological innovation. We contribute to CSCW by providing new empirical evidence of how team practices for innovation may emerge in a novel technology community that is not widely studied before. We also expand a growing research agenda in CSCW on explicating nuanced social behaviors, processes, and consequences of bottom-up technological innovation.},
	number = {CSCW1},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Freeman, Guo and McNeese, Nathan J.},
	month = apr,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {computer-mediated collaboration, game development, indie game development, team practices, technological innovation, virtual game jams},
	pages = {76:1--76:27},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\EWBDD4ZB\\Freeman and McNeese - 2021 - A Tale of Creativity and Struggles Team Practices.pdf:application/pdf},
}

@article{drame-maigne_centralized_2021,
	title = {Centralized, {Distributed}, and {Everything} in between: {Reviewing} {Access} {Control} {Solutions} for the {IoT}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Centralized, {Distributed}, and {Everything} in between},
	url = {https://doi.org/10.1145/3465170},
	doi = {10.1145/3465170},
	abstract = {The Internet of Things is taking hold in our everyday life. Regrettably, the security of IoT devices is often being overlooked. Among the vast array of security issues plaguing the emerging IoT, we decide to focus on access control, as privacy, trust, and other security properties cannot be achieved without controlled access. This article classifies IoT access control solutions from the literature according to their architecture (e.g., centralized, hierarchical, federated, distributed) and examines the suitability of each one for access control purposes. Our analysis concludes that important properties such as auditability and revocation are missing from many proposals while hierarchical and federated architectures are neglected by the community. Finally, we provide an architecture-based taxonomy and future research directions: a focus on hybrid architectures, usability, flexibility, privacy, and revocation schemes in serverless authorization.},
	number = {7},
	urldate = {2024-01-03},
	journal = {ACM Computing Surveys},
	author = {Dramé-Maigné, Sophie and Laurent, Maryline and Castillo, Laurent and Ganem, Hervé},
	month = sep,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Internet of Things, IoT, Access control, security, survey},
	pages = {138:1--138:34},
}

@article{skiadopoulos_dbos_2021,
	title = {{DBOS}: a {DBMS}-oriented operating system},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {{DBOS}},
	url = {https://doi.org/10.14778/3485450.3485454},
	doi = {10.14778/3485450.3485454},
	abstract = {This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.},
	number = {1},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Skiadopoulos, Athinagoras and Li, Qian and Kraft, Peter and Kaffes, Kostis and Hong, Daniel and Mathew, Shana and Bestor, David and Cafarella, Michael and Gadepally, Vijay and Graefe, Goetz and Kepner, Jeremy and Kozyrakis, Christos and Kraska, Tim and Stonebraker, Michael and Suresh, Lalith and Zaharia, Matei},
	month = sep,
	year = {2021},
	note = {Publisher: VLDB Endowment},
	pages = {21--30},
}

@article{burckhardt_durable_2021,
	title = {Durable functions: semantics for stateful serverless},
	volume = {5},
	shorttitle = {Durable functions},
	url = {https://dl.acm.org/doi/10.1145/3485510},
	doi = {10.1145/3485510},
	abstract = {Serverless, or Functions-as-a-Service (FaaS), is an increasingly popular paradigm for application development, as it provides implicit elastic scaling and load based billing. However, the weak execution guarantees and intrinsic compute-storage separation of FaaS create serious challenges when developing applications that require persistent state, reliable progress, or synchronization. This has motivated a new generation of serverless frameworks that provide stateful abstractions. For instance, Azure's Durable Functions (DF) programming model enhances FaaS with actors, workflows, and critical sections. As a programming model, DF is interesting because it combines task and actor parallelism, which makes it suitable for a wide range of serverless applications. We describe DF both informally, using examples, and formally, using an idealized high-level model based on the untyped lambda calculus. Next, we demystify how the DF runtime can (1) execute in a distributed unreliable serverless environment with compute-storage separation, yet still conform to the fault-free high-level model, and (2) persist execution progress without requiring checkpointing support by the language runtime. To this end we define two progressively more complex execution models, which contain the compute-storage separation and the record-replay, and prove that they are equivalent to the high-level model.},
	number = {OOPSLA},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Burckhardt, Sebastian and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S.},
	month = oct,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Serverless, Durable Functions, Programming, Reliable, Service Composition, Services, Workflows},
	pages = {133:1--133:27},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\7WNJK3J6\\Burckhardt et al. - 2021 - Durable functions semantics for stateful serverle.pdf:application/pdf},
}

@article{case_hey_2021,
	title = {Hey {Alexa}! a fun introduction to {AWS} {Lambda}, serverless functions, and {JavaScript} with conversational {AI}: conference workshop},
	volume = {36},
	issn = {1937-4771},
	shorttitle = {Hey {Alexa}! a fun introduction to {AWS} {Lambda}, serverless functions, and {JavaScript} with conversational {AI}},
	url = {https://dl.acm.org/doi/10.5555/3469567.3469574},
	abstract = {Learn how to design and create engaging cutting-edge apps! We'll introduce principles of designing apps that apply freely available conversational AI tools (e.g., Alexa, Google Home, and Siri) [2]. We'll explain the powerful and easy 'serverless' functions that power them - no programming experience required! We'll introduce cloud computing and show the benefits of serverless functions [1]. Along the way, participants will open an Amazon Web Services account and get started as an Alexa developer for free! We'll show how to use AWS Lambda to run code without worrying about servers, complex file transfers, or setup. We'll show you how to easily upload and edit simple web-enabled functions. You'll learn how Alexa can be used to illustrate basic programming concepts like programming flow and enumerated types. Participants get access to the project code for the reference app, checklists for working through the steps to publish, and if they like, can leave with a newly submitted Alexa skill customized for their organization or institution.},
	number = {6},
	journal = {Journal of Computing Sciences in Colleges},
	author = {Case, Denise M.},
	month = apr,
	year = {2021},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {66},
}

@article{liu_operating_2021,
	title = {Operating {Systems} for {Resource}-adaptive {Intelligent} {Software}: {Challenges} and {Opportunities}},
	volume = {21},
	issn = {1533-5399},
	shorttitle = {Operating {Systems} for {Resource}-adaptive {Intelligent} {Software}},
	url = {https://doi.org/10.1145/3425866},
	doi = {10.1145/3425866},
	abstract = {The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is “resource adaptive,” i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the “monolithic OS” that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of “Software-as-a-Service” that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners.},
	number = {2},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Liu, Xuanzhe and Wang, Shangguang and Ma, Yun and Zhang, Ying and Mei, Qiaozhu and Liu, Yunxin and Huang, Gang},
	month = mar,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {machine learning, Operating systems, resource disaggregation, service-oriented},
	pages = {27:1--27:19},
}

@article{yadav_query-driven_2021,
	title = {Query-driven video event processing for the internet of multimedia things},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3476311.3476360},
	doi = {10.14778/3476311.3476360},
	abstract = {Advances in Deep Neural Network (DNN) techniques have revolutionized video analytics and unlocked the potential for querying and mining video event patterns. This paper details GNOSIS, an event processing platform to perform near-real-time video event detection in a distributed setting. GNOSIS follows a serverless approach where its component acts as independent microservices and can be deployed at multiple nodes. GNOSIS uses a declarative query-driven approach where users can write customize queries for spatiotemporal video event reasoning. The system converts the incoming video streams into a continuous evolving graph stream using machine learning (ML) and DNN models pipeline and applies graph matching for video event pattern detection. GNOSIS can perform both stateful and stateless video event matching. To improve Quality of Service (QoS), recent work in GNOSIS incorporates optimization techniques like adaptive scheduling, energy efficiency, and content-driven windows. This paper demonstrates the Occupational Health and Safety query use cases to show the GNOSIS efficacy.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Yadav, Piyush and Salwala, Dhaval and Pontes, Felipe Arruda and Dhingra, Praneet and Curry, Edward},
	month = jul,
	year = {2021},
	note = {Publisher: VLDB Endowment},
	pages = {2847--2850},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\AK7UK4HU\\Yadav et al. - 2021 - Query-driven video event processing for the intern.pdf:application/pdf},
}

@article{krishnan_sdn_2021,
	title = {{SDN} {Enabled} {QoE} and {Security} {Framework} for {Multimedia} {Applications} in {5G} {Networks}},
	volume = {17},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3377390},
	doi = {10.1145/3377390},
	abstract = {The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.},
	number = {2},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Krishnan, Prabhakar and Jain, Kurunandan and Jose, Pramod George and Achuthan, Krishnashree and Buyya, Rajkumar},
	month = apr,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {NFV, SDN, lightweight cryptography, 5th Generation network (5G), Multi-Access Edge Computing (MEC), multimedia communication, network security, network slicing, QoE},
	pages = {39:1--39:29},
}

@article{damiani_blastfunction_2022,
	title = {{BlastFunction}: {A} {Full}-stack {Framework} {Bringing} {FPGA} {Hardware} {Acceleration} to {Cloud}-native {Applications}},
	volume = {15},
	issn = {1936-7406},
	shorttitle = {{BlastFunction}},
	url = {https://doi.org/10.1145/3472958},
	doi = {10.1145/3472958},
	abstract = {“Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.},
	number = {2},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Reconfigurable Technology and Systems},
	author = {Damiani, Andrea and Fiscaletti, Giorgia and Bacis, Marco and Brondolin, Rolando and Santambrogio, Marco D.},
	month = jan,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {cloud-native, autoscaling, Field Programmable Gate Arrays (FPGAs), hardware acceleration, time-sharing},
	pages = {17:1--17:27},
}

@article{das_cdi-e_2022,
	title = {{CDI}-{E}: an elastic cloud service for data engineering},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {{CDI}-{E}},
	url = {https://doi.org/10.14778/3554821.3554825},
	doi = {10.14778/3554821.3554825},
	abstract = {We live in the gilded age of data-driven computing. With public clouds offering virtually unlimited amounts of compute and storage, enterprises collecting data about every aspect of their businesses, and advances in analytics and machine learning technologies, data driven decision making is now timely, cost-effective, and therefore, pervasive. Alas, only a handful of power users can wield today's powerful data engineering tools. For one thing, most solutions require knowledge of specific programming interfaces or libraries. Furthermore, running them requires complex configurations and knowledge of the underlying cloud for cost-effectiveness. We decided that a fundamental redesign is in order to democratize data engineering for the masses at cloud scale. The result is Informatica Cloud Data Integration - Elastic (CDI-E). Since the early 1990s, Informatica has been a pioneer and industry leader in building no-code data engineering tools. Non-experts can express complex data engineering tasks using a graphical user interface (GUI). Informatica CDI-E is built to incorporate the simplicity of GUI in the design layer with an elastic and highly scalable run time to handle data in any format without little to no user input using automated optimizations. Users upload their data to the cloud in any format and can immediately use them in conjunction with their data management and analytic tools of choice using CDI-E GUI. Implementation began in the Spring of 2017, and Informatica CDI-E has been generally available since the Summer of 2019. Today, CDI-E is used in production by a growing number of small and large enterprises to make sense of data in arbitrary formats. In this paper, we describe the architecture of Informatica CDI-E and its novel no-code data engineering interface. The paper highlights some of the key features of CDI-E: simplicity without loss in productivity and extreme elasticity. It concludes with lessons we learned and an outlook of the future.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Das, Prakash and Srivastava, Shivangi and Moskovich, Valentin and Chaturvedi, Anmol and Mittal, Anant and Xiao, Yongqin and Chowdhury, Mosharaf},
	month = aug,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {3319--3331},
}

@article{li_cloud_2022,
	title = {Cloud databases: new techniques, challenges, and opportunities},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Cloud databases},
	url = {https://doi.org/10.14778/3554821.3554893},
	doi = {10.14778/3554821.3554893},
	abstract = {As database vendors are increasingly moving towards the cloud data service, i.e., databases as a service (DBaaS), cloud databases have become prevalent. Compared with the early cloud-hosted databases, the new generation of cloud databases, also known as cloud-native databases, seek for higher elasticity and lower cost by developing new techniques, e.g., compute-storage disaggregation and the log is the database. To better harness the power of these cloud databases, it is important to study and compare the pros and cons of their key techniques. In this tutorial, we offer a comprehensive survey of cloud-native databases. Based on various system architectures, we introduce a taxonomy for the state-of-the-art cloud-native OLTP databases and OLAP databases, respectively. We then take a deep dive into their key techniques regarding storage management, transaction processing, analytical processing, data replication, serverless computing, database recovery, and security. Finally, we discuss the research challenges and opportunities.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Guoliang and Dong, Haowen and Zhang, Chao},
	month = aug,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {3758--3761},
}

@article{da_silva_fog_2022,
	title = {Fog {Computing} {Platforms} for {Smart} {City} {Applications}: {A} {Survey}},
	volume = {22},
	issn = {1533-5399},
	shorttitle = {Fog {Computing} {Platforms} for {Smart} {City} {Applications}},
	url = {https://doi.org/10.1145/3488585},
	doi = {10.1145/3488585},
	abstract = {Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.},
	number = {4},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Da Silva, Thiago Pereira and Batista, Thais and Lopes, Frederico and Neto, Aluizio Rocha and Delicato, Flávia C. and Pires, Paulo F. and Da Rocha, Atslands R.},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {edge computing, Fog computing, smart cities},
	pages = {96:1--96:32},
}

@article{liu_funcpipe_2022,
	title = {{FuncPipe}: {A} {Pipelined} {Serverless} {Framework} for {Fast} and {Cost}-{Efficient} {Training} of {Deep} {Learning} {Models}},
	volume = {6},
	shorttitle = {{FuncPipe}},
	url = {https://dl.acm.org/doi/10.1145/3570607},
	doi = {10.1145/3570607},
	abstract = {Training deep learning (DL) models in the cloud has become a norm. With the emergence of serverless computing and its benefits of true pay-as-you-go pricing and scalability, systems researchers have recently started to provide support for serverless-based training. However, the ability to train DL models on serverless platforms is hindered by the resource limitations of today's serverless infrastructure and DL models' explosive requirement for memory and bandwidth. This paper describes FuncPipe, a novel pipelined training framework specifically designed for serverless platforms that enable fast and low-cost training of DL models. FuncPipe is designed with the key insight that model partitioning can be leveraged to bridge both memory and bandwidth gaps between the capacity of serverless functions and the requirement of DL training. Conceptually simple, we have to answer several design questions, including how to partition the model, configure each serverless function, and exploit each function's uplink/downlink bandwidth. In particular, we tailor a micro-batch scheduling policy for the serverless environment, which serves as the basis for the subsequent optimization. Our Mixed-Integer Quadratic Programming formulation automatically and simultaneously configures serverless resources and partitions models to fit within the resource constraints. Lastly, we improve the bandwidth efficiency of storage-based synchronization with a novel pipelined scatter-reduce algorithm. We implement FuncPipe on two popular cloud serverless platforms and show that it achieves 7\%-77\% cost savings and 1.3X-2.2X speedup compared to state-of-the-art serverless-based frameworks.},
	number = {3},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
	author = {Liu, Yunzhuo and Jiang, Bo and Guo, Tian and Huang, Zimeng and Ma, Wenhao and Wang, Xinbing and Zhou, Chenghu},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {distributed training, pipeline parallelism, serverless function},
	pages = {47:1--47:30},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\MUWIWXUX\\Liu et al. - 2022 - FuncPipe A Pipelined Serverless Framework for Fas.pdf:application/pdf;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\Q7WYPDQ6\\Liu et al. - 2022 - FuncPipe A Pipelined Serverless Framework for Fas.pdf:application/pdf},
}

@article{romero_integration_2022,
	title = {Integration of {DevOps} {Practices} on a {Noise} {Monitor} {System} with {CircleCI} and {Terraform}},
	volume = {13},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/3505228},
	doi = {10.1145/3505228},
	abstract = {Lowering pollution levels is one of the main principles of Sustainable Development goals dictated by the United Nations. Consequently, developments on noise monitoring contribute in great manner to this purpose, since they give the opportunity to governments and institutions to maintain track on the matter. While developing a software product for this purpose, with the growth in terms of functional and non-functional requirements, elements such as infrastructure, source code, and others also scale up. Consequently if there are not good practices to face the new challenges of the software product, then it could become more complex to refactor, maintain, and scale, causing a decrease on delivery rate and the quality of the product. DevOps is an emerging concept but still hazy, which involves a set of practices that helps organizations to speed up delivery time, improve software quality and collaboration between teams. The aim of this article is to document the implementation of some DevOps practices such as IaC, continuous integration and deployment, code quality control, and collaboration on a noise monitor system to increase the product quality and automation of deployment. The final result is a set of automated pipelines that represents the entire integration and deployment cycle of the software integrated with platforms to improve quality and maintainability of the software components.},
	number = {4},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Management Information Systems},
	author = {Romero, Esteban Elias and Camacho, Carlos David and Montenegro, Carlos Enrique and Acosta, Óscar Esneider and Crespo, Rubén González and Gaona, Elvis Eduardo and Martínez, Marcelo Herrera},
	month = aug,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {serverless, CI/CD, classification, DeVOps, sound},
	pages = {36:1--36:24},
}

@article{poppe_moneyball_2022,
	title = {Moneyball: proactive auto-scaling in {Microsoft} {Azure} {SQL} database serverless},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Moneyball},
	url = {https://doi.org/10.14778/3514061.3514073},
	doi = {10.14778/3514061.3514073},
	abstract = {Microsoft Azure SQL Database is among the leading relational database service providers in the cloud. Serverless compute automatically scales resources based on workload demand. When a database becomes idle its resources are reclaimed. When activity returns, resources are resumed. Customers pay only for resources they used. However, scaling is currently merely reactive, not proactive, according to customers' workloads. Therefore, resources may not be immediately available when a customer comes back online after a prolonged idle period. In this work, we focus on reducing this delay in resource availability by predicting the pause/resume patterns and proactively resuming resources for each database. Furthermore, we avoid taking away resources for short idle periods to relieve the back-end from ineffective pause/resume workflows. Results of this study are currently being used worldwide to find the middle ground between quality of service and cost of operation.},
	number = {6},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Poppe, Olga and Guo, Qun and Lang, Willis and Arora, Pankaj and Oslake, Morgan and Xu, Shize and Kalhan, Ajay},
	month = feb,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {1279--1287},
}

@article{burckhardt_netherite_2022,
	title = {Netherite: efficient execution of serverless workflows},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Netherite},
	url = {https://doi.org/10.14778/3529337.3529344},
	doi = {10.14778/3529337.3529344},
	abstract = {Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck. To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts. Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.},
	number = {8},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Burckhardt, Sebastian and Chandramouli, Badrish and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S. and Zhu, Xiangfeng},
	month = apr,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {1591--1604},
}

@article{xu_nlubroker_2022,
	title = {{NLUBroker}: {A} {QoE}-driven {Broker} {System} for {Natural} {Language} {Understanding} {Services}},
	volume = {22},
	issn = {1533-5399},
	shorttitle = {{NLUBroker}},
	url = {https://doi.org/10.1145/3497807},
	doi = {10.1145/3497807},
	abstract = {Cloud-based Natural Language Understanding (NLU) services are becoming more popular with the development of artificial intelligence. More applications are integrated with cloud-based NLU services to enhance the way people communicate with machines. However, with NLU services provided by different companies powered by unrevealed AI technology, how to choose the best one is a problem for developers. Existing tools that can provide guidance to developers and make recommendations based on their needs are severely limited. This article comprehensively evaluates multiple state-of-the-art NLU services, and the results indicate that there is no absolute winner for different usage requirements. Motivated by this observation, we provide several insights and propose NLUBroker, a Quality of Experience-driven (QoE-driven) broker system, to select the proper service according to the environment. NLUBroker senses the client and service status and leverages a solution to the multi-armed bandit problem to conduct online learning, aiming to achieve maximum expected QoE. The performance of NLUBroker is evaluated in both simulation and real-world environments, and the evaluation results demonstrate that NLUBroker is an efficient solution for selecting NLU services. It is adaptive to changes in the environment, outperforms three baseline methods we evaluated and improves overall QoE up to 1.5× for the evaluated state-of-the-art NLU services.},
	number = {3},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Xu, Lanyu and Iyengar, Arun and Shi, Weisong},
	month = feb,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Quality of experience, service broker},
	pages = {69:1--69:29},
}

@article{ali_optimizing_2022,
	title = {Optimizing inference serving on serverless platforms},
	volume = {15},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3547305.3547313},
	doi = {10.14778/3547305.3547313},
	abstract = {Serverless computing is gaining popularity for machine learning (ML) serving workload due to its autonomous resource scaling, easy to use and pay-per-use cost model. Existing serverless platforms work well for image-based ML inference, where requests are homogeneous in service demands. That said, recent advances in natural language processing could not fully benefit from existing serverless platforms as their requests are intrinsically heterogeneous. Batching requests for processing can significantly increase ML serving efficiency while reducing monetary cost, thanks to the pay-per-use pricing model adopted by serverless platforms. Yet, batching heterogeneous ML requests leads to additional computation overhead as small requests need to be "padded" to the same size as large requests within the same batch. Reaching effective batching decisions (i.e., which requests should be batched together and why) is non-trivial: the padding overhead coupled with the serverless auto-scaling forms a complex optimization problem. To address this, we develop Multi-Buffer Serving (MBS), a framework that optimizes the batching of heterogeneous ML inference serving requests to minimize their monetary cost while meeting their service level objectives (SLOs). The core of MBS is a performance and cost estimator driven by analytical models supercharged by a Bayesian optimizer. MBS is prototyped and evaluated on AWS using bursty workloads. Experimental results show that MBS preserves SLOs while outperforming the state-of-the-art by up to 8 x in terms of cost savings while minimizing the padding overhead by up to 37 x with 3 x less number of serverless function invocations.},
	number = {10},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
	month = jun,
	year = {2022},
	note = {Publisher: VLDB Endowment},
	pages = {2071--2084},
}

@article{tirumalasetty_reducing_2022,
	title = {Reducing {Minor} {Page} {Fault} {Overheads} through {Enhanced} {Page} {Walker}},
	volume = {19},
	issn = {1544-3566},
	url = {https://dl.acm.org/doi/10.1145/3547142},
	doi = {10.1145/3547142},
	abstract = {Application virtual memory footprints are growing rapidly in all systems from servers down to smartphones. To address this growing demand, system integrators are incorporating ever larger amounts of main memory, warranting rethinking of memory management. In current systems, applications produce page fault exceptions whenever they access virtual memory regions that are not backed by a physical page. As application memory footprints grow, they induce more and more minor page faults. Handling of each minor page fault can take a few thousands of CPU cycles and blocks the application till the OS kernel finds a free physical frame. These page faults can be detrimental to the performance when their frequency of occurrence is high and spread across application runtime. Specifically, lazy allocation-induced minor page faults are increasingly impacting application performance. Our evaluation of several workloads indicates an overhead due to minor page faults as high as 29\% of execution time. In this article, we propose to mitigate this problem through a hardware, software co-design approach. Specifically, we first propose to parallelize portions of the kernel page allocation to run ahead of fault time in a separate thread. Then we propose the Minor Fault Offload Engine (MFOE), a per-core hardware accelerator for minor fault handling. MFOE is equipped with a pre-allocated page frame table that it uses to service a page fault. On a page fault, MFOE quickly picks a pre-allocated page frame from this table, makes an entry for it in the TLB, and updates the page table entry to satisfy the page fault. The pre-allocation frame tables are periodically refreshed by a background kernel thread, which also updates the data structures in the kernel to account for the handled page faults. We evaluate this system in the gem5 architectural simulator with a modified Linux kernel running on top of simulated hardware containing the MFOE accelerator. Our results show that MFOE improves the average critical path fault handling latency by 33× and tail critical path latency by 51×. Among the evaluated applications, we observed an improvement of runtime by an average of 6.6\%.},
	number = {4},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Tirumalasetty, Chandrahas and Chou, Chih Chieh and Reddy, Narasimha and Gratz, Paul and Abouelwafa, Ayman},
	month = sep,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Function-as-a-Service (FaaS), Paging, Translate Look-aside Buffer (TLB), virtualization},
	pages = {57:1--57:26},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\SFTSE3W3\\Tirumalasetty et al. - 2022 - Reducing Minor Page Fault Overheads through Enhanc.pdf:application/pdf;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\N4LB34M7\\Tirumalasetty et al. - 2022 - Reducing Minor Page Fault Overheads through Enhanc.pdf:application/pdf},
}

@article{barcelona-pons_stateful_2022,
	title = {Stateful {Serverless} {Computing} with {Crucial}},
	volume = {31},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3490386},
	doi = {10.1145/3490386},
	abstract = {Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial, a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k-means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18\%–40\% faster). We also use Crucial to port (part of) a state-of-the-art multi-threaded ML library to serverless. The ported application is up to 30\% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6\% of changes in the code bases of the evaluated applications.},
	number = {3},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Barcelona-Pons, Daniel and Sutra, Pierre and Sánchez-Artigas, Marc and París, Gerard and García-López, Pedro},
	month = mar,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {FaaS, Serverless, in-memory, stateful, synchronization},
	pages = {39:1--39:38},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\E76AAFWX\\Barcelona-Pons et al. - 2022 - Stateful Serverless Computing with Crucial.pdf:application/pdf},
}

@article{wecker_tikkoun_2022,
	title = {Tikkoun {Sofrim}: {Making} {Ancient} {Manuscripts} {Digitally} {Accessible}: {The} {Case} of {Midrash} {Tanhuma}},
	volume = {15},
	issn = {1556-4673},
	shorttitle = {Tikkoun {Sofrim}},
	url = {https://doi.org/10.1145/3476776},
	doi = {10.1145/3476776},
	abstract = {Making ancient handwritten manuscripts accessible to the general public is challenging, for several reasons. Foremost, they are handwritten. Each and every one is unique, so there is a need for manual transcription for providing enough examples for training a machine-learning-based algorithm to automatically transcribe the handwritten text. Moreover, the quality of the text is diverse—over time the ink faded, pages were damaged, and so forth. Furthermore, the boundaries of the textual regions on a page and the lines of text are not standard. Sometimes there are corrections above the lines, the lines are curved, there are comments and annotations on the margins, and more. A possible solution for these challenges is having a “person in the loop.” However, manual correction brings with it another challenge—how to address disagreement between annotations (as usually several corrections are considered before a decision is taken about the correct transcription). Tikkoun-Sofrim is a system that integrates automatic handwritten text recognition with manual, crowdsourced error correction, introducing an automatic decision process about when to stop asking for additional transcription and selecting the best transcription, declaring it as the recommended agreed reading. The system was applied to several manuscripts of “Midrash Tanhuma,” a medieval Hebrew rabbinic homiletic text, achieving a high level of success.},
	number = {2},
	urldate = {2024-01-03},
	journal = {Journal on Computing and Cultural Heritage},
	author = {Wecker, Alan J. and Raziel-Kretzmer, Vered and Kiessling, Benjamin and Ezra, Daniel Stökl Ben and Lavee, Moshe and Kuflik, Tsvi and Elovits, Dror and Schorr, Moshe and Schor, Uri and Jablonski, Pawel},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {CATTI, crowd-sourcing, handwritten text recognition, HTR, transcription},
	pages = {20:1--20:20},
}

@article{mahgoub_wisefuse_2022,
	title = {{WISEFUSE}: {Workload} {Characterization} and {DAG} {Transformation} for {Serverless} {Workflows}},
	volume = {6},
	shorttitle = {{WISEFUSE}},
	url = {https://dl.acm.org/doi/10.1145/3530892},
	doi = {10.1145/3530892},
	abstract = {We characterize production workloads of serverless DAGs at a major cloud provider. Our analysis highlights two major factors that limit performance: (a) lack of efficient communication methods between the serverless functions in the DAG, and (b) stragglers when a DAG stage invokes a set of parallel functions that must complete before starting the next DAG stage. To address these limitations, we propose WISEFUSE, an automated approach to generate an optimized execution plan for serverless DAGs for a user-specified latency objective or budget. We introduce three optimizations: (1) Fusion combines in-series functions together in a single VM to reduce the communication overhead between cascaded functions. (2) Bundling executes a group of parallel invocations of a function in one VM to improve resource sharing among the parallel workers to reduce skew. (3) Resource Allocation assigns the right VM size to each function or function bundle in the DAG to reduce the E2E latency and cost. We implement WISEFUSE to evaluate it experimentally using three popular serverless applications with different DAG structures, memory footprints, and intermediate data sizes. Compared to competing approaches and other alternatives, WISEFUSE shows significant improvements in E2E latency and cost. Specifically, for a machine learning pipeline, WISEFUSE achieves P95 latency that is 67\% lower than Photons, 39\% lower than Faastlane, and 90\% lower than SONIC without increasing the cost.},
	number = {2},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
	author = {Mahgoub, Ashraf and Yi, Edgardo Barsallo and Shankar, Karthick and Minocha, Eshaan and Elnikety, Sameh and Bagchi, Saurabh and Chaterji, Somali},
	month = jun,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {serverless, dag transformation, workload characterization},
	pages = {26:1--26:28},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\5ALSK8ZE\\Mahgoub et al. - 2022 - WISEFUSE Workload Characterization and DAG Transf.pdf:application/pdf},
}

@article{picone_flexible_2023,
	title = {A {Flexible} and {Modular} {Architecture} for {Edge} {Digital} {Twin}: {Implementation} and {Evaluation}},
	volume = {4},
	shorttitle = {A {Flexible} and {Modular} {Architecture} for {Edge} {Digital} {Twin}},
	url = {https://dl.acm.org/doi/10.1145/3573206},
	doi = {10.1145/3573206},
	abstract = {IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.},
	number = {1},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet of Things},
	author = {Picone, Marco and Mamei, Marco and Zambonelli, Franco},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Internet of Things, edge computing, Digital Twin},
	pages = {8:1--8:32},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\Z74JWNAF\\Picone et al. - 2023 - A Flexible and Modular Architecture for Edge Digit.pdf:application/pdf},
}

@article{zhang_low-code_2023,
	title = {A {Low}-code {Development} {Framework} for {Cloud}-native {Edge} {Systems}},
	volume = {23},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3563215},
	doi = {10.1145/3563215},
	abstract = {Customizing and deploying an edge system are time-consuming and complex tasks because of hardware heterogeneity, third-party software compatibility, diverse performance requirements, and so on. In this article, we present TinyEdge, a holistic framework for the low-code development of edge systems. The key idea of TinyEdge is to use a top-down approach for designing edge systems. Developers select and configure TinyEdge modules to specify their interaction logic without dealing with the specific hardware or software. Taking the configuration as input, TinyEdge automatically generates the deployment package and estimates the performance with sufficient profiling. TinyEdge provides a unified development toolkit to specify module dependencies, functionalities, interactions, and configurations. We implement TinyEdge and evaluate its performance using real-world edge systems. Results show that: (1) TinyEdge achieves rapid customization of edge systems, reducing 44.15\% of development time and 67.79\% of lines of code on average compared with the state-of-the-art edge computing platforms; (2) TinyEdge builds compact modules and optimizes the latent circular dependency detection and message routing efficiency; (3) TinyEdge performance estimation has low absolute errors in various settings.},
	number = {1},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Zhang, Wenzhao and Zhang, Yuxuan and Fan, Hongchang and Gao, Yi and Dong, Wei},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Edge computing, cloud-native, low-code development},
	pages = {15:1--15:22},
}

@article{rashid_survey_2023,
	title = {A survey on social-physical sensing: {An} emerging sensing paradigm that explores the collective intelligence of humans and machines},
	volume = {2},
	issn = {2633-9137},
	shorttitle = {A survey on social-physical sensing},
	url = {https://doi.org/10.1177/26339137231170825},
	doi = {10.1177/26339137231170825},
	abstract = {Propelled by the omnipresence of versatile data capture, communication, and computing technologies, physical sensing has revolutionized the avenue for decisively interpreting the real world. However, various limitations hinder physical sensing’s effectiveness in critical scenarios such as disaster response and urban anomaly detection. Meanwhile, social sensing is contriving as a pervasive sensing paradigm leveraging observations from human participants equipped with portable devices and ubiquitous Internet connectivity to perceive the environment. Despite its virtues, social sensing also inherently suffers from a few drawbacks (e.g., inconsistent reliability and uncertain data provenance). Motivated by the complementary strengths of the two sensing modes, social-physical sensing (SPS) is protruding as an emerging sensing paradigm that explores the collective intelligence of humans and machines to reconstruct the “state of the world,” both physically and socially. While a good number of interesting SPS applications have been studied, several critical unsolved challenges still exist in SPS. In this paper, we provide a comprehensive survey of SPS, emphasizing its definition, key enablers, state-of-the-art applications, potential research challenges, and roadmap for future work. This paper intends to bridge the knowledge gap of existing sensing-focused survey papers by thoroughly examining the various aspects of SPS crucial for building potent SPS systems.},
	language = {en},
	number = {2},
	urldate = {2024-01-03},
	journal = {Collective Intelligence},
	author = {Rashid, Md Tahmid and Wei, Na and Wang, Dong},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Sage Publications, Inc.},
	keywords = {collective intelligence, crowdsensing, physical sensing, social media, Social sensing, social-physical sensing},
	pages = {26339137231170825},
	file = {SAGE PDF Full Text:C\:\\Users\\brian\\Zotero\\storage\\RVPVIHME\\Rashid et al. - 2023 - A survey on social-physical sensing An emerging s.pdf:application/pdf},
}

@article{van_renen_cloud_2023,
	title = {Cloud {Analytics} {Benchmark}},
	volume = {16},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3583140.3583156},
	doi = {10.14778/3583140.3583156},
	abstract = {The cloud facilitates the transition to a service-oriented perspective. This affects cloud-native data management in general, and data analytics in particular. Instead of managing a multi-node database cluster on-premise, end users simply send queries to a managed cloud data warehouse and receive results. While this is obviously very attractive for end users, database system architects still have to engineer systems for this new service model. There are currently many competing architectures ranging from self-hosted (Presto, PostgreSQL), over managed (Snowflake, Amazon Redshift) to query-as-a-service (Amazon Athena, Google BigQuery) offerings. Benchmarking these architectural approaches is currently difficult, and it is not even clear what the metrics for a comparison should be. To overcome these challenges, we first analyze a real-world query trace from Snowflake and compare its properties to that of TPC-H and TPC-DS. Doing so, we identify important differences that distinguish traditional benchmarks from real-world cloud data warehouse workloads. Based on this analysis, we propose the Cloud Analytics Benchmark (CAB). By incorporating workload fluctuations and multi-tenancy, CAB allows evaluating different designs in terms of user-centered metrics such as cost and performance.},
	number = {6},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {van Renen, Alexander and Leis, Viktor},
	month = feb,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {1413--1425},
}

@article{li_enhancing_2023,
	title = {Enhancing {Blockchain} {Adoption} through {Tailored} {Software} {Engineering}: {An} {Industrial}-grounded {Study} in {Education} {Credentialing}},
	volume = {2},
	shorttitle = {Enhancing {Blockchain} {Adoption} through {Tailored} {Software} {Engineering}},
	url = {https://doi.org/10.1145/3632532},
	doi = {10.1145/3632532},
	abstract = {Recent years have witnessed a marked increase in both academic proposals and industrial adoptions of blockchain technology. However, a majority of the projects remain at the stage of prototype proposals and their real-world deployment has not met the anticipated level. This gap can be attributed to three major barriers - technical difficulties, human factors, and social context. Most of the existing research leans towards addressing the technical challenges, leaving the human and social aspects inadequately explored. Moreover, a lack of practical insights in the existing blockchain software engineering frameworks further exacerbates the adoption problem. To address these gaps, we introduce a Blockchain-oriented Software Engineering Approach for Higher Adoption Possibility (BOSE-HAP). This approach emphasizes collaboration, reflective thinking, and iterative development, aiming to bolster implementation consistency and stimulate industry adoption. We have applied this approach in the design, development, and launch of a blockchain credentialing product, CValid.org, in the context of a university-level summer school. The product achieves industry-accepted System Usability Score and has seen successful real-world deployment. In addition, this study embed usability considerations throughout the process, involved a total of 112 stakeholders across different development stages, with 25 of them participating in our in-depth interviews and usability testing. Drawing from our firsthand experience and industrial-grounded findings, we deliver eight reflections and propose five best practice suggestions relevant to blockchain adoption. We believe these insights will provide invaluable guidance for both academic researchers and industry practitioners involved in the field of blockchain technology.},
	number = {4},
	urldate = {2024-01-03},
	journal = {Distributed Ledger Technologies: Research and Practice},
	author = {Li, Zoey Ziyi and Wang, Han and Gasevic, Dragan and Yu, Jiangshan and Liu, Joseph K.},
	month = dec,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Blockchain, design-based thinking, education credentialing, software engineering, STGT, usability testing},
	pages = {27:1--27:24},
}

@article{kallas_executing_2023,
	title = {Executing {Microservice} {Applications} on {Serverless}, {Correctly}},
	volume = {7},
	url = {https://dl.acm.org/doi/10.1145/3571206},
	doi = {10.1145/3571206},
	abstract = {While serverless platforms substantially simplify the provisioning, configuration, and management of cloud applications, implementing correct services on top of these platforms can present significant challenges to programmers. For example, serverless infrastructures introduce a host of failure modes that are not present in traditional deployments. Individual serverless instances can fail while others continue to make progress, correct but slow instances can be killed by the cloud provider as part of resource management, and providers will often respond to such failures by re-executing requests. For functions with side-effects, these scenarios can create behaviors that are not observable in serverful deployments. In this paper, we propose mu2sls, a framework for implementing microservice applications on serverless using standard Python code with two extra primitives: transactions and asynchronous calls. Our framework orchestrates user-written services to address several challenges, such as failures and re-executions, and provides formal guarantees that the generated serverless implementations are correct. To that end, we present a novel service specification abstraction and formalization of serverless implementations that facilitate reasoning about the correctness of a given application’s serverless implementation. This formalization forms the basis of the mu2sls prototype, which we then use to develop a few real-world microservice applications and show that the performance of the generated serverless implementations achieves significant scalability (3-5× the throughput of a sequential implementation) while providing correctness guarantees in the context of faults, re-execution, and concurrency.},
	number = {POPL},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Kallas, Konstantinos and Zhang, Haoran and Alur, Rajeev and Angel, Sebastian and Liu, Vincent},
	month = jan,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {microservices, stateful serverless, transactions},
	pages = {13:367--13:395},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\DXHR97GH\\Kallas et al. - 2023 - Executing Microservice Applications on Serverless,.pdf:application/pdf},
}

@article{liu_faaslight_2023,
	title = {{FaaSLight}: {General} {Application}-level {Cold}-start {Latency} {Optimization} for {Function}-as-a-{Service} in {Serverless} {Computing}},
	volume = {32},
	issn = {1049-331X},
	shorttitle = {{FaaSLight}},
	url = {https://doi.org/10.1145/3585007},
	doi = {10.1145/3585007},
	abstract = {Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency. In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95\%, 28.78\% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05\% (19.21\% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency.},
	number = {5},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Liu, Xuanzhe and Wen, Jinfeng and Chen, Zhenpeng and Li, Ding and Chen, Junkai and Liu, Yi and Wang, Haoyu and Jin, Xin},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Serverless computing, cold start, optional function elimination, performance optimization},
	pages = {119:1--119:29},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\BBDDBMSH\\Liu et al. - 2023 - FaaSLight General Application-level Cold-start La.pdf:application/pdf},
}

@article{wu_facilitating_2023,
	title = {Facilitating {Serverless} {Match}-based {Online} {Games} with {Novel} {Blockchain} {Technologies}},
	volume = {23},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3565884},
	doi = {10.1145/3565884},
	abstract = {Applying peer-to-peer (P2P) architecture to online video games has already attracted both academic and industrial interests, since it removes the need for expensive server maintenance. However, there are two major issues preventing the use of a P2P architecture, namely how to provide an effective distributed data storage solution, and how to tackle potential cheating behaviors. Inspired by emerging blockchain techniques, we propose a novel consensus model called Proof-of-Play (PoP) to provide a decentralized data storage system that incorporates an anti-cheating mechanism for P2P games, by rewarding players that interact with the game as intended, along with consideration of security measures to address the Nothing-at-stake Problem and the Long-range Attack. To validate our design, we utilize a game-theory model to show that under certain assumptions, the integrity of the PoP system would not be undermined due to the best interests of any user. Then, as a proof-of-concept, we developed a P2P game (Infinity Battle) to demonstrate how a game can be integrated with PoP in practice. Finally, experiments were conducted to study PoP in comparison with Proof-of-Work (PoW) to show its advantages in various aspects.},
	number = {1},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Internet Technology},
	author = {Wu, Feijie and Yuen, Ho Yin and Chan, Henry and Leung, Victor C. M. and Cai, Wei},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {blockchain, consensus model, Peer-to-peer game},
	pages = {10:1--10:26},
}

@article{arora_flexible_2023,
	title = {Flexible {Resource} {Allocation} for {Relational} {Database}-as-a-{Service}},
	volume = {16},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3625054.3625058},
	doi = {10.14778/3625054.3625058},
	abstract = {Oversubscription is an essential cost management strategy for cloud database providers, and its importance is magnified by the emerging paradigm of serverless databases. In contrast to general purpose techniques used for oversubscription in hypervisors, operating systems and cluster managers, we develop techniques that leverage our understanding of how DBMSs use resources and how resource allocations impact database performance. Our techniques are designed to flexibly redistribute resources across database tenants at the node and cluster levels with low overhead. We have implemented our techniques in a commercial cloud database service: Azure SQL Database. Experiments using microbenchmarks, industry-standard benchmarks and real-world resource usage traces show that using our approach, it is possible to tightly control the impact on database performance even with a relatively high degree of oversubscription.},
	number = {13},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Arora, Pankaj and Chaudhuri, Surajit and Das, Sudipto and Dong, Junfeng and George, Cyril and Kalhan, Ajay and König, Arnd Christian and Lang, Willis and Li, Changsong and Li, Feng and Liu, Jiaqi and Maas, Lukas M. and Mata, Akshay and Menache, Ishai and Moeller, Justin and Narasayya, Vivek and Olma, Matthaios and Oslake, Morgan and Rezai, Elnaz and Shan, Yi and Syamala, Manoj and Xu, Shize and Zois, Vasileios},
	month = sep,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {4202--4215},
}

@article{svingos_foreign_2023,
	title = {Foreign {Keys} {Open} the {Door} for {Faster} {Incremental} {View} {Maintenance}},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3588720},
	doi = {10.1145/3588720},
	abstract = {Serverless cloud-based warehousing systems enable users to create materialized views in order to speed up predictable and repeated query workloads. Incremental view maintenance (IVM) minimizes the time needed to bring a materialized view up-to-date. It allows the refresh of a materialized view solely based on the base table changes since the last refresh. In serverless cloud-based warehouses, IVM uses computations defined as SQL scripts that update the materialized view based on updates to its base tables. However, the scripts set up for materialized views with inner joins are not optimal in the presence of foreign key constraints. For instance, for a join of two tables, the state of the art IVM computations use a UNION ALL operator of two joins - one computing the contributions to the join from updates to the first table and the other one computing the remaining contributions from the second table. Knowing that one of the join keys is a foreign-key would allow us to prune all but one of the UNION ALL branches and obtain a more efficient IVM script. In this work, we explore ways of incorporating knowledge about foreign key into IVM in order to speed up its performance. Experiments in Redshift showed that the proposed technique improved the execution times of the whole refresh process up to 2 times, and up to 2.7 times the process of calculating the necessary changes that will be applied into the materialized view.},
	number = {1},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Svingos, Christoforos and Hernich, Andre and Gildhoff, Hinnerk and Papakonstantinou, Yannis and Ioannidis, Yannis},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {foreign key constraints, incrementally updated materialized views, relational databases},
	pages = {40:1--40:25},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\2F9WAUG7\\Svingos et al. - 2023 - Foreign Keys Open the Door for Faster Incremental .pdf:application/pdf},
}

@article{park_hardware_2023,
	title = {Hardware {Hardened} {Sandbox} {Enclaves} for {Trusted} {Serverless} {Computing}},
	issn = {1544-3566},
	url = {https://dl.acm.org/doi/10.1145/3632954},
	doi = {10.1145/3632954},
	abstract = {In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this paper proposes an extended TEE model called Cloister, designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves which is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this paper shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware.},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Park, Joongun and Kang, Seunghyo and Lee, Sanghyeon and Kim, Taehoon and Park, Jongse and Kwon, Youngjin and Huh, Jaehyuk},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Security, Serverless computing, Hardware, Trusted Execution Environment},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\ATJYPIU2\\Park et al. - 2023 - Hardware Hardened Sandbox Enclaves for Trusted Ser.pdf:application/pdf},
}

@article{zhang_infinistore_2023,
	title = {{InfiniStore}: {Elastic} {Serverless} {Cloud} {Storage}},
	volume = {16},
	issn = {2150-8097},
	shorttitle = {{InfiniStore}},
	url = {https://doi.org/10.14778/3587136.3587139},
	doi = {10.14778/3587136.3587139},
	abstract = {Cloud object storage such as AWS S3 is cost-effective and highly elastic but relatively slow, while high-performance cloud storage such as AWS ElastiCache is expensive and provides limited elasticity. We present a new cloud storage service called ServerlessMemory, which stores data using the memory of serverless functions. ServerlessMemory employs a sliding-window-based memory management strategy inspired by the garbage collection mechanisms used in the programming language to effectively segregate hot/cold data and provides fine-grained elasticity, good performance, and a pay-per-access cost model with extremely low cost. We then design and implement InfiniStore, a persistent and elastic cloud storage system, which seamlessly couples the function-based ServerlessMemory layer with a persistent, inexpensive cloud object store layer. InfiniStore enables durability despite function failures using a fast parallel recovery scheme built on the auto-scaling functionality of a FaaS (Function-as-a-Service) platform. We evaluate InfiniStore extensively using both microbenchmarking and two real-world applications. Results show that InfiniStore has more performance benefits for objects larger than 10 MB compared to AWS ElastiCache and Anna, and InfiniStore achieves 26.25\% and 97.24\% tenant-side cost reduction compared to InfiniCache and ElastiCache, respectively.},
	number = {7},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhang, Jingyuan and Wang, Ao and Ma, Xiaolong and Carver, Benjamin and Newman, Nicholas John and Anwar, Ali and Rupprecht, Lukas and Tarasov, Vasily and Skourtis, Dimitrios and Yan, Feng and Cheng, Yue},
	month = mar,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {1629--1642},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\YIKUFK93\\Zhang et al. - 2023 - InfiniStore Elastic Serverless Cloud Storage.pdf:application/pdf},
}

@article{baresi_neptune_2023,
	title = {{NEPTUNE}: a {Comprehensive} {Framework} for {Managing} {Serverless} {Functions} at the {Edge}},
	issn = {1556-4665},
	shorttitle = {{NEPTUNE}},
	url = {https://dl.acm.org/doi/10.1145/3634750},
	doi = {10.1145/3634750},
	abstract = {Applications that are constrained by low-latency requirements can hardly be executed on cloud infrastructures, given the high network delay required to reach remote servers. Multi-access Edge Computing (MEC) is the reference architecture for executing applications on nodes that are located close to users (i.e., at the edge of the network). This way, the network overhead is reduced but new challenges emerge. The resources available on edge nodes are limited, workloads fluctuate since users can rapidly change location, and complex tasks are becoming widespread (e.g., machine learning inference). To address these issues, this article presents NEPTUNE, a serverless-based framework that automates the management of large-scale MEC infrastructures. In particular, NEPTUNE provides i) the placement of serverless functions on MEC nodes according to users’ location, ii) the resolution of resource contention scenarios by avoiding that single nodes be saturated, and iii) the dynamic allocation of CPUs and GPUs to meet foreseen execution times. To assess NEPTUNE, we built a prototype based on K3S, an edge-dedicated version of Kubernetes, and executed a comprehensive set of experiments. Results show that NEPTUNE obtains a significant reduction in terms of response time, network overhead, and resource consumption compared to five state-of-the-art solutions.},
	urldate = {2024-01-03},
	journal = {ACM Transactions on Autonomous and Adaptive Systems},
	author = {Baresi, Luciano and Hu, Davide Yi Xian and Quattrocchi, Giovanni and Terracciano, Luca},
	month = dec,
	year = {2023},
	note = {Just Accepted},
	keywords = {edge computing, control theory, dynamic resource allocation, GPU, k3s, kubernetes, placement, serverless, vertical scaling},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\F5DBNHMN\\Baresi et al. - 2023 - NEPTUNE a Comprehensive Framework for Managing Se.pdf:application/pdf},
}

@article{dhalla_performance_2023,
	title = {Performance {Testing} of a {Web} {Application} {Using} {Azure} {Serverless} {Functions} and {Apache} {JMeter}},
	volume = {39},
	issn = {1937-4771},
	url = {https://dl.acm.org/doi/10.5555/3636988.3636995},
	abstract = {Software testing is one of the most critical phases in the software development life cycle. It is of utmost importance to learn how to verify and validate a software application. Moreover, the performance of the software is also pivotal for the success of any software application. This tutorial aims to provide participants with a comprehensive understanding of performance testing methodologies using Apache JMeter, with a specific focus on leveraging Azure Serverless Functions as endpoints. I will demonstrate the use of Apache JMeter automation testing tool to create test plans and to generate artificial workload for real-world scenarios of performance testing. Moreover, the participants will learn the impact of resource allocation variations using cloud infrastructure on the performance of web applications.},
	number = {3},
	journal = {Journal of Computing Sciences in Colleges},
	author = {Dhalla, Hardeep Kaur},
	month = oct,
	year = {2023},
	note = {Place: Evansville, IN, USA
Publisher: Consortium for Computing Sciences in Colleges},
	pages = {26},
}

@article{bian_using_2023,
	title = {Using {Cloud} {Functions} as {Accelerator} for {Elastic} {Data} {Analytics}},
	volume = {1},
	url = {https://doi.org/10.1145/3589306},
	doi = {10.1145/3589306},
	abstract = {Cloud function (CF) services, such as AWS Lambda, have been applied as the new computing infrastructure in implementing analytical query engines. For bursty and sparse workloads, CF-based query engine is more elastic than the traditional query engines running in servers, i.e., virtual machines (VMs), and might provide a higher performance/price ratio. However, it is still controversial whether CF services are good suites for general analytical workloads, in respect of the limitations of CFs in storage, network, and lifetime, as well as the much higher resource unit prices than VMs. In this paper, we first present micro-benchmark evaluations of the features of CF and VM. We reveal that for query processing, though CF is more elastic than VM, it is less scalable and is more expensive for continuous workloads. Then, to get the best of both worlds, we propose Pixels-Turbo - a hybrid query engine that processes queries in a scalable VM cluster by default and invokes CFs to accelerate the processing of unpredictable workload spikes. In the query engine, we propose several optimizations to improve the performance and scalability of the CF-based operators and a cost-based optimizer to select the appropriate algorithm and parallelism for the physical query plan. Evaluations on TPC-H and real-world workload show that our query engine has a 1-2 orders of magnitude higher performance/price ratio than state-of-the-art serverless query engines for sustained workloads while not compromising the elasticity for workload spikes.},
	number = {2},
	urldate = {2024-01-03},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Bian, Haoqiong and Sha, Tiannan and Ailamaki, Anastasia},
	month = jun,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data lake, OLAP, serverless, cloud databases, cloud function, cloud storage, column store, cost efficiency, data warehouse, elasticity, FAAS, QAAS, query optimization, query processing},
	pages = {161:1--161:27},
}

@article{saxena_story_2023,
	title = {The {Story} of {AWS} {Glue}},
	volume = {16},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3611540.3611547},
	doi = {10.14778/3611540.3611547},
	abstract = {AWS Glue is Amazon's serverless data integration cloud service that makes it simple and cost effective to extract, clean, enrich, load, and organize data. Originally launched in August 2017, AWS Glue began as an extract-transform-load (ETL) service designed to relieve developers and data engineers of the undifferentiated heavy lifting needed to load databases, data warehouses, and build data lakes on Amazon S3. Since then, it has evolved to serve a larger audience including ETL specialists and data scientists, and includes a broader suite of data integration capabilities. Today, hundreds of thousands of customers use AWS Glue every month. In this paper, we describe the use cases and challenges cloud customers face in preparing data for analytics and the tenets we chose to drive Glue's design. We chose early on to focus on ease-of-use, scale, and extensibility. At its core, Glue offers serverless Apache Spark and Python engines backed by a purpose-built resource manager for fast startup and auto-scaling. In Spark, it offers a new data structure --- DynamicFrames --- for manipulating messy schema-free semi-structured data such as event logs, a variety of transformations and tooling to simplify data preparation, and a new shuffle plugin to offload to cloud storage. It also includes a Hivemetastore compatible Data Catalog with Glue crawlers to build and manage metadata, e.g. for data lakes on Amazon S3. Finally, Glue Studio is its visual interface for authoring Spark and Python-based ETL jobs. We describe the innovations that differentiate AWS Glue and drive its popularity and how it has evolved over the years.},
	number = {12},
	urldate = {2024-01-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Saxena, Mohit and Sowell, Benjamin and Alamgir, Daiyan and Bahadur, Nitin and Bisht, Bijay and Chandrachood, Santosh and Keswani, Chitti and Krishnamoorthy, G. and Lee, Austin and Li, Bohou and Mitchell, Zach and Porwal, Vaibhav and Chappidi, Maheedhar Reddy and Ross, Brian and Sekiyama, Noritaka and Zaki, Omer and Zhang, Linchi and Shah, Mehul A.},
	month = aug,
	year = {2023},
	note = {Publisher: VLDB Endowment},
	pages = {3557--3569},
}

@article{robertson_cloud-based_2022,
	title = {A {Cloud}-{Based} {Computing} {Framework} for {Artificial} {Intelligence} {Innovation} in {Support} of {Multidomain} {Operations}},
	volume = {69},
	issn = {1558-0040},
	url = {https://ieeexplore.ieee.org/document/9497678},
	doi = {10.1109/TEM.2021.3088382},
	abstract = {The DoD’s artificial intelligence (AI) strategy requires the delivery of transformative and disruptive capabilities that impact the “character of the future battlefield and the pace of threats” that US forces must be prepared to handle. Candidate frameworks must also address key mission areas while enabling partnerships with the private sector, academia, and global allies. To meet these challenges, a flexible, cost-effective, and scalable computing infrastructure that incorporates cutting edge technologies and complies with stringent information assurance requirements is necessary. The DoD AI strategy mandates the agile employment of innovative AI capabilities that “rapidly and iteratively” execute experimentation with new operating concepts, and leverage lessons learned in subsequent experiments. Using cloud computing, we present a flexible approach to solve complex systems problems. Promoting “rapid experimentation” and collaboration on problems such as recursive algorithm implementation, deep learning, and inference in neural networks has enabled inherent advantages over existing computing frameworks. Leveraging the cloud to implement shared responsibility security models, serverless architectures, and high-performance virtual machines, aspects of the AI lifecycle including build, deploy, and monitor have resulted in an adaptable and scalable computing framework that is not only disruptive to the current computing paradigm but also promotes enhanced and productive collaboration.},
	number = {6},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Engineering Management},
	author = {Robertson, James and Fossaceca, John M and Bennett, Kelly W.},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Engineering Management},
	pages = {3913--3922},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2M9EF2AP\\9497678.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2BW4W7KS\\9497678.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3T8ZVL3M\\9497678.html:text/html},
}

@article{che_decentralized_2022,
	title = {A {Decentralized} {Federated} {Learning} {Framework} via {Committee} {Mechanism} {With} {Convergence} {Guarantee}},
	volume = {33},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/9870745},
	doi = {10.1109/TPDS.2022.3202887},
	abstract = {Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this article, we propose a novel serverless federated learning framework Committee Mechanism based Federated Learning (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results.},
	number = {12},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Che, Chunjiang and Li, Xiaoli and Chen, Chuan and He, Xiaoyu and Zheng, Zibin},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {4783--4800},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HUPB7HSA\\9870745.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8KQ866HT\\9870745.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RW6RH7IM\\9870745.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\U2NYJE78\\Che et al. - 2022 - A Decentralized Federated Learning Framework via C.pdf:application/pdf},
}

@article{cui_fast_2022,
	title = {A {Fast} {Blockchain}-{Based} {Federated} {Learning} {Framework} {With} {Compressed} {Communications}},
	volume = {40},
	issn = {1558-0008},
	url = {https://ieeexplore.ieee.org/document/9917527},
	doi = {10.1109/JSAC.2022.3213345},
	abstract = {Recently, blockchain-based federated learning (BFL) has attracted intensive research attention due to that the training process is auditable and the architecture is serverless avoiding the single point failure of the parameter server in vanilla federated learning (VFL). Nevertheless, BFL tremendously escalates the communication traffic volume because all local model updates (i.e., changes of model parameters) obtained by BFL clients will be transmitted to all miners for verification and to all clients for aggregation. In contrast, the parameter server and clients in VFL only retain aggregated model updates. Consequently, the huge communication traffic in BFL will inevitably impair the training efficiency and hinder the deployment of BFL in reality. To improve the practicality of BFL, we are among the first to propose a fast blockchain-based communication-efficient federated learning framework by compressing communications in BFL, called BCFL. Meanwhile, we derive the convergence rate of BCFL with non-convex loss. To maximize the final model accuracy, we further formulate the problem to minimize the training loss of the convergence rate subject to a limited training time with respect to the compression rate and the block generation rate, which is a bi-convex optimization problem and can be efficiently solved. To the end, to demonstrate the efficiency of BCFL, we carry out extensive experiments with standard CIFAR-10 and FEMNIST datasets. Our experimental results not only verify the correctness of our analysis, but also manifest that BCFL can remarkably reduce the communication traffic by 95–98\% or shorten the training time by 90–95\% compared with BFL.},
	number = {12},
	urldate = {2024-01-06},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Cui, Laizhong and Su, Xiaoxin and Zhou, Yipeng},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	pages = {3358--3372},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\P98VA5LC\\9917527.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LXSU4BC4\\9917527.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KIK357HL\\9917527.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\4UUZNPAI\\Cui et al. - 2022 - A Fast Blockchain-Based Federated Learning Framewo.pdf:application/pdf},
}

@inproceedings{bharti_scalable_2022,
	title = {A {Scalable} {Design} {Approach} for {State} {Propagation} in {Serverless} {Workflow}},
	url = {https://ieeexplore.ieee.org/document/9972158},
	doi = {10.1109/GCAT55367.2022.9972158},
	abstract = {Serverless development is challenging as applications are composed of stateless and short-lived functions. Many workflows require time-bound functions to transfer their state to other function before termination. The serverless Function-as-a-Service offerings lack state management support; therefore, it must be handled at application-level. In this paper, we propose a scalable design approach that simplifies development of workflows that require sharing of ephemeral intermediate data. Our design uses object serialization/deserialization with cloud object storage to share state across functions. It provides a mechanism for fine-grained support for state propagation and synchronization in a serverless workflow. This solution is cost-effective and efficient as it does not depend on any external database or cache for state management. The design has been validated by implementing ‘Word Count’- a classic MapReduce use case. Our results show that the proposed scalable design can process input of any size and can handle state propagation in complex serverless workflow.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 3rd {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Bharti, Urmil and Goel, Anita and Gupta, S. C.},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)},
	keywords = {FaaS, Serverless, Function-as-a-service, Serverless workflow, Work-flows, Digital storage, Faas, Cost effectiveness, Design, Design approaches, MapReduce, Scalable design, Serverless composition, Serverless Composition, Serverless stateful application, Serverless Stateful Application, Serverless Workflows, State management, State Management},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GEINQKAU\\9972158.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2NSAQRVU\\9972158.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Q9XM9AQL\\9972158.html:text/html},
}

@inproceedings{nastic_serverless_2022,
	title = {A {Serverless} {Computing} {Fabric} for {Edge} \& {Cloud}},
	url = {https://ieeexplore.ieee.org/document/10063372},
	doi = {10.1109/CogMI56440.2022.00011},
	abstract = {Serverless computing has been establishing itself as a compelling paradigm for the development and of modern cloud-native applications. Serverless represents the next step in the evolution of cloud programming models, services and platforms, which is especially appealing due to its low management overhead, easy deployment, scale-to-zero and the promise of optimized costs. Recently, due to the advantages it offers, the serverless paradigm has been growing beyond traditional clouds, making its way to the Edge. The natural evolutionary step for serverless computing is to unify the Edge and the Cloud into what we refer to as Edge-Cloud Continuum. In this paper, we outline our vision of the Serverless Computing Fabric (SCF) for the Edge-Cloud continuum. We introduce the reference architecture for the SCF and show how it unlocks the full potential of the Edge-Cloud continuum. We also discuss main opportunities and challenges, which need to be overcome in order to achieve the vision of the Serverless Computing Fabric. Finally, we introduce key design principles together with core enabling runtime mechanisms, which are intended to serve as a research road map towards the Serverless Computing Fabric for Edge-Cloud continuum.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 4th {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Nastic, Stefan and Raith, Philipp and Furutanpey, Alireza and Pusztai, Thomas and Dustdar, Schahram},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 4th International Conference on Cognitive Machine Intelligence (CogMI)},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VYRW4UKR\\10063372.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4A6BHTXI\\10063372.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5UXYLQ5U\\10063372.html:text/html},
}

@inproceedings{kusnierz_serverless_2022,
	title = {A {Serverless} {Engine} for {High} {Energy} {Physics} {Distributed} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9826036},
	doi = {10.1109/CCGrid54584.2022.00067},
	abstract = {The Large Hadron Collider (LHC) at CERN has generated in the last decade an unprecedented volume of data for the High-Energy Physics (HEP) field. Scientific collaborations interested in analysing such data very often require computing power beyond a single machine. This issue has been tackled traditionally by running analyses in distributed environments using stateful, managed batch computing systems. While this approach has been effective so far, current estimates for future computing needs of the field present large scaling challenges. Such a managed approach may not be the only viable way to tackle them and an interesting alternative could be provided by serverless architectures, to enable an even larger scaling potential. This work describes a novel approach to running real HEP scientific applications through a distributed serverless computing engine. The engine is built upon ROOT, a well-established HEP data analysis software, and distributes its computations to a large pool of concurrent executions on Amazon Web Services Lambda Serverless Platform. Thanks to the developed tool, physicists are able to access datasets stored at CERN (also those that are under restricted access policies) and process it on remote infrastructures outside of their typical environment. The analysis of the serverless functions is monitored at runtime to gather performance metrics, both for data- and computation-intensive workloads.},
	urldate = {2024-01-06},
	booktitle = {2022 22nd {IEEE} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Kuśnierz, Jacek and Padulano, Vincenzo E. and Malawski, Maciej and Burkiewicz, Kamil and Saavedra, Enric Tejedor and Alonso-Jordá, Pedro and Pitt, Michael and Avati, Valentina},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {575--584},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\U9AYL8CV\\9826036.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Z2RE9WIV\\9826036.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CFY782HE\\9826036.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\YWBVS3JR\\Kuśnierz et al. - 2022 - A Serverless Engine for High Energy Physics Distri.pdf:application/pdf;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\9EBHD3DU\\Kuśnierz et al. - 2022 - A Serverless Engine for High Energy Physics Distri.pdf:application/pdf},
}

@inproceedings{bauer_actor-oriented_2022,
	title = {Actor-{Oriented} {Scalable} {Domain}-{Specific} {Cluster} {Architecture} for {Cloud}-{Applications}},
	isbn = {2577-1647},
	url = {https://ieeexplore.ieee.org/document/9968983},
	doi = {10.1109/IECON49645.2022.9968983},
	abstract = {Nowadays, applications in the cloud are based on a microservice architecture. Depending on the problem to be solved, they tend to grow complex, and the maintenance is more complicated. For this, a scalable domain-specific (application-related) cluster architecture is conceptualized, which should fulfill the requirements of flexibility, scalability and elasticity, cost-effectiveness, and reliability. Each instance within the cluster covers an application domain. It is possible to upload service subdomains dynamically to an application domain at runtime. A subdomain consists of a group of actors (also called a pod). The development of a subdomain can be assigned to a team. A service subdomain can be scaled individually through replication or sharding (within the instance or the cluster). The proposed solution is expected to achieve a better result than a microservice or FaaS architecture. A single instance prototype was already developed, and a demo application was created.},
	urldate = {2024-01-06},
	booktitle = {{IECON} 2022 – 48th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	author = {Bauer, David Alessandro and Mäkiö, Juho},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YQN2DMZR\\9968983.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RF5TAIZR\\9968983.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7YXG99TK\\9968983.html:text/html},
}

@inproceedings{zhang_adaptive_2022,
	title = {Adaptive {Auto}-{Scaling} of {Delay}-{Sensitive} {Serverless} {Services} with {Reinforcement} {Learning}},
	isbn = {0730-3157},
	url = {https://ieeexplore.ieee.org/document/9842605},
	doi = {10.1109/COMPSAC54236.2022.00137},
	abstract = {Serverless services such as image recognition and natural language processing have strict response-time constraints. The incoming workloads and resource requirements of a newly deployed serverless service are always unpredictable due to the lack of available historical tracing data. Therefore, making effective auto-scaling decisions for these services is challenging. Open source serverless platforms often work in a best-effort manner, which cannot guarantee the response delay. Moreover, existing studies usually adopt threshold-based methods by configuring additional resource, which cannot well balance the trade-off between the quality of service and resource efficiency. To address the above issues, we propose an adaptive auto-scaling approach for delay-sensitive serverless services with reinforcement learning. First, we characterize the service's resource profile by exploring the performance improvement of different resource allocations with the reinforcement learning method. Then, we propose an adaptive auto-scaling method combining both horizontal and vertical scaling strategies based on the characterized profile to dynamically adjust the resource allocation. Finally, we select three typical services to validate our approach by comparing with two existing state-of-the-art auto-scaling methods. The experimental results show that our approach can accurately characterize services' resource profile, and effectively ensure the response delay constraints while achieving about 10.50\% reduction of cost on average.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 46th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Zhang, Zhiyu and Wang, Tao and Li, An and Zhang, Wenbo},
	month = jun,
	year = {2022},
	note = {ISSN: 0730-3157},
	pages = {866--871},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MW7C7SIQ\\9842605.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DL4GIPCR\\9842605.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4G8BMICT\\9842605.html:text/html},
}

@inproceedings{golec_aiblock_2022,
	title = {{AIBLOCK}: {Blockchain} based {Lightweight} {Framework} for {Serverless} {Computing} using {AI}},
	shorttitle = {{AIBLOCK}},
	url = {https://ieeexplore.ieee.org/document/9826025},
	doi = {10.1109/CCGrid54584.2022.00106},
	abstract = {Artificial intelligence (AI)-based studies have been carried out recently for the early detection of COVID-19. The goal is to prevent the spread of the disease and the number of fatal cases. In AI-based COVID-19 diagnostic studies, the integrity of the data is critical to obtain reliable results. In this paper, we propose a Blockchain-based framework called AIBLOCK, to offer the data integrity required for applications such as Industry 4.0, healthcare, and online banking. In addition, the proposed framework is integrated with Google Cloud Platform (GCP)-Cloud Functions, a serverless computing platform that automatically manages resources by offering dynamic scalability. The performance of five different machine learning models is evaluated and compared in terms of Accuracy, Precision, Recall, F-Score and Area under the curve (AUC). The experimental results show that decision trees gives the best results in terms of accuracy (98.4 \%). Further, it has been identified that utilization of Blockchain technology can increase the load on memory.},
	urldate = {2024-01-06},
	booktitle = {2022 22nd {IEEE} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Golec, Muhammed and Chowdhury, Deepraj and Jaglan, Shivam and Gill, Sukhpal Singh and Uhlig, Steve},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {886--892},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\V95G39BA\\9826025.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QJNG2AD9\\9826025.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\U89QIAKL\\9826025.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\WLPQMKID\\Golec et al. - 2022 - AIBLOCK Blockchain based Lightweight Framework fo.pdf:application/pdf},
}

@inproceedings{sabbioni_architectural_2022,
	title = {An {Architectural} {Approach} for {Heterogeneous} {Data} {Access} in {Serverless} {Platforms}},
	url = {https://ieeexplore.ieee.org/document/10000963},
	doi = {10.1109/GLOBECOM48099.2022.10000963},
	abstract = {The continuous digitalization and application of modern ICT technologies in the Smart City and Tourism domains are paving the way to new, integrated and connected experiences with strong economic and social impact. However, the integration of services and data coming from different providers is hindered by a rapidly evolving ecosystem of tools and techniques, introducing substantial delays and costs in solution design, deployment, testing, and refinement. Serverless computing is a novel cloud computing model where the customers' business logic, structured as lightweight functions, is automatically put into execution in response to an incoming event. This emergent paradigm promises to be an appealing solution, lowering the development and management barrier for the adaptation, integration, and roll-out of services. However, the ephemeral nature of serverless functions clashes with the necessity of creating and maintaining persistent connections to the various data providers. In this work, we present the Serverless Persistence Support (SPS) service, a novel, distributed, and scalable service implementing an adaptation layer able to improve data access performance from serverless functions. To validate our proposal, we conduct a thorough analysis on a realistic testbed, contrasting various data layer supports and assessing SPS performance when compared to the classic approach where connections and operations are executed inside the business logic. The experimental analysis shows that our solution exhibits better performance both in terms of latency and throughput, while also improving resource utilization.},
	urldate = {2024-01-06},
	booktitle = {{GLOBECOM} 2022 - 2022 {IEEE} {Global} {Communications} {Conference}},
	author = {Sabbioni, Andrea and Bujari, Armir and Romeo, Stefano and Foschini, Luca and Corradi, Antonio},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: GLOBECOM 2022 - 2022 IEEE Global Communications Conference},
	pages = {129--134},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FBT6KVYB\\10000963.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8IP8YUZK\\10000963.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BZREEGXL\\10000963.html:text/html},
}

@inproceedings{stahlbock_optimization_2022,
	title = {An {Optimization} {Approach} of {Container} {Startup} {Times} for {Time}-{Sensitive} {Embedded} {Systems}},
	url = {https://ieeexplore.ieee.org/document/10074694},
	doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00300},
	abstract = {Containers are a lightweight virtualization method that is widely adopted in Cloud Environments, Internet of Things (IoT) and embedded devices. Apart from virtualization, containers provide an efficient way of building, storing and distributing layered filesystems for applications. This is one reason why containers became state of the art for service development, deployment and administration in IT systems. Although containers are lightweight, the image layering and container creation add overhead to the application startup time causing a cold start problem in serverless computing. Therefore, the overhead must be kept as small as possible, especially for resource-constrained devices in time-sensitive systems. We present a concept to decrease container startup times and perform analysis on a resource constrained device, a Renesas RCar H3. The study shows that using our concept, the average container startup time can be reduced by up to 64\% on target hardware. Furthermore, we evaluate how the solution scales on different hardware setups and determine the impact of container configuration parameters on startup times.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 24th {Int} {Conf} on {High} {Performance} {Computing} \& {Communications}; 8th {Int} {Conf} on {Data} {Science} \& {Systems}; 20th {Int} {Conf} on {Smart} {City}; 8th {Int} {Conf} on {Dependability} in {Sensor}, {Cloud} \& {Big} {Data} {Systems} \& {Application} ({HPCC}/{DSS}/{SmartCity}/{DependSys})},
	author = {Stahlbock, Lukas and Weber, Jan and Köster, Frank},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 24th Int Conf on High Performance Computing \& Communications; 8th Int Conf on Data Science \& Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud \& Big Data Systems \& Application (HPCC/DSS/SmartCity/DependSys)},
	pages = {2019--2026},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A43YSP26\\10074694.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B8JN6DSR\\10074694.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FR3AX6LG\\10074694.html:text/html},
}

@inproceedings{kumari_analysis_2022,
	title = {Analysis {Of} {Cloud} {Computing} {Security} {Threats} and {Countermeasures}},
	url = {https://ieeexplore.ieee.org/document/9964632},
	doi = {10.1109/ICRITO56286.2022.9964632},
	abstract = {The demand for Cloud Computing (CC) is increasing at a higher growth rate because of the emerging technologies like machine learning, artificial intelligence, big data analytics, IoT, mobile supercomputing, serverless computing, and the industrial need for reducing operational costs. Cloud computing features such as customized offerings, pay-as-you-go or pay-per-use model, availability, seamless scalability, and flexibility have accelerated the demand for cloud services. Despite these many benefits, security is the top challenge in the adoption of cloud services. However, the recent research studies lack the gap between analyzing potential security threats and an exhaustive list of countermeasures proposed for mitigating the threats. The main objective of this paper is to provide a comprehensive view of cloud security threats and to identify potential (more vulnerable) threats. Further, the taxonomy of proposed solutions is provided for data security and privacy on the basis of analysis of research papers in the last decade.},
	urldate = {2024-01-06},
	booktitle = {2022 10th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Kumari, Sushila and Solanki, Kamna and Dalal, Sandeep and Dhankhar, Amita},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IXMUZHTG\\9964632.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7986TTXP\\9964632.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VHGVQJXU\\9964632.html:text/html},
}

@inproceedings{dantas_application_2022,
	title = {Application {Deployment} {Strategies} for {Reducing} the {Cold} {Start} {Delay} of {AWS} {Lambda}},
	isbn = {2159-6190},
	url = {https://ieeexplore.ieee.org/document/9860368},
	doi = {10.1109/CLOUD55607.2022.00016},
	abstract = {Serverless computing has emerged in recent years as the new computing paradigm adopted by key players in the industry for software development. This new paradigm has seen rapid growth in adoption due to its unique billing model and scaling characteristics. Public cloud providers such as Amazon Web Services (AWS) offer several configurations and language runtimes for their serverless functions. Although extensively explored by the research community, this field still lacks current studies that address the many challenges developers face when leveraging serverless functions for real-world applications. One of these challenges that are often overseen by many programmers is the cold start problem which is present in any serverless application. For this reason, we propose the first study to characterize the underlying cold start impacts caused by the choice of language runtime, application size, memory size and deployment type on AWS Lambda. In this paper, we analyze the performance of the container-based deployment and ZIP-based deployment of AWS Lambda using a variety of language runtimes and applications running with different function configurations; then we propose guidelines for developers and cloud managers to consider when deploying/managing the workloads on the cloud.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Dantas, Jaime and Khazaei, Hamzeh and Litoiu, Marin},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KN5Y24E9\\9860368.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L3EXFLRL\\9860368.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L6TN487H\\9860368.html:text/html},
}

@article{jarachanthan_astrea_2022,
	title = {Astrea: {Auto}-{Serverless} {Analytics} {Towards} {Cost}-{Efficiency} and {QoS}-{Awareness}},
	volume = {33},
	issn = {1558-2183},
	shorttitle = {Astrea},
	url = {https://ieeexplore.ieee.org/document/9767624},
	doi = {10.1109/TPDS.2022.3172069},
	abstract = {With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astrea, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21\% to 69\% under a given budget constraint, while saving cost by 20\% to 84\% without violating performance requirements.},
	number = {12},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {3833--3849},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5NICIQQM\\9767624.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S6MBQCB7\\9767624.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\388IQQND\\9767624.html:text/html},
}

@inproceedings{choochotkaew_autodeck_2022,
	title = {{AutoDECK}: {Automated} {Declarative} {Performance} {Evaluation} and {Tuning} {Framework} on {Kubernetes}},
	isbn = {2159-6190},
	shorttitle = {{AutoDECK}},
	url = {https://ieeexplore.ieee.org/document/9860408},
	doi = {10.1109/CLOUD55607.2022.00053},
	abstract = {Containerization and application variety bring many challenges in automating evaluations for performance tuning and comparison among infrastructure choices. Due to the tightly-coupled design of benchmarks and evaluation tools, the present automated tools on Kubernetes are limited to trivial microbenchmarks and cannot be extended to complex cloudnative architectures such as microservices and serverless, which are usually managed by customized operators for setting up workload dependencies. In this paper, we propose AutoDECK, a performance evaluation framework with a fully declarative manner. The proposed framework automates configuring, deploying, evaluating, summarizing, and visualizing the benchmarking workload. It seamlessly integrates mature Kubernetes-native systems and extends multiple functionalities such as tracking the image-build pipeline, and auto-tuning. We present five use cases of evaluations and analysis through various kinds of bench-marks including microbenchmarks and HPC/AI benchmarks. The evaluation results can also differentiate characteristics such as resource usage behavior and parallelism effectiveness between different clusters. Furthermore, the results demonstrate the benefit of integrating an auto-tuning feature in the proposed framework, as shown by the 10\% transferred memory bytes in the Sysbench benchmark.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Choochotkaew, Sunyanan and Chiba, Tatsuhiro and Trent, Scott and Yoshimura, Takeshi and Amaral, Marcelo},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {309--314},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LWIUIENG\\9860408.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I2MBFYAN\\9860408.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GQRT5VM9\\9860408.html:text/html},
}

@inproceedings{winzinger_automatic_2022,
	title = {Automatic {Test} {Case} {Generation} for {Serverless} {Applications}},
	isbn = {2642-6587},
	url = {https://ieeexplore.ieee.org/document/9912636},
	doi = {10.1109/SOSE55356.2022.00015},
	abstract = {Testing is an important part of software development helping detect faults and gain confidence in the quality of the software. However, serverless computing, a new cloud computing model which emerged with Amazon’s introduction of AWS Lambda, still lacks testing support. In serverless computing, applications consist of several components mainly built around serverless functions whose integration has to be tested. In particular for regression tests and stress testing, many test cases are needed which have to cover different parts of the application. Therefore, we investigated how test cases can be built for serverless applications and what is needed for an automatic test case generation. Based on these requirements represented by a model, we built a tool which is able to create integration tests for some coverage criteria automatically and evaluated their potential on two applications. The results showed that integration test cases can be built for the supported criteria with a high coverage by our tool which can be helpful for regression or stress tests where many different test cases are needed.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Winzinger, Stefan and Wirtz, Guido},
	month = aug,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)},
	pages = {77--84},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8D548PW9\\9912636.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LDCNDIUN\\9912636.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\P44IC5A4\\9912636.html:text/html},
}

@inproceedings{wang_autoscaling_2022,
	title = {Autoscaling cracker: an efficient asymmetric {DDoS} attack on serverless functions},
	shorttitle = {Autoscaling cracker},
	url = {https://ieeexplore.ieee.org/document/10001386},
	doi = {10.1109/GLOBECOM48099.2022.10001386},
	abstract = {Serverless computing has brought new changes to cloud computing. The decoupled serverless functions have more flexible scheduling methods and use resources efficiently with the help of autoscaling. However, it exposes more attack surfaces. If an insecure function becomes a serverless function, a significant security risk will be brought to its service. This paper analyzes the risk of asymmetric DDoS attacks faced by insecure serverless functions. These attacks can occupy a large amount of CPU or memory resources without redundant connections. They can affect the quality of service, delay response time, or even interrupt the service. Autoscaling lacks resilience to such attacks. We test the effects of these attacks in experimental environments and Alibaba Cloud's serverless application engine (SAE). In SAE, we increase the response time from 0.2 seconds to 25 seconds or crash the target function within 6 seconds. Compared with traditional DDoS attacks, asymmetric DDoS attacks are more effective for serverless applications. Finally, we design solutions to mitigate asymmetric DDoS attacks for applications with long and short response times in serverless environments.},
	urldate = {2024-01-06},
	booktitle = {{GLOBECOM} 2022 - 2022 {IEEE} {Global} {Communications} {Conference}},
	author = {Wang, Dengzhe and Chen, Xingshu and Wang, Qixu and Wang, Shengkai and Xu, Feiyu and Zheng, Tao},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: GLOBECOM 2022 - 2022 IEEE Global Communications Conference},
	pages = {4179--4184},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZHNHY4ND\\10001386.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KX37D8QZ\\10001386.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IULQ3C46\\10001386.html:text/html},
}

@inproceedings{keisel_babelfish_2022,
	title = {{BabelFish}: {A} {Seamless} {Solution} to {Communicate} with {Multi}-{Lingual} {Individuals}},
	shorttitle = {{BabelFish}},
	url = {https://ieeexplore.ieee.org/document/9796846},
	doi = {10.1109/IETC54973.2022.9796846},
	abstract = {The goal of our work is to build a tool that allows any number of people to communicate comfortably in their native language with anyone around the globe in real time. As globalization and the mixing of economies grows throughout the world people from different locales will need to constantly be communicating with each other to collaborate and get work done. This collaboration will oftentimes include crucial conversations where language barriers might be an obstacle that prevents efficient work from getting done. We want to demonstrate that a server-less implementation of the MQTT protocol and by leveraging google’s translation engine, that seamless native communication can happen between multiple individuals who speak different languages. To accomplish this goal we will develop a rudimentary chat application which will allow one user to type a message in their native language and have that language translate in transit to the other users preferred language. By offering this feature users from around the globe will be able to feel confident that they can communicate complex thoughts and ideas to anyone anywhere and will be a huge boon to industry as our global economy continues to get more tightly coupled.},
	urldate = {2024-01-06},
	booktitle = {2022 {Intermountain} {Engineering}, {Technology} and {Computing} ({IETC})},
	author = {Keisel, Clay and Sajal, Sayeed},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 Intermountain Engineering, Technology and Computing (IETC)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XV9IBM25\\9796846.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HGN7BDCB\\9796846.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UCX8RR7R\\9796846.html:text/html},
}

@inproceedings{zuk_call_2022,
	title = {Call {Scheduling} to {Reduce} {Response} {Time} of a {FaaS} {System}},
	isbn = {2168-9253},
	url = {https://ieeexplore.ieee.org/document/9912714},
	doi = {10.1109/CLUSTER51413.2022.00031},
	abstract = {In an overloaded FaaS cluster, individual worker nodes strain under lengthening queues of requests. Although the cluster might be eventually horizontally-scaled, adding a new node takes dozens of seconds. As serving applications are tuned for tail serving latencies, and these greatly increase under heavier loads, the current workaround is resource over-provisioning. In fact, even though a service can withstand a steady load of, e.g., 70\% CPU utilization, the autoscaler is triggered at, e.g., 30–40\% (thus the service uses twice as many nodes as it would be needed). We propose an alternative: a worker-level method handling heavy load without increasing the number of nodes. FaaS executions are not interactive, compared to, e.g., text editors: end-users do not benefit from the CPU allocated to processes often, yet for short periods. Inspired by scheduling methods for High Performance Computing, we take a radical step of replacing the classic OS preemption by (1) queuing requests based on their historical characteristics; (2) once a request is being processed, setting its CPU limit to exactly one core (with no CPU oversubscription). We extend OpenWhisk and measure the efficiency of the proposed solutions using the SeBS benchmark. In a loaded system, our method decreases the average response time by a factor of 4. The improvement is even higher for shorter requests, as the average stretch is decreased by a factor of 18. This leads us to show that we can provide better response-time statistics with 3 machines compared to a 4-machine baseline.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Zuk, Paweł and Przybylski, Bartłomiej and Rzadca, Krzysztof},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Cluster Computing (CLUSTER)},
	pages = {172--182},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R3B5P5GT\\9912714.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VTH7KGVQ\\9912714.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4P4QGNR9\\9912714.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\B9MDX6I2\\Zuk et al. - 2022 - Call Scheduling to Reduce Response Time of a FaaS .pdf:application/pdf},
}

@inproceedings{gusev_cardiohpc_2022,
	title = {{CardioHPC}: {Serverless} {Approaches} for {Real}-{Time} {Heart} {Monitoring} of {Thousands} of {Patients}},
	shorttitle = {{CardioHPC}},
	url = {https://ieeexplore.ieee.org/document/10023939},
	doi = {10.1109/WORKS56498.2022.00015},
	abstract = {We analyze a heart monitoring center for patients wearing electrocardiogram sensors outside hospitals. This prevents serious heart damages and increases life expectancy and health-care efficiency. In this paper, we address a problem to provide a scalable infrastructure for the real-time processing scenario for at least 10,000 patients simultaneously, and efficient fast processing architecture for the postponed scenario when patients upload data after realized measurements. CardioHPC is a project to realize a simulation of these two scenarios using digital signal processing algorithms and artificial intelligence-based detection and classification software for automated reporting and alerting. We elaborate the challenges we met in experimenting with different serverless implementations: 1) container-based on Google Cloud Run, and 2) Function-as-a-Service (FaaS) on AWS Lambda. Experimental results present the effect of overhead in the request and transfer time, and speedup achieved by analyzing the response time and throughput on both container-based and FaaS implementations as serverless workflows.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE}/{ACM} {Workshop} on {Workflows} in {Support} of {Large}-{Scale} {Science} ({WORKS})},
	author = {Gusev, Marjan and Ristov, Sashko and Amza, Andrei and Hohenegger, Armin and Prodan, Radu and Mileski, Dimitar and Gushev, Pano and Temelkov, Goran},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM Workshop on Workflows in Support of Large-Scale Science (WORKS)},
	pages = {76--83},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FDC8RC5G\\10023939.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SSSC95RE\\10023939.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZXQQ8IDQ\\10023939.html:text/html},
}

@article{zhang_charmseeker_2022,
	title = {{CharmSeeker}: {Automated} {Pipeline} {Configuration} for {Serverless} {Video} {Processing}},
	volume = {30},
	issn = {1558-2566},
	shorttitle = {{CharmSeeker}},
	url = {https://ieeexplore.ieee.org/document/9802908},
	doi = {10.1109/TNET.2022.3183231},
	abstract = {Video processing plays an essential role in a wide range of cloud-based applications. It typically involves multiple pipelined stages, which well fits the latest fine-grained serverless computing paradigm if properly configured to match the cost and delay constraints of video. Existing configuration tools, however, are primarily developed for traditional virtual machine clusters with general workloads. This paper presents CharmSeeker, an automated configuration tuning tool for serverless video processing pipelines. We first carefully examine the key steps and the performance bottlenecks for video processing over modern serverless platforms. Then, we identify the configuration space for processing pipelines and leverage a carefully designed Sequential Bayesian Optimization search scheme to identify promising configurations. We further address the practical challenges toward integrating our solution into real-world systems and develop a prototype with AWS Lambda. Evaluation results show that CharmSeeker can find out the optimal or near-optimal configurations that improve the relative processing time up to 408.77\%. It is also more robust and scalable to various video processing pipelines compared with state-of-the-art solutions.},
	number = {6},
	urldate = {2024-01-06},
	journal = {IEEE/ACM Transactions on Networking},
	author = {Zhang, Miao and Zhu, Yifei and Liu, Jiangchuan and Wang, Feng and Wang, Fangxin},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE/ACM Transactions on Networking},
	pages = {2730--2743},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5358REQY\\9802908.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B4K8VTAE\\9802908.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NB2BVQIF\\9802908.html:text/html},
}

@inproceedings{nagy_cloud-based_2022,
	title = {Cloud-based {Serverless} {Solution} for {Facilitating} the {Organisation} of {Athletics} {Competitions}},
	isbn = {1949-0488},
	url = {https://ieeexplore.ieee.org/document/10036272},
	doi = {10.1109/SISY56759.2022.10036272},
	abstract = {The Athletimeter project was inspired by the annually held Béla Török Memorial athletics competition in Odorheiu Secuiesc, Romania, which brings together children from the surrounding areas to compete. So far, the organizers have not used any digital solution to maintain the data related to the competition. It was managed on paper, which did not provide the necessary transparency and secure maintenance for the organizers, and there was no opportunity for the spectators to follow the results. Therefore, the authors' aim was to develop a cloud-based system that could facilitate the data management needed to run an athletics competition by digitizing the process. The system provides a web and a mobile interface to its users, and is based on a cloud-based server that introduces the concept of serverless architecture. The server is only accessible during the competition, so the amount spent on resources is significantly reduced. Through the web interface, organizers can manage age groups, contestants, events, and results. In addition, the platform allows spectators to track the results of the different events in real-time. The mobile application provides an efficient alternative for recording results and identifying contestants. It is equipped with a built-in stopwatch to record the results of time-based events and a QR code reader that can associate a competitor with a result by scanning the code on the jersey. The paper aims to present the system, detailing its functionalities, architecture, and different components, as well as the technologies and tools used.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 20th {Jubilee} {International} {Symposium} on {Intelligent} {Systems} and {Informatics} ({SISY})},
	author = {Nagy, Tifani Franciska and Csibi, Zsolt and Jánosi, Borbála and Simon, Károly and Hegedüs, Hunor and Szász, Erika},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 20th Jubilee International Symposium on Intelligent Systems and Informatics (SISY)},
	pages = {000145--000150},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CIKBHBN7\\10036272.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YENV2MTM\\10036272.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RLYQHE6S\\10036272.html:text/html},
}

@inproceedings{zou_clustering-based_2022,
	title = {Clustering-{Based} {Serverless} {Edge} {Computing} {Assisted} {Federated} {Learning} for {Energy} {Procurement}},
	isbn = {2576-8565},
	url = {https://ieeexplore.ieee.org/document/9919944},
	doi = {10.23919/APNOMS56106.2022.9919944},
	abstract = {Prosumers nowadays are capable of consuming and generating renewable energy along with providing charging services for public electric vehicles (EVs) through EV support equipment (EVSE). However, the energy demand of prosumers and EVs as well as the renewable energy generation of prosumers have uncertain nature, which causes difficulty for each prosumer to purchase the proper energy at a lower price in advance. Thus, it is paramount important to do energy procurement prediction (EPP) for each prosumer. Nevertheless, submitting data from each prosumer to a centralized server for EPP will result in communication delay and need to consume a huge amount of network bandwidth and energy. Therefore, in this paper, a clustering-based serverless edge computing-assisted federated learning (FL) approach is proposed for EPP, where the objective is to minimize the Huber loss between the predicted and the real value per prosumer. In particular, firstly, normalized Laplacian-based spectral clustering is leveraged to group the prosumers with a similar energy procurement pattern to solve the problem of biased energy procurement forecast caused by updating the model among all the clients. Secondly, long short-term memory (LSTM) in the federated learning setting is utilized to train the global model of each clustered group, where the model aggregation occurs in the serverless edge computing ability-enhanced local edge server with the best performance. The evaluation results demonstrate the proposed method can achieve the lowest Huber loss compared with the baseline methods.},
	urldate = {2024-01-06},
	booktitle = {2022 23rd {Asia}-{Pacific} {Network} {Operations} and {Management} {Symposium} ({APNOMS})},
	author = {Zou, Luyao and Munir, Md. Shirajum and Tun, Ye Lin and Hong, Choong Seon},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 23rd Asia-Pacific Network Operations and Management Symposium (APNOMS)},
	pages = {01--06},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PRZIHJFA\\9919944.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SURS9JAB\\9919944.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\C4LNIPSL\\9919944.html:text/html},
}

@article{zuppelli_code_2022,
	title = {Code {Layering} for the {Detection} of {Network} {Covert} {Channels} in {Agentless} {Systems}},
	volume = {19},
	issn = {1932-4537},
	url = {https://ieeexplore.ieee.org/document/9779347},
	doi = {10.1109/TNSM.2022.3176752},
	abstract = {The growing interest in agentless and serverless environments for the implementation of virtual/container network functions makes monitoring and inspection of network services challenging tasks. A major requirement concerns the agility of deploying security agents at runtime, especially to effectively address emerging and advanced attack patterns. This work investigates a framework leveraging the extended Berkeley Packet Filter to create ad-hoc security layers in virtualized architectures without the need of embedding additional agents. To prove the effectiveness of the approach, we focus on the detection of network covert channels, i.e., hidden/parasitic network conversations difficult to spot with legacy mechanisms. Experimental results demonstrate that different types of covert channels can be revealed with a good accuracy while using limited resources compared to existing cybersecurity tools (i.e., Zeek and libpcap).},
	number = {3},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Zuppelli, Marco and Repetto, Matteo and Schaffhauser, Andreas and Mazurczyk, Wojciech and Caviglione, Luca},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	pages = {2282--2294},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\3GVSTFLM\\Zuppelli et al. - 2022 - Code Layering for the Detection of Network Covert .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2ZCFMI8U\\9779347.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DE7AL448\\9779347.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F4Y9D7ME\\9779347.html:text/html},
}

@inproceedings{rodriguez_creation_2022,
	title = {Creation of serverless applications in the cloud},
	url = {https://ieeexplore.ieee.org/document/10035685},
	doi = {10.1109/CIMPS57786.2022.10035685},
	abstract = {Serverless computing has gained much importance in the last decade, offering features such as cost reduction, low latency, scalability, and avoiding server administration [1]. Serverless computing was introduced in 2014 by Amazon with lambdas. Later it was adopted by other providers [2]. This model allows developers to focus on the application logic, because a traditional architecture contains a server as a monolithic system, while serverless works by events being different pieces of software working independently. This article presents the advantages and disadvantages of working with a cloud server from the perspective of quality development.},
	urldate = {2024-01-06},
	booktitle = {2022 11th {International} {Conference} {On} {Software} {Process} {Improvement} ({CIMPS})},
	author = {Rodríguez, Mario Alberto Negrete and Martínez, Felipe Uriel Infante},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 11th International Conference On Software Process Improvement (CIMPS)},
	pages = {216--218},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7XU6A455\\10035685.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JINTNH9W\\10035685.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9CBTE7F2\\10035685.html:text/html},
}

@inproceedings{scheuner_crossfit_2022,
	title = {{CrossFit}: {Fine}-grained {Benchmarking} of {Serverless} {Application} {Performance} across {Cloud} {Providers}},
	shorttitle = {{CrossFit}},
	url = {https://ieeexplore.ieee.org/document/10061777},
	doi = {10.1109/UCC56403.2022.00016},
	abstract = {Serverless computing emerged as a promising cloud computing paradigm for deploying cloud-native applications but raises new performance challenges. Existing performance evaluation studies focus on micro-benchmarking to measure an individual aspect of serverless functions, such as CPU speed, but lack an in-depth analysis of differences in application performance across cloud providers. This paper presents CrossFit, an approach for detailed and fair cross-provider performance benchmarking of serverless applications based on a providerindependent tracing model. Our case study demonstrates how detailed distributed tracing enables drill-down analysis to explain performance differences between two leading cloud providers, AWS and Azure. The results for an asynchronous application show that trigger time contributes most delay to the end-to-end latency and explains the main performance difference between cloud providers. Our results further reveal how increasing and bursty workloads affect performance stability, median latency, and tail latency.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE}/{ACM} 15th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Scheuner, Joel and Deng, Rui and Steghöfer, Jan-Philipp and Leitner, Philipp},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)},
	pages = {51--60},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NG2RQNJZ\\10061777.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FPX2ZDK5\\10061777.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\E4CVW6JU\\10061777.html:text/html},
}

@inproceedings{benedetti_datax_2022,
	title = {{DataX} {Allocator}: {Dynamic} resource management for stream analytics at the {Edge}},
	isbn = {2832-3033},
	shorttitle = {{DataX} {Allocator}},
	url = {https://ieeexplore.ieee.org/document/10061998},
	doi = {10.1109/IOTSMS58070.2022.10061998},
	abstract = {Serverless edge computing aims to deploy and manage applications so that developers are unaware of challenges associated with dynamic management, sharing, and maintenance of the edge infrastructure. However, this is a non-trivial task because the resource usage by various edge applications varies based on the content in their input sensor data streams. We present a novel reinforcement-learning (RL) technique to maximize the processing rates of applications by dynamically allocating resources (like CPU cores or memory) to microservices in these applications. We model applications as analytics pipelines consisting of several microservices, and a pipeline’s processing rate directly impacts the accuracy of insights from the application. In our unique problem formulation, the state space or the number of actions of RL is independent of the type of workload in the microservices, the number of microservices in a pipeline, or the number of pipelines. This enables us to learn the RL model only once and use it many times to improve the accuracy of insights for a diverse set of AI/ML engines like action recognition or face recognition and applications with varying microservices.Our experiments with real-world applications, i.e., face recognition and action recognition, show that our approach outperforms other widely-used alternative approaches and achieves up to 2.5X improvement in the overall application processing rate. Furthermore, when we apply our RL model trained on a face recognition pipeline to a different and more complex action recognition pipeline, we obtain a 2X improvement in processing rate, thus showing the versatility and robustness of our RL model to pipeline changes.},
	urldate = {2024-01-06},
	booktitle = {2022 9th {International} {Conference} on {Internet} of {Things}: {Systems}, {Management} and {Security} ({IOTSMS})},
	author = {Benedetti, Priscilla and Coviello, Giuseppe and Rao, Kunal and Chakradhar, Srimat},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 9th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I432VEL6\\10061998.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SGEDZZ7L\\10061998.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IF8JMU7N\\10061998.html:text/html},
}

@inproceedings{roy_daydream_2022,
	title = {{DayDream}: {Executing} {Dynamic} {Scientific} {Workflows} on {Serverless} {Platforms} with {Hot} {Starts}},
	isbn = {2167-4337},
	shorttitle = {{DayDream}},
	url = {https://ieeexplore.ieee.org/document/10046081},
	doi = {10.1109/SC41404.2022.00027},
	abstract = {HPC applications are increasingly being designed as dynamic workflows for the ease of development and scaling. This work demonstrates how the serverless computing model can be leveraged for efficient execution of complex, real-world scientific workflows, although serverless computing was not originally designed for executing scientific workflows. This work characterizes, quantifies, and improves the execution of three real-world, complex, dynamic scientific workflows: ExaFEL (workflow for investigating the molecular structures via X-Ray diffraction), Cosmoscout-Vr(workflow for large scale virtual reality simulation), and Core Cosmology Library (a cosmology workflow for investigating dark matter). The proposed technique, DayDream, employs the hot start mechanism for warming up the components of the workflows by decoupling the runtime environment from the component function code to mitigate cold start overhead. DayDream optimizes the service time and service cost jointly to reduce the service time by 45\% and service cost by 23\% over the state-of-the-art HPC workload manager.},
	urldate = {2024-01-06},
	booktitle = {{SC22}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Roy, Rohan Basu and Patel, Tirthak and Tiwari, Devesh},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages = {1--18},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YTATBDLZ\\10046081.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6HSWFJ6A\\10046081.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ACW4HU2Q\\10046081.html:text/html},
}

@article{deng_dependent_2022,
	title = {Dependent {Function} {Embedding} for {Distributed} {Serverless} {Edge} {Computing}},
	volume = {33},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/document/9665233},
	doi = {10.1109/TPDS.2021.3137380},
	abstract = {Edge computing is booming as a promising paradigm to extend service provisioning from the centralized cloud to the network edge. Benefit from the development of serverless computing, an edge server can be configured as a carrier of limited serverless functions, in the way of deploying Docker runtime and Kubernetes engine. Meanwhile, an application generally takes the form of directed acyclic graphs (DAGs), where vertices represent dependent functions and edges represent data traffic. The status quo of minimizing the completion time (a.k.a. makespan) of the application motivates the study on optimal function placement. However, current approaches lose sight of proactively splitting and mapping the traffic to the logical data paths between the heterogeneous edge servers, which could affect the makespan significantly. To remedy that, we propose an algorithm, termed as Dependent Function Embedding (DPE), to get the optimal edge server for each function to execute and the moment it starts executing. DPE finds the best segmentation of each data traffic by exquisitely solving several infinity norm minimization problems. DPE is theoretically verified to achieve the global optimality. Extensive experiments on Alibaba cluster trace show that DPE significantly outperforms two baseline algorithms in makespan by 43.19\% and 40.71\%, respectively.},
	number = {10},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Deng, Shuiguang and Zhao, Hailiang and Xiang, Zhengzhe and Zhang, Cheng and Jiang, Rong and Li, Ying and Yin, Jianwei and Dustdar, Schahram and Zomaya, Albert Y.},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {2346--2357},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Q93IH6DL\\9665233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9VIGJE37\\9665233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XJSKWG2D\\9665233.html:text/html},
}

@inproceedings{yodjaiphet_development_2022,
	title = {Development of {Low} {Cost} 3 {Phases} {IoT} {Electrical} {Power} {Meter}},
	url = {https://ieeexplore.ieee.org/document/10067539},
	doi = {10.1109/InCIT56086.2022.10067539},
	abstract = {In this paper, 3 phases of IoT electrical power meter is developed. The aim of this development is to create a 3 phases low-cost meter with IoT that can measure the true RMS value. The design of both hardware and software focuses on easy design, fewer components, uncomplicated processing, power component values, and using calculations according to numerical calculation method according to the basic theoretical principles. The dsPIC30F4011 16-bits microcontroller is used as the main microcontroller. The ESP8266 is used as the transmitting device. The information of the measurement is sent from dsPIC30F4011 to the Thinkspeak platform due to the serverless concept. After that, the information is displayed on the website via the dashboard. By comparing with the commercial product, this DIY is cheaper and has more flexible that the user can adjust the function and dash board as wish.},
	urldate = {2024-01-06},
	booktitle = {2022 6th {International} {Conference} on {Information} {Technology} ({InCIT})},
	author = {Yodjaiphet, Anusorn and Ayutaya, Ronnachai Sretawat Na and Chansareewittaya, Suppakarn},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 6th International Conference on Information Technology (InCIT)},
	pages = {78--82},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IJMTT72V\\10067539.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5G9BL3GH\\10067539.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VP8J94VE\\10067539.html:text/html},
}

@inproceedings{fingler_dgsf_2022,
	title = {{DGSF}: {Disaggregated} {GPUs} for {Serverless} {Functions}},
	isbn = {1530-2075},
	shorttitle = {{DGSF}},
	url = {https://ieeexplore.ieee.org/document/9820659},
	doi = {10.1109/IPDPS53621.2022.00077},
	abstract = {Ease of use and transparent access to elastic resources have attracted many applications away from traditional platforms toward serverless functions. Many of these applications, such as machine learning, could benefit significantly from GPU acceleration. Unfortunately, GPUs remain inaccessible from serverless functions in modern production settings. We present DGSF, a platform that transparently enables serverless functions to use GPUs through general purpose APIs such as CUDA. DGSF solves provisioning and utilization challenges with disaggregation, serving the needs of a potentially large number of functions through virtual GPUs backed by a small pool of physical GPUs on dedicated servers. Disaggregation allows the provider to decouple GPU provisioning from other resources, and enables significant benefits through consolidation. We describe how DGSF solves GPU disaggregation challenges including supporting API transparency, hiding the latency of communication with remote GPUs, and load-balancing access to heavily shared GPUs. Evaluation of our prototype on six workloads shows that DGSF's API remoting optimizations can improve the runtime of a function by up to 50\% relative to unoptimized DGSF. Such optimizations, which aggressively remove GPU runtime and object management latency from the critical path, can enable functions running over DGSF to have a lower end-to-end time than when running on a GPU natively. By enabling GPU sharing, DGSF can reduce function queueing latency by up to 53\%. We use DGSF to augment AWS Lambda with GPU support, showing similar benefits.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Fingler, Henrique and Zhu, Zhiting and Yoon, Esther and Jia, Zhipeng and Witchel, Emmett and Rossbach, Christopher J.},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {739--750},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XUTXU6LH\\9820659.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8LXZXZSN\\9820659.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HPUNTG2M\\9820659.html:text/html},
}

@inproceedings{mileski_distributed_2022,
	title = {Distributed {Denial} of {Wallet} {Attack} on {Serverless} {Pay}-as-you-go {Model}},
	url = {https://ieeexplore.ieee.org/document/9983732},
	doi = {10.1109/℡FOR56187.2022.9983732},
	abstract = {The serverless pay-as-you-go model in the cloud enables payment of services during execution and resources used at the smallest, most granular level, as was the initial idea when setting the foundations and concepts of the pay-as-you-go model in the cloud. The disadvantage of this method of payment during execution and the resources used is that it is subject to financial damage if we have an attack on serverless services. This paper defines notions for three types of attacks that can cause financial damage to the serverless pay-as-you-go model and are experimentally validated. The first attack is Blast DDoW - Distributed Denial of Wallet, the second attack is Continual Inconspicuous DDoW, and the third one is Background Chained DDoW. We discussed financial damages and the consequences of each type of attack.},
	urldate = {2024-01-06},
	booktitle = {2022 30th {Telecommunications} {Forum} (℡{FOR})},
	author = {Mileski, Dimitar and Mihajloska, Hristina},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 30th Telecommunications Forum (TELFOR)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SSPECJ2H\\9983732.html:text/html},
}

@article{tang_distributed_2022,
	title = {Distributed {Task} {Scheduling} in {Serverless} {Edge} {Computing} {Networks} for the {Internet} of {Things}: {A} {Learning} {Approach}},
	volume = {9},
	issn = {2327-4662},
	shorttitle = {Distributed {Task} {Scheduling} in {Serverless} {Edge} {Computing} {Networks} for the {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/9757233},
	doi = {10.1109/JIOT.2022.3167417},
	abstract = {By delegating the infrastructure management, such as provisioning or scaling to third-party providers, serverless edge computing has recently been widely adopted in several applications, especially Internet of Things (IoT) applications. Task scheduling is a critical issue in serverless edge computing as it significantly impacts the quality of user experience. In contrast to the centralized scheduling in the cloud center, serverless edge task scheduling is more challenging due to the heterogeneous and resource-constrained nature of edge resources. This article aims to study the distributed task scheduling for the IoT in serverless edge computing networks, in which heterogeneous serverless edge computing nodes are rational individuals with interests to optimize their own scheduling utility while the nodes only have access to local observations. The task scheduling competition process is formulated as a partially observable stochastic game (POSG) to enable serverless edge computing nodes to noncooperatively schedule tasks and allocate computing resources depending on their locally observed system state, which takes into account the associated task generation state, data queue state, communication channel state, and previous computing resource allocation state. To solve the proposed POSG and deal with the partial observability, a multiagent task scheduling algorithm based on the dueling double deep recurrent Q -network (D3RQN) method is developed to approximate the optimal task scheduling and resource allocation solution. Finally, extensive simulation experiments are conducted to validate the effectiveness and superiority of the proposed scheme.},
	number = {20},
	urldate = {2024-01-06},
	journal = {IEEE Internet of Things Journal},
	author = {Tang, Qinqin and Xie, Renchao and Yu, Fei Richard and Chen, Tianjiao and Zhang, Ran and Huang, Tao and Liu, Yunjie},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {19634--19648},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GSN8C27D\\9757233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UQKUR5MG\\9757233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NVTPBD63\\9757233.html:text/html},
}

@inproceedings{patel_dsserve_2022,
	title = {{DSServe} - {Data} {Science} using {Serverless}},
	url = {https://ieeexplore.ieee.org/document/10020441},
	doi = {10.1109/BigData55660.2022.10020441},
	abstract = {AI Applications uses various data science tools such as Jupyter notebook to prescribe a series of steps, commonly referred as workflow, for building AI Solutions. The steps in workflow can be as simple as loading the data from remote storage, visualize the data for better understanding or conducting data quality study, or it can be as complex as generating features for modeling, best model discovery processes, etc. Clearly, different steps of the data science workflow has varying requirement of compute resources. Moreover, the execution of steps in workflow are Adhoc and Subjective. With wider availability of various Serverless technology, in this paper, we demonstrate a generalized framework that can be used to provide on demand scale out capability for the Data Science Workflow. In particular, we selected the most common AI operation, namely Automatic Model Selection, as an example to demonstrate benefits of serverless computing. We conducted a detailed experimental results using IBM Code Engine technology to validate the benefits of our proposed approach.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Patel, Dhaval and Lin, Shuxin and Kalagnanam, Jayant},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Big Data (Big Data)},
	keywords = {Work-flows, Digital storage, Remote storage, On demands, AI applications, Best model, Compute resources, Data quality, Data Science, Model discoveries, Science tools, Simple++},
	pages = {2343--2345},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6ZWQQQ5M\\10020441.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I5ZZPTE2\\10020441.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NE5B3BCV\\10020441.html:text/html},
}

@inproceedings{rajput_edgefaasbench_2022,
	title = {{EdgeFaaSBench}: {Benchmarking} {Edge} {Devices} {Using} {Serverless} {Computing}},
	isbn = {2767-9918},
	shorttitle = {{EdgeFaaSBench}},
	url = {https://ieeexplore.ieee.org/document/9860334},
	doi = {10.1109/EDGE55608.2022.00024},
	abstract = {Due to the development of small-size, energy-efficient, and powerful CPUs and GPUs for single board computers, various edge devices are widely adopted for hosting real-world applications, including real-time object detection, autonomous driving, and sensor stream processing. At the same time, serverless computing receives increasing attention as a new application deployment model because of its simplicity, scalability, event-driven processing, and short-lived computation. Therefore, there is a growing demand for applying serverless computing to edge computing environments. However, due to the lack of characterization of serverless edge computing (e.g., application performance and impact from resource heterogeneity), researchers and practitioners have to conduct tedious measurements to understand the performance of serverless applications on edge devices in non-systematic ways.We create EdgeFaaSBench, a novel benchmark suite for serverless computing on edge devices, to bridge this gap. EdgeFaaSBench is developed on top of Apache OpenFaaS with Docker Swarm and can run various serverless benchmark workloads on edge devices with different hardware specifications (e.g., GPUs). EdgeFaaSBench contains 14 different benchmark workloads running on heterogeneous edge devices and captures various system-level, application-level, and serverless-specific metrics, including system utilization, response time, cold/warm start times, and impact of concurrent function executions. Experimental studies are conducted on two widely used edge devices, Raspberry Pi 4B and Jetson Nano, to show EdgeFaaSBench’s capabilities to benchmark serverless computing on edge devices.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Edge} {Computing} and {Communications} ({EDGE})},
	author = {Rajput, Kaustubh Rajendra and Kulkarni, Chinmay Dilip and Cho, Byungjin and Wang, Wei and Kim, In Kee},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Edge Computing and Communications (EDGE)},
	pages = {93--103},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JUWIH8QN\\9860334.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JP78PGGU\\9860334.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\68CKTXYF\\9860334.html:text/html},
}

@inproceedings{aslanpour_energy-aware_2022,
	title = {Energy-{Aware} {Resource} {Scheduling} for {Serverless} {Edge} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9826101},
	doi = {10.1109/CCGrid54584.2022.00028},
	abstract = {In this paper, we present energy-aware scheduling for Serverless edge computing. Energy awareness is critical since edge nodes, in many Internet of Things (IoT) domains, are meant to be powered by renewable energy sources that are variable, making low-powered and/or overloaded (bottleneck) nodes unavailable and not operating their services. This awareness is also required since energy challenges have not been previously addressed by Serverless, largely due to its origin in cloud computing. To achieve this, we formally model an energy-aware resource scheduling problem in Serverless edge computing, given a cluster of battery-operated and renewable-energy powered nodes. Then, we devise zone-oriented and priority-based algorithms to improve the operational availability of bottleneck nodes. As assets, our algorithm coins terms “sticky offloading” and “warm scheduling” in the interest of the Quality of Service (QoS). We evaluate our proposal against well-known benchmarks using real-world implementations on a cluster of Raspberry Pis enabled with container orchestration, Kubernetes, and Serverless computing, OpenFaaS, where edge nodes are powered by real-world solar irradiation. Experimental results achieve significant improvements, up to 33\%, in helping bottleneck node's operational availability while preserving the QoS. With energy awareness, now Serverless can unconditionally offer its resource efficiency and portability at the edge.},
	urldate = {2024-01-06},
	booktitle = {2022 22nd {IEEE} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Aslanpour, Mohammad Sadegh and Toosi, Adel N. and Cheema, Muhammad Aamir and Gaire, Raj},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {190--199},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZMDNKELS\\9826101.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7SLPAHGY\\9826101.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G3W2PU54\\9826101.html:text/html},
}

@inproceedings{baughman_exploring_2022,
	title = {Exploring {Tradeoffs} in {Federated} {Learning} on {Serverless} {Computing} {Architectures}},
	url = {https://ieeexplore.ieee.org/document/9973734},
	doi = {10.1109/eScience55777.2022.00074},
	abstract = {Federated learning is driving the development of new techniques to efficiently and securely use data across multiple sites while using diverse resources. One of these techniques is the use of the serverless computing paradigm to abstract away resource specific configurations, allowing federated learning across heterogeneous environments. However, deploying federated learning across edge resources, the cloud, and traditional HPC sites will require specialized approaches in order to best account for the weaknesses and strengths of each resource. In this work, we explore the new tradeoffs presented by managing a federated learning task across heterogeneous resources and demonstrate these tradeoffs with experiments using a serverless federated learning framework.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Baughman, Matt and Foster, Ian and Chard, Kyle},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 18th International Conference on e-Science (e-Science)},
	pages = {433--434},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WSPJYHJ4\\9973734.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9BJA4KTG\\9973734.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A59T2CSF\\9973734.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\N6D2KI9D\\Baughman et al. - 2022 - Exploring Tradeoffs in Federated Learning on Serve.pdf:application/pdf},
}

@inproceedings{smith_fado_2022,
	title = {{FaDO}: {FaaS} {Functions} and {Data} {Orchestrator} for {Multiple} {Serverless} {Edge}-{Cloud} {Clusters}},
	shorttitle = {{FaDO}},
	url = {https://ieeexplore.ieee.org/document/9799194},
	doi = {10.1109/ICFEC54809.2022.00010},
	abstract = {Function-as-a-Service (FaaS) is an attractive cloud computing model that simplifies application development and deployment. However, current serverless compute platforms do not consider data placement when scheduling functions. With the growing demand for edge-cloud continuum, multi-cloud, and multi-serverless applications, this flaw means serverless technologies are still ill-suited to latency-sensitive operations like media streaming. This work proposes a solution by presenting a tool called FaDO: FaaS Functions and Data Orchestrator, designed to allow data-aware functions scheduling across multi-serverless compute clusters present at different locations, such as at the edge and in the cloud. FaDO works through header-based HTTP reverse proxying and uses three load-balancing algorithms: 1) The Least Connections, 2) Round Robin, and 3) Random for load balancing the invocations of the function across the suitable serverless compute clusters based on the set storage policies. FaDO further provides users with an abstraction of the serverless compute cluster’s storage, allowing users to interact with data across different storage services through a unified interface. In addition, users can configure automatic and policy-aware granular data replications, causing FaDO to spread data across the clusters while respecting location constraints. Load testing results show that it is capable of load balancing high-throughput workloads, placing functions near their data without contributing any significant performance overhead.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} 6th {International} {Conference} on {Fog} and {Edge} {Computing} ({ICFEC})},
	author = {Smith, Christopher Peter and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael and Benedict, Shajulin},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 6th International Conference on Fog and Edge Computing (ICFEC)},
	keywords = {Serverless, Function-as-a-service, Edge computing, Orchestration, Scheduling, Digital storage, Edge clouds, Data-aware, Edge-computing, Load-Balancing, Media streaming, Multi-cloud, Multi-clouds, Service data, Service functions},
	pages = {17--25},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4PDUTWM7\\9799194.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5TTNGW6B\\9799194.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XWN9M9QS\\9799194.html:text/html},
}

@inproceedings{cordingly_function_2022,
	title = {Function {Memory} {Optimization} for {Heterogeneous} {Serverless} {Platforms} with {CPU} {Time} {Accounting}},
	url = {https://ieeexplore.ieee.org/document/9946331},
	doi = {10.1109/IC2E55432.2022.00019},
	abstract = {Serverless Function-as-a-Service (FaaS) platforms often abstract the underlying infrastructure configuration into the single option of specifying a function's memory reservation size. This resource abstraction of coupling configurations options (e.g. vCPUs, memory, disk), combined with the lack of profiling, leaves developers to make ad hoc decisions on how to configure functions. Solutions are needed to mitigate exhaustive brute force searches of large parameter input spaces to find optimal configurations which can incur high costs. To address these challenges, we propose CPU Time Accounting Memory Selection (CPU-TAMS). CPU-TAMS is a workload agnostic memory selection method that utilizes CPU time accounting principles and regression modeling to recommend memory settings that reduce function runtime and subsequently, cost. Comparing CPU-TAMS to eight existing selection methods, we find that CPU-TAMS finds maximum value memory settings with only 8\% runtime and 5\% cost error compared to brute force testing while only requiring a single profiling run to evaluate function resource requirements. We adapt CPU-TAMS for use on four commercial FaaS platforms demonstrating efficacy to optimize function memory configurations where platforms feature heterogeneous infrastructure management policies.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Cordingly, Robert and Xu, Sonia and Lloyd, Wes},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Cloud Engineering (IC2E)},
	keywords = {Serverless Computing, Serverless computing, Function-as-a-Service, Function-as-a-service, Service platforms, Function evaluation, Performance Evaluation, Performances evaluation, CPU time, Memory optimization, Performance Modeling, Resource abstraction, Runtimes, Selection methods},
	pages = {104--115},
}

@inproceedings{schirmer_fusionize_2022,
	title = {Fusionize: {Improving} {Serverless} {Application} {Performance} through {Feedback}-{Driven} {Function} {Fusion}},
	shorttitle = {Fusionize},
	url = {https://ieeexplore.ieee.org/document/9946241},
	doi = {10.1109/IC2E55432.2022.00017},
	abstract = {Serverless computing increases developer productivity by removing operational concerns such as managing hardware or software runtimes. Developers, however, still need to partition their application into functions, which can be error-prone and adds complexity: Using a small function size where only the smallest logical unit of an application is inside a function maximizes flexibility and reusability. Yet, having small functions leads to invocation overheads, additional cold starts, and may increase cost due to double billing during synchronous invocations. In this paper we present Fusionize, a framework that removes these concerns from developers by automatically fusing the application code into a multi-function orchestration with varying function size. Developers only need to write the application code following a lightweight programming model and do not need to worry how the application is turned into functions. Our framework automatically fuses different parts of the application into functions and manages their interactions. Leveraging monitoring data, the framework optimizes the distribution of application parts to functions to optimize deployment goals such as end-to-end latency and cost. Using two example applications, we show that Fusionizecan automatically and iteratively improve the deployment artifacts of the application.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Schirmer, Trever and Scheuner, Joel and Pfandzelter, Tobias and Bermbach, David},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {85--95},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y5VWSZN9\\9946241.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZK2IMCT6\\9946241.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\W4VHAFLU\\9946241.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\SK2DH3TM\\Schirmer et al. - 2022 - Fusionize Improving Serverless Application Perfor.pdf:application/pdf},
}

@article{mi_general_2022,
	title = {General and {Fast} {Inter}-{Process} {Communication} via {Bypassing} {Privileged} {Software}},
	volume = {71},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/9627571},
	doi = {10.1109/TC.2021.3130751},
	abstract = {IPC (Inter-Process Communication) is a widely used operating system (OS) technique that allows one process to invoke the services of other processes. The IPC participants may share the same OS (internal IPC) or use a separate OS (external IPC). Even though a long line of researches has optimized the performance of IPC, it is still a major factor of the run-time overhead of IPC-intensive applications. Furthermore, there is no one-size-fits-all solution for both internal and external IPC. This paper presents SkyBridge, a general communication technique designed and optimized for both types of IPC. SkyBridge requires no involvement of the privileged software (the kernel or the hypervisor) and enables a process to directly switch to the virtual address space of the target process, regardless of whether they are running on the same OS or not. We have implemented SkyBridge on two microkernels (seL4 and Google Zircon) as well as an open-source serverless hypervisor (Firecracker). The evaluation results show that SkyBridge improves the latency of internal IPC and external IPC by up to 19.6x and 1265.7x, respectively.},
	number = {10},
	urldate = {2024-01-06},
	journal = {IEEE Transactions on Computers},
	author = {Mi, Zeyu and Zhuang, Haoqi and Zang, Binyu and Chen, Haibo},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {2435--2448},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\J7XH2HHQ\\9627571.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D8W38JJB\\9627571.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PB7RQYUV\\9627571.html:text/html},
}

@inproceedings{ristov_godeploy_2022,
	title = {{GoDeploy}: {Portable} {Deployment} of {Serverless} {Functions} in {Federated} {FaaS}},
	shorttitle = {{GoDeploy}},
	url = {https://ieeexplore.ieee.org/document/9973070},
	doi = {10.1109/CloudSummit54781.2022.00012},
	abstract = {Federated Function-as-a-Service (FaaS) offers higher scalability, better resilience and cost-performance trade-off than running serverless applications in a single cloud region. However, existing Infrastructure-as-Code (IaC) tools are mainly focused on the FaaS provider, rather than on applications, which increases developer effort to code multiple times the same data in order to deploy a serverless function on various cloud regions in federated FaaS. To bridge this gap, this paper introduces GoDeploy, a framework that simplifies coding the deployment of serverless functions in Federated FaaS. Using the design principle “code once, deploy everywhere”, GoDeploy offers developers a domain-specific language, which introduces a three-levels hierarchy serverless function → FaaS providers →cloud regions of FaaS provider, rather than existing either the two-levels hierarchy FaaS provider → serverless functions or flat horizontal structure. Moreover, GoDeploy hides the complexity and requirements of each FaaS provider to store deployment packages (zip) of serverless functions on their storages. With this approach, GoDeploy reduces deployment script length measured in lines of code (LoC) compared to the recent FaaSifier M2FaaS by up to 33.33\% for deployment on three cloud regions of AWS. When deploying a single function on three cloud regions of each of three FaaS providers AWS, IBM, Google, LoC are reduced by up to 72.34\% compared to the state-of-the-art IaC tool Terraform. The improvement is higher when a serverless function needs to be deployed on multiple cloud regions because GoDeploy's three-level hierarchy requires a single LoC per cloud region, compared to multiple LoC in Terraform's and M2FaaS DSLs.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {Cloud} {Summit}},
	author = {Ristov, Sashko and Brandacher, Simon and Felderer, Michael and Breu, Ruth},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE Cloud Summit},
	keywords = {Automation, Function-as-a-Service, Function-as-a-service, Portability, Infrastructure as a service (IaaS), Economic and social effects, Cost performance, Domain Specific Language, Domains specific languages, High scalabilities, Infrastructure-as-a-code, Infrastructure-as-a-Code, Line of codes, portability, Problem oriented languages, Service provider, Terraform, Three-level},
	pages = {38--43},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VHMY7GGT\\9973070.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T37RTUUJ\\9973070.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GTRJ6N84\\9973070.html:text/html},
}

@inproceedings{werner_hardless_2022,
	title = {{HARDLESS}: {A} {Generalized} {Serverless} {Compute} {Architecture} for {Hardware} {Processing} {Accelerators}},
	shorttitle = {{HARDLESS}},
	url = {https://ieeexplore.ieee.org/document/9946318?denied=},
	doi = {10.1109/IC2E55432.2022.00016},
	abstract = {The increasing use of hardware processing accelerators tailored for specific applications, such as the Vision Processing Unit (VPU) for image recognition, further increases developers' configuration, development, and management over-head. Developers have successfully used fully automated elastic cloud services such as serverless computing to counter these additional efforts and shorten development cycles for applications running on CPUs. Unfortunately, current cloud solutions do not yet provide these simplifications for applications that require hardware acceleration. However, as the development of special-ized hardware acceleration continues to provide performance and cost improvements, it will become increasingly important to enable ease of use in the cloud. In this paper, we present an initial design and implemen-tation of Hardless, an extensible and generalized serverless computing architecture that can support workloads for arbitrary hardware accelerators. We show how Hardless can scale across different commodity hardware accelerators and support a variety of workloads using the same execution and programming model common in serverless computing today.},
	urldate = {2024-01-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Werner, Sebastian and Schirmer, Trever},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {79--84},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZAXIJLQS\\9946318.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8JYWKTV6\\9946318.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G5Z7QQFV\\9946318.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\CKQJD3U2\\Werner and Schirmer - 2022 - HARDLESS A Generalized Serverless Compute Archite.pdf:application/pdf},
}

@inproceedings{athreya_implementation_2022,
	title = {Implementation of {Serverless} {E}-{Commerce} {Mobile} {Application}},
	url = {https://ieeexplore.ieee.org/document/9847829},
	doi = {10.1109/CONIT55038.2022.9847829},
	abstract = {In recent years, the internet ecosystem has improved many folds giving rise to multiple avenues for businesses to reach their customers. Technology adoption in the global arena has exponentially increased due to the evolution of smartphones. Due to the above-mentioned factors, E-Commerce has become one of the most widely used transaction methods worldwide to purchase products and services. Traditional Established Businesses can manage E-Commerce without a technology team with the help of EaaS (E-Commerce as a Service) Providers like Shopify, WooCommerce, BigCommerce and Magento which have very high development, subscription and ongoing cloud costs. Small and Medium Scale businesses need a cost-effective, high-performance solution to build the business. To build such a system we have performed research and analysis on available technology paradigms (monolith vs microservices) and Cloud paradigm (IaaS vs PaaS) for finalizing the architecture of the E-Commerce application. In this paper, we showcase the analysis results and elaborate on the implementation of our E-Commerce Application.},
	urldate = {2024-01-06},
	booktitle = {2022 2nd {International} {Conference} on {Intelligent} {Technologies} ({CONIT})},
	author = {Athreya, Shriram and Kurian, Sanmith and Dange, Afrin and Bhatsangave, Suvarna},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 2nd International Conference on Intelligent Technologies (CONIT)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EY46HGAK\\9847829.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YR88SU2M\\9847829.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YB7MCM3N\\9847829.html:text/html},
}

@inproceedings{wahal_iot_2022,
	title = {{IoT} based {Chatbots} using {NLP} and {SVM} {Algorithms}},
	url = {https://ieeexplore.ieee.org/document/9853095},
	doi = {10.1109/ICIEM54221.2022.9853095},
	abstract = {As of late, there has been a developing revenue in creating AI-empowered chatbot-based indication checker (CSC) applications in the medical services market. CSC applications give expected determinations to clients and help them with self-triaging in light of Artificial Intelligence (AI) procedures utilizing human-like discussions. The review presents an original PC application going about as an individual virtual specialist that has been ideally planned and broadly prepared to collaborate with patients like people. This application depends on a serverless engineering and it totals the administrations of a specialist by giving preventive measures, home cures, intuitive directing meetings, medical care tips, and side effects covering the most pervasive sicknesses in provincial India. Man-made reasoning (AI) is progressively being utilized in medical services. Here, AI-based chatbot frameworks can go about as robotized conversational specialists, equipped for advancing wellbeing, giving schooling, and conceivably provoking conduct change. Investigating the inspiration to utilize wellbeing chatbots is expected to foresee take-up. This conversational application has brought about diminishing the hindrances for admittance to medical services offices and secures shrewd interviews from a distance to permit opportune consideration and quality therapy, along these lines really helping the general public. This conversational application has achieved decreasing the obstructions for permission to clinical benefits workplaces and ties down canny meetings from a distance to allow perfect thought and quality treatment, thusly truly helping the overall population [1]. Therefore, in this research paper, we have discussed about the different roles of artificial intelligence, the way it uniquely involves itself in chatbots and cover it and make it to the most important working part of the model. Specifically, in this, we have discussed about the role in medical healthcare related chatbots along with the integration of IoT devices such as batteries and sensors in this case with the chatbot.},
	urldate = {2024-01-06},
	booktitle = {2022 3rd {International} {Conference} on {Intelligent} {Engineering} and {Management} ({ICIEM})},
	author = {Wahal, Asthha and Aggarwal, Muskan and Poongodi, T.},
	month = apr,
	year = {2022},
	note = {Journal Abbreviation: 2022 3rd International Conference on Intelligent Engineering and Management (ICIEM)},
	pages = {484--489},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UNXJCN4D\\9853095.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CZ38TTHV\\9853095.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QHKTXZ77\\9853095.html:text/html},
}

@inproceedings{abad_keynote_2022,
	title = {Keynote: {Designing} {Serverless} {Platforms} to {Support} {Emerging} {Applications}},
	shorttitle = {Keynote},
	url = {https://ieeexplore.ieee.org/document/9775198},
	doi = {10.1109/PerComWorkshops53856.2022.9775198},
	abstract = {Serverless computing offerings from cloud providers have gained significant traction in recent years due to the advantages that these platforms bring with their flexible pricing models, built-in scalability, and minimal operational requirements. In a recent survey of serverless use cases, we found a wide variety of applications that depend on these services, including implementing the core functionality at the backend of mobile applications, automating the DevOps tasks of complex distributed applications, real-time processing of IoT streaming data, and scientific applications. To properly support these applications, the platforms should be fast, self-managing, and provide support for diverse QoS requirements. As a result, novel improvements to serverless platforms are rapidly being proposed and adopted. Evaluating these solutions necessitates application-based, workload-aware benchmarking tools that the community can rely on. This talk addresses these challenges and our research efforts on tackling them, presenting a performance engineering perspective about the current state and future challenges of serverless computing research. I will describe our solutions in autonomic resource management for serverless platforms, focusing on solutions that improve performance or reduce costs via scheduling, caching, and right-sizing of resources, along with our ongoing efforts in developing an application-driven serverless benchmark.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	author = {Abad, Cristina L.},
	month = mar,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7DHA3VC2\\9775198.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\548IW2MK\\9775198.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D4AWCKEP\\9775198.html:text/html},
}

@inproceedings{szewczyk_leaps_2022,
	title = {Leaps and bounds: {Analyzing} {WebAssembly}’s performance with a focus on bounds checking},
	shorttitle = {Leaps and bounds},
	url = {https://ieeexplore.ieee.org/document/9975418},
	doi = {10.1109/IISWC55918.2022.00030},
	abstract = {WebAssembly is gaining more and more popularity, finding applications beyond the Web browser for which it was initially designed. However, its performance, which developers intended to be comparable with native, has not been extensively studied to identify overheads and pinpoint their causes. This paper identifies that WebAssembly’s bounds-checked memory access safety mechanism may introduce up to a 650\% overhead, and requires further tuning. Based on that, we extend four popular WebAssembly runtimes with modern bounds checking mechanisms and compare the performance of each with native compiled code. The runtimes are evaluated on three different instruction set architectures: x86-64, Armv8, and RISC-V RV64GC. We show that, for simple numerical kernels from Poly-Bench/C, there are no significant differences in the bounds checking performance overheads across different instruction set architectures. With the default bounds checking mechanism, performance-oriented runtimes are able to achieve execution times within 20\% of native on x86-64 platforms, within 35\% on Armv8 platforms, and within 17\% on RISC-V. We also show that, when scaling the tested runtimes to multiple threads, the default bounds checking approach taken by WAVM, Wasmtime, and V8 of using the mprotect syscall to resize memory can cause excessive locking in the Linux kernel. Such scaling might be used to quickly start up serverless instances for a single function without the overhead of spawning new processes. We present an alternative userfaultf-based solution to mitigate this issue. We share our results, tools, and scripts under an open source license for other researchers to replicate and use to monitor the progress that WebAssembly runtimes make as they evolve.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Szewczyk, Raven and Stonehouse, Kimberley and Barbalace, Antonio and Spink, Tom},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Symposium on Workload Characterization (IISWC)},
	pages = {256--268},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\7AZ6M7R4\\Szewczyk et al. - 2022 - Leaps and bounds Analyzing WebAssembly’s performa.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RDG4DWEC\\9975418.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RC3RHDJF\\9975418.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IZ8TI7V8\\9975418.html:text/html},
}

@inproceedings{mutengeni_local_2022,
	title = {Local {Area} {Network} {Based} {Collaboration} {Using} {Distributed} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10045858},
	doi = {10.1109/ZCICT55726.2022.10045858},
	abstract = {Collab is an application that uses distributed computing techniques for effective, real-time collaboration over a Local Area Network (LAN) in either a home, educational, or workplace environment. Users can collaborate by joining a secure collaboration session in the absence of an internet connection. Within a session, users can share messages, digital content, and reviews. Most existing LANbased applications lack important collaboration features. It is against this backdrop that we propose the use of distributed computing techniques in a small-scale, serverless LAN to empower users to collaborate effectively in local teams. Thus, we design and implement a computer application that can be used for collaboration on a LAN in the absence of a centralised web service.},
	urldate = {2024-01-07},
	booktitle = {2022 1st {Zimbabwe} {Conference} of {Information} and {Communication} {Technologies} ({ZCICT})},
	author = {Mutengeni, Joseph and Musasa, Alec and Mutunhu, Belinda},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 1st Zimbabwe Conference of Information and Communication Technologies (ZCICT)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H7RHAGLW\\10045858.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RTXK5L93\\10045858.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PR5EF4XN\\10045858.html:text/html},
}

@inproceedings{abdelbari_mda_2022,
	title = {{MDA}: {Multiple} {Decentralized} {Anchors} for {Hiding} {Communication} {Information}},
	isbn = {2770-7962},
	shorttitle = {{MDA}},
	url = {https://ieeexplore.ieee.org/document/9932770},
	doi = {10.1109/ISMSIT56059.2022.9932770},
	abstract = {This paper presents a novel network architecture based on The Onion Routing (TOR) network and BitTorrent concepts to advance privacy and anonymity in communication networks. In the traditional TOR network, when the transmitter sends a message, any external intruder or the first proxy knows that a message has been sent from a specific location or IP address. The same issue is on the receiver side. This can reveal some privacy about who using the network, the time of activity, and the density of communications. Another issue with the services that provide end-to-end encryption is that the powerful entities can force the service's owner to deliver their encryption keys and other related data that can reveal information about the communications within the network. Therefore, the new architecture decentralizes the network entirely in which nodes are the only fundamental component of the network. The nodes themselves act as hops and control the rules of the network by using a distributed hash table (DHT). There are no proxy servers working as hops in TOR networks or servers to hold DHTs as trackers in the BitTorrent network. The nodes act as an end client and a hop server for other nodes that want to join, leave, and communicate. The actual implementation of the MDA network is under development for peer-to-peer (P2P) messaging. Further development can be made for web browsing, file sharing and video streaming.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Symposium} on {Multidisciplinary} {Studies} and {Innovative} {Technologies} ({ISMSIT})},
	author = {Abdelbari, Amr},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)},
	pages = {591--596},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UFFZMNNY\\9932770.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FUY74PJ3\\9932770.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WCGTPN5L\\9932770.html:text/html},
}

@inproceedings{he_medtator_2022,
	title = {{MedTator}: {A} {Serverless} {Web}-based {Tool} for {Corpus} {Annotation}},
	isbn = {2575-2634},
	shorttitle = {{MedTator}},
	url = {https://ieeexplore.ieee.org/document/9874521},
	doi = {10.1109/ICHI54592.2022.00099},
	abstract = {Annotation tools play an important role in building high-quality annotation corpus. Although existing text annotation tools may provide many features to meet different needs for text annotation, an easy-to-install tool for annotating clinical narrative documents is still demanded. In response, we developed MedTator, a serverless web-based tool for corpus annotation.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 10th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {He, Huan and Fu, Sunyang and Wang, Liwei and Wen, Andrew and Liu, Sijia and Liu, Hongfang},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)},
	pages = {530--531},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QLMBFW6S\\9874521.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A7RV746B\\9874521.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XYZN663Q\\9874521.html:text/html},
}

@inproceedings{byrne_microfaas_2022,
	title = {{MicroFaaS}: {Energy}-efficient {Serverless} on {Bare}-metal {Single}-board {Computers}},
	isbn = {1558-1101},
	shorttitle = {{MicroFaaS}},
	url = {https://ieeexplore.ieee.org/document/9774688},
	doi = {10.23919/DATE54114.2022.9774688},
	abstract = {Serverless function-as-a-service (FaaS) platforms offer a radically-new paradigm for cloud software development, yet the hardware infrastructure underlying these platforms is based on a decades-old design pattern. The rise of FaaS presents an opportunity to reimagine cloud infrastructure to be more energy-efficient, cost-effective, reliable, and secure. In this paper, we show how replacing handfuls of x86-based rack servers with hundreds of ARM-based single-board computers could lead to a virtualization-free, energy-proportional cloud that achieves this vision. We call our systematically-designed implementation MicroFaaS, and we conduct a thorough evaluation and cost analysis comparing MicroFaaS to a throughput-matched FaaS platform implemented in the style of conventional virtualization-based cloud systems. Our results show a 5.6x increase in energy efficiency and 34.2\% decrease in total cost of ownership compared to our baseline.},
	urldate = {2024-01-07},
	booktitle = {2022 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Byrne, Anthony and Pang, Yanni and Zou, Allen and Nadgowda, Shripad and Coskun, Ayse K.},
	month = mar,
	year = {2022},
	note = {Journal Abbreviation: 2022 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
	pages = {754--759},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UAIVG93E\\9774688.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VYDG96Q2\\9774688.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R6I6BBYV\\9774688.html:text/html},
}

@inproceedings{huang_mobility-aware_2022,
	title = {Mobility-aware {Seamless} {Virtual} {Function} {Migration} in {Deviceless} {Edge} {Computing} {Environments}},
	isbn = {2575-8411},
	url = {https://ieeexplore.ieee.org/document/9912171},
	doi = {10.1109/ICDCS54860.2022.00050},
	abstract = {Serverless Computing and Function-as-a-Service (FaaS) offer convenient and transparent services to developers and users. The deployment and resource allocation of services are managed by the cloud service providers. Meanwhile, the development of smart mobile devices and network technology enables the collection and transmission of a huge amount of data, which creates the mobile edge computing shifting tasks to the network edge for mobile users. In this paper, we propose a deviceless edge computing system targeting the mobility of end users. We focus on the migration of virtual functions to provide uninterrupted services to mobile users. We introduce the deviceless edge computing model and propose a seamless migration scheme of virtual functions with limited involvement of function developers. We formulate the migration decision problem into integer linear programming and use receding horizon control (RHC) for online solutions. We implement the migration system and algorithm to support delay-sensitive scenarios over real edge devices and develop a streaming game as the virtual function to test the performance. Extensive experiments in real scenarios exhibit the system has the ability to support high-mobility and delay-sensitive application scenarios. Extensive simulation results also show its applicability over large-scale networks.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 42nd {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Huang, Yaodong and Lin, Zelin and Yao, Tingting and Shang, Xiaojun and Cui, Laizhong and Huang, Joshua Zhexue},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)},
	pages = {447--457},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5KTP4HWN\\9912171.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VRGZDIQ3\\9912171.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QH4SLFXA\\9912171.html:text/html},
}

@article{perez_monitoring_2022,
	title = {Monitoring {Platform} {Evolution} {Toward} {Serverless} {Computing} for {5G} and {Beyond} {Systems}},
	volume = {19},
	issn = {1932-4537},
	url = {https://ieeexplore.ieee.org/document/9709528},
	doi = {10.1109/TNSM.2022.3150586},
	abstract = {Fifth generation (5G) and beyond systems require flexible and efficient monitoring platforms to guarantee optimal key performance indicators (KPIs) in various scenarios. Their applicability in Edge computing environments requires lightweight monitoring solutions. This work evaluates different candidate technologies to implement a monitoring platform for 5G and beyond systems in these environments. For monitoring data plane technologies, we evaluate different virtualization technologies, including bare metal servers, virtual machines, and orchestrated containers. We show that containers not only offer superior flexibility and deployment agility, but also allow obtaining better throughput and latency. In addition, we explore the suitability of the Function-as-a-Service (FaaS) serverless paradigm for deploying the functions used to manage the monitoring platform. This is motivated by the event oriented nature of those functions, designed to set up the monitoring infrastructure for newly created services. When the FaaS warm start mode is used, the platform gives users the perception of resources that are always available. When a cold start mode is used, containers running the application’s modules are automatically destroyed when the application is not in use. Our analysis compares both of them with the standard deployment of microservices. The experimental results show that the cold start mode produces a significant latency increase, along with potential instabilities. For this reason, its usage is not recommended despite the potential savings of computing resources. Conversely, when the warm start mode is used for executing configuration tasks of monitoring infrastructure, it can provide similar execution times to a microservice-based deployment. In addition, the FaaS approach significantly simplifies the code logic in comparison with microservices, reducing lines of code to less than 38\%, thus reducing development time. Thus, FaaS in warm start mode represents the best candidate technology to implements such management functions.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Perez, Ramon and Benedetti, Priscilla and Pergolesi, Matteo and Garcia-Reinoso, Jaime and Zabala, Aitor and Serrano, Pablo and Femminella, Mauro and Reali, Gianluca and Steenhaut, Kris and Banchs, Albert},
	month = jun,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	pages = {1489--1504},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\2F575I4C\\Perez et al. - 2022 - Monitoring Platform Evolution Toward Serverless Co.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EHZF653Y\\9709528.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7TNEB25L\\9709528.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YYXJS96R\\9709528.html:text/html},
}

@inproceedings{baresi_neptune_2022,
	title = {{NEPTUNE}: {Network}- and {GPU}-aware {Management} of {Serverless} {Functions} at the {Edge}},
	isbn = {2157-2321},
	shorttitle = {{NEPTUNE}},
	url = {https://ieeexplore.ieee.org/document/9799943},
	doi = {10.1145/3524844.3528051},
	abstract = {Nowadays a wide range of applications is constrained by low-latency requirements that cloud infrastructures cannot meet. Multi-access Edge Computing (MEC) has been proposed as the reference architecture for executing applications closer to users and reducing latency, but new challenges arise: edge nodes are resource-constrained, the workload can vary significantly since users are nomadic, and task complexity is increasing (e.g., machine learning inference). To overcome these problems, the paper presents NEPTUNE, a serverless-based framework for managing complex MEC solutions. NEPTUNE i) places functions on edge nodes according to user locations, ii) avoids the saturation of single nodes, iii) exploits GPUs when available, and iv) allocates resources (CPU cores) dynamically to meet foreseen execution times. A prototype, built on top of K3S, was used to evaluate NEPTUNE on a set of experiments that demonstrate a significant reduction in terms of response time, network overhead, and resource consumption compared to three well-known approaches. CCS CONCEPTS • Theory of computation → Scheduling algorithms; • Computing methodologies →Distributed computing methodologies; • Computer systems organization →Distributed architectures.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Symposium} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems} ({SEAMS})},
	author = {Baresi, Luciano and Hu, Davide Yi Xian and Quattrocchi, Giovanni and Terracciano, Luca},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)},
	pages = {144--155},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\NRLBKJ8Z\\Baresi et al. - 2022 - NEPTUNE Network- and GPU-aware Management of Serv.pdf:application/pdf;Full Text:C\:\\Users\\brian\\Zotero\\storage\\E4ELR5LR\\Baresi et al. - 2022 - NEPTUNE Network- and GPU-aware Management of Serv.pdf:application/pdf;Full Text:C\:\\Users\\brian\\Zotero\\storage\\7KINE3NV\\Baresi et al. - 2022 - NEPTUNE Network- and GPU-aware Management of Serv.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HJB5LNM4\\9799943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UWEJRZWE\\9799943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3TUA6EZM\\9799943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\59EMW9SH\\9799943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\L9CIBCYX\\9799943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H9DLWGZK\\9799943.html:text/html},
}

@inproceedings{li_joint_2022,
	title = {On the {Joint} {Optimization} of {Function} {Assignment} and {Communication} {Scheduling} toward {Performance} {Efficient} {Serverless} {Edge} {Computing}},
	isbn = {1548-615X},
	url = {https://ieeexplore.ieee.org/document/9812887},
	doi = {10.1109/IWQoS54832.2022.9812887},
	abstract = {Serverless edge computing is booming as an efficient carrier of deploying complex applications composed of dependent functions, whose assignment decisions highly influence the application performance. Although similar problem has been widely studied, none of existing approaches considers the diversity of communication styles, which is specially introduced in serverless computing and also imposes high influence to the performance efficiency. We compare two communication styles, called direct-passing and remote-storage, to transmit intermediate data between functions. We find that there is no single communication style that can prevail under all scenarios and the optimal selection depends on several factors, such as fanout degree, data size, and network bandwidth. Hence, how to select the appropriate communication style for each inter-function communication link, together with the function assignment decision, is essential to the application performance. To this end, we propose a Priority-based ASsignment and Selection (PASS) algorithm with joint consideration of function assignment and communication style selection. We theoretically analyze the approximation ratio of PASS algorithm and extensive experiments on real-world applications show that PASS can averagely reduce the completion time by 24.1\% in comparison with state-of-the-art approaches.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 30th {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	author = {Li, Yuepeng and Zeng, Deze and Gu, Lin and Wang, Kun and Guo, Song},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YLCHW3ML\\9812887.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NB3S4F7A\\9812887.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6DIJMB8I\\9812887.html:text/html},
}

@inproceedings{alhindi_power_2022,
	title = {On the {Power} {Consumption} of {Serverless} {Functions}: {An} {Evaluation} of {OpenFaaS}},
	shorttitle = {On the {Power} {Consumption} of {Serverless} {Functions}},
	url = {https://ieeexplore.ieee.org/document/10061779},
	doi = {10.1109/UCC56403.2022.00064},
	abstract = {The rapid growth in cloud-based technologies has introduced the need for very large data centres to meet the increasing demand for cloud services. One of the main challenges in managing these data centres is the sharp increase of power consumption. Research has therefore tackled the issue of power/energy efficiency in cloud data centres. Serverless computing is a cloud computing execution model that gives software developers the option to deploy their code without the need to configure servers, operating systems or runtime libraries, thus allowing them to invest less effort and capital in infrastructure management. This paper investigates whether serverless computing has the ability to support power efficiency. To this aim, a number of experiments are conducted to compare the power consumption of a serverless platform, OpenFaaS, against Docker containers with the consideration of applications and benchmarks. The experimental results show that OpenFaaS is more power efficient than Docker when the processor and memory are under stress.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 15th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Alhindi, Abdulaziz and Djemame, Karim and Heravan, Fatemeh Banaie},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)},
	pages = {366--371},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\DL6N8QYN\\Alhindi et al. - 2022 - On the Power Consumption of Serverless Functions .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EPKTUBKV\\10061779.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ECGDIGW8\\10061779.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\546BS5Q2\\10061779.html:text/html},
}

@inproceedings{sicari_openwolf_2022,
	title = {{OpenWolf}: {A} {Serverless} {Workflow} {Engine} for {Native} {Cloud}-{Edge} {Continuum}},
	shorttitle = {{OpenWolf}},
	url = {https://ieeexplore.ieee.org/document/9927926},
	doi = {10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927926},
	abstract = {Nowadays, Serverless computing is emerging as one of the most used Cloud services. In particular, the Function as Service (FaaS) is bringing to Cloud consumers, developers, and devops many advantages in terms of service costs, speed of development, and ease of deployment. In fact, it stands to be a key technology for enabling the Cloud-Edge Continuum. Regardless of these features, it is still not possible to build FaaS native applications without a Cloud broker that coordinates the functions. Therefore, FaaS usage is limited to very simple and specific jobs. In this work, we brush up on the concept of Scientific Workflow using the FaaS paradigm, in order to realize full Native Serverless Workflows-based applications. We define a custom Workflow Manifest DSL used to describe function interactions, then we describe the implementation of an agent able to deploy architecture-independent functions and coordinate them according to the Manifest. Finally, federating the Cloud-Fog-Edge tiers in a single Continuum environment, we allow functions to take advantage of the Continuum tier’s characteristics where they are deployed. This project is called OpenWolf, it’s repository is published on GitHub, under GNU General Public License v3.0.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {Intl} {Conf} on {Dependable}, {Autonomic} and {Secure} {Computing}, {Intl} {Conf} on {Pervasive} {Intelligence} and {Computing}, {Intl} {Conf} on {Cloud} and {Big} {Data} {Computing}, {Intl} {Conf} on {Cyber} {Science} and {Technology} {Congress} ({DASC}/{PiCom}/{CBDCom}/{CyberSciTech})},
	author = {Sicari, Christian and Carnevale, Lorenzo and Galletta, Antonino and Villari, Massimo},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XSCHE83C\\9927926.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2BHPRHQL\\9927926.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EEBIDF3E\\9927926.html:text/html},
}

@inproceedings{nanda_performance_2022,
	title = {Performance {Analysis} of {Geospatial} {Serverless} {Computing} for {Geospatial} {Big} {Data} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9885470},
	doi = {10.1109/ICESC54411.2022.9885470},
	abstract = {Nowadays, real-world applications are primarily concerned with collecting the physical characteristics that correspond to various geographical phenomena, and information plays a critical role in the analysis and forecast of various occurrences that occur. Real-time analysis is used in a variety of application domains, including traffic flow monitoring, healthcare monitoring, and so on. This research study has considered the existing geospatial serverless computing framework and proposed an analytical model to enable the geospatial serverless platform to compute and operate at different workloads and the tradeoff between the cost and performance are then determined by the users’ preferences. Here, the accuracy of the model is validated in Amazon Web Service (AWS) Lambda environment and the model calculations are shown based on the performance metrics by including average response time, probability of cold start as well as the average amount of functional instances in the steady state by using the queuing theory. Here, the performance modelling is carried out in the geospatial serverless environment for different workloads, which results in delivering better performance at a minimal cost.},
	urldate = {2024-01-07},
	booktitle = {2022 3rd {International} {Conference} on {Electronics} and {Sustainable} {Communication} {Systems} ({ICESC})},
	author = {Nanda, Sandeep and Ranjan Mishra, Manoj and Narayan Brahma, Aditya and Chandra Swain, Sarat and Shekhar Patra, Sudhansu and Kumar Barik, Rabindra},
	month = aug,
	year = {2022},
	note = {Journal Abbreviation: 2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)},
	pages = {716--720},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D548EN89\\9885470.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2JUWFWTX\\9885470.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IEX25R5D\\9885470.html:text/html},
}

@inproceedings{trieu_performance_2022,
	title = {Performance {Evaluation} of {Serverless} {Edge} {Computing} for {Machine} {Learning} {Applications}},
	url = {https://ieeexplore.ieee.org/document/10061771},
	doi = {10.1109/UCC56403.2022.00025},
	abstract = {Next generation technologies such as smart health-care, self-driving cars, and smart cities require new approaches to deal with the network traffic generated by the Internet of Things (IoT) devices, as well as efficient programming models to deploy machine learning techniques. Serverless edge computing is an emerging computing paradigm from the integration of two recent technologies, edge computing and serverless computing, that can possibly address these challenges. However, there is little work to explore the capability and performance of such a technology. In this paper, a comprehensive performance analysis of a serverless edge computing system using popular open-source frameworks, namely, Kubeless, OpenFaaS, Fission, and funcX is presented. The experiments considered different programming languages, workloads, and the number of concurrent users. The machine learning workloads have been used to evaluate the performance of the system under different working conditions to provide insights into the best practices. The evaluation results revealed some of the current challenges in serverless edge computing and open research opportunities in this emerging technology for machine learning applications.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 15th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Trieu, Quoc Lap and Javadi, Bahman and Basilakis, Jim and Toosi, Adel N.},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)},
	pages = {139--144},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\33TKM2NV\\10061771.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9VLQ2D4H\\10061771.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PRFVBZMH\\10061771.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\9JRRYIAR\\Trieu et al. - 2022 - Performance Evaluation of Serverless Edge Computin.pdf:application/pdf},
}

@article{mahmoudi_performance_2022,
	title = {Performance {Modeling} of {Serverless} {Computing} {Platforms}},
	volume = {10},
	issn = {2168-7161},
	url = {https://ieeexplore.ieee.org/document/9238484},
	doi = {10.1109/TCC.2020.3033373},
	abstract = {Analytical performance models have been leveraged extensively to analyze and improve the performance and cost of various cloud computing services. However, in the case of serverless computing, which is projected to be the dominant form of cloud computing in the future, we have not seen analytical performance models to help with the analysis and optimization of such platforms. In this work, we propose an analytical performance model that captures the unique details of serverless computing platforms. The model can be leveraged to improve the quality of service and resource utilization and reduce the operational cost of serverless platforms. Also, the proposed performance model provides a framework that enables serverless platforms to become workload-aware and operate differently for different workloads to provide a better trade-off between the cost and performance depending on the user's preferences. The current serverless offerings require the user to have extensive knowledge of the internals of the platform to perform efficient deployments. Using the proposed analytical model, the provider can simplify the deployment process by calculating the performance metrics for users even before physical deployments. We validate the applicability and accuracy of the proposed model by extensive experimentation on AWS Lambda. We show that the proposed model can calculate essential performance metrics such as average response time, probability of cold start, and the average number of function instances in the steady-state. Also, we show how the performance model can be used to tune the serverless platform for each workload, which will result in better performance or lower cost without scarifying the other. The presented model assumes no non-realistic restrictions, so that it offers a high degree of fidelity while maintaining tractability at large scale.},
	number = {4},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Mahmoudi, Nima and Khazaei, Hamzeh},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Cloud Computing},
	pages = {2834--2847},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2C4ATTWG\\9238484.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XMVUR5AH\\9238484.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JXFEM82H\\9238484.html:text/html},
}

@inproceedings{yao_performance_2022,
	title = {Performance {Optimization} in {Serverless} {Edge} {Computing} {Environment} using {DRL}-{Based} {Function} {Offloading}},
	url = {https://ieeexplore.ieee.org/document/9776166},
	doi = {10.1109/CSCWD54268.2022.9776166},
	abstract = {Serverless computing/Function as a Service (FaaS) has emerged as a new paradigm for running short-lived applications in the cloud. Serverless edge computing is recently adopting serverless computing at edge to run event-driven tasks and supporting application running of resource-constrained Internet of Things (IoT) devices by offloading their tasks to the edge. However, traditional task offloading methods are mainly based on heuristic algorithms for one-shot optimization, which leads to performance degradation in long-term operation. Fortunately, deep reinforcement learning techniques combining reinforcement learning and deep neural networks provide a promising alternative. Therefore, a function offloading algorithm DRLFO is proposed with a deep reinforcement learning algorithm based on actor-critic framework in this paper. The function offloading process in serverless edge computing environment is modeled as a Markov Decision Process. Finally, the experimental results show that the proposed algorithm can successfully converge and outperform the compared baseline algorithm in terms of function success rate and reduce the average latency by 4.6\%-22.6\%.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 25th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	author = {Yao, Xuyi and Chen, Ningjiang and Yuan, Xuemei and Ou, Pingjie},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
	pages = {1390--1395},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EIQ3Z4M4\\9776166.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HMACKG2C\\9776166.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8NAMDFQG\\9776166.html:text/html},
}

@inproceedings{balla_predictable_2022,
	title = {Predictable {Open} {Source} {FaaS} {Function} {Chains}},
	isbn = {2771-5663},
	url = {https://ieeexplore.ieee.org/document/9978820},
	doi = {10.1109/CloudNet55617.2022.9978820},
	abstract = {Function as a Service (FaaS) provides a way to execute modular pieces of code in cloud environments. FaaS also supports the implementation of distributed cloud applications by deploying function chains.In this paper, we introduce a simulation framework for function chains using asynchronous invocations. By using our simulator the completion time distribution of the individual function invocations can be estimated as well as the completion time of the distribution of the whole function chain.We evaluate the results of our simulation framework by using compute-intensive, Python3 based function chains that implement image transforming operations. We also propose cost decreasing methods for function chains with asynchronous invocations.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 11th {International} {Conference} on {Cloud} {Networking} ({CloudNet})},
	author = {Balla, David and Maliosz, Markosz and Simon, Csaba},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 11th International Conference on Cloud Networking (CloudNet)},
	pages = {186--194},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A2Y38ERS\\9978820.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IVJIJI8D\\9978820.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KUUHT77B\\9978820.html:text/html},
}

@inproceedings{martinez_predictive_2022,
	title = {Predictive function placement for distributed serverless environments},
	isbn = {2472-8144},
	url = {https://ieeexplore.ieee.org/document/9758140},
	doi = {10.1109/ICIN53892.2022.9758140},
	abstract = {Serverless computing has gained popularity in recent years, and it is being used in an increasing number of use cases. It is still limited by some of its challenges, such as high latency, statelessness, vendor lock-in or difficulty to test. However, it presents advantages that could open new opportunities for more applications, such as event-driven IoT or mobile applications that could benefit from the serverless elasticity and resilience and reduce operating costs. Networks are becoming more geographically distributed and adopting a hybrid infrastructure of cloud and edge nodes, for example on 5G base stations. If the latency is high, a wide range of mobile applications cannot make use of serverless computing. In this paper, we propose an approach to reduce the end-to-end latency perceived by an application using serverless computing by predicting the resource utilization of the available nodes to decide the location to deploy the serverless function instances. In this first approach, we apply a Kalman filter to predict the CPU load of each of the nodes. The results of the experiments with two serverless nodes show a latency reduction that increases as the requested computation becomes more complex, reaching a 17\% reduction compared to resource-based load balancer with direct measurement.},
	urldate = {2024-01-07},
	booktitle = {2022 25th {Conference} on {Innovation} in {Clouds}, {Internet} and {Networks} ({ICIN})},
	author = {Martinez, Maria Mora and Pandey, Sanjeet Raj},
	month = mar,
	year = {2022},
	note = {Journal Abbreviation: 2022 25th Conference on Innovation in Clouds, Internet and Networks (ICIN)},
	pages = {86--90},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QYYQZZGE\\9758140.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HQ9DLB76\\9758140.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\57WN2PW9\\9758140.html:text/html},
}

@inproceedings{dennis_proprov_2022,
	title = {{ProProv}: {A} {Language} and {Graphical} {Tool} for {Specifying} {Data} {Provenance} {Policies}},
	shorttitle = {{ProProv}},
	url = {https://ieeexplore.ieee.org/document/10063380},
	doi = {10.1109/TPS-ISA56441.2022.00040},
	abstract = {The Function-as-a-Service cloud computing paradigm has made large-scale application development convenient and efficient as developers no longer need to deploy or manage the necessary infrastructure themselves. However, as a consequence of this abstraction, developers lose insight into how their code is executed and data is processed. Cloud providers currently offer little to no assurance of the integrity of customer data. One approach to robust data integrity verification is the analysis of data provenance—logs that describe the causal history of data, applications, users, and non-person entities. This paper introduces ProProv, a new domain-specific language and graphical user interface for specifying policies over provenance metadata to automate provenance analyses.To evaluate the convenience and usability of the new ProProv interface, 61 individuals were recruited to construct provenance policies using both ProProv and the popular, general-purpose policy specification language Rego—used as a baseline for comparison. We found that, compared to Rego, the ProProv interface greatly increased the number of policies successfully constructed, improved the time taken to construct those policies, and reduced the failed-attempt rate. Participants successfully constructed 73\% of the requested policies using ProProv, compared to 41\% using Rego. To further evaluate the usability of the tools, participants were given a 10-question questionnaire measured using the System Usability Scale (SUS). The median SUS score for the graphical ProProv interface was above average and fell into the “excellent” category, compared to below average and “OK” for Rego. These results highlight the impacts that graphical domain-specific tools can have on the accuracy and speed of policy construction.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 4th {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems}, and {Applications} ({TPS}-{ISA})},
	author = {Dennis, Kevin and Engram, Shamaria and Kaczmarek, Tyler and Ligatti, Jay},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)},
	pages = {266--275},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3RHSDMGP\\10063380.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NV2ZV2XF\\10063380.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WTYEPTX2\\10063380.html:text/html},
}

@inproceedings{cicconetti_qkd-secure_2022,
	title = {{QKD}-{Secure} {ETSI} {MEC}},
	url = {https://ieeexplore.ieee.org/document/9882872},
	doi = {10.1109/WOLTE55422.2022.9882872},
	abstract = {Edge computing is a leading trend in the deployment of computation and networking infrastructures, where processing is done closer to the users, compared to more traditional cloud-based solutions relying on remote data centers. On the other hand, Quantum Key Distribution (QKD) technologies, offering unconditional security between the communicating parties, is expected to become affordable in the near future and it will unlock new opportunities, e.g., for applications with long-term confidentiality requirements. In this paper we study how to enable ETSI ISG QKD key delivery within an ETSI MEC edge computing scenario, in the practical use case of cloud-native applications based on Function-as-a-Service (FaaS). This work is part of the activities of the PON project “Development of quantum systems and technologies for IT security in communication networks” (QUANCOM) which aims to the realization of a metropolitan quantum communication network through the collaboration between universities, research centers and companies operating in the communication market area.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {Workshop} on {Low} {Temperature} {Electronics} ({WOLTE})},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th Workshop on Low Temperature Electronics (WOLTE)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PGT46YQ9\\9882872.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HAAX5F7J\\9882872.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4PMB8MYY\\9882872.html:text/html},
}

@inproceedings{sheshadri_qos_2022,
	title = {{QoS} aware {FaaS} for {Heterogeneous} {Edge}-{Cloud} continuum},
	volume = {2022-July},
	isbn = {2159-6190},
	url = {https://ieeexplore.ieee.org/document/9860835},
	doi = {10.1109/CLOUD55607.2022.00023},
	abstract = {Function as a Service (FaaS) is one of the widely used serverless computing service offerings to build and deploy applications on the Cloud. The platform is popular for its "pay-as-you-go" billing model, microservice-based design, event-driven executions, and autonomous scaling. Although it has its firm roots in Cloud computing service offerings, it is considerably explored in the Edge computing layer. The efficient resource management of FaaS is attractive to Edge computing because of the limited nature of resources. Existing literature on Edge-Cloud FaaS platforms orchestrates compute workloads based on factors such as data locality, resource availability, network costs, and bandwidth. However, the state-of-the-art platforms lack a comprehensive way to address the challenges of managing heterogeneous resources in the FaaS platform. The resource specification in a heterogeneous setting, lack of Quality of Service (QoS) driven resource provisioning, and function deployment exacerbate the problem of resource selection, and function deployment in FaaS platforms with a heterogeneous resource pool. To address these gaps, the current work presents a novel heterogeneous FaaS platform that deduces function resource specification using Machine Learning (ML) methods, performs smart function placement on Edge/Cloud based on a user-specified QoS requirement, and exploit data locality by caching appropriate data for function executions. Experimental results based on real-world workloads on a video surveillance application show that the proposed platform brings efficient resource utilization and cost savings at the Cloud by reducing the resource usage by up to 30\%, while improving the performance of function executions by up to 25\% at Edge and Cloud.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Sheshadri, K R and Lakshmi, J},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	keywords = {serverless computing, Serverless computing, FaaS, Function as a Service, Function as a service, Edge computing, Quality of service, Service platforms, Cloud platforms, Edge Cloud, Edge cloud continuum, Edge Cloud continuum, Edge Cloud FaaS, Edge cloud function as a service, Edge clouds, Heterogeneous edge cloud platform, Heterogeneous Edge Cloud platforms, Heterogeneous FaaS platforms, Heterogeneous function as a service platform, QoS, QoS aware FaaS for Heterogeneous Edge Cloud continuum, Quality of Service, Quality of service aware function as a service for heterogeneous edge cloud continuum, Quality-of-service, Security systems, Service-aware, Specifications},
	pages = {70--80},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\N2FAY2YP\\9860835.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JGJW2E6N\\9860835.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TWFGCVFL\\9860835.html:text/html},
}

@inproceedings{n_real-time_2022,
	title = {Real-{Time} {Object} {Detection} with {Tensorflow} {Model} {Using} {Edge} {Computing} {Architecture}},
	url = {https://ieeexplore.ieee.org/document/9782169},
	doi = {10.1109/ICSSS54381.2022.9782169},
	abstract = {This paper presents the capturing of objects using Wi-Fi enabled modular esp32 camera and processes the captured stream of data using machine learning and computer vision techniques, then sends the processed data to the cloud, there are major cloud providers in the market who occupied more than 80\% of the global public market the cloud providers are Google Cloud, Amazon AWS, Microsoft Azure. Google Cloud Platform (GCP) is been our primary choice because of its good documentation availability, The Cloud IoT-Core Gateway, as well as a serverless cloud layer to store all of the data. The cloud functions help to trigger the notifications to the users when the cameras detect what we have trained the model. The Edge computing project uses an ESP32 With cameras as a device listener and a raspberry pi as an edge server which has an image classifier model trained with TensorFlow.},
	urldate = {2024-01-07},
	booktitle = {2022 8th {International} {Conference} on {Smart} {Structures} and {Systems} ({ICSSS})},
	author = {N, Mahiban Lindsay and Rao, Alla Eswara and Kalyan, Madaka Pavan},
	month = apr,
	year = {2022},
	note = {Journal Abbreviation: 2022 8th International Conference on Smart Structures and Systems (ICSSS)},
	pages = {01--04},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8NLUAIYD\\9782169.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I4LQE2K4\\9782169.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9MYZ6SGQ\\9782169.html:text/html},
}

@inproceedings{birillo_reflekt_2022,
	title = {Reflekt: a {Library} for {Compile}-{Time} {Reflection} in {Kotlin}},
	shorttitle = {Reflekt},
	url = {https://ieeexplore.ieee.org/document/9793932},
	doi = {10.1145/3510457.3513053},
	abstract = {Reflection in Kotlin is a powerful mechanism to introspect program behavior during its execution at run-time. However, among the variety of practical tasks involving reflection, there are scenarios when the poor performance of run-time approaches becomes a significant disadvantage. This problem manifests itself in Kotless, a popular framework for developing serverless applications, because the faster the applications launch, the less their cloud infrastructure costs. In this paper, we present Reflekt - a compile-time reflection library which allows to perform the search among classes, object expressions (which in Kotlin are implemented as singleton classes), and functions in Kotlin code based on the given search query. It comes with a convenient DSL and better performance comparing to the existing run-time reflection approaches. Our experiments show that replacing run-time reflection calls with Reflekt in serverless applications created with Kotless resulted in a significant performance boost in start-up time of these applications.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Birillo, Anastasiia and Lyulina, Elena and Malysheva, Maria and Tankov, Vladislav and Bryksin, Timofey},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
	pages = {231--240},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R6Z95T6X\\9793932.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\32EGUI9G\\9793932.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YGJC5RMV\\9793932.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\XLH4SQRE\\Birillo et al. - 2022 - Reflekt a Library for Compile-Time Reflection in .pdf:application/pdf},
}

@inproceedings{nikolov_repository_2022,
	title = {Repository {Platform} for {RESTful} {Web} {Services}},
	url = {https://ieeexplore.ieee.org/document/9960066},
	doi = {10.1109/ICAI55857.2022.9960066},
	abstract = {With the growing usage of web services across the globe, demand for advanced repositories has emerged that manage web service discovery and facilitate their use by different domains. The rapid development of mobile applications and the adoption of Internet of Things technologies and serverless computing have led to the popularity of the Representational State Transfer (REST) architecture, a preferable approach. Due to their lightweight nature, the RESTful web services area is preferable over Simple Object Access Protocol (SOAP) web services for the development of distributed applications. Thus, new approaches for the design and development of web service repositories are needed to overcome the limitations of the current web service discovery mechanisms and consider the advanced RESTful technologies. This paper addresses such a need by proposing a repository platform for RESTful web services. It supports the validation of web service definitions against OpenAPI specification, the generation of client SDKs and server stubs, the invocation of web service operations for testing purposes and the assessment of web service availability, customer support level and rating. The architecture, functionality and validation of the platform are presented, giving insight into its features, and proving its feasibility.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Conference} {Automatics} and {Informatics} ({ICAI})},
	author = {Nikolov, Alexander and Petrova-Antonova, Dessislava},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Conference Automatics and Informatics (ICAI)},
	pages = {292--297},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8D2G6R8B\\9960066.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\25CYBFAF\\9960066.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R29KJSRN\\9960066.html:text/html},
}

@inproceedings{manner_resource_2022,
	title = {Resource {Scaling} {Strategies} for {Open}-{Source} {FaaS} {Platforms} compared to {Commercial} {Cloud} {Offerings}},
	isbn = {2159-6190},
	url = {https://ieeexplore.ieee.org/document/9860370},
	doi = {10.1109/CLOUD55607.2022.00020},
	abstract = {Open-source offerings are often investigated when comparing their features to commercial cloud offerings. However, performance benchmarking is rarely executed for open-source tools hosted on-premise nor is it possible to conduct a fair cost comparison due to a lack of resource settings equivalent to cloud scaling strategies.Therefore, we firstly list implemented resource scaling strategies for public and open-source FaaS platforms. Based on this we propose a methodology to calculate an abstract performance measure to compare two platforms with each other. Since all open-source platforms suggest a Kubernetes deployment, we use this measure for a configuration of open-source FaaS platforms based on Kubernetes limits. We tested our approach with CPU intensive functions, considering the difference between single-threaded and multi-threaded functions to avoid wasting resources. With regard to this, we also address the noisy neighbor problem for open-source FaaS platforms by conducting an instance parallelization experiment. Our approach to limit resources leads to consistent results while avoiding an overbooking of resources.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Manner, Johannes and Wirtz, Guido},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {40--48},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RNV7W2N5\\9860370.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D9BJP7F9\\9860370.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NAMAE7CP\\9860370.html:text/html},
}

@inproceedings{souza_sapparchi_2022,
	title = {{SAPPARCHI}: an {Osmotic} {Platform} to {Execute} {Scalable} {Applications} on {Smart} {City} {Environments}},
	isbn = {2159-6190},
	shorttitle = {{SAPPARCHI}},
	url = {https://ieeexplore.ieee.org/document/9860412},
	doi = {10.1109/CLOUD55607.2022.00051},
	abstract = {In the Smart Cities context, a plethora of Middle-ware Platforms had been proposed to support applications execution and data processing. Despite all the progress already made, the vast majority of solutions have not met the requirements of Applications’ Runtime, Development, and Deployment when related to Scalability. Some studies point out that just 1 of 97 (1\%) reported platforms reach this all this set of requirements at same time. This small number of platforms may be explained by some reasons: i) Big Data: The huge amount of processed and stored data with various data sources and data types, ii) Multi-domains: many domains involved (Economy, Traffic, Health, Security, Agronomy, etc.), iii) Multiple processing methods like Data Flow, Batch Processing, Services, and Microservices, and 4) High Distributed Degree: The use of multiple IoT and BigData tools combined with execution at various computational levels (Edge, Fog, Cloud) leads applications to present a high level of distribution. Aware of those great challenges, we propose Sapparchi, an integrated architectural model for Smart Cities applications that defines multi-processing levels (Edge, Fog, and Cloud). Also, it presents the Sapparchi middleware platform for developing, deploying, and running applications in the smart city environment with an osmotic multi-processing approach that scales applications from Cloud to Edge. Finally, an experimental evaluation exposes the main advantages of adopting Sapparchi.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Souza, Arthur and Cacho, Nélio and Batista, Thais and Ranjan, Rajiv},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {289--298},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FZGMVDUM\\9860412.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NGK7M933\\9860412.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TKJE2J22\\9860412.html:text/html},
}

@inproceedings{yu_scorpius_2022,
	title = {Scorpius: {Proactive} {Code} {Preparation} to {Accelerate} {Function} {Startup}},
	isbn = {1548-615X},
	shorttitle = {Scorpius},
	url = {https://ieeexplore.ieee.org/document/9812868},
	doi = {10.1109/IWQoS54832.2022.9812868},
	abstract = {Massive enterprises deploy their applications on public clouds to relieve infrastructure management burden. However, applications are faced with highly fluctuating workloads, while clouds provision exclusive resources at coarse time granularity, resulting in severely low resource efficiency. Function-as-a-Service (FaaS) platform enables fine-grained resource multiplexing, which has the potential to improve efficiency. However, FaaS platforms could consume several seconds to start functions and the long startup latency can severely hurt the performance of applications. In this paper, we measure the FaaS platforms and find that most startup latency is occupied by code preparation. To reduce the code preparation latency with little resource overhead, we propose Scorpius, a FaaS platform that proactively prepares code based on the historical data of functions. It combines two optimization categories: (1) To reduce the code size, Scorpius proposes to proactively prepare partial libraries over servers and run functions on the server with most library sharing. (2) To advance the start time, Scorpius proposes to predict the function overload with a simple model and proactively scale code to more servers. We have implemented a prototype of Scorpius and conducted extensive experiments. Evaluation results demonstrate that compared with state-of-the-art methods, Scorpius can reduce the code preparation latency by 87.6\% with only 9.3\% storage overhead.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 30th {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	author = {Yu, Heng and Shen, Junxian and Zhang, Han and Wang, Jilong and Miao, Congcong and Xu, Mingwei},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)},
	pages = {1--10},
}

@inproceedings{fakinos_sequence_2022,
	title = {Sequence {Clock}: {A} {Dynamic} {Resource} {Orchestrator} for {Serverless} {Architectures}},
	isbn = {2159-6190},
	shorttitle = {Sequence {Clock}},
	url = {https://ieeexplore.ieee.org/document/9860832},
	doi = {10.1109/CLOUD55607.2022.00024},
	abstract = {Function-as-a-service (FaaS) represents the next frontier in the evolution of cloud computing being an emerging paradigm that removes the burden of configuration and management issues from users. This is achieved by replacing the well-established monolithic approach with graphs of standalone, small, stateless, event-driven components called functions. At the same time, from the cloud providers’ perspective, problems such as availability, load balancing and scalability need to be resolved without being aware of the functionality, behavior or resource requirements of their tenants’ code. However, in this context, functions’ containers coexist with others inside a host of finite resources, where a passive resource allocation technique does not guarantee a well-defined quality of service (QoS) in regards to time latency. In this paper, we present Sequence Clock, an expandable latency targeting tool that actively monitors serverless invocations in a cluster and offers execution of a sequential chain of functions, also known as pipelines or sequences, while achieving the targeted time latency. Two regulation methods were utilized, with one of them achieving up to 82\% decrease in the severity of time violations and in some cases even eliminating them completely.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Fakinos, Ioannis and Tzenetopoulos, Achilleas and Masouros, Dimosthenis and Xydis, Sotirios and Soudris, Dimitrios},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {81--90},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\578PSKXE\\9860832.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GIILEPG8\\9860832.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R9UY8JYV\\9860832.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\P27L2UQ5\\Fakinos et al. - 2022 - Sequence Clock A Dynamic Resource Orchestrator fo.pdf:application/pdf},
}

@inproceedings{b_serverless_2022,
	title = {Serverless {Blockchain}-based {AI}-{Powered} {Financial} {Transaction} {Management} {System} on {Cloud}},
	url = {https://ieeexplore.ieee.org/document/10100227},
	doi = {10.1109/ICSTCEE56972.2022.10100227},
	abstract = {The Finance Manager Application is used to automate the existing finance tracking system and to offer the user to have a more engaging and aesthetic user experience with the bill tracking feature. The existing systems are used to record several financial transactions and events. They are not giving an overview of the amount spent and in which category the user has transacted more amount and hence it does not support expense management. Mainly the existing finance tracking systems are not using AI-based authentication which is more convenient for the user. Our web application is proposed to give transaction charts and graphs that give more clarity of the income, expenses, and transfers. In addition, we can analyse the transactions in category-wise, and we can track the history. Along with form-data authentication, we are providing OAuth and Zero-effort authentication that is more user friendly. Our SPA provides the facility to add multiple transactions at a time and it is to show the balance in the wallet on the future according to the planned transactions. Also, it can automate the data entry for recurrent transactions.},
	urldate = {2024-01-07},
	booktitle = {2022 {Third} {International} {Conference} on {Smart} {Technologies} in {Computing}, {Electrical} and {Electronics} ({ICSTCEE})},
	author = {B, Sri Sathya K and A, Nithyasri and E, Amirtha and S, Fowjiya},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 Third International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\E3CNDE2R\\10100227.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9HED2PM5\\10100227.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XATKE7SV\\10100227.html:text/html},
}

@inproceedings{hluchy_serverless_2022,
	title = {Serverless {Computing} and {FaaS} for {Airport} {Meteorology}},
	url = {https://ieeexplore.ieee.org/document/9925943},
	doi = {10.1109/KI55792.2022.9925943},
	abstract = {Serverless computing and Function-as-a-Service are programming paradigms that have many advantages for modern, distributed and highly modular applications. However, the situation for older applications with more monolithic architectures is more complex. It may be questionable whether the obvious advantages received from their porting into cloud and FaaS can outweigh the effort and resources spent on the transformation itself. In this paper, we present our initial effort aimed at transforming one such application, successfully being used for several years for visibility determination at airports - a significant component of air traffic safety. We have chosen to modularize the application, divide it into parts that can be implemented as functions in the FaaS paradigm, and implement simple cloud-based data management. The tools that we are using in the initial stage are OpenWhisk for FaaS and Airflow for workflow management (events production for OpenWhisk, which has an event-driven programming model), while we are also exploring the application of OpenFaaS and OSCAR. The work is still in initial stages and only a small part of the application has been implemented as OpenWhisk actions (FaaS functions) so far.},
	urldate = {2024-01-07},
	booktitle = {2022 {Cybernetics} \& {Informatics} ({K}\&{I})},
	author = {Hluchý, Ladislav and Habala, Ondrej and Bobák, Martin and Tran, Viet and Ivica, Lukáš},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 Cybernetics \& Informatics (K\&I)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CE8AA3FJ\\9925943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BRRZPJF5\\9925943.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5GPIW8QH\\9925943.html:text/html},
}

@inproceedings{bac_serverless_2022,
	title = {Serverless {Computing} {Approach} for {Deploying} {Machine} {Learning} {Applications} in {Edge} {Layer}},
	isbn = {1976-7684},
	url = {https://ieeexplore.ieee.org/document/9687209},
	doi = {10.1109/ICOIN53446.2022.9687209},
	abstract = {Serverless computing-a stateless cloud computing model, is an emerging solution that has shown significant benefits to efficiency and cost for event-driven applications in the cloud environment, including artificial intelligence (AI), machine learning applications. With serverless computing, the machine learning system’s complexity is minimized, flexible and straightforward in management. However, operating and managing serverless machine learning services on clouds faces many limitations such as latency and data privacy. Local distributed edge computing nodes which are closed to users can address these challenges of cloud-serverless AI applications. Based on this motivation, in this paper, we propose an architecture for deploying machine learning workload as serverless functions in the edge environment. We illustrate our proposed approach and evaluate its performance and effectiveness by exploiting a holistic end-to-end image classifier, a famous machine learning use case in the MNIST dataset. Our proof of concept provides comprehensive assessments that prove its effectiveness in latency reduction and distributed machine learning deployment.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Conference} on {Information} {Networking} ({ICOIN})},
	author = {Bac, Ta Phuong and Tran, Minh Ngoc and Kim, YoungHan},
	month = jan,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Conference on Information Networking (ICOIN)},
	pages = {396--401},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\V4YB39G9\\9687209.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2HAZAPLH\\9687209.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y4XRHMLV\\9687209.html:text/html},
}

@inproceedings{mora_serverless_2022,
	title = {Serverless {Computing} at the {Edge} for {AIoT} {Applications}},
	url = {https://ieeexplore.ieee.org/document/10121879},
	doi = {10.1109/ICAIoT57170.2022.10121879},
	abstract = {Serverless computing is a new trend for developing Cloud hosted applications. This paradigm takes advantage of the scalability and flexibility of the management of infrastructure from the Cloud provider to offer advantages in cost and maintenance of Cloud applications. Although it was originally designed for Cloud deployments, some proposals have been made to give its benefits closer where the data is generated and provide service to IoT applications. However, some important challenges remain in order to provide a suitable solution for all cases. This work proposes a computational model for designing a serverless architecture at the edge side of the network. The model is especially focused on Artificial Intelligence (AI) functions of IoT applications, where serverless could achieve its full potential. The results show that the proposal allows to overcome the main drawbacks raised while maintains the cloud processing support.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Conference} on {Artificial} {Intelligence} of {Things} ({ICAIoT})},
	author = {Mora, H. and Mora-Gimeno, F.J. and Jimeno-Morenilla, A. and Macia-Lillo, A. and Elouali, A.},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Conference on Artificial Intelligence of Things (ICAIoT)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BQ28R4P5\\10121879.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XJ5YU43U\\10121879.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UG4T6F7S\\10121879.html:text/html},
}

@inproceedings{mileski_serverless_2022,
	title = {Serverless {Implementations} of {Real}-time {Embarrassingly} {Parallel} {Problems}},
	url = {https://ieeexplore.ieee.org/document/9983710},
	doi = {10.1109/℡FOR56187.2022.9983710},
	abstract = {In this paper, we conduct experiments to deploy a scalable serverless computing solution for real-time monitoring of thousands of patients with streaming electrocardiograms as an example of embarrassingly parallel tasks originally executed on two virtual machines. The research question is to find the speedup of such solution versus classical virtual machine approaches with sequential or parallel threads.The challenge of migrating an existing service to a serverless solution is to adapt and reconfigure the code for serverless platform, to write the code to invoke the service in parallel and asynchronously, and to use other services in the cloud that are needed for the whole solution to be functional and scalable. Evaluation of developing various solutions matching migration challenges to Google Cloud Run, Google Cloud Compute Engine, and Google Cloud Storage (customization of code, the configuration of services) shows that greater speedups can be achieved by dividing the Embarrassingly Parallel tasks into sub-tasks executed as a serverless service. We achieved highest speedup of almost 40 for Serverless solution compared to a sequential execution on a virtual machine solution, and speedup of 23 for Serverless solution compared to a Parallel execution using virtual machines.},
	urldate = {2024-01-07},
	booktitle = {2022 30th {Telecommunications} {Forum} (℡{FOR})},
	author = {Mileski, Dimitar and Gusev, Marjan},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 30th Telecommunications Forum (TELFOR)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NJ8C2ZUY\\9983710.html:text/html},
}

@inproceedings{rohan_serverless_2022,
	title = {Serverless {Video} {Analysis} {Pipeline} for {Autonomous} {Remote} {Monitoring} {System}},
	url = {https://ieeexplore.ieee.org/document/10068884},
	doi = {10.1109/ICETECC56662.2022.10068884},
	abstract = {Cloud computing is the delivery of computing services including servers, storage, databases, networking, software, and analytics over the Internet. The businesses can store the data and run applications in the cloud with a high availability, accessibility from anywhere with a web browser, and on pay-per-use basis. Public cloud platforms make it possible for anyone to deploy end-to-end applications easily and economically. The cloud hosting for business applications provides the additional benefits of security, elasticity, and logging compared to an in-house private cloud.In this paper, we describe the implementation of a serverless remote video monitoring solution on the Amazon Web Services (AWS) cloud. It can provide surveillance of an area against unauthorized access, identification of objects in the scene, and investigating security incidents. This autonomous prototype application provides a frame-by-frame object detection of the live video. A Short Message Service (SMS) notification alert is generated and sent to the third-party on detecting an object of interest. It provides a flexible and economical solution for remote monitoring.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Conference} on {Emerging} {Technologies} in {Electronics}, {Computing} and {Communication} ({ICETECC})},
	author = {Rohan, Mohammad and Ahmed, Shurjeel and Kaleem, Mohammad and Nazir, Sajid},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)},
	pages = {1--6},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\WCSXHDI5\\Rohan et al. - 2022 - Serverless Video Analysis Pipeline for Autonomous .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\82NQKQQN\\10068884.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AGT833GJ\\10068884.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RYS7CVIH\\10068884.html:text/html},
}

@inproceedings{taibi_serverless_2022,
	title = {Serverless: {From} {Bad} {Practices} to {Good} {Solutions}},
	isbn = {2642-6587},
	shorttitle = {Serverless},
	url = {https://ieeexplore.ieee.org/document/9912641},
	doi = {10.1109/SOSE55356.2022.00016},
	abstract = {Serverless computing is increasing its popularity in the industry. However, practitioners still have issues when using it, also because serverless bad practices, bad smells, and anti-patterns have not been deeply investigated. In this work, we identify the main bad practices experienced by practitioners during the development of serverless-based applications. We interviewed 91 experienced practitioners and we performed a focus group to analyze the solutions they adopted to solve the issues generated by the bad practice. Moreover, we propose the most appropriate solutions based on our professional experience. The results can be helpful to other practitioners to avoid facing the same issues, to understand how to overcome them, and to researchers that can better validate them and propose alternative solutions.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Taibi, Davide and Kehoe, Ben and Poccia, Danilo},
	month = aug,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Service-Oriented System Engineering (SOSE)},
	pages = {85--92},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XU696VYW\\9912641.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5DKPRPWP\\9912641.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XQ5FUEBP\\9912641.html:text/html},
}

@inproceedings{safaryan_slam_2022,
	title = {{SLAM}: {SLO}-{Aware} {Memory} {Optimization} for {Serverless} {Applications}},
	isbn = {2159-6190},
	shorttitle = {{SLAM}},
	url = {https://ieeexplore.ieee.org/document/9860980},
	doi = {10.1109/CLOUD55607.2022.00019},
	abstract = {Serverless computing paradigm has become more ingrained into the industry, as it offers a cheap alternative for application development and deployment. This new paradigm has also created new kinds of problems for the developer, who needs to tune memory configurations for balancing cost and performance. Many researchers have addressed the issue of minimizing cost and meeting Service Level Objective (SLO) requirements for a single FaaS function, but there has been a gap for solving the same problem for an application consisting of many FaaS functions, creating complex application workflows.In this work, we designed a tool called SLAM to address the issue. SLAM uses distributed tracing to detect the relationship among the FaaS functions within a serverless application. By modeling each of them, it estimates the execution time for the application at different memory configurations. Using these estimations, SLAM determines the optimal memory configuration for the given serverless application based on the specified SLO requirements and user-specified objectives (minimum cost or minimum execution time). We demonstrate the functionality of SLAM on AWS Lambda by testing on four applications. Our results show that the suggested memory configurations guarantee that more than 95\% of requests are completed within the predefined SLOs.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 15th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Safaryan, Gor and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael},
	month = jul,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 15th International Conference on Cloud Computing (CLOUD)},
	pages = {30--39},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4SFVFK4Z\\9860980.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S634H3FQ\\9860980.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AJUETCR9\\9860980.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\K7KSRVYK\\Safaryan et al. - 2022 - SLAM SLO-Aware Memory Optimization for Serverless.pdf:application/pdf},
}

@inproceedings{sushma_smart_2022,
	title = {Smart {Water} {Flow} {Meter} for {Improved} {Measurement} of {Water} {Usage} in a {Smart} {City}},
	url = {https://ieeexplore.ieee.org/document/9808041},
	doi = {10.1109/ICAECT54875.2022.9808041},
	abstract = {Water shortage is one of the predominant problems which influences billions of humans throughout the world. With the cities transforming to dwelling places of smart infrastructure, controlling, analyzing and reducing the water consumption of the houses has proved to be challenging. Existing smart water monitoring devices are prone to theft, damage and can be quite expensive. As a result, developing countries are facing a difficult time in acquiring and maintaining high quality monitoring tools. Thereby water information stays erroneous and generally depends on isolated estimations. Hence, there is an increased demand for low-cost, reliable and precise flow sensors that are easy to use. To meet this necessity a low-cost, remote monitored smart water meter is proposed in this article to monitor water usage in urban private residences. A water flow sensor along with Raspberry Pi Pico is used that continuously monitors water usage. With the ability to monitor and visualize data with ease, the designed smart water meter is capable of sensing even moderate pipeline flow rates with a serverless architecture. The proposed meter also records monthly average water requirement of a house.},
	urldate = {2024-01-07},
	booktitle = {2022 {Second} {International} {Conference} on {Advances} in {Electrical}, {Computing}, {Communication} and {Sustainable} {Technologies} ({ICAECT})},
	author = {Sushma, N. and Suresh, H. N. and Lakshmi, J. Mohana},
	month = apr,
	year = {2022},
	note = {Journal Abbreviation: 2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2HJ5I5RC\\9808041.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\245ADTD2\\9808041.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QKAMV55T\\9808041.html:text/html},
}

@inproceedings{yuan_smpi_2022,
	title = {{SMPI}: {Scalable} {Serverless} {MPI} {Computing}},
	isbn = {2374-9628},
	shorttitle = {{SMPI}},
	url = {https://ieeexplore.ieee.org/document/9894339},
	doi = {10.1109/IPCCC55026.2022.9894339},
	abstract = {Running HPC in the cloud has gained more and more practice. As a new cloud paradigm, serverless is highly attractive for HPC service providers due to its distinctive benefits such as scalability. However, it is difficult for serverless to meet the demands of MPI programming and running, resulting in that MPI programs cannot scale with serverless functions. We introduce a serverless parallel function model to solve the problems. It divides parallelism at function and worker levels to bridge gaps in programming and running between serverless and MPI. Then we present the SMPI framework atop the model. For programming, SMPI redefines the function generation pipeline for parallel functions to prepare metadata for MPI parallel functions. For running, SMPI employs the parallel function gateway and scheduler to realize parallel function invocation and instantiation for MPI parallel functions. It is implemented and evaluated with OpenFaaS. Experiments show that SMPI supports MPI programming and running in a complete serverless manner. Compared to server-centric methods, it reduces efforts on cluster maintenance, provides scalable serverless MPI computing with competitive performance (0.559–1.048s slower of start-up time, and 0.145s–0.945s slower of computing time than best-behaved baseline), and is potential to scale on multiple clusters for higher scalability.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Performance}, {Computing}, and {Communications} {Conference} ({IPCCC})},
	author = {Yuan, Yuxin and Shi, Xiao and Lei, Zhengyu and Wang, Xiaohong and Zhao, Xiaofang},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)},
	pages = {275--282},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MMHUYQ8D\\9894339.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BKS73Z8T\\9894339.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7M2NW4I6\\9894339.html:text/html},
}

@inproceedings{pfandzelter_streaming_2022,
	title = {Streaming vs. {Functions}: {A} {Cost} {Perspective} on {Cloud} {Event} {Processing}},
	shorttitle = {Streaming vs. {Functions}},
	url = {https://ieeexplore.ieee.org/document/9946366},
	doi = {10.1109/IC2E55432.2022.00015},
	abstract = {In cloud event processing, data generated at the edge is processed in real-time by cloud resources. Both distributed stream processing (DSP) and Function-as-a-Service (FaaS) have been proposed to implement such event processing applications. FaaS emphasizes fast development and easy operation, while DSP emphasizes efficient handling of large data volumes. Despite their architectural differences, both can be used to model and implement loosely-coupled job graphs. In this paper, we consider the selection of FaaS and DSP from a cost perspective. We implement stateless and stateful workflows from the Theodolite benchmarking suite using cloud FaaS and DSP. In an extensive evaluation, we show how application type, cloud service provider, and runtime environment can influence the cost of application deployments and derive decision guidelines for cloud engineers.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Pfandzelter, Tobias and Henning, Sören and Schirmer, Trever and Hasselbring, Wilhelm and Bermbach, David},
	month = sep,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {67--78},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ERCC2Q77\\9946366.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TBM2S45U\\9946366.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YVQA4E8T\\9946366.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\CPCRNK5X\\Pfandzelter et al. - 2022 - Streaming vs. Functions A Cost Perspective on Clo.pdf:application/pdf},
}

@inproceedings{zhao_supporting_2022,
	title = {Supporting {Multi}-{Cloud} in {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10061782},
	doi = {10.1109/UCC56403.2022.00051},
	abstract = {Serverless computing is a widely adopted cloud execution model composed of Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS) offerings. The increased level of abstraction makes vendor lock-in inherent to serverless computing, raising more concerns than previous cloud paradigms. Multicloud serverless is a promising emerging approach against vendor lock-in, yet multiple challenges must be overcome to tap its potential. First, we need to be aware of both the performance and cost of each FaaS provider. Second, a multi-cloud architecture needs to be proposed before deploying a multi-cloud workflow. Domain-specific serverless offerings must then be integrated into the multi-cloud architecture to improve performance and/or save costs. Finally, we require workload portability support for serverless multi-cloud. In this paper, we present a multi-cloud library for crossserverless offerings. We develop an analysis system to support comparison among public FaaS providers in terms of performance and cost. Moreover, we present how to alleviate data gravity with domain-specific serverless offerings. Finally, we deploy workloads on these architectures to evaluate several public FaaS offerings.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE}/{ACM} 15th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Zhao, Haidong and Benomar, Zakaria and Pfandzelter, Tobias and Georgantas, Nikolaos},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE/ACM 15th International Conference on Utility and Cloud Computing (UCC)},
	keywords = {Computer architecture, Serverless, Performance, serverless, Lock-in, Locks (fasteners), vendor lock-in, Vendor lock-in, Multi-clouds, Service provider, Architecture, Cloud architectures, Cloud computing architecture, Cost benefit analysis, Domain specific, multi-cloud, performance and cost, Performance and cost, Service offering, Taps},
	pages = {285--290},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3VH2LKBA\\10061782.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3NPK3MJZ\\10061782.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4S9MK6DS\\10061782.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\Z5WY543X\\Zhao et al. - 2022 - Supporting Multi-Cloud in Serverless Computing.pdf:application/pdf},
}

@article{garbugli_tempos_2022,
	title = {{TEMPOS}: {QoS} {Management} {Middleware} for {Edge} {Cloud} {Computing} {FaaS} in the {Internet} of {Things}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {{TEMPOS}},
	url = {https://ieeexplore.ieee.org/document/9770777},
	doi = {10.1109/ACCESS.2022.3173434},
	abstract = {Several classes of advanced Internet of Things (IoT) applications, e.g., in the industrial manufacturing domain, call for Quality of Service (QoS) management to guarantee/control performance indicators, even in presence of many sources of “stochastic noise” in real deployment environments, from scarcely available bandwidth in a time window to concurrent usage of virtualized processing resources. This paper proposes a novel IoT-oriented middleware that i) considers and coordinates together different aspects of QoS monitoring, control, and management for different kinds of virtualized resources (from networking to processing) in a holistic way, and ii) specifically targets deployment environments where edge cloud resources are employed to enable the Serverless paradigm in the cloud continuum. The reported experimental results show how it is possible to achieve the desired QoS differentiation by coordinating heterogeneous mechanisms and technologies already available in the market. This demonstrates the feasibility of effective QoS-aware management of virtualized resources in the cloud-to-things continuum when considering a Serverless provisioning scenario, which is completely original in the related literature to the best of our knowledge.},
	urldate = {2024-01-07},
	journal = {IEEE Access},
	author = {Garbugli, Andrea and Sabbioni, Andrea and Corradi, Antonio and Bellavista, Paolo},
	year = {2022},
	note = {Conference Name: IEEE Access},
	pages = {49114--49127},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\Q39HN7CM\\Garbugli et al. - 2022 - TEMPOS QoS Management Middleware for Edge Cloud C.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GKA3S5WU\\9770777.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NKBZ3US8\\9770777.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FBZP97R4\\9770777.html:text/html},
}

@inproceedings{rinta-jaskari_testing_2022,
	title = {Testing {Approaches} {And} {Tools} {For} {AWS} {Lambda} {Serverless}-{Based} {Applications}},
	url = {https://ieeexplore.ieee.org/document/9767473},
	doi = {10.1109/PerComWorkshops53856.2022.9767473},
	abstract = {With serverless-based applications are increasing their popularity, little is known on testing practices and tools available to test serverless functions. This work aims to identify testing approaches for serverless functions built for the Amazon Web Services cloud platform, and to demonstrate how to implement them to a full-stack application. For this purpose, we implemented unit, integration and system tests to an existing open source application providing insights of the testing practices and and tools applicable. Results shows that all the testing practices are applicable, even if there is a lack of tools to support end-to-end tests, especially for debugging.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	author = {Rinta-Jaskari, Eetu and Allen, Christopher and Meghla, Tamara and Taibi, Davide},
	month = mar,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)},
	keywords = {Testing, Function-as-a-Service, Function-as-a-service, serverless function, Infrastructure as a service (IaaS), Open source software, Lambda's, Application programs, Amazon web services, Amazon Web Services, faas test, Faas test, Infrastructure-as-code, Infrastructure-as-Code, Integration testing, Lambda, lambda test, Lambda test, Open systems, Serverless function, serverless testing, Serverless testing, software testing, Software testings, Web services, Websites},
	pages = {686--692},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VY2D3235\\9767473.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZMCXXKD9\\9767473.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MHZB2AMX\\9767473.html:text/html},
}

@inproceedings{patel_next_2022,
	title = {The {Next} {Generation} {Edge} {Technologies}: {Architectures}, {Challenges}, and {Research} {Directions}},
	shorttitle = {The {Next} {Generation} {Edge} {Technologies}},
	url = {https://ieeexplore.ieee.org/document/10011000},
	doi = {10.1109/ICAISS55157.2022.10011000},
	abstract = {Advanced computing technology like Edge Computing moves the data to the edge of the network for processing and storage. With the advancement of communication technologies such as 5th Generation (5G) networks, the number of computing devices connected over internet and the data generated by these devices are also increased, which demand for this variation of cloud computing (CC). Edge computing was introduced with the goal to overcomes the limitations of cloud computing such as significant communication overhead and overall communication time in networks with many computing devices that produce and consume huge amounts of data. Moreover, Edge Computing boosts network support for mobility and confidentiality. The article discusses Edge Computing, its architectures and gives brief summary of its concept, core characteristics and challenges.},
	urldate = {2024-01-07},
	booktitle = {2022 {International} {Conference} on {Augmented} {Intelligence} and {Sustainable} {Systems} ({ICAISS})},
	author = {Patel, Akash and Nayak, Amit},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)},
	pages = {851--855},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G2ECIVN3\\10011000.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HAYAVP6J\\10011000.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6I6PMFTW\\10011000.html:text/html},
}

@article{toka_shape_2022,
	title = {The {Shape} of {Your} {Cloud}: {How} to {Design} and {Run} {Polylithic} {Cloud} {Applications}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {The {Shape} of {Your} {Cloud}},
	url = {https://ieeexplore.ieee.org/document/9889714},
	doi = {10.1109/ACCESS.2022.3206433},
	abstract = {Nowadays the major trend in IT dictates deploying applications in the cloud, cutting the monolithic software into small, easily manageable and developable components, and running them in a microservice scheme. With these choices come the questions: which cloud service types to choose from the several available options, and how to distribute the monolith in order to best resonate with the selected cloud features. We propose a model that presents monolithic applications in a novel way and focuses on key properties that are crucial in the development of cloud-native applications. The model focuses on the organization of scaling units, and it accounts for the cost of provisioned resources in scale-out periods and invocation delays among the application components. We analyze dis-aggregated monolithic applications that are deployed in the cloud, offering both Container-as-a-Service (CaaS) and Function-as-a-Service (FaaS) platforms. We showcase the efficiency of our proposed optimization solution by presenting the reduction in operation costs as an illustrative example. We propose to group similarly low scale components together in CaaS, while running dynamically scaled components in FaaS. By doing so, the price is decreased as unnecessary memory provisioning is eliminated, while application response time does not show any degradation.},
	urldate = {2024-01-07},
	journal = {IEEE Access},
	author = {Toka, Laszlo},
	year = {2022},
	note = {Conference Name: IEEE Access},
	pages = {97971--97982},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\DGM8FKLD\\Toka - 2022 - The Shape of Your Cloud How to Design and Run Pol.pdf:application/pdf},
}

@inproceedings{khochare_toward_2022,
	title = {Toward {Scientific} {Workflows} in a {Serverless} {World}},
	url = {https://ieeexplore.ieee.org/document/9973585},
	doi = {10.1109/eScience55777.2022.00057},
	abstract = {Serverless computing and FaaS have gained popularity due to their ease of design, deployment, scaling and billing on clouds. However, when used to compose and orchestrate scientific workflows, they pose limitations due to cold starts, message indirection, vendor lock-in and lack of provenance support. Here, we propose a design for a Ser verless Scientific Workflow Orchestrator that overcomes these challenges using techniques like function fusion, pilot invocations and data fabrics.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Khochare, Aakash and Simmhan, Yogesh and Mehta, Sameep and Agarwal, Arvind},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 18th International Conference on e-Science (e-Science)},
	pages = {399--400},
	file = {Accepted Version:C\:\\Users\\brian\\Zotero\\storage\\AWHGDM4E\\Khochare et al. - 2022 - Toward Scientific Workflows in a Serverless World.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XWVUDM48\\9973585.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SHSGLIW8\\9973585.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JUDC5G5I\\9973585.html:text/html},
}

@inproceedings{ferry_towards_2022,
	title = {Towards a {Model}-{Based} {Serverless} {Platform} for the {Cloud}-{Edge}-{IoT} {Continuum}},
	url = {https://ieeexplore.ieee.org/document/9826113},
	doi = {10.1109/CCGrid54584.2022.00101},
	abstract = {One of the most prominent implementations of the serverless programming model is Function-as-a-Service (FaaS). Using FaaS, application developers provide source code of serverless functions, typically describing only parts of a larger application, and define triggers for executing these functions on infrastructure components managed by the FaaS provider. There are still challenges that hinder the wider adoption of the FaaS model across the whole Cloud-Edge-IoT continuum. These include the high heterogeneity of the Edge and IoT infrastructure, vendor lock-in, the need to deploy and adapt serverless functions as well as their supporting services and software stacks into their cyber-physical execution environment. As a first step towards addressing these challenges, we introduce the SERVERLEss4I0T platform for the design, deployment, and maintenance of applications over the Cloud-Edge-IoT continuum. In particular, our platform enables the specification and deployment of serverless functions on Cloud and Edge resources, as well as the deployment of their supporting services and software stacks over the whole Cloud-Edge-IoT continuum.},
	urldate = {2024-01-07},
	booktitle = {2022 22nd {IEEE} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Ferry, Nicolas and Dautov, Rustem and Song, Hui},
	month = may,
	year = {2022},
	note = {Journal Abbreviation: 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	keywords = {Internet of Things, Cloud Computing, Edge computing, Internet of things, Deployment, Cloud-computing, Application developers, Edge Computing, Model-based OPC, Model-driven Engineering, Model-Driven Engineering, Programming models, Service stack, Services applications, Software stacks},
	pages = {851--858},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T337PXC3\\9826113.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FAY7B4SQ\\9826113.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SK8PTEP6\\9826113.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\Q454U4U6\\Ferry et al. - 2022 - Towards a Model-Based Serverless Platform for the .pdf:application/pdf},
}

@inproceedings{prodan_towards_2022,
	title = {Towards {Extreme} and {Sustainable} {Graph} {Processing} for {Urgent} {Societal} {Challenges} in {Europe}},
	url = {https://ieeexplore.ieee.org/document/9973125},
	doi = {10.1109/CloudSummit54781.2022.00010},
	abstract = {The Graph-Massivizer project, funded by the Horizon Europe research and innovation program, researches and develops a high-performance, scalable, and sustainable platform for information processing and reasoning based on the massive graph (MG) representation of extreme data. It delivers a toolkit of five open-source software tools and FAIR graph datasets covering the sustainable lifecycle of processing extreme data as MGs. The tools focus on holistic usability (from extreme data ingestion and MG creation), automated intelligence (through analytics and reasoning), performance modelling, and environmental sustainability tradeoffs, supported by credible data-driven evidence across the computing continuum. The automated operation uses the emerging serverless computing paradigm for efficiency and event responsiveness. Thus, it supports experienced and novice stakeholders from a broad group of large and small organisations to capitalise on extreme data through MG programming and processing. Graph-Massivizer validates its innovation on four complementary use cases considering their extreme data properties and coverage of the three sustainability pillars (economy, society, and environment): sustainable green finance, global environment protection foresight, green AI for the sustainable automotive industry, and data centre digital twin for exascale computing. Graph-Massivizer promises 70\% more efficient analytics than AliGraph, and 30 \% improved energy awareness for extract, transform and load storage operations than Amazon Redshift. Furthermore, it aims to demonstrate a possible two-fold improvement in data centre energy efficiency and over 25 \% lower greenhouse gas emissions for basic graph operations.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {Cloud} {Summit}},
	author = {Prodan, Radu and Kimovski, Dragi and Bartolini, Andrea and Cochez, Michael and Iosup, Alexandru and Kharlamov, Evgeny and Rožanec, Jože and Vasiliu, Laurenţiu and Vărbănescu, Ana Lucia},
	month = oct,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE Cloud Summit},
	pages = {23--30},
}

@inproceedings{russo_towards_2022,
	title = {Towards {QoS}-{Aware} {Function} {Composition} {Scheduling} in {Apache} {OpenWhisk}},
	url = {https://ieeexplore.ieee.org/document/9767299},
	doi = {10.1109/PerComWorkshops53856.2022.9767299},
	abstract = {Function-as-a-Service (FaaS) is increasingly popular thanks to the benefits provided to application developers and operators. Besides commercial Cloud-based offerings, open-source solutions have emerged enabling FaaS deployment on private infrastructures and possibly at the edge of the network. When moving from the Cloud to Fog/Edge environments, optimizing resource allocation for function execution becomes a critical challenge. Unfortunately, existing FaaS platforms have little or no support for fine-grained scheduling and resource allocation, nor allow users to enforce Quality-of-Service (QoS) requirements. We take a first step towards the development of a QoS-aware FaaS platform. We design and implement new mechanisms to support differentiated classes of services within Apache OpenWhisk, a popular open-source FaaS framework. Our experiments show that our prototype efficiently supports state-of-the-art scheduling policies and provides throughput improvements when dealing with function compositions under high load scenarios.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	author = {Russo, Gabriele Russo and Milani, Alfredo and Iannucci, Stefano and Cardellini, Valeria},
	month = mar,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)},
	pages = {693--698},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MAXQJN56\\9767299.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XIKWR4IH\\9767299.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6I8M54RJ\\9767299.html:text/html},
}

@inproceedings{spataru_tufa_2022,
	title = {{TUFA}: {A} {TOSCA} extension for the specification of accelerator-aware applications in the {Cloud} {Continuum}},
	isbn = {0730-3157},
	shorttitle = {{TUFA}},
	url = {https://ieeexplore.ieee.org/document/9842714},
	doi = {10.1109/COMPSAC54236.2022.00185},
	abstract = {A Distributed Application Topology is a valuable commodity built on the strength of a long and iterative design process. A topology is generally refined over time, other topologies can use it as a component, and the community may share it. To reproduce a deployment, several properties must be recorded such as data origin, processing steps, configuration settings, and hardware requirements. This paper presents an extension to the TOSCA specification that allows for the definition of accelerator-aware services that can span from Cloud to Edge. Additionally, we introduce the concept of Abstract Applications that contain at least one abstract service definition. The process of Service Optimization replaces the abstract sertvices, creating an explicit topology deployable under hybrid deployment models (Virtual Machines, Containers, HPC) residing on the Cloud Continuum spectrum.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 46th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Spătaru, Adrian and Iuhasz, Gabriel and Panica, Silviu},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)},
	pages = {1178--1183},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AYZI5MIG\\9842714.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WYGBBVHM\\9842714.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LMTVH3NJ\\9842714.html:text/html},
}

@inproceedings{xie_workflow_2022,
	title = {Workflow {Scheduling} {Using} {Hybrid} {PSO}-{GA} {Algorithm} in {Serverless} {Edge} {Computing} for the {Internet} of {Things}},
	isbn = {2577-2465},
	url = {https://ieeexplore.ieee.org/document/9860395},
	doi = {10.1109/VTC2022-Spring54318.2022.9860395},
	abstract = {In this paper, we design a task scheduling scheme for Internet of Things (IoT) workflow applications in serverless edge computing. Notice the fact that complex applications in traditional serverless computing are decomposed into several stateless, dependent functions, whose execution environments are pre-deployed at the resource-finite edge domain, we model the workflow application as Directed Acyclic Graph (DAG) by considering the distribution of edge resources and the deployment of serverless functions. We further formulate the scheduling problem as a multi-objective optimization problem to reduce the time consumption, energy consumption, and cost simultaneously. Then, considering the diversity of solution space and the fast convergence to optimal solutions, an improved hybrid algorithm that combines Particle Swarm Optimization and Genetic Algorithm (PSO–GA) is introduced and utilized to make the scheduling decision. Finally, extensive simulation experiments are conducted to validate the superiority of the proposed scheme.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 95th {Vehicular} {Technology} {Conference}: ({VTC2022}-{Spring})},
	author = {Xie, Renchao and Gu, Dier and Tang, Qinqin and Huang, Tao and Yu, F. Richard},
	month = jun,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KC3LZPQ2\\9860395.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\48Y2LN8G\\9860395.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H4CNNMF6\\9860395.html:text/html},
}

@inproceedings{kumari_workflow_2022,
	title = {Workflow {Sensitive} {Access} {Management} in {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10051255},
	doi = {10.1109/iSSSC56467.2022.10051255},
	abstract = {In recent years, serverless computing has been emerging as a most profitable cloud framework, which drastically improves the development and deployment policy of online services, but as a result, it is highly exposed to tempting targets for attackers. These attackers are proposing innovative strategies to get beyond the transitory nature of serverless activities by taking advantage of container reuse for the execution of stateless functions. The external request for function invocation must be extensively verified to protect the valuable resources from attackers. Traditional access management policy usually checks the individual inbound request for function invocation by ignoring other dependencies associated with the complete workflow. In this paper, we have proposed a two-phase workflow sensitive access management (WAM) policy that provides authentication tokens and checks whether the incoming request possesses all the necessary permission or not. WAM is based on a state-dependency graph which is a representation of allowable permission required to make transitions among the functions in the workflow. AWS Lambda is considered as the base framework where WAM policy is integrated. The effectiveness of WAM is verified using four real-world serverless applications and the performance is extensively compared with other standard serverless frameworks like Openwhisk, Openfaas, and Microsoft Azure.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 2nd {International} {Symposium} on {Sustainable} {Energy}, {Signal} {Processing} and {Cyber} {Security} ({iSSSC})},
	author = {Kumari, Anisha and Akram Khan, Md. and Sahoo, Bibhudatta},
	month = dec,
	year = {2022},
	note = {Journal Abbreviation: 2022 IEEE 2nd International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EBEFDED7\\10051255.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AGZEC3CB\\10051255.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BB5FJGUC\\10051255.html:text/html},
}

@inproceedings{kumar_wos_2022,
	title = {{WoS} {Bibliometric}-based {Review} on {Serverless} {Computing} model},
	isbn = {2573-3079},
	url = {https://ieeexplore.ieee.org/document/10053142},
	doi = {10.1109/PDGC56933.2022.10053142},
	abstract = {The Serverless architecture is a cloud-native development paradigm that allows developers to build and run programs. Servers are still there in serverless, but they are not involved in the creation of apps. A cloud provider often takes care of the server infrastructure’s provisioning, upkeep, and expansion. Developers merely need to bundle their code in containers in order to deploy it. It also aids in the construction of "cloud-naive computing model". The term "cloud-naive" describes the possible advantages for the environment that IT services provided online may present to society. The number of resources used by the IT services is causing a resource deficit. We use the last 6 years of WoS articles, which were published between 2017 and 2022. Overall analysis is performed in a RStudio environment using the "bibliometrix" library. Even so, we use AMSTAR and PRISMA check-lists to evaluate the methodological quality of the systematic review technique. We compute the outcomes based on eight different factors, including publisher, year, citation, sources, technology, country adoption, research impact, and research gaps.},
	urldate = {2024-01-07},
	booktitle = {2022 {Seventh} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Kumar, Ajay and Gupta, Rohan and Bhandari, Rahul},
	month = nov,
	year = {2022},
	note = {Journal Abbreviation: 2022 Seventh International Conference on Parallel, Distributed and Grid Computing (PDGC)},
	pages = {600--605},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6XX4HG7H\\10053142.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZCDI6GSI\\10053142.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\59SS2SET\\10053142.html:text/html},
}

@article{xu_dnn_2022,
	title = {λ{DNN}: {Achieving} {Predictable} {Distributed} {DNN} {Training} {With} {Serverless} {Architectures}},
	volume = {71},
	issn = {1557-9956},
	shorttitle = {λ{DNN}},
	url = {https://ieeexplore.ieee.org/document/9336272},
	doi = {10.1109/TC.2021.3054656},
	abstract = {Serverless computing is becoming a promising paradigm for Distributed Deep Neural Network (DDNN) training in the cloud, as it allows users to decompose complex model training into a number of functions without managing virtual machines or servers. Though provided with a simpler resource interface (i.e., function number and memory size), inadequate function resource provisioning (either under-provisioning or over-provisioning) easily leads to unpredictable DDNN training performance in serverless platforms. Our empirical studies on AWS Lambda indicate that, such unpredictable performance of serverless DDNN training is mainly caused by the resource bottleneck of Parameter Servers (PS) and small local batch size. In this article, we design and implement λλDNN, a cost-efficient function resource provisioning framework to provide predictable performance for serverless DDNN training workloads, while saving the budget of provisioned functions. Leveraging the PS network bandwidth and function CPU utilization, we build a lightweight analytical DDNN training performance model to enable our design of λλDNN resource provisioning strategy, so as to guarantee DDNN training performance with serverless functions. Extensive prototype experiments on AWS Lambda and complementary trace-driven simulations demonstrate that, λλDNN can deliver predictable DDNN training performance and save the monetary cost of function resources by up to 66.7 percent, compared with the state-of-the-art resource provisioning strategies, yet with an acceptable runtime overhead.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Computers},
	author = {Xu, Fei and Qin, Yiling and Chen, Li and Zhou, Zhi and Liu, Fangming},
	month = feb,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {450--463},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CPQYBIAF\\9336272.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4VTVRPUP\\9336272.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AQDYG8K2\\9336272.html:text/html},
}

@inproceedings{madheswaran_11_2023,
	title = {11 things about {Securing} {Microservice}},
	isbn = {9798350331325},
	url = {https://ieeexplore.ieee.org/document/10305626},
	doi = {10.1109/SecDev56634.2023.00019},
	abstract = {Microservices are a modern-era software development approach to creating REST APIs as a small independent process that is loosely coupled, performs business specific operation or capabilities, and is owned by a small team. Microservices are light weight components that are easy to develop, deploy and scale based on business requirements. Some of our customers use microservice with container-based deployment, some implement regular legacy process build and deploy on the webserver and some serverless in private, public, or hybrid cloud. Microservices are evolving and distributing the business requirement logic to multiple services which increases application complexity, maintainability and security while interacting with other services. Each organization follows multiple ways to secure their Microservices. Here, we will go through some of the security implementations and finally will review securing Microservices in zero trust.},
	language = {English},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {Secure} {Development} {Conference} ({SecDev})},
	publisher = {IEEE Computer Society},
	author = {Madheswaran, Yuvaraj},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE Secure Development Conference (SecDev)},
	pages = {51--53},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XENB9KYK\\10305626.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YLAX946S\\10305626.html:text/html},
}

@inproceedings{eder_comparison_2023,
	title = {A {Comparison} of {Distributed} {Tracing} {Tools} in {Serverless} {Applications}},
	isbn = {2642-6587},
	url = {https://ieeexplore.ieee.org/document/10254754},
	doi = {10.1109/SOSE58276.2023.00018},
	abstract = {Serverless computing can favor the emergence of complex and error-prone applications. In order to gain observability in such applications, distributed tracing can be used. However, as serverless computing relies on the pay-per-use billing model, utilizing distributed tracing tools can have a noticeable impact on the resulting costs. Therefore, this paper investigates the impact of distributed tracing in serverless applications by exploring and comparing the efficiency characteristics of three selected distributed tracing tools - Zipkin, OpenTelemetry, and SkyWalking. In particular, the runtime, the memory usage, and the initialization duration were examined by benchmarking AWS Lambda function invocations. In the experiments, Zipkin imposed the lowest runtime overhead with an average of 10.73 \%, while SkyWalking introduced the highest overhead with an average runtime overhead of 50.67 \%. OpenTelemetry added 24.19 \% additional runtime. Besides runtime overheads, significantly higher memory usage and initialization durations were detected for all tools. Therefore, the results suggest that distributed tracing can significantly impact the efficiency of serverless applications. Nevertheless, differences could be observed concerning tracing mechanisms and use cases. This helps developers to carefully select the most suitable tracing tool considering factors such as runtime overhead, memory usage, and initialization durations.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Eder, Christina and Winzinger, Stefan and Lichtenthäler, Robin},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Service-Oriented System Engineering (SOSE)},
	pages = {98--105},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MDL3WS9S\\10254754.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B6UBL7MA\\10254754.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GTRNDD3R\\10254754.html:text/html},
}

@inproceedings{su_review_2023,
	title = {A {Review} of {Multimedia} {Video} {Services} {Based} on {Serverless} {Cloud} {Computing}},
	isbn = {2155-5052},
	url = {https://ieeexplore.ieee.org/document/10211228},
	doi = {10.1109/BMSB58369.2023.10211228},
	abstract = {Cloud computing technology has achieved rapid growth in recent years. Serverless cloud computing has been applied in more and more scenarios due to its pay-as-you-go, event-driven, and flexible features. This article introduces the application of serverless cloud computing technology in multimedia video services, analyzes the key capabilities, and looks forward to the key development directions in the future.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Symposium} on {Broadband} {Multimedia} {Systems} and {Broadcasting} ({BMSB})},
	author = {Su, Yue and Sang, Liu and Zhao, Weibo and Li, Wei and Yuan, Changqing},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TRVMT6C5\\10211228.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6PYKDZJ4\\10211228.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\N5EK577F\\10211228.html:text/html},
}

@inproceedings{wang_design_2023,
	title = {A {Design} of {Edge} {Distributed} {Video} {Analysis} {System} {Based} on {Serverless} {Computing} {Service}},
	isbn = {2155-5052},
	url = {https://ieeexplore.ieee.org/document/10211177},
	doi = {10.1109/BMSB58369.2023.10211177},
	abstract = {The paper introduces the design of an edge distributed video analysis system based on serverless computing services. Firstly, we briefly describe the challenges faced by the traditional multimedia video intelligent analysis system based on centralize-cloud, and analyze the idea of deploying it as a edge distributed system. Secondly, we proposed a serverless edge distributed video intelligent analysis system, introduced the system architecture and key components. Meanwhile the resource matching scheme between the stateless video analysis function and the actual edge computing device is presented. Finally, by comparing with the small-scale experimental cluster and centralized video analysis system, we verified the efficiency and effectiveness of the system design scheme by this paper.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Symposium} on {Broadband} {Multimedia} {Systems} and {Broadcasting} ({BMSB})},
	author = {Wang, Yutong and Yang, Mingchuan and Ding, Peng and Shen, Yun and Shi, Xiaohou and Dai, Meiling},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4DJBUNU5\\10211177.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B6QQUQTB\\10211177.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AE6PX2GA\\10211177.html:text/html},
}

@inproceedings{yekta_review_2023,
	title = {A {Review} on {Machine} {Learning} {Methods} for {Workload} {Prediction} in {Cloud} {Computing}},
	isbn = {2643-279X},
	url = {https://ieeexplore.ieee.org/document/10326297},
	doi = {10.1109/ICCKE60553.2023.10326297},
	abstract = {Workload prediction is one of the critical parts of resource provisioning in cloud computing and its evolved branches such as serverless and edge computing. Effective resource provisioning stands as a crucial element within the realm of edge-cloud computing. Accurate prediction of cloud workloads is essential for the effective allocation of resources. Workload prediction plays a crucial role in enhancing efficiency, reducing costs, optimizing cloud performance, maintaining a high level of quality of service, and minimizing energy consumption. In this paper, we conduct a comprehensive review of state-of-the-art Machine Learning (ML) and Deep Learning (DL) algorithms employed in workload prediction in cloud computing and other similar platforms such as edge computing. We compared the selected papers in terms of utilized methods and techniques, predicted factors, accuracy metrics, and the dataset. Additionally, to facilitate usability and comparison, articles sharing similar advantages and disadvantages are organized into a table. Finally, the paper concludes by addressing current challenges and future research directions.},
	urldate = {2024-01-07},
	booktitle = {2023 13th {International} {Conference} on {Computer} and {Knowledge} {Engineering} ({ICCKE})},
	author = {Yekta, Mohammad and Shahhoseini, Hadi Shahriar},
	month = nov,
	year = {2023},
	note = {Journal Abbreviation: 2023 13th International Conference on Computer and Knowledge Engineering (ICCKE)},
	pages = {306--311},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\46B4GC4I\\10326297.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\M3IDCS63\\10326297.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X2BMWRAR\\10326297.html:text/html},
}

@inproceedings{zhang_node_2023,
	title = {A {Node} {Agent} for {Fast} and {Safe} {Execution} of {Computing} {Tasks} under {Kubernetes}},
	url = {https://ieeexplore.ieee.org/document/10242630},
	doi = {10.1109/MICCIS58901.2023.00020},
	abstract = {Current cloud platforms host workloads by creating a container instance and destroying the instance when the call is complete. However, the use of containers brings much overhead, which is not suitable for performance-sensitive computing tasks such as emerging IoT applications with real-time latency constraints. In this paper, we discuss the existing solutions for performing computational tasks in Kubernetes, a software that can automatically manage, scale, and maintain the required state of multiple workloads, as well as unresolved performance challenges in resource-constrained nodes. To achieve higher performance, we propose and implement a node agent that performs computing tasks quickly and safely. Compared with traditional container-based node agent, the performance of our node agent is improved by about 100 times and 1.1 times in the cold start and hot start workload scenarios, respectively. Compared with the latest solution using Webassembly, it improves the cold start and hot start time by about 10\%-50\%, and can be applied to richer scenarios.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Mobile} {Internet}, {Cloud} {Computing} and {Information} {Security} ({MICCIS})},
	author = {Zhang, Zhiwei and Pan, Qijun and Zhou, Jun and Zhang, Chen},
	month = apr,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Mobile Internet, Cloud Computing and Information Security (MICCIS)},
	pages = {86--94},
}

@inproceedings{baseer_secure_2023,
	title = {A {Secure} {Resale} {Management} {System} using {Cloud} {Services} and {ReactJS}},
	url = {https://ieeexplore.ieee.org/document/10085477},
	doi = {10.1109/ICEARS56392.2023.10085477},
	abstract = {Resale Management System (RMS) is a convenient approach to buy or to sell products from the comfort of home through the internet. Current Scenario of reselling products involves a person who buys the products from customers and sells them to others which impacts on price. In most of the cases customers need to manually visit the store to buy or sell the products which leads to more time consuming, man power, marketing skills, cost ineffective, etc. Generally, few stores allow customers to place an order through phone call then the product is sent to the customer but there is no guarantee that it satisfies the customer. So, the resale management system is a platform for customers to find right products they need or to sell the products. This project aims to create a customized e-commerce system that allows users to buy or to sell products such as books, phones, gadgets, etc. Resale Management System (RMS) developed by using Amazon Web Services (Cognito, API Gateway, S3 Bucket, Lambdas, DynamoDB, Node.js, Serverless and Express) and ReactJS . To test the system, a case study has been considered on resale of books. Now-a-days people are finding difficulties in buying books at reasonable-prices and cannot even afford to buy books due to their high price in present market and if they can get them in second hand, the price will be favourable to the needy. Finally, a comparative study with respect to properties of existing and proposed system has been measured with predefined metrics and suggested that our proposed RMS with Cloud services will provide an optimal solution to resolve the existing issues.},
	urldate = {2024-01-07},
	booktitle = {2023 {Second} {International} {Conference} on {Electronics} and {Renewable} {Systems} ({ICEARS})},
	author = {Baseer, K. K. and Jahir Pasha, M. and Srinivasulu, B. V. and Moon, Shaik Ali},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)},
	pages = {727--734},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YF4E3FQW\\10085477.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\82V47CVT\\10085477.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UCU9QK5P\\10085477.html:text/html},
}

@inproceedings{ravi_novel_2023,
	title = {A {Novel} {Technique} to {Improve} {Latency} and {Response} {Time} of {AI} {Models} using {Serverless} {Infrastructure}},
	isbn = {2767-7788},
	url = {https://ieeexplore.ieee.org/document/10134379},
	doi = {10.1109/ICICT57646.2023.10134379},
	abstract = {Response time for AI-based models in time-critical applications is always a matter of concern. The situation is challenging when the model is deployed in a cloud-based infrastructure. To address this issue, a cloud-native development methodology called serverless infrastructure enables developers to create and execute applications without having to worry about managing servers. Generally, traditional systems require manual scaling, constant maintenance, and dedicated hardware resulting in high costs. To solve this, the AI model is deployed in serverless infrastructure services for hosting APIs that is to identify marine animals which have the potential to attack human beings at the seashores. The serverless infrastructure suffers from an initializing delay called cold start for occasional requests and hence the response time will be delayed even if the lambda function is free. The problem of cold starts is mitigated using the scheduler in the lambda function. The scheduler sends dummy requests to the server to keep the server warm and active. The AI model used as a test case utilizes Convolutional Neural Network Algorithms and Transfer Learning Technique, for detecting predators near the seashore. The model is deployed in a serverless infrastructure and has the benefits of automatic scaling, pay-per-use pricing, and decreased operational costs. The AI model is implemented in both serverless infrastructure and in Elastic Cloud Compute EC2. The performance of both systems was done for cost, latency, and response time. The proposed system provides promising results when compared to traditional server systems.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Ravi, Bhanu Sankar and Madhukanthi, Chepuri and Sivasankar, P and Prasanna, John Deva},
	month = apr,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Inventive Computation Technologies (ICICT)},
	pages = {428--433},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GJRF5JGS\\10134379.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6BC6IZFA\\10134379.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G7DWT862\\10134379.html:text/html},
}

@inproceedings{farrow_serverless_2023,
	title = {A {Serverless} {Electroencephalogram} {Data} {Retrieval} and {Preprocessing} {Framework}},
	isbn = {2835-5776},
	url = {https://ieeexplore.ieee.org/document/10229382},
	doi = {10.1109/IRI58017.2023.00045},
	abstract = {Electroencephalogram (EEG) research continues to rely heavily on data silos used in isolated physical lab environments. However, as a part of the digital transformation, the EEG community has begun its exploration of the public cloud to determine how it can be best utilized to increase collaboration and accelerate research outcomes. The growing number of online repositories for data and tools has provided additional computational resources but the process of downloading data and software along with the installation and configuration requirements is cumbersome and prone to error. To break away from this research paradigm, we present a novel application of cloud technologies to provide reusable EEG data acquisition and preprocessing software as a service (SaaS) that eliminates data and software downloading prerequisites. We utilize the Amazon Web Services (AWS) cloud platform and serverless technologies to create a distributed, highly scalable and extensible solution for EEG signal data preprocessing that is more conducive to effective collaboration and data reproducibility with the potential to expedite neurotechnology breakthroughs.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 24th {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Farrow, Bathsheba and Jayarathna, Sampath},
	month = aug,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)},
	keywords = {AWS, SaaS, Serverless, Microservices, Computer software reusability, Application programs, Amazon web services, Web services, Data acquisition, Data preprocessing, Data retrieval, Data silos, Digital transformation, Electroencephalography, Electrophysiology, Microservice, Pipeline, PREP, Saa, Software as a service (SaaS), Software Reuse, Software-reuse},
	pages = {221--226},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WC9UN3NE\\10229382.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PIAZ3G4V\\10229382.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZXVZHIEW\\10229382.html:text/html},
}

@inproceedings{ferreira_dos_santos_alternative_2023,
	title = {An {Alternative} to {FaaS} {Cold} {Start} {Latency} of {Low} {Request} {Frequency} {Applications}},
	url = {https://ieeexplore.ieee.org/document/10253389},
	doi = {10.1109/ICECCME57830.2023.10253389},
	abstract = {Serverless applications are those that do not require the developer to provide or manage any servers. The developer can focus on core product logic and development and just release their code into a container at the service provider. However, when initializing a container there may be a latency called cold start. This article aims to use the Node.js language as an alternative to the Java language as a prevention against the cold start scenario in applications that have a low frequency of use. With the development of lambda functions with the same functionality in both languages. Noting that the node.js language had an 82\% reduction in startup time compared to java.},
	urldate = {2024-01-07},
	booktitle = {2023 3rd {International} {Conference} on {Electrical}, {Computer}, {Communications} and {Mechatronics} {Engineering} ({ICECCME})},
	author = {Ferreira Dos Santos, Paulo Otavio and Jorge de Moura Costa, Humberto and Leithardt, Valderi R. Q. and Jorge Silveira Ferreira, Paulo},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 3rd International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\64VNEV2W\\10253389.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WN4F5YTS\\10253389.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GC3KZ9SP\\10253389.html:text/html},
}

@inproceedings{lakhai_improved_2023,
	title = {An improved method for increasing maintainability in terms of serverless architecture application},
	isbn = {2766-3639},
	url = {https://ieeexplore.ieee.org/document/10324273},
	doi = {10.1109/CSIT61576.2023.10324273},
	abstract = {The use of new and improvement of existing software development methods becomes a necessity due to high quality requirements, complexity and importance of ease of maintenance. Based on the most common software development approach that improves maintainability, this paper develops a proprietary approach to improve maintainability and demonstrates its advantages over the existing approach.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 18th {International} {Conference} on {Computer} {Science} and {Information} {Technologies} ({CSIT})},
	author = {Lakhai, Vladyslav and Kuzmych, Oleh and Seniv, Maksym},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 18th International Conference on Computer Science and Information Technologies (CSIT)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JHLTNDHW\\10324273.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\467WQ922\\10324273.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KFTP69CA\\10324273.html:text/html},
}

@inproceedings{paval_openstack_2023,
	title = {An {OpenStack} {Cloud} {Solution} for a {Community} {Database} with {Handwritten} {Characters} {Used} in {Developing} {OCR} {Algorithms}},
	isbn = {2473-5698},
	url = {https://ieeexplore.ieee.org/document/10308492},
	doi = {10.1109/ICSTCC59206.2023.10308492},
	abstract = {Most research addressing OCR through machine learning techniques is focused on the actual algorithms and on using the MNIST data set as the de facto benchmark. Little effort was made to extend the data set or to build an entirely new one. Furthermore, support for characters other than English ones is mostly limited. This paper presents an OpenStack based approach that aims to overcome this last limitation by providing a community-oriented solution for developing and maintaining richer, language agnostic, community-shared data sets for OCR based applications. The proposed architecture is integrated with OpenStack services and relies on new Cloud perspectives, such as Function-as-a-Service (FaaS), to achieve a greater degree of flexibility. The included modules allow users to upload their own data sets, select or fine-tune their desired pre-processing methods, and derive the required features for their target character set. Both the input and the output data are stored using OpenStack specific data services and are shared for all the users of the Cloud deployment. An interesting feature is that the underlying FaaS functionality would also allow interested parties to upload their own pre-processing and feature extraction stages.},
	urldate = {2024-01-07},
	booktitle = {2023 27th {International} {Conference} on {System} {Theory}, {Control} and {Computing} ({ICSTCC})},
	author = {Pavăl, Mihaela-Irina and Alexandrescu, Adrian and Archip, Alexandru},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 27th International Conference on System Theory, Control and Computing (ICSTCC)},
	pages = {327--332},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IGXYGU3H\\10308492.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ILBWGLBP\\10308492.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RMI9ABRU\\10308492.html:text/html},
}

@inproceedings{saravana_kumar_survey_2023,
	title = {A {Survey} and {Implementation} on {Using} {A} {Runtime} {Overhead} {To} {Enable} {Serverless} {Deployment}},
	volume = {1},
	isbn = {2575-7288},
	url = {https://ieeexplore.ieee.org/document/10113032},
	doi = {10.1109/ICACCS57279.2023.10113032},
	abstract = {Serverless computing is an Important paradigm for cloud computing that may greatly decrease application development complexity and releasing programmers of the burden of operational responsibilities. This more sophisticated form of serverless computing, also known as “Pay what is used,” runs charges users and clusters on a “Pay as you go” basis. It is attracting a lot of attention from researchers as a symbol of the development of cloud programming paradigms. This new methodology’s acceptance as the industry standard, its effect, and the transfer of older applications are all covered. In this essay, we assess the descriptions of serverless apps in the literature. We divide applications into categories and go into great length on the aims, viability, and difficulties that the serverless paradigm poses in each of those disciplines. Evaluate the performance profile of the serverless ecosystem in a low latency, rising environment and offer conclusions about overall performance for enhancing serverless designs. Our investigation is restricted to One element of AWS Lambda’s serverless architecture is available. Our results show that the Lambda-based architectures' performance characteristics can be changed, and we cover aspects such as chilly beginnings and possible delay characteristics that can be caused by a variety of factors, incorporating systems and external events. To eventually offer a set of explicit references that will let the use of serverless computing to address a wider range of issues, We offer a wide range of tactics, suggestions, and techniques that, Simultaneously work to its advantages when utilized and executed properly. We cover both the challenging challenges and the subjects that the science community must continue studying.},
	urldate = {2024-01-07},
	booktitle = {2023 9th {International} {Conference} on {Advanced} {Computing} and {Communication} {Systems} ({ICACCS})},
	author = {Saravana Kumar, N. and Selvakumara Samy, S.},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)},
	pages = {497--501},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KGTA4U9E\\10113032.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\C8WFER4G\\10113032.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZQYS728N\\10113032.html:text/html},
}

@inproceedings{tiwari_automation_2023,
	title = {Automation of {FaaS} {Serverless} {Frameworks} {OpenFaaS} and {OpenWhisk} in {Private} {Cloud}},
	url = {https://ieeexplore.ieee.org/document/10235008},
	doi = {10.1109/WCONF58270.2023.10235008},
	abstract = {A methodology for cloud-native development called serverless facilitates the creation and execution of programmes by developers without worrying about maintaining servers. As no automation is available to implement FaaS serverless frameworks in private clouds and manual implementation is a tedious job. If the same tasks are performed manually, there is a greater risk of human error as well as an increase in cognitive strain that can take up to 4-5 hours for installation and configuration. Therefore, we are going to provide a command line tool to automate the process to implement FaaS serverless frameworks Open-FaaS and OpenWhisk in private clouds, which is easy to configure and straight forward to use. The proposed tool has the potential to save significant time and cost for customers compared to hiring a DevOps engineer to manually perform the installation and configuration. Additionally, the potential revenue generated from the tool’s use in the financial industry alone suggests a promising market for this type of automation tool. Overall, this paper highlights the usefulness and importance of automation in facilitating the adoption and implementation of serverless architecture in private clouds.},
	urldate = {2024-01-07},
	booktitle = {2023 {World} {Conference} on {Communication} \& {Computing} ({WCONF})},
	author = {Tiwari, Pankaj and Sharma, Sangeeta},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 World Conference on Communication \& Computing (WCONF)},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WX85ZUI4\\10235008.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Q248G27S\\10235008.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RXQPBR7F\\10235008.html:text/html},
}

@inproceedings{jarachanthan_acts_2023,
	title = {{ACTS}: {Autonomous} {Cost}-{Efficient} {Task} {Orchestration} for {Serverless} {Analytics}},
	isbn = {2766-8568},
	shorttitle = {{ACTS}},
	url = {https://ieeexplore.ieee.org/document/10188782},
	doi = {10.1109/IWQoS57198.2023.10188782},
	abstract = {Serverless computing has become increasingly popular for cloud applications, due to its compelling properties of high-level abstractions, lightweight runtime, high elasticity and pay-per-use billing. In this revolutionary computing paradigm shift, challenges arise when adapting data analytics applications to the serverless environment, due to the lack of support for efficient state sharing, which attract ever-growing research attention. In this paper, we aim to exploit the advantages of task-level orchestration and fine-grained resource provisioning for data analytics on serverless platforms, with the hope of fulfilling the promise of serverless deployment to the maximum extent. To this end, we present ACTS, an autonomous cost-efficient task orchestration framework for serverless analytics. ACTS judiciously schedules and coordinates function tasks to mitigate cold-start latency and state sharing overhead. In addition, ACTS explores the optimization space of fine-grained workload distribution and function resource configuration for cost efficiency. We have deployed and implemented ACTS on AWS Lambda, evaluated with various data analytics workloads. Results from extensive experiments demonstrate that ACTS achieves up to 98\% monetary cost reduction while maintaining superior job completion time performance, in comparison with the state-of-the-art baselines.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE}/{ACM} 31st {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RKJWUX6R\\10188782.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5LZLLB9P\\10188782.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\59SAH52E\\10188782.html:text/html},
}

@inproceedings{luo_behavior_2023,
	title = {Behavior {Tree}-based {Workflow} {Modeling} and {Scheduling} for {Serverless} {Edge} {Computing}},
	isbn = {2575-8411},
	url = {https://ieeexplore.ieee.org/document/10272475},
	doi = {10.1109/ICDCS57875.2023.00100},
	abstract = {Despite the popularity of Serverless computing, there are insufficient efforts dedicated to Serverless workflows (i.e., Serverless function orchestration), particularly for Serverless edge computing. In this paper, we first identify the challenges of deploying the state-of-the-art cloud-oriented Serverless workflow scheduling on resource-constrained edge devices, then propose to model Serverless workflows with behavior trees, and finally reveal our key observations and preliminary results for behavior tree-based Serverless workflow scheduling.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 43rd {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Luo, Ke and Ouyang, Tao and Zhou, Zhi and Chen, Xu},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)},
	pages = {955--956},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JGSVEW8G\\10272475.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RBP89PTX\\10272475.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\84IXI4Q4\\10272475.html:text/html},
}

@inproceedings{singh_challenges_2023,
	title = {“{Challenges} and {Solution} for {Security} \& {Living} in {High} {Altitude} {Area} {Siachen}”},
	url = {https://ieeexplore.ieee.org/document/10303613},
	doi = {10.1109/ICSEIET58677.2023.10303613},
	abstract = {This abstract discusses the challenges faced by soldiers at high-altitude border fronts and proposes a solution to address these issues. The soldiers endure extreme weather conditions, including strong winds of 300 KM/Hour and bone-chilling temperatures of -60 degrees Celsius, along with blizzards and avalanches. Food scarcity is common, necessitating snow boiling for drinking water, and limited time for eating due to freezing conditions. Communication with loved ones is challenging due to the lack of mobile networks in such remote locations. To tackle these problems, the author introduces a Multipurpose cable system that provides electrical power, telecommunication lines for base camp connectivity, and optical fiber for data sharing. The cable is designed to withstand extreme weather conditions and is spread within the snow using specific alloys to ensure stability at temperatures ranging from -75°C to -100°C. The cable includes self-security features to prevent tampering and provides GPS navigation capabilities. The proposed solution enables the supply of frozen food to soldiers, which can be cooked on electric heaters, making the process environmentally friendly. Additionally, an Indestructible Secure Communication Network is suggested, utilizing optical and telecommunication cables covered with metal repair paste for added security. This network establishes an unhackable serverless system at critical locations, facilitating secure communication for defense forces, research labs, and ministries. The proposed system utilizes two specially designed cables for different terrains and risk levels, effectively connecting high-altitude posts to bases without disturbing the environment. By implementing this solution, the aim is to improve the living conditions and security of armed forces stationed at challenging high-altitude border fronts.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Sustainable} {Emerging} {Innovations} in {Engineering} and {Technology} ({ICSEIET})},
	author = {Singh, Anirudh Pratap and Rajput, Anushree and Kumar, Anil and Jain, Dhyanendra and Verma, Pranshi},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Sustainable Emerging Innovations in Engineering and Technology (ICSEIET)},
	pages = {140--143},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\57ZHU58G\\10303613.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WQFCCPVD\\10303613.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8UZQH6VL\\10303613.html:text/html},
}

@inproceedings{veuvolu_cloud_2023,
	title = {Cloud {Computing} {Based} ({Serverless} computing) using {Serverless} architecture for {Dynamic} {Web} {Hosting} and cost {Optimization}},
	isbn = {2473-7577},
	url = {https://ieeexplore.ieee.org/document/10128286},
	doi = {10.1109/ICCCI56745.2023.10128286},
	abstract = {Serverless is a cloud-based code execution model where cloud carrie copes with servers and computing useful resource control instead of builders. There are not any virtual machines or bodily servers: they’re deployed routinely inside the cloud by companies. Cloud carriers cope with provisioning, keeping, and scaling the serverless architecture. What’s more, the serverless structure permits launching apps as wished: you don’t pay for ‘constantly-on’ server components to run your app when it’s no longer getting used. Instead, on a few occasiotriggerers app code, and the resources are dynamically allocated for that code. You forestall paying as quickly as the code is carried out. So, in a nutshell, serverless architecture is a manner to construct your cloud-primarily based software without coping with infrastructure. It eliminates the want for ordinary duties like protection patches, ability control, load balancing, scaling, and many others. Still, serverless does not mean there are no servers in any respect. The time period is truly elusive. Servers are definitely removed from the app development seeing that they are managed by the companies.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Computer} {Communication} and {Informatics} ({ICCCI})},
	author = {Veuvolu, Rakesh and Suryadevar, Anirudh and Vignesh, T. and Avthu, Nikhil Reddy},
	month = jan,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Computer Communication and Informatics (ICCCI)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G7LYEHSI\\10128286.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MXHGH27D\\10128286.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LAEWNU9D\\10128286.html:text/html},
}

@inproceedings{ward_cloud_2023,
	title = {Cloud {Services} {Enable} {Efficient} {AI}-{Guided} {Simulation} {Workflows} across {Heterogeneous} {Resources}},
	url = {https://ieeexplore.ieee.org/document/10196576},
	doi = {10.1109/IPDPSW59300.2023.00018},
	abstract = {Applications that fuse machine learning and simulation can benefit from the use of multiple computing resources, with, for example, simulation codes running on highly parallel supercomputers and AI training and inference tasks on specialized accelerators. Here, we present our experiences deploying two AI-guided simulation workflows across such heterogeneous systems. A unique aspect of our approach is our use of cloud-hosted management services to manage challenging aspects of cross-resource authentication and authorization, function-as-a-service (FaaS) function invocation, and data transfer. We show that these methods can achieve performance parity with systems that rely on direct connection between resources. We achieve parity by integrating the FaaS system and data transfer capabilities with a system that passes data by reference among managers and workers, and a user-configurable steering algorithm to hide data transfer latencies. We anticipate that this ease of use can enable routine use of heterogeneous resources in computational science.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Ward, Logan and Pauloski, J. Gregory and Hayot-Sasson, Valerie and Chard, Ryan and Babuji, Yadu and Sivaraman, Ganesh and Choudhury, Sutanay and Chard, Kyle and Thakur, Rajeev and Foster, Ian},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	pages = {32--41},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UA3BCLYP\\10196576.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YQCNPTA4\\10196576.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9Q43Y2MC\\10196576.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\NE5VIIQ7\\Ward et al. - 2023 - Cloud Services Enable Efficient AI-Guided Simulati.pdf:application/pdf},
}

@inproceedings{townend_cognit_2023,
	title = {{COGNIT}: {Challenges} and {Vision} for a {Serverless} and {Multi}-{Provider} {Cognitive} {Cloud}-{Edge} {Continuum}},
	volume = {2023-July},
	isbn = {2767-9918},
	shorttitle = {{COGNIT}},
	url = {https://ieeexplore.ieee.org/document/10234326},
	doi = {10.1109/EDGE60047.2023.00015},
	abstract = {Use of the serverless paradigm in cloud application development is growing rapidly, primarily driven by its promise to free developers from the responsibility of provisioning, operating, and scaling the underlying infrastructure. However, modern cloud-edge infrastructures are characterized by large numbers of disparate providers, constrained resource devices, platform heterogeneity, infrastructural dynamicity, and the need to orchestrate geographically distributed nodes and devices over public networks. This presents significant management complexity that must be addressed if serverless technologies are to be used in production systems. This position paper introduces COGNIT, a major new European initiative aiming to integrate AI technology into cloud-edge management systems to create a Cognitive Cloud reference framework and associated tools for serverless computing at the edge. COGNIT aims to: 1) support an innovative new serverless paradigm for edge application management and enhanced digital sovereignty for users and developers; 2) enable on-demand deployment of large-scale, highly distributed and self-adaptive serverless environments using existing cloud resources; 3) optimize data placement according to changes in energy efficiency heuristics and application demands and behavior; 4) enable secure and trusted execution of serverless runtimes. We identify and discuss seven research challenges related to the integration of serverless technologies with multi-provider Edge infrastructures and present our vision for how these challenges can be solved. We introduce a high-level view of our reference architecture for serverless cloud-edge continuum systems, and detail four motivating real-world use cases that will be used for validation, drawing from domains within Smart Cities, Agriculture and Environment, Energy, and Cybersecurity.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Edge} {Computing} and {Communications} ({EDGE})},
	author = {Townend, Paul and Martí, Alberto P. and De La Iglesia, Idoia and Matskanis, Nikolaos and Timoudas, Thomas Ohlson and Hallmann, Torsten and Lalaguna, Antonio and Swat, Kaja and Renzi, Francesco and Bocheński, Dominik and Mancini, Marco and Bhuyan, Monowar and González-Hierro, Marco and Dupont, Sébastien and Kristiansson, Johan and Montero, Rubén S. and Elmroth, Erik and Valdés, Iván and Massonet, Philippe and Olsson, Daniel and Llorente, Ignacio M. and Östberg, Per-Olov and Abdou, Michael},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Edge Computing and Communications (EDGE)},
	keywords = {FaaS, Serverless, edge computing, Edge computing, Resource management, serverless, Application development, Faas, Scalings, Cloud applications, Open systems, cognitive cloud, Cognitive cloud, Cognitive systems, Energy efficiency, Information management, multi-provider, Multi-provider, open source, Open-source, resource management},
	pages = {12--22},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KDMCJ8E5\\10234326.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I2HDBIWI\\10234326.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IAFADQHK\\10234326.html:text/html},
}

@inproceedings{czentye_cost-optimal_2023,
	title = {Cost-optimal {Operation} of {Latency} {Constrained} {Serverless} {Applications}: {From} {Theory} to {Practice}},
	isbn = {2374-9709},
	shorttitle = {Cost-optimal {Operation} of {Latency} {Constrained} {Serverless} {Applications}},
	url = {https://ieeexplore.ieee.org/document/10154412},
	doi = {10.1109/NOMS56928.2023.10154412},
	abstract = {Serverless computing and the function as a service model are new paradigms enabling the fine granular, bottomup construction of cloud-native applications. It can significantly reduce operating costs while shifting the management tasks from developers and application providers towards the cloud operators. But these benefits are provided at the cost of less control over the underlying infrastructure and the application performance, including the end-to-end latency. However, grouping of functions into deployable serverless software artifacts remains still under our control, which has a considerable impact on performance and operation costs. In this paper, we propose fast and efficient algorithms that can partition an application’s functions into separate deployment artifacts in a cost-optimal way while meeting user-defined average end-to-end latency bounds. Moreover, our approach supports the dynamic redesign and reconfiguration of the current deployment setup in response to changes in monitored metrics. Our main contribution is threefold. First, we establish the relevant theoretical models capturing the behavior of the serverless ecosystem and we define the main problem. In addition, the concept of the integrated application management is introduced. Second, we propose novel algorithms providing optimal solutions for different variants of the core problem and the complexity of the methods are analyzed. Third, we demonstrate the applicability and the benefits of our solution by evaluating different deployment scenarios of a realistic use case in Amazon’s public cloud environment.},
	urldate = {2024-01-07},
	booktitle = {{NOMS} 2023-2023 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Czentye, János and Pelle, István and Sonkoly, Balázs},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LRUXLYSZ\\10154412.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BNKSSHAS\\10154412.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DHQ2MZ6F\\10154412.html:text/html},
}

@inproceedings{bayerlein_cloud-based_2023,
	title = {Cloud-{Based} {Serverless} {Computing} {Enables} {Accelerated} {Monte} {Carlo}-{Based} {Scatter} {Correction} for {Positron} {Emission} {Tomography}},
	isbn = {2577-0829},
	url = {https://ieeexplore.ieee.org/document/10337966},
	doi = {10.1109/NSSMICRTSD49126.2023.10337966},
	abstract = {This study investigates the potential of cloud-based serverless computing to accelerate Monte Carlo (MC)-based scatter correction (SC) in positron emission tomography (PET) imaging. Especially with longer axial field of view, wider acceptance angles and increased number of lines-of-response, SC can pose high computational burden for image reconstruction methods – even when executed on a modern multi-core computing server. In this work we investigate the computational performance of a cloud-based serverless MC simulation using Amazon Web Service (AWS) Lambda and provide a comparison to the computational performance of modern on-premises multi-thread reconstruction server. We find that for our specific application cloud-based SC can outperform local server-based computations by more than an order of magnitude. Adopting cloud-based serverless computing in PET imaging facilities can significantly improve processing times and overall workflow efficiency, with future research exploring additional enhancements through optimized configurations and computational methods.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {Nuclear} {Science} {Symposium}, {Medical} {Imaging} {Conference} and {International} {Symposium} on {Room}-{Temperature} {Semiconductor} {Detectors} ({NSS} {MIC} {RTSD})},
	author = {Bayerlein, R. and Swarnakar, V. and Spencer, B. and Selfridge, A. and Leung, E. K.S. and Nardo, L. and Cherry, S. R. and Badawi, R. D.},
	month = nov,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UA7H53Y6\\10337966.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XS3IUEKH\\10337966.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5977IB5B\\10337966.html:text/html},
}

@inproceedings{liu_cost-sensitive_2023,
	title = {Cost-{Sensitive} {Cold} {Start} {Latency} {Optimization} {Mechanism} in {Function}-as-a-{Service}},
	url = {https://ieeexplore.ieee.org/document/10284800},
	doi = {10.1109/NaNA60121.2023.00081},
	abstract = {Function-as-a-Service (FaaS), as a cloud computing service, builds, runs, and manages application packages directly in a functional way, greatly improving development and delivery efficiency, and is a major trend in the future development of cloud services. However, FaaS is executed via event-driven execution and has a cold-start problem at runtime. Most of the existing research focuses on function runtime optimization and ignores cold start time. For business scenarios with high real-time requirements, prolonged cold starts can affect business results. Therefore, cold-start optimization is particularly important for the application of function computing in latency-sensitive scenarios. To reduce the impact of cold start latency on services, this paper proposes a memory configuration to reduce cold start. Firstly, a memory-cost model is constructed based on memory resource rules and service computation time rules, and the model is optimized using a gradient descent algorithm. The results of large-scale simulations show that the memory selection scheme proposed in this paper can reduce the cold start latency by about 25\% compared to the memory selection of conventional function services in existing cases.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Networking} and {Network} {Applications} ({NaNA})},
	author = {Liu, Ruiyan and Ma, Tengchao and Huang, Yiting and An, Qingzhao and Yan, Lin and Li, Jiangyuan},
	month = aug,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Networking and Network Applications (NaNA)},
	pages = {453--458},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S32KX8QL\\10284800.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\359NPT7J\\10284800.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\72DS4UII\\10284800.html:text/html},
}

@inproceedings{zhou_container_2023,
	title = {Container {Image} {Similarity}-{Aware} {Resource} {Provisioning} for {Serverless} {Edge} {Computing}},
	isbn = {2836-3868},
	url = {https://ieeexplore.ieee.org/document/10248269},
	doi = {10.1109/ICWS60048.2023.00047},
	abstract = {Container-enabled serverless computing has become a widely adopted approach for resource provisioning in the edge cloud. However, traffic incurred by container image pulling heavily burdens the already congested back-haul network. To relieve the problem, we do an analysis on Docker Hub, and find that instance deployment strategy has a significant impact on the back-haul traffic due to the varying similarity levels of different images. We incorporate this feature into task offloading decision and resource provisioning, and formulate the problem with a mixed integer non-linear programming (MINLP) problem. To address the challenges arising from the coupling and contradiction of instance deployment, image pulling, offloading decision, and resource allocation, we employ multi-agent deep reinforcement learning to decompose the problem into several simpler sub-problems, and design an algorithm for each sub-problem individually by exploiting convex optimization and fractional programming techniques. Simulations are conducted to validate the effectiveness of the proposed algorithm. The experiment results illustrate that our algorithm outperforms current notable solutions and improves the global utility by 13\%–74\%.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Zhou, Ao and Li, Sisi and Ma, Xiao and Zhang, Yiran and Wang, Shangguang},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Web Services (ICWS)},
	pages = {278--288},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NAPALUGU\\10248269.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UGU2AIX5\\10248269.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JLSQS7RN\\10248269.html:text/html},
}

@inproceedings{park_crow_2023,
	title = {Crow {API}: {Cross}-device {I}/{O} {Sharing} in {Web} {Applications}},
	isbn = {2641-9874},
	shorttitle = {Crow {API}},
	url = {https://ieeexplore.ieee.org/document/10228950},
	doi = {10.1109/INFOCOM53939.2023.10228950},
	abstract = {Although cross-device input/output (I/O) sharing is useful for users who own multiple computing devices, previous solutions had a platform-dependency problem. The meta-platform characteristics of web applications could provide a viable solution. In this paper, we propose the Crow application programming interface (API) that allows web applications to access other devices’ I/O through standard web APIs without modifying operating systems or browsers. The provision of cross-device I/O should resolve two key challenges. First, the web environment lacks support for device discovery when making a device-to-device connection. This requires a significant effort for developers to implement and maintain signaling servers. To address this challenge, we propose a serverless Crow connectivity mechanism using devices’ I/O-specific communication schemes. Second, JavaScript runtimes have limitations in supporting cross-device inter-process communication (IPC). To solve the problem, we propose a web IPC scheme, called Crow IPC, which introduces a proxy interface that relays the cross-device IPC connection. Crow IPC also provides a mechanism for ensuring functional consistency. We implemented the Crow API as a JavaScript library with which developers can easily develop their applications. An extensive evaluation showed that the Crow API provides cross-device I/O sharing functionality effectively and efficiently on various web applications and platforms.},
	urldate = {2024-01-07},
	booktitle = {{IEEE} {INFOCOM} 2023 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Park, Seonghoon and Lee, Jeho and Cha, Hojung},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: IEEE INFOCOM 2023 - IEEE Conference on Computer Communications},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VS3GG4SG\\10228950.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GKIXLFD4\\10228950.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IEN8L5VC\\10228950.html:text/html},
}

@inproceedings{satapathy_disprotrack_2023,
	title = {{DisProTrack}: {Distributed} {Provenance} {Tracking} over {Serverless} {Applications}},
	isbn = {2641-9874},
	shorttitle = {{DisProTrack}},
	url = {https://ieeexplore.ieee.org/document/10228884},
	doi = {10.1109/INFOCOM53939.2023.10228884},
	abstract = {Provenance tracking has been widely used in the recent literature to debug system vulnerabilities and find the root causes behind faults, errors, or crashes over a running system. However, the existing approaches primarily developed graph-based models for provenance tracking over monolithic applications running directly over the operating system kernel. In contrast, the modern DevOps-based service-oriented architecture relies on distributed platforms, like serverless computing that uses container-based sandboxing over the kernel. Provenance tracking over such a distributed micro-service architecture is challenging, as the application and system logs are generated asynchronously and follow heterogeneous nomenclature and logging formats. This paper develops a novel approach to combining system and micro-services logs together to generate a Universal Provenance Graph (UPG) that can be used for provenance tracking over serverless architecture. We develop a Loadable Kernel Module (LKM) for runtime unit identification over the logs by intercepting the system calls with the help from the control flow graphs over the static application binaries. Finally, we design a regular expression-based log optimization method for reverse query parsing over the generated UPG. A thorough evaluation of the proposed UPG model with different benchmarked serverless applications shows the system’s effectiveness.},
	urldate = {2024-01-07},
	booktitle = {{IEEE} {INFOCOM} 2023 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Satapathy, Utkalika and Thakur, Rishabh and Chattopadhyay, Subhrendu and Chakraborty, Sandip},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: IEEE INFOCOM 2023 - IEEE Conference on Computer Communications},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WRNMZM9A\\10228884.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7BGFS2ZC\\10228884.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R8RS38CF\\10228884.html:text/html},
}

@article{du_decentralized_2023,
	title = {Decentralized {Federated} {Learning} {With} {Markov} {Chain} {Based} {Consensus} for {Industrial} {IoT} {Networks}},
	volume = {19},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/document/9833351},
	doi = {10.1109/TII.2022.3192297},
	abstract = {Federated learning (FL) provides a novel framework to collaboratively train a shared model in a distribution fashion by virtue of a central server. However, FL is inappropriate for a serverless scenario and also suffers from some major drawbacks in Industrial Internet of Things (IIoT) networks, such as unresilience to network failures and communication bottleneck effect. In this article, we propose a novel decentralized federated learning (DFL) approach for IIoT devices to achieve model consensus by exchanging model parameters only with their neighbors rather than a central server. We firstly formulate the problem of model consensus in DFL as a fastest mixing Markov chain problem and then optimize the consensus matrix to improve the convergence rate. Meanwhile, a practical medium access control protocol with time slotted channel hopping is taken into account to implement the proposed approach. Furthermore, we also propose an accumulated update compression method to alleviate communication cost. Finally, extensive simulation results demonstrate that the proposed approach improves accuracy and reduces communication cost especially under the nonindependent identically distribution data distribution.},
	number = {4},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Du, Mengxuan and Zheng, Haifeng and Feng, Xinxin and Chen, Youjia and Zhao, Tiesong},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	pages = {6006--6015},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UBYZEI8G\\9833351.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A8J7N45M\\9833351.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y9WM9GDU\\9833351.html:text/html},
}

@inproceedings{escobar_decentralized_2023,
	title = {Decentralized {Serverless} {IoT} {Dataflow} {Architecture} for the {Cloud}-to-{Edge} {Continuum}},
	isbn = {2472-8144},
	url = {https://ieeexplore.ieee.org/document/10073502},
	doi = {10.1109/ICIN56760.2023.10073502},
	abstract = {The advent of new computing and communication trends that link pervasive data sources and consumers, such as Edge Computing, 5G and IIoT, has led to the development of the Cloud-to-Edge Continuum in order to take advantage of the resources available in massive IoT scenarios, and to conduct data analysis to leverage intelligence at all levels. This paper outlines the challenging requirements of this novel IoT context and presents an innovative IoT framework to develop dataflow applications for data-centric environments. The proposed design takes advantage of decentralized Pub/Sub communication and serverless nanoservice architecture, using novel technologies such as Zenoh and WebAssembly, respectively, to implement lightweight services along the Cloud-to-Edge infrastructure. We also describe some use cases to illustrate the benefits and concerns of the coming IoT generation.},
	urldate = {2024-01-07},
	booktitle = {2023 26th {Conference} on {Innovation} in {Clouds}, {Internet} and {Networks} and {Workshops} ({ICIN})},
	author = {Escobar, Juan José López and Gil-Castiñeira, Felipe and Díaz Redondo, Rebeca P.},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 26th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)},
	pages = {42--49},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5XULJRYN\\10073502.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8LK5XZIN\\10073502.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QASB5T3I\\10073502.html:text/html},
}

@article{bartan_distributed_2023,
	title = {Distributed {Sketching} for {Randomized} {Optimization}: {Exact} {Characterization}, {Concentration}, and {Lower} {Bounds}},
	volume = {69},
	issn = {1557-9654},
	shorttitle = {Distributed {Sketching} for {Randomized} {Optimization}},
	url = {https://ieeexplore.ieee.org/document/10049613},
	doi = {10.1109/TIT.2023.3247559},
	abstract = {We consider distributed optimization methods for problems where forming the Hessian is computationally challenging and communication is a significant bottleneck. We leverage randomized sketches for reducing the problem dimensions as well as preserving privacy and improving straggler resilience in asynchronous distributed systems. We derive novel approximation guarantees for classical sketching methods and establish tight concentration results that serve as both upper and lower bounds on the error. We then extend our analysis to the accuracy of parameter averaging for distributed sketches. Furthermore, we develop unbiased parameter averaging methods for randomized second order optimization in regularized problems that employ sketching of the Hessian. Existing works do not take the bias of the estimators into consideration, which limits their application to massively parallel computation. We provide closed-form formulas for regularization parameters and step sizes that provably minimize the bias for sketched Newton directions. Additionally, we demonstrate the implications of our theoretical findings via large scale experiments on a serverless cloud computing platform.},
	number = {6},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Information Theory},
	author = {Bartan, Burak and Pilanci, Mert},
	month = jun,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Information Theory},
	pages = {3850--3879},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\R9UBU9H6\\10049613.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JR9DFIAQ\\10049613.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5C5MF3H6\\10049613.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\T8V98Z4H\\Bartan and Pilanci - 2023 - Distributed Sketching for Randomized Optimization.pdf:application/pdf},
}

@inproceedings{tsenos_energy_2023,
	title = {Energy {Efficient} {Scheduling} for {Serverless} {Systems}},
	url = {https://ieeexplore.ieee.org/document/10336044},
	doi = {10.1109/ACSOS58161.2023.00020},
	abstract = {Serverless computing, also referred to as Function-as-a-Service (FaaS), is a cloud computing model that has attracted significant attention and has been widely adopted in recent years. The serverless computing model offers an intuitive, event-based interface that makes the development and deployment of scalable cloud-based applications easier and cost-effective. An important aspect that has not been examined in these systems is their energy consumption during the application execution. One way to deal with this issue is to schedule the function invocations in an energy-efficient way. However, efficient scheduling of applications in a multi-tenant environment, like FaaS systems, poses significant challenges. The trade-off between the server’s energy usage and the hosted functions’ performance requirements needs to be taken into consideration. In this work, we propose an Energy Efficient Scheduler for orchestrating the execution of serverless functions so that it minimizes energy consumption while it satisfies the applications’ performance demands. Our approach considers real-time performance measurements and historical data and applies a novel DVFS technique to minimize energy consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Autonomic} {Computing} and {Self}-{Organizing} {Systems} ({ACSOS})},
	author = {Tsenos, Michail and Peri, Aristotelis and Kalogeraki, Vana},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)},
	pages = {27--36},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T5PQ6KCL\\10336044.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RRS4VDVT\\10336044.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JTYE79KC\\10336044.html:text/html},
}

@inproceedings{fatouros_enhancing_2023,
	title = {Enhancing {Smart} {Agriculture} {Scenarios} with {Low}-code, {Pattern}-oriented functionalities for {Cloud}/{Edge} collaboration},
	isbn = {2325-2944},
	url = {https://ieeexplore.ieee.org/document/10257242},
	doi = {10.1109/DCOSS-IoT58021.2023.00055},
	abstract = {The integration of cloud computing and Internet of Things (IoT) technologies has brought significant advancements in the agriculture domain. However, the implementation of such systems often requires significant time and resources, making it challenging for smart agriculture providers to offer optimized yet affordable services for small and medium-sized farmers at scale. Low-code development platforms can be a viable solution to address these challenges, enabling non-experts to adapt or enhance existing applications with minimal coding. This paper presents a low-code approach to enhance smart agriculture scenarios with pattern-oriented functionality blocks for cloud/edge collaboration. It highlights the usage of a pattern collection for redesigning the implementation of smart agriculture applications that can enhance the data collection process as well as real-time decision-making and efficient resource management in the continuum. The effectiveness of the presented approach is demonstrated through the implementation of a case study in smart agriculture greenhouses. Evaluation results show that this approach can significantly reduce the time and effort required to deploy smart agriculture applications and provide data resilience.},
	urldate = {2024-01-07},
	booktitle = {2023 19th {International} {Conference} on {Distributed} {Computing} in {Smart} {Systems} and the {Internet} of {Things} ({DCOSS}-{IoT})},
	author = {Fatouros, Georgios and Kousiouris, George and Lohier, Theophile and Makridis, Georgios and Polyviou, Ariana and Soldatos, John and Kyriazis, Dimosthenis},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)},
	pages = {285--292},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Y2BELPEI\\10257242.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JNNS7Y7Y\\10257242.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\493VJ5NE\\10257242.html:text/html},
}

@inproceedings{wang_edge-assisted_2023,
	title = {Edge-{Assisted} {Adaptive} {Configuration} for {Serverless}-{Based} {Video} {Analytics}},
	isbn = {2575-8411},
	url = {https://ieeexplore.ieee.org/document/10272441},
	doi = {10.1109/ICDCS57875.2023.00058},
	abstract = {The growth of video volumes and increased DNN capabilities have led to a growing desire for video analytics, which demands intensive computation resources. Traditional resource provisioning strategies, such as configuring a cluster per peak utilization, lead to low resource efficiency. Serverless computing is a promising way to avoid wasteful resource provisioning since video analytics regularly encounters bursty input workloads and finegrained video content dynamics. For serverless-based video analytics, the application configuration (frame rate, detection model, and computation resources) will impact several metrics, such as computation cost and analytics accuracy. In this paper, we investigate the joint configuration adjustment problem for video knobs and computation resources provided by the serverless platform. We propose an algorithm that can efficiently adapt configurations for video streams to address two key challenges in serverless-based video analytics systems, including the complex relationships between the configurations and the key performance metrics, and the dynamically best configuration. Our algorithm is developed based on Markov approximation to minimize the computation cost within an accuracy constraint. We have developed a prototype over AWS Lambda and conducted extensive experiments with real-world video streams. The results show that our algorithm can greatly reduce the computation cost under the constraint of target accuracy.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 43rd {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Wang, Ziyi and Zhang, Songyu and Cheng, Jing and Wu, Zhixiong and Cao, Zhen and Cui, Yong},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)},
	pages = {248--258},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\93PJ3J27\\10272441.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QERB97Y7\\10272441.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PJSUUM24\\10272441.html:text/html},
}

@inproceedings{huang_duo_2023,
	title = {Duo: {Improving} {Data} {Sharing} of {Stateful} {Serverless} {Applications} by {Efficiently} {Caching} {Multi}-{Read} {Data}},
	isbn = {1530-2075},
	shorttitle = {Duo},
	url = {https://ieeexplore.ieee.org/document/10177481},
	doi = {10.1109/IPDPS54959.2023.00092},
	abstract = {A growing number of applications are moving to serverless architectures for high elasticity and fine-grained billing. For stateful applications, however, the use of serverless architectures is likely to lead to significant performance degradation, as frequent data sharing between different execution stages involves time-consuming remote storage access. Current platforms leverage memory cache to speed up remote access. However, conventional caching strategies show limited performance improvement. We experimentally find that the reason is that current strategies overlook the stage-dependent access patterns of stateful serverless applications, i.e., data that are read multiple times across stages (denoted as multi-read data) are wrongly evicted by data that are read only once (denoted as read-once data), causing a high cache miss ratio.Accordingly, we propose a new caching strategy, Duo, whose design principle is to cache multi-read data as long as possible. Specifically, Duo contains a large cache list and a small cache list, which act as Leader list and Wingman list, respectively. Leader list ignores the data that is read for the first time to prevent itself from being polluted by massive read-once data at each stage. Wingman list inspects the data that are ignored or evicted by Leader list, and pre-fetches the data that will probably be read again based on the observation that multi-read data usually appear periodically in groups. Compared to the state-of-the-art works, Duo improves hit ratio by 1.1×-2.1× and reduces the data sharing overhead by 25\%-62\%.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Huang, Zhuo and Fan, Hao and Cheng, Chaoyi and Wu, Song and Jin, Hai},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {875--885},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GKFF35QH\\10177481.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WLP87Q3M\\10177481.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XBT5JJV7\\10177481.html:text/html},
}

@inproceedings{liang_edgeorcher_2023,
	title = {{EdgeOrcher}: {Predictive} {Function} {Orchestration} for {Serverless}-{Based} {Edge} {Native} {Applications}},
	isbn = {2575-8411},
	shorttitle = {{EdgeOrcher}},
	url = {https://ieeexplore.ieee.org/document/10272539},
	doi = {10.1109/ICDCS57875.2023.00094},
	abstract = {Serverless computing is becoming prevalent to develop resource-demanding and delay-sensitive edge native applications across the edge and cloud. The unique pricing mechanism of serverless computing brings new opportunities to reduce the cost of edge native applications, by orchestrating function fusion and placement across the edge and cloud. However, function fusion potentially increases the latency of the serverless workflow. To navigate this performance-cost tradeoff, we present an online predictive function orchestration framework which leverages predictions to dynamically optimize the function fusion and placement. Preliminary evaluation results verify the efficacy of the proposed framework.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 43rd {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Liang, Yunkai and Zhou, Zhi and Chen, Xu},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)},
	pages = {1--2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\C6UB4PCU\\10272539.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\C8HX4G44\\10272539.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GJ2YWUW8\\10272539.html:text/html},
}

@inproceedings{xu_enabling_2023,
	title = {Enabling {Age}-{Aware} {Big} {Data} {Analytics} in {Serverless} {Edge} {Clouds}},
	isbn = {2641-9874},
	url = {https://ieeexplore.ieee.org/document/10228905},
	doi = {10.1109/INFOCOM53939.2023.10228905},
	abstract = {With the fast development of artificial intelligence applications, large-volume big data generated in the edge of networks are waiting for real-time analysis, such that the valuable information is unveiled. Analytic developers for big data applications usually face the burden of managing the underlying cloud resources, which greatly drags the speed of analytic development. Serverless Computing is envisioned as an enabling technology to release the management burden of developers and to enable agile big data analytics. That is, big data analytics can be implemented in short-lived functions via the Function-as-a-Service (FaaS) programming paradigm. In this paper, we aim to fill the gap between serverless computing and mobile edge computing, via enabling query evaluations for big data analytics in short-lived functions of a serverless edge cloud (SEC). Specifically, we formulate novel age-aware big data query evaluation problems in an SEC so that the age of data is minimized, where the age of data is defined as the time difference between the current time and the generation time of the dataset. We propose approximation algorithms for the age-aware big data query evaluation problem with a single query, by proposing a novel parameterized virtualization technique that strives for a fine trade-off between short-lived functions and large resource demands of big data queries. We also devise an online learning algorithm with a bounded regret for the problem with multiple queries arriving dynamically and without prior knowledge of resource demands of the queries. We finally evaluate the performance of the proposed algorithms by extensive simulations. Simulation results show that the performance of our algorithms is promising.},
	urldate = {2024-01-07},
	booktitle = {{IEEE} {INFOCOM} 2023 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Xu, Zichuan and Fu, Yuexin and Xia, Qiufen and Li, Hao},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: IEEE INFOCOM 2023 - IEEE Conference on Computer Communications},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\53G735DW\\10228905.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LABUSP36\\10228905.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\I8Q5M98M\\10228905.html:text/html},
}

@inproceedings{cordingly_enabling_2023,
	title = {Enabling {Serverless} {Sky} {Computing}},
	isbn = {2694-0825},
	url = {https://ieeexplore.ieee.org/document/10305821},
	doi = {10.1109/IC2E59103.2023.00038},
	abstract = {The Sky Computing vision represents a unified multi-cloud environment where applications can be deployed to utilize resources from different cloud regions, resource configurations, and cloud providers. Serverless computing platforms have recently emerged, offering automatic elastic scaling, high performance, and reduced costs but often utilize proprietary deployment tools and services locking users into platform-specific services. This research aims to apply Sky Computing to serverless computing platforms offered by major cloud providers such as Amazon Web Services, Google Cloud, Azure, and more. This research will build a serverless sky architecture to enable the aggregation of serverless resources to achieve service-level objectives such as low hosting costs, high performance, fault tolerance, high throughput, and low carbon footprint. The research will focus on evaluating the performance implications of serverless aggregation (Thrust-1), design and evaluation of Sky Computing architectures and aggregation strategies (Thrust-2), and finally autonomous resource aggregation for intelligent self-management of applications deployed to the sky (Thrust-3).},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Cordingly, Robert and Lloyd, Wes},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {232--235},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4QVWPW9G\\10305821.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZHSSMQ4W\\10305821.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MDDP6AA4\\10305821.html:text/html},
}

@inproceedings{barrak_exploring_2023,
	title = {Exploring the {Impact} of {Serverless} {Computing} on {Peer} {To} {Peer} {Training} {Machine} {Learning}},
	isbn = {2694-0825},
	url = {https://ieeexplore.ieee.org/document/10305831},
	doi = {10.1109/IC2E59103.2023.00024},
	abstract = {The increasing demand for computational power in big data and machine learning has driven the development of distributed training methodologies. Among these, peer-to-peer (P2P) networks provide advantages such as enhanced scalability and fault tolerance. However, they also encounter challenges related to resource consumption, costs, and communication overhead as the number of participating peers grows. In this paper, we introduce a novel architecture that combines serverless computing with P2P networks for distributed training and present a method for efficient parallel gradient computation under resource constraints.Our findings show a significant enhancement in gradient computation time, with up to a 97.34\% improvement compared to conventional P2P distributed training methods. As for costs, our examination confirmed that the serverless architecture could incur higher expenses, reaching up to 5.4 times more than instance-based architectures. It is essential to consider that these higher costs are associated with marked improvements in computation time, particularly under resource-constrained scenarios.Despite the cost-time trade-off, the serverless approach still holds promise due to its pay-as-you-go model. Utilizing dynamic resource allocation, it enables faster training times and optimized resource utilization, making it a promising candidate for a wide range of machine learning applications.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Barrak, Amine and Trabelsi, Ranim and Jaafar, Fehmi and Petrillo, Fabio},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {141--152},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ELWMXKM3\\10305831.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MG52SGRW\\10305831.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PQ9FYQKX\\10305831.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\W8F7GB4N\\Barrak et al. - 2023 - Exploring the Impact of Serverless Computing on Pe.pdf:application/pdf},
}

@inproceedings{cavalheiro_exploring_2023,
	title = {Exploring the {Serverless} {First} {Strategy} in {Cloud} {Application} {Development}},
	url = {https://ieeexplore.ieee.org/document/10306039},
	doi = {10.1109/SBAC-PADW60351.2023.00023},
	abstract = {This paper explores the Serverless First strategy in cloud application development. Serverless computing has gained popularity due to its flexibility and scalability. In our work, we provide a systematic review of the literature about the Serverless paradigm in cloud computing and an evaluation of the advantages of this approach by performing a comparative analysis among three ways for the implementation of an application: AWS Lambda, AWS Lambda with Chalice framework, and the traditional form using the Flask framework. The literature review results show the gains in scaling, cost reduction, and ease of maintenance achieved with the Serverless First strategy. However, some limitations and challenges were also highlighted, such as the greater complexity of the environment, less control over resources, resource limitations imposed by the cloud provider, and difficulties in debugging and managing the infrastructure. The case study demonstrates in practice that the Chalice framework provided the most straightforward and rapid implementation, the AWS Lambda without Chalice offered greater flexibility and control, and the Flask version allowed local testing and total control but required more manual setup and lacked automatic scalability.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing} {Workshops} ({SBAC}-{PADW})},
	author = {Cavalheiro, Adriano Prado and Schepke, Claudio},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)},
	pages = {89--94},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TU4FM92R\\10306039.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZAAL9HCC\\10306039.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S77UD3DF\\10306039.html:text/html},
}

@inproceedings{yang_faasctdo_2023,
	title = {{FaaSCTDO}: {Collaborative} {Task}-{Data} {Orchestration} for {Serverless} {Workflows}},
	volume = {2023-July},
	isbn = {2159-6190},
	shorttitle = {{FaaSCTDO}},
	url = {https://ieeexplore.ieee.org/document/10254972},
	doi = {10.1109/CLOUD60044.2023.00070},
	abstract = {The use of Function-as-a-Service (FaaS) platforms for executing complex serverless workflows has gained significant popularity. However, the stateless nature of FaaS requires functions to rely on remote storage to store their state, leading to performance overhead and reduced efficiency. Previous approaches using excess memory resources as caches can cause resource contention and decreased task execution performance. Moreover, the impact of workflow orchestration approaches on latency has not been fully explored. To address these challenges, we propose FaaSCTDO, a framework for collaborative task and data orchestration in serverless workflows. FaaSCTDO provides developers with comprehensive lifecycle management capabilities, treating data as orchestratable objects. It groups tasks and data with high data correlation together and allocates them on the same server node, leveraging data locality to expedite function access to data.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Yang, Neng and Zhang, Haitao and Zhang, Yepeng},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)},
	keywords = {FaaS, Function-as-a-service, Orchestration, Service platforms, Serverless workflow, Work-flows, Collaborative tasks, data locality, Data locality, Digital storage, Life cycle, orchestration, Remote storage, serverless workflows, workflow grouping, Workflow grouping},
	pages = {526--528},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TSJ8L6QU\\10254972.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UU2HR8EK\\10254972.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DVTJ3AD7\\10254972.html:text/html},
}

@article{yu_faasdeliver_2023,
	title = {{FaaSDeliver}: {Cost}-{Efficient} and {QoS}-{Aware} {Function} {Delivery} in {Computing} {Continuum}},
	volume = {16},
	issn = {1939-1374},
	shorttitle = {{FaaSDeliver}},
	url = {https://ieeexplore.ieee.org/document/10122727},
	doi = {10.1109/TSC.2023.3274769},
	abstract = {Serverless Function-as-a-Service (FaaS) is a rapidly growing computing paradigm in the cloud era. To provide rapid service response and save network bandwidth, traditional cloud-based FaaS platforms have been extended to the edge. However, launching functions in a heterogeneous computing continuum (HCC) that includes the cloud, fog, and the edge brings new challenges: determining where functions should be delivered and how many resources should be allocated. To optimize the cost of running functions in the HCC, we propose an adaptive and efficient function delivery engine, named FaaSDeliver, which automatically unearths a cost-efficient function delivery policy (FDP) for each function, including the FaaS platform selection and resource allocation. Real system implementation and evaluations in a practical HCC demonstrate that FaaSDeliver can unearth the most cost-efficient FDPs from among 180,200 FDPs after a few trials. FaaSDeliver reduces the average cost of function execution from 38\% to 78\% compared to some state-of-the-art approaches.},
	number = {5},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Services Computing},
	author = {Yu, Guangba and Chen, Pengfei and Zheng, Zibin and Zhang, Jingrun and Li, Xiaoyun and He, Zilong},
	month = sep,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Services Computing},
	pages = {3332--3347},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K8QIFXRP\\10122727.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\J2WHSKYQ\\10122727.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LLMFYA5E\\10122727.html:text/html},
}

@inproceedings{liu_fcloudless_2023,
	title = {{FCloudless}: {A} {Performance}-{Aware} {Collaborative} {Mechanism} for {JointCloud} {Serverless}},
	shorttitle = {{FCloudless}},
	url = {https://ieeexplore.ieee.org/document/10229726},
	doi = {10.1109/JCC59055.2023.00019},
	abstract = {As a new stage in the development of the cloud computing paradigm, serverless computing has the high-level abstraction characteristic of shielding underlying details. This makes it extremely challenging for users to choose a suitable serverless platform. To address this, targeting the jointcloud computing scenario of heterogeneous serverless platforms across multiple clouds, this paper presents a jointcloud collaborative mechanism called FCloudless with cross-cloud detection of the full lifecycle performance of serverless platforms. Based on the benchmark metrics set that probe performance critical stages of the full lifecycle, this paper proposes a performance optimization algorithm based on detected performance data that takes into account all key stages that affect the performance during the lifecycle of a function and predicts the overall performance by combining the scores of local stages and dynamic weights. We evaluate FCloudless on AWS, AliYun, and Azure. The experimental results show that FCloudless can detect the underlying performance of serverless platforms hidden in the black box and its optimization algorithm can select the optimal scheduling strategy for various applications in a jointcloud environment. FCloudless reduces the runtime by 23.3\% and 24.7\% for cold and warm invocations respectively under cost constraints.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Joint} {Cloud} {Computing} ({JCC})},
	author = {Liu, Jianfei and Wang, Huaimin and Shi, Peichang and Li, Yaojie and Ma, Penghui and Yi, Guodong},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Joint Cloud Computing (JCC)},
	pages = {93--94},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7X3TN8E7\\10229726.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\22SD3ILF\\10229726.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DX8GLNRE\\10229726.html:text/html},
}

@inproceedings{ma_feature_2023,
	title = {Feature and {Performance} {Comparison} of {FaaS} {Platforms}},
	isbn = {2327-0594},
	url = {https://ieeexplore.ieee.org/document/10293015},
	doi = {10.1109/ICSESS58500.2023.10293015},
	abstract = {With serverless computing offering more efficient and cost-effective application deployment, the diversity of serverless platforms presents challenges to users, including platform lock-in and costly migration. Moreover, due to the black box nature of function computing, traditional performance benchmarking methods are not applicable, necessitating new studies. This article presents a detailed comparison of six major public cloud function computing platforms and introduces a benchmarking framework for function computing performance. This framework aims to help users make comprehensive comparisons and select the most suitable platform for their specific needs.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 14th {International} {Conference} on {Software} {Engineering} and {Service} {Science} ({ICSESS})},
	author = {Ma, Penghui and Shi, Peichang and Yi, Guodong},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 14th International Conference on Software Engineering and Service Science (ICSESS)},
	pages = {239--243},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7NAY96N7\\10293015.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PS34YPGH\\10293015.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6HX849UJ\\10293015.html:text/html},
}

@inproceedings{zhang_first_2023,
	title = {{FIRST}: {Exploiting} the {Multi}-{Dimensional} {Attributes} of {Functions} for {Power}-{Aware} {Serverless} {Computing}},
	isbn = {1530-2075},
	shorttitle = {{FIRST}},
	url = {https://ieeexplore.ieee.org/document/10177391},
	doi = {10.1109/IPDPS54959.2023.00091},
	abstract = {Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.In this work, we show that the optimal operating point (OOP) for energy efficiency cannot be attained without synthesizing the multi-dimensional attributes of functions. We present FIRST, a novel mechanism that enables servers to better orchestrate serverless functions. The key feature of FIRST is that it leverages a lightweight Internal Representation and meta-Scheduling (IRS) layer for collecting the maximum potential revenue from the servers. Specifically, FIRST follows a pipeline-style workflow. Its frontend components aim to analyze functions from different angles and expose their key features to the system. Meanwhile, its backend components are able to make informed function assignment decisions to avoid OOP divergence. We further demonstrate the way to create extensions based on FIRST to enable versatile cloud-native power management. In total, our design constitutes a flexible management layer that supports power-aware function deployment. We show that FIRST could allow 94\% functions to be processed under the OOP, which brings up to 24\% energy efficiency improvements.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Zhang, Lu and Li, Chao and Wang, Xinkai and Feng, Weiqi and Yu, Zheng and Chen, Quan and Leng, Jingwen and Guo, Minyi and Yang, Pu and Yue, Shang},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {864--874},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SEKWQJGC\\10177391.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QKE2Q66W\\10177391.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FBZYI8YS\\10177391.html:text/html},
}

@article{xu_feddk_2023,
	title = {{FedDK}: {Improving} {Cyclic} {Knowledge} {Distillation} for {Personalized} {Healthcare} {Federated} {Learning}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {{FedDK}},
	url = {https://ieeexplore.ieee.org/document/10182241},
	doi = {10.1109/ACCESS.2023.3294812},
	abstract = {For most healthcare organizations, a significant challenge today is predicting diseases with incomplete data information, often resulting in isolation. Federated learning (FL) solves the issue of data silos by enabling remote local machines to train a globally optimal model collaboratively without the need for sharing data. In this research, we present FedDK, a serverless framework designed to obtain personalized models for each federation through data from local federations using convolutional neural networks and training through FL. Our approach involves using convolutional neural networks (CNNs) to accumulate common knowledge and transfer it using knowledge distillation, which helps prevent common knowledge forgetting. Additionally, the missing common knowledge is filled circularly between each federation, culminating in a personalized model for each group. This novel design leverages federated, deep, and integrated learning methods to produce more accurate machine-learning models. Our federated model exhibits superior performance to local and baseline FL methods, achieving significant advantages.},
	urldate = {2024-01-07},
	journal = {IEEE Access},
	author = {Xu, Yikai and Fan, Hongbo},
	year = {2023},
	note = {Conference Name: IEEE Access},
	pages = {72409--72417},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\M62MEF99\\Xu and Fan - 2023 - FedDK Improving Cyclic Knowledge Distillation for.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LSLFIEQZ\\10182241.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JZDTCU5Z\\10182241.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2SUBWHV6\\10182241.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\8YC3BHYM\\Xu and Fan - 2023 - FedDK Improving Cyclic Knowledge Distillation for.pdf:application/pdf},
}

@inproceedings{kesavan_firestore_2023,
	title = {Firestore: {The} {NoSQL} {Serverless} {Database} for the {Application} {Developer}},
	isbn = {2375-026X},
	shorttitle = {Firestore},
	url = {https://ieeexplore.ieee.org/document/10184529},
	doi = {10.1109/ICDE55515.2023.00259},
	abstract = {The recent years have seen an explosive growth in web and mobile application development. Such applications typically have rapid development cycles, and their developers expect mobile-friendly features and serverless characteristics such as rapid deployment capabilities (with minimal initialization), scalability to handle workload spikes, and flexible pay-as-you-go billing. Google’s Firestore is a NoSQL serverless database with real-time notification capability, and together with the Firebase ecosystem greatly simplifies common app development challenges by letting application developers focus primarily on their business logic and user experience. This paper presents the Firestore architecture, how it satisfies the aforementioned requirements, and how its real-time notification system works in tandem with Firebase client libraries to allow mobile applications to provide a smooth user experience even in the presence of network connectivity issues.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 39th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Kesavan, Ram and Gay, David and Thevessen, Daniel and Shah, Jimit and Mohan, C.},
	month = apr,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 39th International Conference on Data Engineering (ICDE)},
	pages = {3376--3388},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IQ2PHDMM\\10184529.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RFJ9WJL7\\10184529.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RHQNWWDB\\10184529.html:text/html},
}

@inproceedings{zhao_gpu-enabled_2023,
	title = {{GPU}-enabled {Function}-as-a-{Service} for {Machine} {Learning} {Inference}},
	isbn = {1530-2075},
	url = {https://ieeexplore.ieee.org/document/10177435},
	doi = {10.1109/IPDPS54959.2023.00096},
	abstract = {Function-as-a-Service (FaaS) is emerging as an important cloud computing service model as it can improve the scalability and usability of a wide range of applications, especially Machine-Learning (ML) inference tasks that require scalable resources and complex software configurations. These inference tasks heavily rely on GPUs to achieve high performance; however, support for GPUs is currently lacking in the existing FaaS solutions. The unique event-triggered and short-lived nature of functions poses new challenges to enabling GPUs on FaaS, which must consider the overhead of transferring data (e.g., ML model parameters and inputs/outputs) between GPU and host memory. This paper proposes a novel GPU-enabled FaaS solution that enables ML inference functions to efficiently utilize GPUs to accelerate their computations. First, it extends existing FaaS frameworks such as OpenFaaS to support the scheduling and execution of functions across GPUs in a FaaS cluster. Second, it provides caching of ML models in GPU memory to improve the performance of model inference functions and global management of GPU memories to improve cache utilization. Third, it offers co-designed GPU function scheduling and cache management to optimize the performance of ML inference functions. Specifically, the paper proposes locality-aware scheduling, which maximizes the utilization of both GPU memory for cache hits and GPU cores for parallel processing. A thorough evaluation based on real-world traces and ML models shows that the proposed GPU-enabled FaaS works well for ML inference tasks, and the proposed locality-aware scheduler achieves a speedup of 48x compared to the default, load balancing only schedulers.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Zhao, Ming and Jha, Kritshekhar and Hong, Sungho},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {918--928},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UC228Z9X\\10177435.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MFARUXDC\\10177435.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8AD8U5RK\\10177435.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\RIH2NCYL\\Zhao et al. - 2023 - GPU-enabled Function-as-a-Service for Machine Lear.pdf:application/pdf},
}

@article{golec_healthfaas_2023,
	title = {{HealthFaaS}: {AI}-{Based} {Smart} {Healthcare} {System} for {Heart} {Patients} {Using} {Serverless} {Computing}},
	volume = {10},
	issn = {2327-4662},
	shorttitle = {{HealthFaaS}},
	url = {https://ieeexplore.ieee.org/document/10129153},
	doi = {10.1109/JIOT.2023.3277500},
	abstract = {Heart disease is one of the leading causes of death worldwide, and with early detection, mortality rates can be reduced. Well-known studies have shown that the latest artificial intelligence (AI) can be used to determine the risk of heart disease. However, existing studies did not consider dynamic scalability to get the best performance from these AI models in case of an increasing number of users. To solve this problem, we proposed an AI-powered smart healthcare framework called HealthFaaS, using the Internet of Things (IoT) and a Serverless Computing environment to reduce heart disease-related deaths and prevent financial losses by reducing misdiagnoses. HealthFaaS framework collects health data from users via IoT devices and sends it to AI models deployed on a Google Cloud Platform (GCP)-based serverless computing environment due to its advantages, such as dynamic scalability, less operational complexity, and a pay-as-you-go pricing model. The performance of five different AI models for heart disease risk detection is evaluated and compared based on key parameters, such as accuracy, precision, recall, F -Score, and AUC. Experimental results demonstrate that the light gradient boosting machine model gives the highest success in detecting heart diseases with an accuracy rate of 91.80\%. Further, we have tested the performance of the HealthFaaS framework in terms of Quality-of-Service (QoS) parameters, such as throughput and latency against the increasing number of users and compared it with a non-serverless platform. In addition, we have also evaluated the cold start latency using a serverless platform which determined that the amount of memory and the software language makes a direct impact on the cold start latency.},
	number = {21},
	urldate = {2024-01-07},
	journal = {IEEE Internet of Things Journal},
	author = {Golec, Muhammed and Gill, Sukhpal Singh and Parlikad, Ajith Kumar and Uhlig, Steve},
	month = nov,
	year = {2023},
	note = {Conference Name: IEEE Internet of Things Journal},
	pages = {18469--18476},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4ZUA2VC3\\10129153.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PEGYB6U2\\10129153.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XM6GSVJS\\10129153.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\T4H5DU8E\\Golec et al. - 2023 - HealthFaaS AI-Based Smart Healthcare System for H.pdf:application/pdf},
}

@inproceedings{luk_heterogeneous_2023,
	title = {Heterogeneous {Reconfigurable} {Accelerators}: {Trends} and {Perspectives}},
	shorttitle = {Heterogeneous {Reconfigurable} {Accelerators}},
	url = {https://ieeexplore.ieee.org/document/10247723},
	doi = {10.1109/DAC56929.2023.10247723},
	abstract = {Heterogeneity and reconfigurability have both been adopted by accelerators to improve their flexibility and efficiency for a wide variety of applications, from cloud computing to embedded systems. This paper provides an overview of the trends of heterogeneous reconfigurable accelerators including field-programmable gate arrays and coarse-grained reconfigurable arrays, and the related design automation approaches for enhancing design quality and designer productivity of these accelerators. We shall also discuss how recent advances in technology, such as multi-level co-design, heterogeneous Function-as-a-Service and meta-programming, would help address the challenges in engineering next-generation heterogeneous reconfigurable accelerators and beyond.},
	urldate = {2024-01-07},
	booktitle = {2023 60th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	author = {Luk, Wayne},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 60th ACM/IEEE Design Automation Conference (DAC)},
	pages = {1--2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UJMHVTZ9\\10247723.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RN6XKHS4\\10247723.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D7YRIE5E\\10247723.html:text/html},
}

@inproceedings{taylor_hyron_2023,
	title = {Hyron: {A} {New} {Approach} for {Automating} the {Network} {ACL} {Delivery} {Pipeline}},
	isbn = {2637-9430},
	shorttitle = {Hyron},
	url = {https://ieeexplore.ieee.org/document/10230146},
	doi = {10.1109/ICCCN58024.2023.10230146},
	abstract = {Automated Access Control List (ACL) configuration has remained an area of research interest for a significant period. However, previous research has not addressed the challenges associated with ACL automation in the context of a complex and evolving industry landscape. We examine the existing research literature on this topic and identify a series of key requirements (“success criteria”) that any new system must achieve to be considered an improvement over the status quo. We then design, develop, and demonstrate an approach to ACL automation that embodies these characteristics by combining model-driven ACL synthesis with a modern DevOps-style deployment system. We explain the rationale that drove the design decisions behind the Hyron ACL generation toolkit and how it can enable a fully automated ACL delivery pipeline when integrated with standard developer tools. In contrast to previous research, our design approach reflects automation trends in the industry to ensure mainstream engineers readily adopt our solution. We provide an analysis comparing our approach's benefits to those of previous research.},
	urldate = {2024-01-07},
	booktitle = {2023 32nd {International} {Conference} on {Computer} {Communications} and {Networks} ({ICCCN})},
	author = {Taylor, Jacob Neil and Le, Ngoc Thuy and Baek, Joonsang and Susilo, Willy},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 32nd International Conference on Computer Communications and Networks (ICCCN)},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\Q5UGV886\\10230146.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GRC6VIHB\\10230146.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G4TTARWM\\10230146.html:text/html},
}

@inproceedings{awasthi_innovative_2023,
	title = {Innovative {Approach} of {Data} {Encryption} {Algorithm} for {Securing} {Big} {Data}},
	url = {https://ieeexplore.ieee.org/document/10192702},
	doi = {10.1109/ICCES57224.2023.10192702},
	abstract = {Nowadays, a large amount of data that is growing exponentially is known as Big data. In other words, big data contains more variety and comes in big volume with more velocity. Traditional processing software cannot handle complex and big amounts of data, but this data addresses a lot of business problems, that could not be able to tackle before so to deal with this data, open-source software HDFS (Hadoop Distributed File System) is used as a cloud storage system. Big Data Security is a collective term used for all measures and tools that are used to guard data and analytics processes used for accessing the data. Big data is a joint term and tool for data and data management systems against attacks, thefts, or other malicious activities that could harm or negatively affect them so server and serverless systems both are major concerned with high security of data. The author’s research presents data security in the cloud computing environment in this paper. HDFS does not provide any scheme of encryption and decryption for securing the data, HDFS act as a storage medium that stores data in Avro format. Users use the encryption key and encryption zone to encrypt their data into HDFS.},
	urldate = {2024-01-07},
	booktitle = {2023 8th {International} {Conference} on {Communication} and {Electronics} {Systems} ({ICCES})},
	author = {Awasthi, Shivani and Kohli, Narendra},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 8th International Conference on Communication and Electronics Systems (ICCES)},
	pages = {528--533},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8YZ7QLIT\\10192702.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZMPQLBCD\\10192702.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2TEY3ZQB\\10192702.html:text/html},
}

@inproceedings{ang_legacy_2023,
	title = {Legacy {Moderization}: {A} {Cloud} {Migration} {Strategy} {With} {Serverless} {Microservice} {Architecture}},
	shorttitle = {Legacy {Moderization}},
	url = {https://ieeexplore.ieee.org/document/10302995},
	doi = {10.1109/IC-C57619.2023.00017},
	abstract = {This paper provides an overview, presents the work-in-progress study of how software architecture can be modernized using the latest technologies. We will discuss the reasons why software modernization is required, how to choose an optimal solution from the architecture perspective, related migration strategy and the related points to note. The paper will also discuss some of the common issues faced while reviewing the architecture. These issues would provide some insights to some frequently faced problems in development and deployment. Finally, a discussion on how containerization technology and load balancer could help in resolving some of the common issues faced. In this paper, we have evaluated our proposed approach in a realistic case study involving an actual organization, which was anonymized for identity protection A certain organization would be used to provide the corresponding review mentioned.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Intelligent} {Computing} and {Control} ({IC}\&{C})},
	author = {Ang, Qi Zhi and Yau, Peter ChunYu and Sum, Chin Sean and Cao, Qi and Wong, Dennis},
	month = feb,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Intelligent Computing and Control (IC\&C)},
	pages = {59--63},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\N2IVZJP3\\10302995.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CIJL7UPS\\10302995.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\AMGMVDHQ\\10302995.html:text/html},
}

@article{lakhan_its_2023,
	title = {{ITS} {Based} on {Deep} {Graph} {Convolutional} {Fraud} {Detection} {Network} {Blockchain}-{Enabled} {Fog}-{Cloud}},
	volume = {24},
	issn = {1558-0016},
	url = {https://ieeexplore.ieee.org/document/9714858},
	doi = {10.1109/TITS.2022.3147852},
	abstract = {The advancement in transport applications increases at the everyday progress in technologies. Therefore, intelligent transport systems (ITS) gain a lot of progress at the different vehicle levels and in the vehicular area network. However, privacy and security at the network level are critical issues for ITS applications in the existing mechanism. In this paper, the study devises the cost-efficient and secure Serverless Blockchain Enable Task Scheduling (SBETS) ITS system and algorithm framework. The main goal is to reduce processing and security blockchian costs for ITS applications in the system. The processing cost minimizes based on the new proposed function-based price model and secures the data by a suggested deep graph convolutional neural network scheme in the network. The simulation results show that SBETS outperformed all existing ITS systems and minimized processing costs by 10\% and fraud detection issues by 50\% for transport applications.},
	number = {8},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Lakhan, Abdullah and Mohammed, Mazin Abed and Ibrahim, Dheyaa Ahmed and Kadry, Seifedine and Abdulkareem, Karrar Hameed},
	month = aug,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	pages = {8399--8408},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JY3D3L3C\\9714858.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H5IZ8EGA\\9714858.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CLNTYZWP\\9714858.html:text/html},
}

@inproceedings{baskara_load_2023,
	title = {Load {Testing} and {Cost} {Analysis} of {Deployment} {Environments} of a {Backend} {System} in an {Electronic} {Election} {Recapitulation} {System} {Application}},
	isbn = {2640-0227},
	url = {https://ieeexplore.ieee.org/document/10291494},
	doi = {10.1109/ICoDSE59534.2023.10291494},
	abstract = {Based on a statement by the General Election Commission of the Republic of Indonesia, 190.022.169 voters will vote in the 2024 election. With a very large number of voters and TPS, this manual recapitulation process raises several problems. As an effort to overcome this problem, an electronic election recapitulation system application is raised to become one of the solutions to be used at the 2024 election. An important part of this application is a scalable backend system. This paper aims to provide a design and also build a backend system that is able to serve every request well. Aspects that are considered in this application are database schemes and flow and implementation at the application level, and application deployment environment to support application performance in dealing with large numbers of requests, but mainly focuses on the aspects of load testing and cost analysis of the deployment environments. The solution that was successfully developed is an application using the Golang programming language connected to the PostgreSQL database and deployed both server-based and serverless Kubernetes on Alibaba Cloud and Google Cloud. After the system has been built, load testing is carried out to test the system's ability to deal with many requests at the same time. The results of load testing show that the system has been able to serve peak requests similar to what will be encountered on election day or regional elections in 2024 with variations in results for each different development environment.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Data} and {Software} {Engineering} ({ICoDSE})},
	author = {Baskara, Gregorius Dimas and Perdana, Riza Satria and Akbar, Saiful},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Data and Software Engineering (ICoDSE)},
	pages = {162--167},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZAGVS3FP\\10291494.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2WVJFDEX\\10291494.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4ING6D7E\\10291494.html:text/html},
}

@article{liu_machine_2023,
	title = {Machine {Learning}-{Based} {Non}-{Intrusive} {Digital} {Forensic} {Service} for {Smart} {Homes}},
	volume = {20},
	issn = {1932-4537},
	url = {https://ieeexplore.ieee.org/document/9964107},
	doi = {10.1109/TNSM.2022.3224863},
	abstract = {Security and privacy concerns keep growing with the successful development of Internet of Things (IoT) and the booming deployment of smart homes. IoT devices are utilized cooperatively to enable the interactions between home surroundings and users’ daily lives, containing forensically-valuable information about what happens in smart homes, which can help introduce digital forensics into smart homes to alleviate the growing concerns. However, current IoT devices, apps, and platforms usually do not provide built-in capabilities for digital forensics. To overcome this limitation, we propose a non-intrusive solution (i.e., bringing no modification to IoT devices, apps, and platforms) of digital forensic service to provide Forensics-as-a-Service (FaaS) for smart homes. First, it leverages side-channel analysis on sniffed network traffic to monitor commands, actions, and states of IoT devices. Then, it introduces provenance graphs (i.e., causal graphs) for smart home modeling to provide a holistic and overall explanation of smart homes. Machine learning (ML) techniques are applied to overcome the deficiency of a non-intrusive solution as it suffers from challenges in data collection and smart home modeling. Finally, it conducts forensic analysis based on scalable, reusable policies that are designed for graph-based smart home modeling. We implement a prototype of our forensic service and evaluate it in a real-world smart home. The evaluation results show that our forensic service can effectively collect forensic data for smart home modeling and conduct forensic analysis to explain security risks in smart homes.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Liu, Xuanyu and Fu, Xiao and Du, Xiaojiang and Luo, Bin and Guizani, Mohsen},
	month = jun,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	pages = {945--960},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D2RII3FN\\9964107.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VZYUBNXY\\9964107.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\DL4PGGSM\\9964107.html:text/html},
}

@inproceedings{zhang_maxwells_2023,
	title = {Maxwell’s {Demon} in {Tail}-tolerant, {Resource}-efficient {Serverless} {Computing}},
	isbn = {2690-5965},
	url = {https://ieeexplore.ieee.org/document/10077958},
	doi = {10.1109/ICPADS56603.2022.00104},
	abstract = {Computing systems always face a “resource allocation dilemma” that shows the great difficulties in trading off resource efficiency for tail latency, due to the internal uncertainty of cluster status and execution behavior. Inspired by the imaginary “Maxwell’s demon” in thermodynamics who can reduce the uncertainty through a per-gas molecule-level control policy, we consider the “one-to-one mapping” feature of serverless computing and build a novel resource allocator, named Maxwell, that can achieve low tail latency and high resource efficiency in serverless simultaneously. Like the “Maxwell’s demon Maxwell is able to optimize the resource allocation for every request. It observes the state of each request and makes decisions about the minimum resource allocation through a reinforcement learning predictor. As the per-request-grained control incurs significant overhead, we further design a pipeline for avoiding the accumulated effect on a workflow. Experimental results show that Maxwell not only saves up to 31\% CPU resources but also reduces the standard deviation of latency by 1.9×. Its time overhead is negligible and the resource overhead is also limited when the query per second {\textbackslash}leq500.},
	urldate = {2024-01-07},
	booktitle = {2022 {IEEE} 28th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Zhang, Huanyu and Huang, Wenhao and Zhao, Laiping and Li, Keqiu},
	month = jan,
	year = {2023},
	note = {Journal Abbreviation: 2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)},
	pages = {762--769},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WVGQCZLA\\10077958.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KKNKJW8L\\10077958.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XX2VPPJM\\10077958.html:text/html},
}

@inproceedings{wobuyaga_latency_2023,
	title = {Latency and {Reliability} {Aware} {SDN} {Controller}: {A} {Role} {Delegation} {Function} as a {Service}},
	shorttitle = {Latency and {Reliability} {Aware} {SDN} {Controller}},
	url = {https://ieeexplore.ieee.org/document/10099225},
	doi = {10.1109/CCWC57344.2023.10099225},
	abstract = {The emergency of machine type and ultra-reliable low latency communication is imposing stringent constraints for service provisioning. Addressing such constraints is challenging for network and cloud service providers. As a trending paradigm, software-defined networking (SDN) plays a significant role in future networks and services. However, the classical implementation of the SDN controller has limitations in-terms-of latency and reliability since the controller is decoupled from the forwarding device. Several research works have tried to tackle these challenges by proposing solutions such as Devoflow, DIFANE, and hierarchical and distributed controller deployment. Nonetheless, these approaches are not fully addressing these challenges. This paper tries to address the problem of latency and reliability by proposing a dynamic controller role delegation architecture for forwarding devices. To align with the microservice or multi-agent-based service-based architecture, the role delegation function as a service is proposed. The dynamic role delegation enables to predict and (pre-)installed flow rules in the forwarding devices based on various considerations such as network state, packet type, and service's stringent requirements. The proposed architecture is implemented and evaluated for latency and resiliency performance in comparison to the centralized and distributed deployment of the SDN controller. We used ComNetsEmu, a softwarized network emulation tool, to emulate SDN and NFV (Network Function Virtualization). The result indicated a significant decrease in latency and improved resilience in case of failure, yielding better network performance.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 13th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Wobuyaga, Dinah and Arzo, Sisay T and Kumar, Harsh and Granelli, Fabrizio and Devetsikiotis, Michael},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)},
	pages = {0205--0211},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A3J8Q7FG\\10099225.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YU9EDZD7\\10099225.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7KLPXLPW\\10099225.html:text/html},
}

@article{gill_least-privilege_2023,
	title = {Least-{Privilege} {Calls} to {Amazon} {Web} {Services}},
	volume = {20},
	issn = {1941-0018},
	url = {https://ieeexplore.ieee.org/document/9767574},
	doi = {10.1109/TDSC.2022.3171740},
	abstract = {We address least-privilege in a particular context of public cloud computing: calls to Amazon Web Services (AWS) Application Programming Interfaces (APIs). AWS is, by far, the largest cloud provider, and therefore an important context in which to consider the fundamental security design principle of least-privilege, which states that a thread of execution should possess only those privileges it needs. There have been reports of over-privilege being a root cause of attacks against AWS cloud applications, and a least-privilege set for an API call is a necessary building-block in devising a least-privilege policy for a cloud application. We observe that accurate information on a least-privilege set for an invoker of a method to possess is simply not available for most such methods in AWS. We provide a meaningful characterization of least-privilege in this context. We then propose techniques to determine such sets, and discuss a black-box process we have devised and carried out to identify such sets for all 707 API methods we are able to invoke across five AWS services. We discuss a number of interesting discoveries we have made, some of which are surprising and some alarming, that we have reported to AWS. Our work has resulted in a database of least-privilege sets for API calls to AWS, which we make available publicly. Developers can consult our database when configuring security policies for their cloud applications, and we welcome contributors that augment our database. Also, we discuss example uses of our database via an assessment of two repositories and two full-fledged serverless applications that are available publicly and have policies published alongside. We observe that the vast majority of policies are over-privileged. Our work contributes constructively to securing cloud applications in the largest cloud provider.},
	number = {3},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Gill, Puneet and Dietl, Werner and Tripunitara, Mahesh},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	pages = {2085--2096},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BXYQWKD7\\9767574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IZS87J35\\9767574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6MK2M43S\\9767574.html:text/html},
}

@article{vahidinia_mitigating_2023,
	title = {Mitigating {Cold} {Start} {Problem} in {Serverless} {Computing}: {A} {Reinforcement} {Learning} {Approach}},
	volume = {10},
	issn = {2327-4662},
	shorttitle = {Mitigating {Cold} {Start} {Problem} in {Serverless} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9749611},
	doi = {10.1109/JIOT.2022.3165127},
	abstract = {Serverless computing has revolutionized the world of cloud-based and event-driven applications with the introduction of Function as a Service (FaaS) as the latest cloud computing model. This computational model increases the level of abstraction from the infrastructure and breaks the program into small units called functions. Thus, it brings benefits, such as ease of development, saving resources, and reducing product launch time for enterprises and developers. Thanks to the scale-to-zero feature of this computational model, idle functions with no traffic will be depreciated from memory. However, this cost-saving approach adversely impacts delay leading to the cold start problem. Unfortunately, the existing solutions to alleviate the cold start delay are not resource efficient as they follow a fixed policy over time. Thereby, this article proposes a novel two-layer adaptive approach to tackle this issue. The first layer utilizes a holistic reinforcement learning algorithm to discover the function invocation patterns over time for determining the best time to keep the containers warm. The second layer is designed based on a long short-term memory (LSTM) to predict the function invocation times in the future to determine the required prewarmed containers. The experimental results on the Openwhisk platform show that the proposed approach reduces the memory consumption by 12.73\% and improves the execution invocations on prewarmed containers by 22.65\% compared to the Openwhisk platform.},
	number = {5},
	urldate = {2024-01-07},
	journal = {IEEE Internet of Things Journal},
	author = {Vahidinia, Parichehr and Farahani, Bahar and Aliee, Fereidoon Shams},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Cloud computing, Internet of Things, Computational modeling, Cold start delay, Containers, Costs, Delays, Function as a Service (FaaS), memory consumption, openwhisk, reinforcement learning, serverless computing, Serverless computing, serverless platforms},
	pages = {3917--3927},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NJICARBT\\9749611.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3YR863LX\\9749611.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7R9GR79D\\9749611.html:text/html},
}

@inproceedings{guerreiro_monitoring_2023,
	title = {Monitoring in {Function}-as-a-{Service} {Platforms}},
	isbn = {2166-0727},
	url = {https://ieeexplore.ieee.org/document/10212053},
	doi = {10.23919/CISTI58278.2023.10212053},
	abstract = {Functions deployed in FaaS platforms need to be monitored regarding resource consumption, errors, and application-specific metrics. Platforms like Google Cloud Functions or Azure Functions have dashboards and Web APIs that expose information about the execution of functions. However, the current approach collects data of general metrics about the function as an all and imposes a vendor-specific way for monitoring events. To the best of our knowledge, fine-grained function metrics are not available in FaaS platforms, e.g., the time that takes to execute only part of the function’s code.This work aims to explore how to build a system to provide fine-grained monitoring to FaaS platforms for developers. In this paper we present an architecture for this system, introduce the metrics library we are developing for FaaS platforms, and discuss some challenges.},
	urldate = {2024-01-07},
	booktitle = {2023 18th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Guerreiro, Beatriz and Freitas, Filipe and Simão, José},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 18th Iberian Conference on Information Systems and Technologies (CISTI)},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TLQTI7TB\\10212053.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UHRERQIN\\10212053.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7Y5WECZP\\10212053.html:text/html},
}

@inproceedings{ramesh_multi-objective_2023,
	title = {Multi-{Objective} {Workflow} {Scheduling} to {Serverless} {Architecture} in a {Multi}-{Cloud} {Environment}},
	isbn = {2694-0825},
	url = {https://ieeexplore.ieee.org/document/10305826},
	doi = {10.1109/IC2E59103.2023.00027},
	abstract = {Many complex workflows consist of multiple tasks represented as a directed acyclic graph (DAG). Optimal deployment of such workflows on a cloud using multiple services requires a judicious selection of compute and storage services to minimize the makespan and the cost of deployment. Multi-cloud deployment is emerging as a preferred choice for the deployment of complex workflows for price competitiveness and freedom from vendor lock-in. However, finding an optimal mapping scheme for heterogeneous tasks of a workflow in a multi-cloud environment is a challenge. Furthermore, each participating cloud service provider (CSP) has a unique cost model and maximum deliverable performance. This makes exploration of the optimal configuration of the chosen service daunting. Many algorithms, frameworks, and tools have been proposed to schedule complex workflows on cloud using virtual machines (VMs) available as Infrastructure-as-a-Service (IaaS). However, the use of scalable and cost-effective serverless platforms offered as Function-as-a-Service (FaaS) is still in its infancy.In this work, we use particle swarm optimization (PSO) for mapping complex workflows to cloud services such as computing, and storage in a multi-cloud scenario. We map complex workflows to the serverless platforms and storage services from popular cloud vendors namely Amazon Web Services (AWS), Azure (AZR), and Google Cloud Platform (GCP). The experimental evaluation shows that our approach results in up to 61\% improvement in makespan and 51\% improvement in the cost of workflow deployment as compared to naive and intuition-based mapping in a cloud.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Ramesh, Manju and Chahal, Dheeraj and Phalak, Chetan and Singhal, Rekha},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {173--183},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KMLT4JFZ\\10305826.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VNQJQG8G\\10305826.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\APPY9UVF\\10305826.html:text/html},
}

@inproceedings{lertpongrujikorn_object_2023,
	title = {Object as a {Service} ({OaaS}): {Enabling} {Object} {Abstraction} in {Serverless} {Clouds}},
	isbn = {2159-6190},
	shorttitle = {Object as a {Service} ({OaaS})},
	url = {https://ieeexplore.ieee.org/document/10254994},
	doi = {10.1109/CLOUD60044.2023.00035},
	abstract = {Function as a Service (FaaS) paradigm is becoming widespread and is envisioned as the next generation of cloud computing systems that mitigate the burden for programmers and cloud solution architects. However, the FaaS abstraction only makes the cloud resource management aspects transparent but does not deal with the application data aspects. As such, developers have to intervene and undergo the burden of managing the application data, often via separate cloud services (e.g., AWS S3). Similarly, the FaaS abstraction does not natively support function workflow, hence, the developers often have to work with workflow orchestration services (e.g., AWS Step Functions) to build workflows. Moreover, they have to explicitly navigate the data throughout the workflow. To overcome these inherent problems of FaaS, our hypothesis is to design a higher-level cloud programming abstraction that can hide the complexities and mitigate the burden of developing cloud-native application development. Accordingly, in this research, we borrow the notion of object from object-oriented programming and propose a new abstraction level atop the function abstraction, known as Object as a Service (OaaS). OaaS encapsulates the application data and function into the object abstraction and relieves the developers from resource and data management burdens. It also unlocks opportunities for built-in optimization features, such as software reusability, data locality, and caching. OaaS natively supports dataflow programming such that developers define a workflow of functions transparently without getting involved in data navigation, synchronization, and parallelism aspects. We implemented a prototype of the OaaS platform and evaluated it under real-world settings against state-of-the-art platforms regarding the imposed overhead, scalability, and ease of use. The results demonstrate that OaaS streamlines cloud programming and offers scalability with an insignificant overhead to the underlying cloud system.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Lertpongrujikorn, Pawissanutt and Salehi, Mohsen Amini},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)},
	pages = {238--248},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XHX8EVZ5\\10254994.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IS83SVBC\\10254994.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8FK8SJG7\\10254994.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\764343SY\\Lertpongrujikorn and Salehi - 2023 - Object as a Service (OaaS) Enabling Object Abstra.pdf:application/pdf},
}

@inproceedings{peri_orchestrating_2023,
	title = {Orchestrating the {Execution} of {Serverless} {Functions} in {Hybrid} {Clouds}},
	url = {https://ieeexplore.ieee.org/document/10336026},
	doi = {10.1109/ACSOS58161.2023.00032},
	abstract = {In recent years, serverless computing, especially Function as a Service (FaaS), is rapidly growing in popularity as a cloud programming model. The serverless computing model provides an intuitive interface for developing cloud-based applications, where the development and deployment of scalable microservices has become easier and cost-effective. An increasing number of batch-processing applications are deployed as pipelines that comprise a sequence of functions that must meet their deadline targets to be practical. In this paper, we present our Hybrid Cloud Scheduler (HCS) for orchestrating the execution of serverless batch-processing pipelines deployed over heterogeneous infrastructures. Our framework enables developers to (i) automatically schedule and execute batch-processing applications in heterogeneous environments such as the private edge and public cloud serverless infrastructures, (ii) benefit from cost reduction through the utilization of their own resources in a private cluster, and (iii) significantly improves the probability of meeting the deadline requirements of their applications. Our experimental evaluation demonstrates the efficiency and benefits of our approach.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Autonomic} {Computing} and {Self}-{Organizing} {Systems} ({ACSOS})},
	author = {Peri, Aristotelis and Tsenos, Michail and Kalogeraki, Vana},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)},
	pages = {139--144},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\H3BH8UYM\\10336026.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PQ4SNELF\\10336026.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PT8RMIWD\\10336026.html:text/html},
}

@inproceedings{olariu_overcoming_2023,
	title = {Overcoming {Challenges} in {Migrating} {Modular} {Monolith} from {On}-{Premises} to {AWS} {Cloud}},
	isbn = {2247-5443},
	url = {https://ieeexplore.ieee.org/document/10274946},
	doi = {10.1109/RoEduNet60162.2023.10274946},
	abstract = {Cloud computing has gained popularity for efficient data storage, processing, and application access. This study focuses on migrating Hermit Portal’s modular monolithic web application from on-premises to AWS cloud. The aim is to identify migration patterns, optimize costs, and consider project management constraints (e.g., time, cost, and performance). Considering the Cloud Computing models perspective, the study analyzed Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Serverless models, with a focus on Amazon Web Services (AWS). Initial “lift and shift” using IaaS is recommended, followed by a gradual adoption of PaaS components like AWS Elastic Beanstalk or AWS Lambda for simplified management and reduced infrastructure responsibilities. Additionally, we explored various cost optimization strategies specific to AWS. Utilizing Reserved Instances, optimizing Windows Server and SQL Server licensing, considering PostgreSQL as an alternative to SQL Server, and leveraging Linux-based apps and web servers on AWS, were found to be effective in reducing costs. Also, we realized during the research that conducting a comprehensive analysis within the AWS ecosystem and understanding AWS-specific features before migration is crucial. This avoids unexpected cost increases compared with on premises hosting and accounts for refactoring and acquiring AWS-specific expertise. In conclusion, based on our research, a phased approach with thorough analysis and AWS-Specific strategies is the key to successfully and cost-effectively migrating monolith applications to AWS Cloud. Choosing the right cloud computing model and implementing cost optimization techniques ensures a smooth transition and maximizes the benefits of AWS.},
	urldate = {2024-01-07},
	booktitle = {2023 22nd {RoEduNet} {Conference}: {Networking} in {Education} and {Research} ({RoEduNet})},
	author = {Olariu, Florin},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 22nd RoEduNet Conference: Networking in Education and Research (RoEduNet)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7PTRSDD3\\10274946.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QFZZESV2\\10274946.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\KIUIVYA7\\10274946.html:text/html},
}

@inproceedings{serth_feasibility_2023,
	title = {On the {Feasibility} of {Serverless} {Functions} in the {Context} of {Auto}-{Graders}},
	url = {https://ieeexplore.ieee.org/document/10295106},
	doi = {10.1109/GECon58119.2023.10295106},
	abstract = {Learners interested in acquiring fundamental programming skills may choose from a variety of different offers, including Massive Open Online Courses (MOOCs). Usually, these courses not only include lecture videos and multiple-choice quizzes, but also feature hands-on programming exercises, allowing learners to apply their newly acquired knowledge right away. Since solving these exercises requires access to a programming tool chain, most MOOCs embed their exercises in a web-based environment supplying necessary tools. One of these so-called auto-graders is CodeOcean, which allows learners to write and run code or receive automated feedback. While a web-based auto-grader lowers the entry barrier for learners to get started, providing sufficient resources for all code executions poses an additional challenge for the MOOC provider, especially during high-demand periods. Therefore, we evaluated serverless functions as offered by cloud computing providers for the use in auto-graders and conducted a Randomized Control Trial. Although serverless functions at first appear to be slower compared to our existing containerized execution of learners' code, they convinced with more constant execution times in high-demand periods.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 2nd {German} {Education} {Conference} ({GECon})},
	author = {Serth, Sebastian and Paß, Maximilian and Meinel, Christoph},
	month = aug,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 2nd German Education Conference (GECon)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\J8RW5FRQ\\10295106.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RWA6P56D\\10295106.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S9XEVBHS\\10295106.html:text/html},
}

@inproceedings{shang_online_2023,
	title = {Online {Container} {Scheduling} for {Data}-intensive {Applications} in {Serverless} {Edge} {Computing}},
	isbn = {2641-9874},
	url = {https://ieeexplore.ieee.org/document/10229034},
	doi = {10.1109/INFOCOM53939.2023.10229034},
	abstract = {Introducing the emerging serverless paradigm into edge computing could avoid over- and under-provisioning of limited edge resources and make complex edge resource management transparent to application developers, which largely facilitates the cost-effectiveness, portability, and short time-to-market of edge applications. However, the computation/data dispersion and device/network heterogeneity of edge environments prevent current serverless computing platforms from acclimating to the network edge. In this paper, we address such challenges by formulating a container placement and data flow routing problem, which fully considers the heterogeneity of edge networks and the overhead of operating serverless platforms on resource-limited edge servers. We design an online algorithm to solve the problem. We further show its local optimum for each arriving container and prove its theoretical guarantee to the optimal offline solution. We also conduct extensive simulations based on practical experiment results to show the advantages of the proposed algorithm over existing baselines.},
	urldate = {2024-01-07},
	booktitle = {{IEEE} {INFOCOM} 2023 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Shang, Xiaojun and Mao, Yingling and Liu, Yu and Huang, Yaodong and Liu, Zhenhua and Yang, Yuanyuan},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: IEEE INFOCOM 2023 - IEEE Conference on Computer Communications},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\D5F3WISU\\10229034.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7VEYJSZC\\10229034.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LNSQVYDR\\10229034.html:text/html},
}

@inproceedings{benedict_penalty-enabled_2023,
	title = {Penalty-{Enabled} {Serverless} {Architecture} for {Cloud}-based {Startup} {Solutions}},
	isbn = {2767-7788},
	url = {https://ieeexplore.ieee.org/document/10134026},
	doi = {10.1109/ICICT57646.2023.10134026},
	abstract = {The step to the success of startups is through overcoming competitors by adopting software innovations that improve businesses. Serverless computing model, recently, has intrigued a sizable number of startup professionals belonging to various sectors, including financial or IoT-enabled application developers. One of the main flaws is its heavy dependency on cloud providers, which can still result in hefty pricing to startups and stalling functions in applications. This article proposes a penaltyenabled serverless architecture for startups. The architecture can boost the economy of startups and can analyze the serverlessoriented cost-saving options in applications. The penalty-oriented approach could enable cloud architects, developers, and startups, to rethink the utilization of serverless functions; to gleam of with future innovations.},
	urldate = {2024-01-07},
	booktitle = {2023 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Benedict, Shajulin and Subair, Rubiya and Gupta, Tanya and S.P., Vedanta},
	month = apr,
	year = {2023},
	note = {Journal Abbreviation: 2023 International Conference on Inventive Computation Technologies (ICICT)},
	pages = {497--502},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GIDAR8N3\\10134026.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9F5RI9JE\\10134026.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PNL8VBE9\\10134026.html:text/html},
}

@inproceedings{shrestha_performance_2023,
	title = {Performance {Evaluation} and {Comparison} of {Microservices} and {Serverless} {Deployments} in {Cloud}},
	url = {https://ieeexplore.ieee.org/document/10349142},
	doi = {10.1109/SmartCloud58862.2023.00043},
	abstract = {Microservices and serverless are arguably the two most widely used architectures today for deploying applications in the cloud. With both these technologies, applications can take advantage of faster delivery, lightweight, scalable, and lower development and maintenance costs. However, there are ongoing debates concerning which of these two architectures to use for deploying a given application. This paper evaluates and compares them quantitatively in terms of their performance and cost, based on a use case study for an image processing application. The study was conducted by deploying the application using the two technologies in the two major cloud platforms, Amazon AWS and Google Cloud. Results showed that serverless perform better in terms of performance and cost, whereas microservices show superiority in terms of memory use.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 8th {International} {Conference} on {Smart} {Cloud} ({SmartCloud})},
	author = {Shrestha, Raju and Nisha, Beebu},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 8th International Conference on Smart Cloud (SmartCloud)},
	pages = {202--207},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5RV6PAKM\\10349142.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RZY826MH\\10349142.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\F2XD6G44\\10349142.html:text/html},
}

@article{mahmoudi_performance_2023,
	title = {Performance {Modeling} of {Metric}-{Based} {Serverless} {Computing} {Platforms}},
	volume = {11},
	issn = {2168-7161},
	url = {https://ieeexplore.ieee.org/document/9763051},
	doi = {10.1109/TCC.2022.3169619},
	abstract = {Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads. While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees. Besides, most serverless computing platforms still require developers’ input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback. In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments. In this work, we aim to develop analytical performance models for latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions. Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google). The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome. We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Mahmoudi, Nima and Khazaei, Hamzeh},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Cloud Computing},
	pages = {1899--1910},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QDWAXB4D\\9763051.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VBPYV3ZI\\9763051.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7LW3CY5F\\9763051.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\SJVLQIV4\\Mahmoudi and Khazaei - 2023 - Performance Modeling of Metric-Based Serverless Co.pdf:application/pdf},
}

@inproceedings{tomaras_prediction-driven_2023,
	title = {Prediction-driven resource provisioning for serverless container runtimes},
	url = {https://ieeexplore.ieee.org/document/10336036},
	doi = {10.1109/ACSOS58161.2023.00033},
	abstract = {In recent years Serverless Computing has emerged as a compelling cloud based model for the development of a wide range of data-intensive applications. However, rapid container provisioning introduces non-trivial challenges for FaaS cloud providers, as (i) real-world FaaS workloads may exhibit highly dynamic request patterns, (ii) applications have service-level objectives (SLOs) that must be met, and (iii) container provisioning can be a costly process. In this paper, we present SLOPE, a prediction framework for serverless FaaS platforms to address the aforementioned challenges. Specifically, it trains a neural network model that utilizes knowledge from past runs in order to estimate the number of instances required to satisfy the invocation rate requirements of the serverless applications. In cases that a priori knowledge is not available, SLOPE makes predictions using a graph edit distance approach to capture the similarities among serverless applications. Our experimental results illustrate the efficiency and benefits of our approach, which can reduce the operating costs by 66.25\% on average.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Autonomic} {Computing} and {Self}-{Organizing} {Systems} ({ACSOS})},
	author = {Tomaras, Dimitrios and Tsenos, Michail and Kalogeraki, Vana},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9TJGUU2Y\\10336036.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7CCMLVDE\\10336036.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7QEJNEQD\\10336036.html:text/html},
}

@inproceedings{sankaranarayanan_predicut_2023,
	title = {Predicut - {A} {Machine} {Learning} {Model} {For} {Online} {Prediction} of {Cut}-{In} {Manoeuvre} {For} {Autonomous} {Vehicles}},
	isbn = {2577-2465},
	url = {https://ieeexplore.ieee.org/document/10199227},
	doi = {10.1109/VTC2023-Spring57618.2023.10199227},
	abstract = {When a vehicle changes lane abruptly resulting in low headway distance to the vehicle behind it, it may trigger emergency braking or it may result in a collision. Such events are called cut-ins and they cause a majority of road accidents in the U.S. Autonomous Vehicles (AVs) have to predict such events in advance and react appropriately to such human behavior. We propose Predicut, a stacking classifier model that uses sensor data of an instrumented host vehicle and predicts if any of the neighboring vehicles would cut in front of it in the next 0.5 to 5.5 seconds. We have built and tested the model using the SPMD and NGSIM (I-80, US-101) driving datasets. The model was able to predict cut-ins with a maximum F1 score of 94.3\%. From the datasets, we determine that the model through its advance prediction, avoided up to 90\% of emergency braking scenarios. The model is highly performant with average inference time of 26ms when hosted on a cloud based serverless inference service.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 97th {Vehicular} {Technology} {Conference} ({VTC2023}-{Spring})},
	author = {Sankaranarayanan, Pandeeswari and Ramanujam, Arvind and Sathy, Sruthi and Jayaprakash, Rajesh},
	month = jun,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 97th Vehicular Technology Conference (VTC2023-Spring)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\4GPWKNKC\\10199227.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3A9LNMCI\\10199227.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HZWDTSJB\\10199227.html:text/html},
}

@inproceedings{garg_privflow_2023,
	title = {{PrivFlow}: {Secure} and {Privacy} {Preserving} {Serverless} {Workflows} on {Cloud}},
	shorttitle = {{PrivFlow}},
	url = {https://ieeexplore.ieee.org/document/10171483},
	doi = {10.1109/CCGrid57682.2023.00049},
	abstract = {The recent advancement of serverless computing in the widespread deployment of applications prompts the need to protect serverless workflows against cloud vulnerabilities and threats. We propose PrivFlow, a workflow-centric, privacy preserving framework to protect the information flow in serverless computing applications in semi-honest (S-PrivFlow) and malicious (M-PrivFlow) adversarial settings. An Authenticated Data Structure is used to store the valid workflows encoded in the proposed format. The validation of workflows is performed in a privacy preserving manner that leaks no sensitive information to any unauthorized user. We focus on the two most prevalent attacks on the serverless cloud platforms, namely the Denial-of-Wallet and Wrong Function Invocation attacks. We demonstrate that PrivFlow mitigates both of these attacks. Further, we evaluate PrivFlow on the popular benchmark application- Hello Retail, and a customized scaled application. Though the comparison with the state-of-the-art approaches in terms of the runtime performance shows a latency of 1.6 times for S-PrivFlow and 8 times for M-PrivFlow, the PrivFlow provides high security and privacy. PrivFlow acts as a wrapper to the application resulting in no change to the source code.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE}/{ACM} 23rd {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Garg, Surabhi and Dilip Thakur, Meena Singh and A, Rajan M and Maddali, Lakshmi Padmaja and Ramachandran, Vigneswaran},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	pages = {447--458},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WVHXVSNY\\10171483.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RVS9GGRX\\10171483.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6VXYEQ4J\\10171483.html:text/html},
}

@inproceedings{wu_qos-aware_2023,
	title = {{QoS}-{Aware} and {Cost}-{Efficient} {Dynamic} {Resource} {Allocation} for {Serverless} {ML} {Workflows}},
	isbn = {1530-2075},
	url = {https://ieeexplore.ieee.org/document/10177446},
	doi = {10.1109/IPDPS54959.2023.00093},
	abstract = {Machine Learning (ML) workflows are increasingly deployed on serverless computing platforms to benefit from their elasticity and fine-grain pricing. Proper resource allocation is crucial to achieve fast and cost-efficient execution of serverless ML workflows (specially for hyperparameter tuning and model training). Unfortunately, existing resource allocation methods are static, treat functions equally, and rely on offline prediction, which limit their efficiency. In this paper, we introduce CE-scaling – a Cost-Efficient autoscaling framework for serverless ML work-flows. During the hyperparameter tuning, CE-scaling partitions resources across stages according to their exact usage to minimize resource waste. Moreover, it incorporates an online prediction method to dynamically adjust resources during model training. We implement and evaluate CE-scaling on AWS Lambda using various ML models. Evaluation results show that compared to state-of-the-art static resource allocation methods, CE-scaling can reduce the job completion time and the monetary cost by up to 63\% and 41\% for hyperparameter tuning, respectively; and by up to 58\% and 38\% for model training.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Wu, Hao and Deng, Junxiao and Fan, Hao and Ibrahim, Shadi and Wu, Song and Jin, Hai},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	pages = {886--896},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K264ZLGQ\\10177446.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\RDF4Z5N9\\10177446.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LULUDGAS\\10177446.html:text/html},
}

@article{bartan_randomized_2023,
	title = {Randomized {Polar} {Codes} for {Anytime} {Distributed} {Machine} {Learning}},
	volume = {4},
	issn = {2641-8770},
	url = {https://ieeexplore.ieee.org/document/10239266},
	doi = {10.1109/JSAIT.2023.3310931},
	abstract = {We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.},
	urldate = {2024-01-07},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Bartan, Burak and Pilanci, Mert},
	year = {2023},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	pages = {393--404},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\75YP3UQZ\\10239266.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NPKW4ZL4\\10239266.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FCYE8P65\\10239266.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\76BK5U77\\Bartan and Pilanci - 2023 - Randomized Polar Codes for Anytime Distributed Mac.pdf:application/pdf},
}

@article{szalay_real-time_2023,
	title = {Real-{Time} {FaaS}: {Towards} a {Latency} {Bounded} {Serverless} {Cloud}},
	volume = {11},
	issn = {2168-7161},
	shorttitle = {Real-{Time} {FaaS}},
	url = {https://ieeexplore.ieee.org/document/9714028},
	doi = {10.1109/TCC.2022.3151469},
	abstract = {Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgradability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multi-node systems make real-time scheduling significantly complex since guaranteeing real-time task execution and communication is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm suitable for real-time FaaS platforms of multi-node clusters. We show that our task scheduling heuristics could outperform existing algorithms by 55\%. Furthermore, we propose three conceptual designs to enable the necessary real-time communications. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Szalay, Márk and Mátray, Péter and Toka, László},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Cloud Computing},
	pages = {1636--1650},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2GD5RUAN\\9714028.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XERBSAPD\\9714028.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G849EWUF\\9714028.html:text/html},
}

@inproceedings{gupta_reliable_2023,
	title = {Reliable {Transactions} in {Serverless}-{Edge} {Architecture}},
	isbn = {2375-026X},
	url = {https://ieeexplore.ieee.org/document/10184657},
	doi = {10.1109/ICDE55515.2023.00030},
	abstract = {Modern edge applications demand novel solutions where edge applications do not have to rely on a single cloud provider (which cannot be in the vicinity of every edge device) or dedicated edge servers (which cannot scale as clouds) for processing compute-intensive tasks. A recent computing philosophy, Sky computing, proposes giving each user ability to select between available cloud providers.In this paper, we present our serverless-edge co-design, which extends the Sky computing vision. In our serverless-edge co-design, we expect edge devices to collaborate and spawn required number of serverless functions. This raises several key challenges: (1) how will this collaboration take place, (2) what if some edge devices are compromised, and (3) what if a selected cloud provider is malicious. Hence, we design ServerlessBFT, the first protocol to guarantee Byzantine fault-tolerant (Bft) transactional flow between edge devices and serverless functions. We present an exhaustive list of attacks and their solutions on our serverless-edge co-design. Further, we extensively benchmark our architecture on a variety of parameters.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 39th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Gupta, Suyash and Rahnama, Sajjad and Linsenmayer, Erik and Nawab, Faisal and Sadoghi, Mohammad},
	month = apr,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 39th International Conference on Data Engineering (ICDE)},
	pages = {301--314},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GSKZ7GK8\\10184657.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G7Z6WJTW\\10184657.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PL3D4J8U\\10184657.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\DV8GU92P\\Gupta et al. - 2023 - Reliable Transactions in Serverless-Edge Architect.pdf:application/pdf},
}

@inproceedings{russo_serverledge_2023,
	title = {Serverledge: {Decentralized} {Function}-as-a-{Service} for the {Edge}-{Cloud} {Continuum}},
	isbn = {2474-249X},
	shorttitle = {Serverledge},
	url = {https://ieeexplore.ieee.org/document/10099372},
	doi = {10.1109/PERCOM56429.2023.10099372},
	abstract = {As the Function-as-a-Service (FaaS) paradigm enjoys growing popularity within Cloud-based systems, there is increasing interest in moving serverless functions towards the Edge, to better support geo-distributed and pervasive applications. However, enjoying both the reduced latency of Edge and the scalability of FaaS requires new architectures and implementations to cope with typical Edge challenges (e.g., nodes with limited computational capacity). While first solutions have been proposed for Edge-based FaaS, including light function sandboxing techniques, we lack a platform with the ability to span both Edge and Cloud and adaptively exploit both. In this paper, we present Serverledge, a FaaS platform designed for the Edge-to-Cloud continuum. Serverledge adopts a decentralized architecture, where function invocation requests can be fully served within Edge nodes. To cope with load peaks, Serverledge also supports vertical (i.e., from Edge to Cloud) and horizontal (i.e., among Edge nodes) computation offloading. Our evaluation shows that Serverledge outperforms Apache OpenWhisk in an Edge-like scenario and has competitive performance with state-of-the-art frameworks optimized for the Edge, with the advantage of built-in support for vertical and horizontal offloading.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} ({PerCom})},
	author = {Russo, Gabriele Russo and Mannucci, Tiziana and Cardellini, Valeria and Presti, Francesco Lo},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Pervasive Computing and Communications (PerCom)},
	pages = {131--140},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UK3LJGGM\\10099372.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WDU6DQPK\\10099372.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\YP43CWZP\\10099372.html:text/html},
}

@inproceedings{dey_serverless_2023,
	title = {Serverless {Computing}: {Architectural} {Paradigms}, {Challenges}, and {Future} {Directions} in {Cloud} {Technology}},
	isbn = {2768-0673},
	shorttitle = {Serverless {Computing}},
	url = {https://ieeexplore.ieee.org/document/10290253},
	doi = {10.1109/I-SMAC58438.2023.10290253},
	abstract = {The concept of serverless computing has emerged as a significant paradigm shift in cloud technology, fundamentally transforming the processes involved in application development and deployment. This research paper offers a complete examination of serverless computing, focusing on its architectural principles, identifying the primary issues it poses, and exploring the potential future developments in this fast-progressing domain. The article begins by providing a comprehensive explanation of the basic ideas behind serverless computing. It places particular emphasis on its event-driven nature, scalability, and the paradigm change from infrastructure management to code-centric development. This study examines the architectural paradigms of serverless computing, specifically focusing on Function as a Service (FaaS) and Backend as a Service (BaaS). It does a comparative analysis of these paradigms, considering their respective use cases and benefits. This study not only discusses the existing issues in serverless computing but also presents potential future possibilities in the field. The paper investigates current developments in the field, including the utilization of serverless orchestration, the integration of hybrid cloud systems, and the advancement of tools for serverless development. Moreover, this study explores the possible ramifications of serverless technology on edge computing, Internet of Things (IoT), and machine learning applications. This research paper is a great resource for practitioners, academics, and organizations in the field of cloud technology that are interested in leveraging the advantages of serverless computing. This study amalgamates existing information, discerns areas of research that need further investigation, and provides valuable perspectives on the dynamic development of serverless technology. Consequently, it facilitates a more knowledgeable and inventive approach to the field of cloud computing.},
	urldate = {2024-01-07},
	booktitle = {2023 7th {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC})},
	author = {Dey, Niladri Sekhar and Reddy, Sana Pavan Kumar and G, Lavanya},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)},
	keywords = {Serverless computing, Function as a service, Internet of things, Application development, Architectural paradigm, Baa, Challenge, Cloud technologies, Future direction, Paradigm shifts, Research papers},
	pages = {406--414},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3C2SDMGE\\10290253.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NEBAQFE3\\10290253.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VGBQK383\\10290253.html:text/html},
}

@article{li_serverless_2023,
	title = {Serverless {Computing}: {State}-of-the-{Art}, {Challenges} and {Opportunities}},
	volume = {16},
	issn = {1939-1374},
	shorttitle = {Serverless {Computing}},
	url = {https://ieeexplore.ieee.org/document/9756233},
	doi = {10.1109/TSC.2022.3166553},
	abstract = {Serverless computing is growing in popularity by virtue of its lightweight and simplicity of management. It achieves these merits by reducing the granularity of the computing unit to the function level. Specifically, serverless allows users to focus squarely on the function itself while leaving other cumbersome management and scheduling issues to the platform provider, who is responsible for striking a balance between high-performance scheduling and low resource cost. In this article, we conduct a comprehensive survey of serverless computing with a particular focus on its infrastructure characteristics. Whereby some existing challenges are identified, and the associated cutting-edge solutions are analyzed. With these results, we further investigate some typical open-source frameworks and study how they address the identified challenges. Given the great advantages of serverless computing, it is expected that its deployment would dominate future cloud platforms. As such, we also envision some promising research opportunities that need to be further explored in the future. We hope that our work in this article can inspire those researchers and practitioners who are engaged in related fields to appreciate serverless computing, thereby setting foot in this promising area and making great contributions to its development.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Services Computing},
	author = {Li, Yongkang and Lin, Yanying and Wang, Yang and Ye, Kejiang and Xu, Chengzhong},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Services Computing},
	pages = {1522--1539},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FK67EHJ2\\9756233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WYHS559P\\9756233.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GAYF6GGD\\9756233.html:text/html},
}

@inproceedings{govindarajan_resource_2023,
	title = {Resource {Management} in {Serverless} {Computing} - {Review}, {Research} {Challenges}, and {Prospects}},
	url = {https://ieeexplore.ieee.org/document/10249574},
	doi = {10.1109/ICoAC59537.2023.10249574},
	abstract = {Serverless Computing has become one of the major trends for running cloud applications. The main advantage of serverless computing is that the infrastructure is being completely taken care of by the providers. Users are spared the burden and the complexity of managing the infrastructure. In serverless computing, cloud applications become for the users a collection of multiple cloud functions. Even though the infrastructure is completely under the control of providers, the resource management is very complex in nature and it is essential to take both the users’ and the consumers’ service requirements into consideration to achieve quality of service (QoS). In this research paper, we conduct a literature review on serverless computing, on various opensource frameworks supporting serverless computing, and on complexities involved in the management of serverless computing platform. The paper ends with a proposal for a rankbased cloud function scheduler for the allocation of cloud functions in the serverless computing environment. Based on the existing works and the preliminary experimentation, we are evident that the proposed mechanism will impact the performance metrics of throughput and the success rate of cloud functions execution.},
	urldate = {2024-01-07},
	booktitle = {2023 12th {International} {Conference} on {Advanced} {Computing} ({ICoAC})},
	author = {Govindarajan, Kannan and Tienne, Andrè De},
	month = aug,
	year = {2023},
	note = {Journal Abbreviation: 2023 12th International Conference on Advanced Computing (ICoAC)},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5W8JXTJS\\10249574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SW2ENHCT\\10249574.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SR2SHAXA\\10249574.html:text/html},
}

@article{lakhan_restricted_2023,
	title = {Restricted {Boltzmann} {Machine} {Assisted} {Secure} {Serverless} {Edge} {System} for {Internet} of {Medical} {Things}},
	volume = {27},
	issn = {2168-2208},
	url = {https://ieeexplore.ieee.org/document/9784882},
	doi = {10.1109/JBHI.2022.3178660},
	abstract = {The Internet of things (IoT) is a network of technologies that support a wide variety of healthcare workflow applications to facilitate users’ obtaining real-time healthcare services. Many patients and doctors’ hospitals use different healthcare services to monitor their healthcare and save their records on the servers. Healthcare sensors are widely linked to the outside world for different disease classifications and questions. These applications are extraordinarily dynamic and use mobile devices to roam several locales. However, healthcare apps confront two significant challenges: data privacy and the cost of application execution services. This work presents the mobility-aware security dynamic service composition (MSDSC) algorithmic framework for workflow healthcare based on serverless, serverless, and restricted Boltzmann machine mechanisms. The study suggests the stochastic deep neural network trains probabilistic models at each phase of the process, including service composition, task sequencing, security, and scheduling. The experimental setup and findings revealed that the developed system-based methods outperform traditional methods by 25\% in terms of safety and 35\% in application cost.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Lakhan, Abdullah and Mohammed, Mazin Abed and Rashid, Ahmed N. and Kadry, Seifedine and Abdulkareem, Karrar Hameed and Nedoma, Jan and Martinek, Radek and Razzak, Imran},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	pages = {673--683},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\IC7D2F56\\9784882.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2HHQZ78A\\9784882.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7V83PYER\\9784882.html:text/html},
}

@inproceedings{garde_serverless_2023,
	title = {Serverless {Data} {Protection} in {Cloud}},
	isbn = {2832-143X},
	url = {https://ieeexplore.ieee.org/document/10112206},
	doi = {10.1109/ISCON57294.2023.10112206},
	abstract = {Cloud computing has already gained vast momentum in various fields like healthcare, finance, defense, education, and businesses. With the increasing amount of data generated, most firms have migrated to cloud platforms to reduce operational overhead and infrastructural costs. Corporate data in the cloud is often vulnerable to data breaches and losses. Hence, protecting the data stored on cloud-native assets is quintessential. Over the years, several approaches to disaster recovery have been proposed and most of the solutions focus on a server-based architecture. With the advent of cloud computing, serverless computing is emerging at an exponential rate considering the benefits it offers. Serverless computing lets users focus on developing their business logic rather than managing server configurations. In contrast to a server-based architecture for data protection; a serverless architecture is cost-efficient as it works on a pay-as- you-go model. This paper proposes a microservices-based approach to protect cloud-native assets using a serverless framework. In this approach, cloud assets are first discovered and their snapshot-based backup is taken. The backed-up assets can be later used to recover the asset in case of any disaster or data loss. Further, we also elaborate on the methodology of building a data protection application with Boto3 (Amazon Web Services - Software development Kit for Python) and integrating it with various Amazon Web Services. The proposed serverless and microservices-based approach offers high scalability, reliability, availability with time, and cost-efficiency.},
	urldate = {2024-01-07},
	booktitle = {2023 6th {International} {Conference} on {Information} {Systems} and {Computer} {Networks} ({ISCON})},
	author = {Garde, Anaya and Gandhale, Siddhi and Dharankar, Rutuja and Sangtani, Bhagya Shri and Deshmukh, Nutan and Deshpande, Shilpa and Rathi, Rahul and Srivastava, Astitva},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 6th International Conference on Information Systems and Computer Networks (ISCON)},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5H5Y8GKZ\\10112206.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9WTYKYD7\\10112206.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UYJTV8H5\\10112206.html:text/html},
}

@article{yu_serpens_2023,
	title = {Serpens: {A} {High} {Performance} {FaaS} {Platform} for {Network} {Functions}},
	volume = {34},
	issn = {1558-2183},
	shorttitle = {Serpens},
	url = {https://ieeexplore.ieee.org/document/10089180},
	doi = {10.1109/TPDS.2023.3263272},
	abstract = {More and more enterprises deploy applications on Function-as-a-Service (FaaS) platforms to improve resource efficiency and save monetary costs. Network Functions (NFs) suffer from staggered peaks of traffic patterns and could benefit from fine-grained resource multiplexing in FaaS platform. However, naively exploring existing FaaS platforms to support NFs can introduce significant performance overheads in three aspects, including slow instance startup, remote state access for NFs, and costly packet delivery between NFs. To address these problems, we propose {\textbackslash}sf SerpensSerpens, a high performance FaaS platform for NFs. First, {\textbackslash}sf SerpensSerpens proposes a reusable NF runtime design to slash instance startup overhead. Second, {\textbackslash}sf SerpensSerpens designs a novel state management mechanism to support local state access. Third, {\textbackslash}sf SerpensSerpens introduces an advanced service chaining approach to avoid extra packet delivery. Besides, {\textbackslash}sf SerpensSerpens designs an NF scaling mechanism to minimize performance fluctuation. We have implemented a prototype of {\textbackslash}sf SerpensSerpens and conducted comprehensive experiments. Compared with the NFs and Service Function Chains (SFCs) that run on existing FaaS platforms, {\textbackslash}sf SerpensSerpens can improve the throughput by more than 10× and reduce the latency by more than 90\%.},
	number = {8},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yu, Heng and Zhang, Han and Shen, Junxian and Geng, Yantao and Wang, Jilong and Miao, Congcong and Xu, Mingwei},
	month = aug,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {2448--2463},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BZU6VWQ5\\10089180.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\NF3HVZBM\\10089180.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8EADLTMJ\\10089180.html:text/html},
}

@inproceedings{m_secure_2023,
	title = {Secure {Automated} {Inventory} {Management} system using {Abstracted} {System} {Design} {Microservice} {Architecture}},
	isbn = {2473-7674},
	url = {https://ieeexplore.ieee.org/document/10306924},
	doi = {10.1109/ICCCNT56998.2023.10306924},
	abstract = {Managing inventory has forever been a difficult problem for e-commerce sellers. Even today manual labor is used for the major part of the job. Without much automation in this industry growth has been stagnant because of the same reason.The proposed system works on a microservice-based architecture that enables automated inventory management with the help of a social-media messaging bot in an abstract manner. The bot comprises an integrated hybrid recommender system at the customer’s end for better recommendations based on image and textual data. The entire system is deployed on the cloud following a serverless architecture for resilient service. The proposed solution plays a key role in taking research forward in the direction of micro-service-based system design involving real-world data. The results of the experiment prove to be beneficial for customers and retailers since they are now connected using the micro-service.},
	urldate = {2024-01-07},
	booktitle = {2023 14th {International} {Conference} on {Computing} {Communication} and {Networking} {Technologies} ({ICCCNT})},
	author = {M, Darshan and Raswanth, S.R and Chaitanya, V.S.S.K and Kalavalapalli, Venkata Sridhar Sai and Kumar, Priyanka and Srivastava, Gautam},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5SJ7CEXU\\10306924.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\LGTKPVFY\\10306924.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EG5SL37M\\10306924.html:text/html},
}

@inproceedings{russo_serverless_2023,
	title = {Serverless {Functions} in the {Cloud}-{Edge} {Continuum}: {Challenges} and {Opportunities}},
	isbn = {2377-5750},
	shorttitle = {Serverless {Functions} in the {Cloud}-{Edge} {Continuum}},
	url = {https://ieeexplore.ieee.org/document/10136957},
	doi = {10.1109/PDP59025.2023.00056},
	abstract = {The Function-as-a-Service (FaaS) paradigm is increasingly adopted for the development of Cloud-native applications, which especially benefit from the seamless scalability and attractive pricing models of serverless deployments. With the continuous emergence of latency-sensitive applications and services, including Internet-of-Things and augmented reality, it is now natural to wonder whether and how the FaaS paradigm can be efficiently exploited in the Cloud-Edge Continuum, where serverless functions may benefit from reduced network delay between their invoking users and the FaaS platform. In this paper, we illustrate the key challenges that must be faced to effectively deploy serverless functions in the Cloud-Edge Continuum and review recent contributions proposed by the research community towards overcoming those challenges. We also discuss the key issues that currently remain unsolved and highlight a few research opportunities for better support of FaaS in the Compute Continuum.},
	urldate = {2024-01-07},
	booktitle = {2023 31st {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	author = {Russo, Gabriele Russo and Cardellini, Valeria and Presti, Francesco Lo},
	month = mar,
	year = {2023},
	note = {Journal Abbreviation: 2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
	keywords = {Serverless, edge computing, Edge computing, Service platforms, Augmented reality, compute continuum, Compute continuum, Key Issues, Network-delay, Pricing models, Research communities, Sensitive application, Service paradigm},
	pages = {321--328},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TEM8I7PX\\10136957.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\EJSXJSF3\\10136957.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\93Z53FBW\\10136957.html:text/html},
}

@inproceedings{busto_serverless_2023,
	title = {Serverless {Web} {Application} for {The} {Life} {Cycle} of {Software} {Development} {Projects} using {Scrum} in {South} {America}},
	url = {https://ieeexplore.ieee.org/document/10153878},
	doi = {10.1109/APCT58752.2023.00008},
	abstract = {Serverless models are one of the latest architecture models provided by Cloud vendors such as AWS and Microsoft; we are exploring serverless applications to develop a progressive web application that will help future developers and project leaders to manage their projects better by having a tool that will help incorporate Scrum properly into their projects. As seen in the investigation done, we can see that South America is below the global average in completing projects on time or within the planned budget. In this work we explain the agile life cycle of the web application developed, focusing on the Scrum aspects of the tool design, the serverless architecture of the app and its following development, while also applying an analysis of the current projects to measure the level of effectiveness that a proper Scrum method can have on finishing a project correctly on time. With our initial surveys landing a score of 80 in the positive version of the System Usability Scale and getting good results in the completion of our main quality metrics, we can see that there is indeed an interest in finding a tool that helps use Scrum properly by adding video calls as a mean of direct communication between users.},
	urldate = {2024-01-07},
	booktitle = {2023 2nd {Asia}-{Pacific} {Computer} {Technologies} {Conference} ({APCT})},
	author = {Busto, Pablo Josué Francia Del and Tambra, Rodson Vladimir Ayme and Moroco, Juan Antonio Flores},
	month = jan,
	year = {2023},
	note = {Journal Abbreviation: 2023 2nd Asia-Pacific Computer Technologies Conference (APCT)},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5NAWPS4D\\10153878.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\259IE955\\10153878.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JVUR5IDZ\\10153878.html:text/html},
}

@inproceedings{bandopadhyay_speech_2023,
	title = {Speech {Recognition} and {Neural} {Networks} based {Talking} {Health} {Care} {Bot} ({THCB}): {Medibot}},
	shorttitle = {Speech {Recognition} and {Neural} {Networks} based {Talking} {Health} {Care} {Bot} ({THCB})},
	url = {https://ieeexplore.ieee.org/document/10084191},
	doi = {10.1109/ICCMC56507.2023.10084191},
	abstract = {The COVID-19 pandemic has affected healthcare in several ways. Some patients were unable to make it to appointments due to curfews, transportation restrictions, and stay-at-home directives, while less urgent procedures were postponed or cancelled. Others steered clear of hospitals out of fear of contracting an infection. With the use of a conversational artificial intelligence-based program, the Talking Health Care Bot (THCB) could be useful during the pandemic by allowing patients to receive supportive care without physically visiting a hospital. Therefore, the THCB will drastically and quickly change in-person care to patient consultation through the internet. To give patients free primary healthcare and to narrow the supply-demand gap for human healthcare professionals, this work created a conversational bot based on artificial intelligence and machine learning. The study proposes a revolutionary computer program that serves as a patient's personal virtual doctor. The program was carefully created and thoroughly trained to communicate with patients as if they were real people. Based on a serverless architecture, this application predicts the disease based on the symptoms of the patients. A Talking Healthcare chatbot confronts several challenges, but the user's accent is by far the most challenging. This study has then evaluated the proposed model by using one hundred different voices and symptoms, achieving an accuracy rate of 77\%.},
	urldate = {2024-01-07},
	booktitle = {2023 7th {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	author = {Bandopadhyay, Dwaipayan and Ghosh, Rajdeep and Chatterjee, Rajdeep and Das, Nabanita and Sadhukhan, Bikash},
	month = feb,
	year = {2023},
	note = {Journal Abbreviation: 2023 7th International Conference on Computing Methodologies and Communication (ICCMC)},
	pages = {399--404},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8CMQ48YF\\10084191.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BYSXRGIL\\10084191.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\JP5NQ4D5\\10084191.html:text/html},
}

@inproceedings{barrak_spirt_2023,
	title = {{SPIRT}: {A} {Fault}-{Tolerant} and {Reliable} {Peer}-to-{Peer} {Serverless} {ML} {Training} {Architecture}},
	isbn = {2693-9177},
	shorttitle = {{SPIRT}},
	url = {https://ieeexplore.ieee.org/document/10366723},
	doi = {10.1109/QRS60937.2023.00069},
	abstract = {The advent of serverless computing has ushered in notable advancements in distributed machine learning, particularly within parameter server-based architectures. Yet, the integration of serverless features within peer-to-peer (P2P) distributed networks remains largely uncharted. In this paper, we introduce SPIRT, a fault-tolerant, reliable, scalable and secure serverless P2P ML training architecture. designed to bridge this existing gap. Capitalizing on the inherent robustness and reliability innate to P2P systems, we emphasized Intra-peer scalability for concurrent gradient to mitigate communication overhead from increased peer interactions. SPIRT, employs RedisAI for in-database operations, achieves an 82\% reduction in model update times. This architecture showcases resilience against peer failures and adeptly manages the integration of new peers. Furthermore, SPIRT ensures secure communication between peers, enhancing the reliability of distributed machine learning tasks. Even in the face of Byzantine attacks, the system’s robust aggregation algorithms maintain high levels of accuracy. These findings illuminate the promising potential of serverless architectures in P2P distributed machine learning, offering a significant stride towards the development of more efficient, scalable, and resilient applications.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 23rd {International} {Conference} on {Software} {Quality}, {Reliability}, and {Security} ({QRS})},
	author = {Barrak, Amine and Jaziri, Mayssa and Trabelsi, Ranim and Jaafar, Fehmi and Petrillo, Fabio},
	month = oct,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)},
	pages = {650--661},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\B3LWMGAW\\10366723.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9ZE48VVL\\10366723.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\QT5745QH\\10366723.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\8UAKWYE5\\Barrak et al. - 2023 - SPIRT A Fault-Tolerant and Reliable Peer-to-Peer .pdf:application/pdf},
}

@inproceedings{muliarevych_cloud-based_2023,
	title = {The {Cloud}-{Based} {Optimization} for {Automated} {Warehouse} {Design}},
	volume = {1},
	isbn = {2770-4254},
	url = {https://ieeexplore.ieee.org/document/10348891},
	doi = {10.1109/IDAACS58523.2023.10348891},
	abstract = {The paper delves into the process of warehouse design and the methodologies applicable for creating an automated warehouse design system. Focusing on the calculation of entry and shipment zones, this study highlights the challenges faced in seeking optimal design solutions, given the numerous combinations generated from input data, parameters, and constraints. The presented a two-phase design approach, which forms the basis of demonstrating automated warehouse design system. This system incorporates cutting-edge cloud technology advancements, such as predictive modeling and serverless computing, to enhance performance. The article compares the performance of various cloud computing architectures, including monolithic, horizontally auto-scalable microservices, the suggested approach grounded in lightweight context-based functional processing.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 12th {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Muliarevych, Oleksandr},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)},
	pages = {380--384},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8QXKWWHV\\10348891.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\5A9FJLDM\\10348891.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VLE5KD29\\10348891.html:text/html},
}

@inproceedings{nguyen_storm-rts_2023,
	title = {Storm-{RTS}: {Stream} {Processing} with {Stable} {Performance} for {Multi}-{Cloud} and {Cloud}-edge},
	isbn = {2159-6190},
	shorttitle = {Storm-{RTS}},
	url = {https://ieeexplore.ieee.org/document/10254965},
	doi = {10.1109/CLOUD60044.2023.00015},
	abstract = {Stream Processing Engines (SPEs) traditionally de-ploy applications on a set of shared workers (e.g., threads, processes, or containers) requiring complex performance man-agement by SPEs and application developers. We explore a new approach that replaces workers with Rate-based Abstract Ma-chines (RBAMs). This allows SPEs to translate stream operations into FaaS invocations, and exploit guaranteed invocation rates to manage performance. This approach enables SPE applications to achieve transparent and predictable performance. We realize the approach in the Storm-RTS system. Exploring 36 stream processing scenarios over 5 different hardware config-urations, we demonstrate several key advantages. First, Storm-RTS provides stable application performance and can enable flexible reconfiguration across cloud resource configurations. Sec-ond, SPEs built on RBAM can be resource-efficient and scalable. Finally, Storm-RTS allows the stream-processing paradigm to be extended from the cloud to the edge, using its performance stability to hide edge heterogeneity and resource competition. An experiment with 4 cloud and edge sites over 300 cores shows how Storm-RTS can support flexible reconfiguration and simple high-level declarative policies that optimize resource cost or other criteria.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Nguyen, Hai Duc and Chien, Andrew A.},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)},
	pages = {45--57},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HC932F2M\\10254965.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MKSNI9V5\\10254965.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\CL684T8F\\10254965.html:text/html},
}

@article{mars_jaseci_2023,
	title = {The {Jaseci} {Programming} {Paradigm} and {Runtime} {Stack}: {Building} {Scale}-{Out} {Production} {Applications} {Easy} and {Fast}},
	volume = {22},
	issn = {1556-6064},
	shorttitle = {The {Jaseci} {Programming} {Paradigm} and {Runtime} {Stack}},
	url = {https://ieeexplore.ieee.org/document/10129141},
	doi = {10.1109/LCA.2023.3274038},
	abstract = {Today's production scale-out applications include many sub-application components, such as storage backends, logging infrastructure and AI models. These components have drastically different characteristics, are required to work in collaboration, and interface with each other as microservices. This leads to increasingly high complexity in developing, optimizing, configuring, and deploying scale-out applications, raising the barrier to entry for most individuals and small teams. We developed a novel co-designed runtime system, Jaseci, and programming language, Jac, which aims to reduce this complexity. The key design principle throughout Jaseci's design is to raise the level of abstraction by moving as much of the scale-out data management, microservice componentization, and live update complexity into the runtime stack to be automated and optimized automatically. We use real-world AI applications to demonstrate Jaseci's benefit for application performance and developer productivity.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Computer Architecture Letters},
	author = {Mars, Jason and Kang, Yiping and Daynauth, Roland and Li, Baichuan and Mahendra, Ashish and Flautner, Krisztian and Tang, Lingjia},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Computer Architecture Letters},
	pages = {101--104},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WNDU3LRX\\10129141.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\HZWI6HD7\\10129141.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\UC9JK6N3\\10129141.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\42GQ6XRA\\Mars et al. - 2023 - The Jaseci Programming Paradigm and Runtime Stack.pdf:application/pdf},
}

@inproceedings{gu_time_2023,
	title = {Time and {Cost}-{Efficient} {Cloud} {Data} {Transmission} based on {Serverless} {Computing} {Compression}},
	isbn = {2641-9874},
	url = {https://ieeexplore.ieee.org/document/10229090},
	doi = {10.1109/INFOCOM53939.2023.10229090},
	abstract = {Nowadays, there exists a lot of cross-region data transmission demand on cloud. It is promising to use serverless computing for compressing data to save the transmission data amount. However, it is challenging to estimate the data transmission time and monetary cost with serverless compression. In addition, minimizing the data transmission cost is non-trivial due to enormous parameter space and joint optimization. This paper focuses on this problem and makes the following contributions: (1) We propose empirical data transmission time and monetary cost models based on serverless compression. (2) For single-task cloud data transmission, we propose two efficient parameter search methods based on Sequential Quadratic Programming (SQP ) and Eliminate then Divide and Conquer (EDC), which are theoretically proven with error upper bounds. (3) Furthermore, for multi-task cloud data transmission, a parameter search method based on dynamic programming and numerical computation is proposed to reduce the algorithm complexity from exponential to linear complexity. We have implemented the entire actual system and evaluated it with various workloads and application cases on the real-world AWS serverless computing platform. Experimental results on cross-region public cloud show that the proposed approach can improve the parameter search efficiency by more than 3× compared with the state-of-art parameter search methods and achieves better parameter quality. Compared with other competing cloud data transmission approaches, our approach is able to achieve higher time efficiency and lower monetary cost.},
	urldate = {2024-01-07},
	booktitle = {{IEEE} {INFOCOM} 2023 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Gu, Rong and Chen, Xiaofei and Dai, Haipeng and Wang, Shulin and Wang, Zhaokang and Tu, Yaofeng and Huang, Yihua and Chen, Guihai},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: IEEE INFOCOM 2023 - IEEE Conference on Computer Communications},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TXYANRG8\\10229090.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\9I6YV7UH\\10229090.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K2ZK3V3I\\10229090.html:text/html},
}

@inproceedings{andreoli_towards_2023,
	title = {Towards a {Holistic} {Cloud} {System} with {End}-to-{End} {Performance} {Guarantees}},
	isbn = {2694-0825},
	url = {https://ieeexplore.ieee.org/document/10305824},
	doi = {10.1109/IC2E59103.2023.00039},
	abstract = {Computing technologies are undergoing a relentless evolution from both the hardware and software sides, incorporating new mechanisms for low-latency networking, virtualization, operating systems, hardware acceleration, smart services orchestration, serverless computing, hybrid private-public Cloud solutions and others. Therefore, Cloud infrastructures are becoming increasingly attractive for deploying a wider and wider range of applications, including those with more and more stringent timing constraints, like the emerging use case of deploying time-critical applications. However, despite the availability of a number of public Cloud offerings, and of products (or open-source suites) for deploying in-house private Cloud infrastructures, still there are no solutions readily available for managing time-critical software components with predictable end-to-end timing requirements in the range of hundreds or even tens of milliseconds. The goal of this discussion is to present the multi-domain challenges associated with orchestrating a holistic Cloud system with end-to-end guarantees, which is the subject of my current PhD investigations.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Andreoli, Remo and Cucinotta, Tommaso},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {236--238},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\8DXRTPIX\\10305824.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GRVCJDNF\\10305824.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\J5SZ8JHQ\\10305824.html:text/html},
}

@article{kwak_toward_2023,
	title = {Toward {Sustainable} {Smart} {City}: {Lessons} {From} 20 {Years} of {Korean} {Programs}},
	volume = {70},
	issn = {1558-0040},
	shorttitle = {Toward {Sustainable} {Smart} {City}},
	url = {https://ieeexplore.ieee.org/document/9381515},
	doi = {10.1109/TEM.2021.3060956},
	abstract = {South Korea has a long history of the planning, development, and management of smart cities to integrate emerging technological advances into complex physical infrastructure. This article explores lessons learned from smart city programs in South Korea to better understand the challenges and opportunities of future sustainable smart city innovation and development. This article conducted a comprehensive review and analysis of South Korea's smart city programs and conceptualized a sustainable smart city framework that will assist policymakers, planners, citizens, and other key stakeholders. This research proposed Governance, Policy, and Services (GPS) as the three pillars of a successful smart city framework, in addition to integrating physical and cyber infrastructures. This article argues that a smart city should function as a service platform that incubates and delivers long-term services to citizens and society. This article also emphasizes that a strong groundwork of the GPS framework will lead to the successful adaptation of innovative technologies and ideas for future smart city programs.},
	number = {2},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Engineering Management},
	author = {Kwak, Young Hoon and Lee, Jaehyun},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Engineering Management},
	pages = {740--754},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\VNAKHYJ7\\9381515.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\ZM9QSAHM\\9381515.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GIW2SP2I\\9381515.html:text/html},
}

@inproceedings{angelelli_towards_2023,
	title = {Towards a {Multi}-objective {Scheduling} {Policy} for {Serverless}-based {Edge}-{Cloud} {Continuum}},
	url = {https://ieeexplore.ieee.org/document/10171469},
	doi = {10.1109/CCGrid57682.2023.00052},
	abstract = {The cloud is extended towards the edge to form a computing continuum while managing resources' heterogeneity. The serverless technology simplified how to build cloud applications and use resources, becoming a driving force in consolidating the continuum with the deployment of small functions with short execution. However, the adaptation of serverless to the edge-cloud continuum brings new challenges mainly related to resource management and scheduling. Standard cloud scheduling policies are based on greedy algorithms that do not efficiently handle platforms' heterogeneity nor deal with problems such as cold start delays. This work introduces a new scheduling policy that tries to address these issues. It is based on multi-objective optimization for data transfers and makespan while considering heterogeneity. Using simulations that vary workloads, platforms, and heterogeneity levels, we study the system utilization, the trade-offs between the targets, and the impacts of considering platforms' heterogeneity. We perform comparisons with a baseline inspired by a Kubernetes-based policy, representing greedy algorithms. Our experiments show considerable gaps between the efficiency of a greedy-based scheduling policy and a multi-objective-based one. The last outperforms the baseline by reducing makespan, data transfers, and system utilization by up to two orders of magnitudes in relevant cases for the edge-cloud continuum.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE}/{ACM} 23rd {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Angelelli, Luc and da Silva, Anderson Andrei and Georgiou, Yiannis and Mercier, Michael and Mounié, Gregory and Trystram, Denis},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	keywords = {Serverless Computing, Serverless computing, Edge clouds, Data transfer, Economic and social effects, Edge-cloud continuum, Edge-Cloud Continuum, Greedy algorithms, Heterogeneous platforms, Heterogeneous Platforms, Makespan, Managing resources, Multi-objective scheduling, Multiobjective optimization, Scheduling policies, Scheduling Policies, Simulation platform, System utilization},
	pages = {485--497},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\TEX5WNNQ\\10171469.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\SREVMWS5\\10171469.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\7IPF3E4I\\10171469.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\PWEITZRT\\Angelelli et al. - 2023 - Towards a Multi-objective Scheduling Policy for Se.pdf:application/pdf},
}

@inproceedings{cordingly_towards_2023,
	title = {Towards {Serverless} {Sky} {Computing}: {An} {Investigation} on {Global} {Workload} {Distribution} to {Mitigate} {Carbon} {Intensity}, {Network} {Latency}, and {Cost}},
	isbn = {2694-0825},
	shorttitle = {Towards {Serverless} {Sky} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10305816},
	doi = {10.1109/IC2E59103.2023.00015},
	abstract = {The high demand for energy consumption and the resulting carbon footprint of the cloud pose significant sustainability challenges, as cloud data centers consume vast amounts of energy. The emergence of serverless cloud computing platforms has opened up new avenues for more sustainable cloud computing. Serverless Function-as-a-Service (FaaS) cloud computing platforms facilitate deploying applications as decoupled microservices to leverage automatic rapid scaling, high availability, fault tolerance, and on-demand pricing. The absence of always-on hosting costs associated with virtual machines enables serverless functions to be deployed with many different function configurations and cloud regions to achieve high performance, low network latency, and reduced costs. In this paper, we investigate the utility of a global sky computing platform where serverless resources are aggregated between up to 19 distinct cloud regions. We prototype a serverless load distribution system to distribute client requests across serverless aggregations to minimize performance objectives, including network latency, runtime, hosting costs, and carbon footprint. To evaluate our serverless distribution system's ability to meet performance objectives, we continuously executed large experiments across 19 regions around the world from November 2022 through March 2023. Our serverless load distribution approach using aggregated resources reduced the carbon intensity of a globally distributed serverless application by up to 99.8\%, network latency by 65\%, or hosting costs by 58\% by optimizing function routing to deployments with optimal hardware configurations.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Cordingly, Robert and Kaur, Jasleen and Dwivedi, Divyansh and Lloyd, Wes},
	month = sep,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Cloud Engineering (IC2E)},
	pages = {59--69},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\23MNCVHY\\10305816.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\P8UHQFIR\\10305816.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\MJKPZRHS\\10305816.html:text/html},
}

@inproceedings{hluchy_transformation_2023,
	title = {Transformation of a {Legacy} {Airport} {Meteorology} {Application} into a {Serverless} {Cloud} {Application}},
	isbn = {2765-818X},
	url = {https://ieeexplore.ieee.org/document/10158660},
	doi = {10.1109/SACI58269.2023.10158660},
	abstract = {In this article we describe our continuing work on transforming a legacy application dealing with airport meteorology into a cloud application, using the concepts of serverless computing and Function-as-a-Service. In our previous work [1] we have described the architecture and concepts of our solution, and initial experiments with several FaaS tools. Here we continue the experiments, concentrating on the Apache OpenWhisk framework, which we have selected as our target serverless environment. We have further developed some of the application’s components and tried to execute them via OpenWhisk. During these experiments we have found multiple practical problems of transforming and executing legacy applications as serverless functions in OpenWhisk, and have devised solutions for these problems.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} 17th {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics} ({SACI})},
	author = {Hluchý, Ladislav and Habala, Ondrej and Bobák, Martin and Šeleng, Martin},
	month = may,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE 17th International Symposium on Applied Computational Intelligence and Informatics (SACI)},
	keywords = {Cloud computing, cloud computing, serverless computing, Serverless computing, Function-as-a-Service, Function-as-a-service, Cloud-computing, Cloud applications, Airports, Apache openwhisk, Apache OpenWhisk, Application components, legacy application, Legacy applications, Meteorology, Practical problems},
	pages = {000637--000642},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\A2GSTA85\\10158660.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\S5KAJBCP\\10158660.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\XEG7S59M\\10158660.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WBFAK9XP\\10158660.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\3HVM5N63\\10158660.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WP99PHLX\\10158660.html:text/html},
}

@article{eklund_using_2023,
	title = {Using a {Digital} {Twin} as the {Objective} {Function} for {Evolutionary} {Algorithm} {Applications} in {Large} {Scale} {Industrial} {Processes}},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10064259},
	doi = {10.1109/ACCESS.2023.3254896},
	abstract = {In this paper, we describe how the up-to-date state of a digital twin, and its corresponding simulation model, can be used as a fitness function of an evolutionary algorithm for optimizing a large-scale industrial process. An ICT architecture is presented for solving the computational challenges that arise when the fitness function evaluation takes considerable amount of time. Parallel computation of the fitness function in a cloud computing environment is proposed and the evolutionary algorithm is connected to the computational environment using the Function-as-a-Service approach. A case-study was conducted on the district heating network of Espoo, the second largest city in Finland. The study shows that the architecture is suited for optimizing the operating costs of the large district heating network, with over 800 km of water pipes and over 14 heat producers, reaching a cost-saving of an average of 2\%, and up-to 4\%, over the current industrial state-of-the-art method in use at the city of Espoo.},
	urldate = {2024-01-07},
	journal = {IEEE Access},
	author = {Eklund, Miro and Sierla, Seppo A. and Niemistö, Hannu and Korvola, Timo and Savolainen, Jouni and Karhela, Tommi A.},
	year = {2023},
	note = {Conference Name: IEEE Access},
	pages = {24185--24202},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\SIZIDAV7\\M. Eklund et al. - 2023 - Using a Digital Twin as the Objective Function for.pdf:application/pdf;Full Text:C\:\\Users\\brian\\Zotero\\storage\\MVYG6GAW\\Eklund et al. - 2023 - Using a Digital Twin as the Objective Function for.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\BA9HHSU4\\10064259.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\G7NJ35W8\\10064259.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\GJG4BRLC\\10064259.html:text/html},
}

@inproceedings{jin_when_2023,
	title = {When {Edge} {Meets} {FaaS}: {Opportunities} and {Challenges}},
	isbn = {2767-9918},
	shorttitle = {When {Edge} {Meets} {FaaS}},
	url = {https://ieeexplore.ieee.org/document/10234282},
	doi = {10.1109/EDGE60047.2023.00016},
	abstract = {The proliferation of edge devices and the rapid growth of IoT data have called forth the edge computing paradigm. Function-as-a-service (FaaS) is a promising computing paradigm to realize edge computing. This paper explores the feasibility and advantages of FaaS-based edge computing. It also studies the research challenges that should be addressed in the design of such systems, which are 1) the quick decomposing and recomposing of applications, 2) the trade-off between performance and isolation of sandbox mechanisms, and 3) distributed scheduling. The challenges are illustrated by evaluating existing FaaS-based edge platforms, AWS IoT Greengrass, and OpenFaaS.},
	urldate = {2024-01-07},
	booktitle = {2023 {IEEE} {International} {Conference} on {Edge} {Computing} and {Communications} ({EDGE})},
	author = {Jin, Runyu and Yang, Qirui and Zhao, Ming},
	month = jul,
	year = {2023},
	note = {Journal Abbreviation: 2023 IEEE International Conference on Edge Computing and Communications (EDGE)},
	pages = {23--25},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\PQAUDW8Y\\10234282.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\WNM37Q6H\\10234282.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6WS2FG6L\\10234282.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\B97CCP2X\\Jin et al. - 2023 - When Edge Meets FaaS Opportunities and Challenges.pdf:application/pdf},
}

@article{ristov_xafclxafcl_2023,
	title = {{xAFCLxAFCL}: {Run} {Scalable} {Function} {Choreographies} {Across} {Multiple} {FaaS} {Systems}},
	volume = {16},
	issn = {1939-1374},
	shorttitle = {{xAFCLxAFCL}},
	url = {https://ieeexplore.ieee.org/document/9616383/algorithms?tabFilter=dataset#algorithms},
	doi = {10.1109/TSC.2021.3128137},
	abstract = {Most well-known cloud providers offer advanced support for serverless applications that goes beyond single function invocation by enabling developers to build entire workflows, which are known as serverless function choreographies (FCs). Current support for FCs by many FaaS systems uncovered important problems including maximum number of parallel function executions, unexpected considerable delays, and provider lock-in. These limitations can result in longer execution times or even failure to execute individual functions or entire FCs. To overcome some of these limitations, we introduce a scalable middleware service xAFCLxAFCL that can schedule and execute different functions of the same FC across multiple FaaS systems (currently supporting all top five providers). In order to support scheduling under xAFCLxAFCL, we introduce a novel FaaS model which estimates the completion time of functions by considering FaaS system limitations, submission delays, and overheads for executing functions. Experimental results demonstrate that xAFCLxAFCL’s FaaS model shows very low inaccuracy of up to 2.9\%2.9\% for AWS and 20\%20\% for IBM for real-life BWA data-bound FC that uses S3. Moreover, xAFCLxAFCL outperforms an earliest start time (EST) scheduler by up to 43\%43\% for makespan and 2.7{\textbackslash}times2.7× for throughput.},
	number = {1},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Services Computing},
	author = {Ristov, Sasko and Pedratscher, Stefan and Fahringer, Thomas},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Services Computing},
	pages = {711--723},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\X5F56TCH\\algorithms.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FLFY3LMG\\algorithms.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\T2CYHYJW\\algorithms.html:text/html},
}

@article{xie_workflow_2023,
	title = {Workflow {Scheduling} in {Serverless} {Edge} {Computing} for the {Industrial} {Internet} of {Things}: {A} {Learning} {Approach}},
	volume = {19},
	issn = {1941-0050},
	shorttitle = {Workflow {Scheduling} in {Serverless} {Edge} {Computing} for the {Industrial} {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/9931467},
	doi = {10.1109/TII.2022.3217477},
	abstract = {Serverless edge computing is seen as a promising enabler to execute differentiated Industrial Internet of Things (IIoT) applications without managing the underlying servers and clusters. In IIoT serverless edge computing, IIoT workflow scheduling for cloud-edge collaborative processing is closely related to the service quality of users. However, serverless functions decomposed by IIoT applications are limited in their deployment at the edge due to the resource-constrained nature of edge infrastructures. In addition, the scheduling of complex IIoT applications supported by serverless computing is more challenging. Therefore, considering the limited function deployment and the complex dependencies of serverless workflows, we model the workflow application as directed acyclic graph and formulate the scheduling problem as a multiobjective optimization problem. A dueling double deep Q-network-based solution is proposed to make scheduling decisions under dynamically changing systems. Extensive simulation experiments are conducted to validate the superiority of the proposed scheme.},
	number = {7},
	urldate = {2024-01-07},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Xie, Renchao and Gu, Dier and Tang, Qinqin and Huang, Tao and Yu, Fei Richard},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	pages = {8242--8252},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\6VYDZQHU\\9931467.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\2FNI8ZPZ\\9931467.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\FLUV7YY4\\9931467.html:text/html},
}

@inproceedings{agache_firecracker_2020,
	address = {USA},
	series = {{NSDI}'20},
	title = {Firecracker: lightweight virtualization for serverless applications},
	isbn = {978-1-939133-13-7},
	shorttitle = {Firecracker},
	url = {https://dl.acm.org/doi/10.5555/3388242.3388273},
	doi = {10.5555/3388242.3388273},
	abstract = {Serverless containers and functions are widely used for deploying and managing software in the cloud. Their popularity is due to reduced cost of operations, improved utilization of hardware, and faster scaling than traditional deployment methods. The economics and scale of serverless applications demand that workloads from multiple customers run on the same hardware with minimal overhead, while preserving strong security and performance isolation. The traditional view is that there is a choice between virtualization with strong security and high overhead, and container technologies with weaker security and minimal overhead. This tradeoff is unacceptable to public infrastructure providers, who need both strong security and minimal overhead. To meet this need, we developed Firecracker, a new open source Virtual Machine Monitor (VMM) specialized for serverless workloads, but generally useful for containers, functions and other compute workloads within a reasonable set of constraints. We have deployed Firecracker in two publically-available serverless compute services at Amazon Web Services (Lambda and Fargate), where it supports millions of production workloads, and trillions of requests per month. We describe how specializing for serverless informed the design of Firecracker, and what we learned from seamlessly migrating Lambda customers to Firecracker.},
	language = {en},
	urldate = {2024-01-14},
	booktitle = {Proceedings of the 17th {Usenix} {Conference} on {Networked} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Agache, Alexandru and Brooker, Marc and Florescu, Andreea and Iordache, Alexandra and Liguori, Anthony and Neugebauer, Rolf and Piwonka, Phil and Popa, Diana-Maria},
	year = {2020},
	pages = {419--434},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\IX2D4KEF\\Agache et al. - 2020 - Firecracker Lightweight Virtualization for Server.pdf:application/pdf},
}

@inproceedings{aslanpour_serverless_2021,
	address = {New York, NY, USA},
	series = {{ACSW} '21},
	title = {Serverless {Edge} {Computing}: {Vision} and {Challenges}},
	isbn = {978-1-4503-8956-3},
	shorttitle = {Serverless {Edge} {Computing}},
	url = {https://doi.org/10.1145/3437378.3444367},
	doi = {10.1145/3437378.3444367},
	abstract = {Born from a need for a pure “pay-per-use” model and highly scalable platform, the “Serverless” paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things (IoT) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and IoT applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 2021 {Australasian} {Computer} {Science} {Week} {Multiconference}},
	publisher = {Association for Computing Machinery},
	author = {Aslanpour, Mohammad S. and Toosi, Adel N. and Cicconetti, Claudio and Javadi, Bahman and Sbarski, Peter and Taibi, Davide and Assuncao, Marcos and Gill, Sukhpal Singh and Gaire, Raj and Dustdar, Schahram},
	month = feb,
	year = {2021},
	pages = {1--10},
	file = {Full Text:C\:\\Users\\brian\\Zotero\\storage\\ACC8N7CR\\Aslanpour et al. - 2021 - Serverless Edge Computing Vision and Challenges.pdf:application/pdf},
}

@misc{javed_serverless_2021,
	title = {Serverless {Platforms} on the {Edge}: {A} {Performance} {Analysis}},
	shorttitle = {Serverless {Platforms} on the {Edge}},
	url = {http://arxiv.org/abs/2111.06563},
	doi = {10.48550/arXiv.2111.06563},
	abstract = {The exponential growth of Internet of Things (IoT) has given rise to a new wave of edge computing due to the need to process data on the edge, closer to where it is being produced and attempting to move away from a cloud-centric architecture. This provides its own opportunity to decrease latency and address data privacy concerns along with the ability to reduce public cloud costs. The serverless computing model provides a potential solution with its event-driven architecture to reduce the need for ever-running servers and convert the backend services to an as-used model. This model is an attractive prospect in edge computing environments with varying workloads and limited resources. Furthermore, its setup on the edge of the network promises reduced latency to the edge devices communicating with it and eliminates the need to manage the underlying infrastructure. In this book chapter, first, we introduce the novel concept of serverless edge computing, then, we analyze the performance of multiple serverless platforms, namely, OpenFaaS, AWS Greengrass, Apache OpenWhisk, when set up on the single-board computers (SBCs) on the edge and compare it with public cloud serverless offerings, namely, AWS Lambda and Azure Functions, to deduce the suitability of serverless architectures on the network edge. These serverless platforms are set up on a cluster of Raspberry Pis and we evaluate their performance by simulating different types of edge workloads. The evaluation results show that OpenFaaS achieves the lowest response time on the SBC edge computing infrastructure while serverless cloud offerings are the most reliable with the highest success rate.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Javed, Hamza and Toosi, Adel N. and Aslanpour, Mohammad S.},
	month = nov,
	year = {2021},
	note = {arXiv:2111.06563 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\brian\\Zotero\\storage\\NTLYL2MG\\Javed et al. - 2021 - Serverless Platforms on the Edge A Performance An.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\brian\\Zotero\\storage\\LHV7T33B\\2111.html:text/html},
}

@inproceedings{nguyen_real-time_2019,
	address = {New York, NY, USA},
	series = {{WOSC} '19},
	title = {Real-time {Serverless}: {Enabling} {Application} {Performance} {Guarantees}},
	isbn = {978-1-4503-7038-7},
	shorttitle = {Real-time {Serverless}},
	url = {https://dl.acm.org/doi/10.1145/3366623.3368133},
	doi = {10.1145/3366623.3368133},
	abstract = {Today's serverless provides "function-as-a-service" with dynamic scaling and fine-grained resource charging, enabling new cloud applications. Serverless functions are invoked as a best-effort service. We propose an extension to serverless, called real-time serverless that provides an invocation rate guarantee, a service-level objective (SLO) specified by the application, and delivered by the underlying implementation. Real-time serverless allows applications to guarantee real-time performance. We study real-time serverless behavior analytically and empirically to characterize its ability to support bursty, real-time cloud and edge applications efficiently. Finally, we use a case study, traffic monitoring, to illustrate the use and benefits of real-time serverless, on our prototype implementation.},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Serverless} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Hai Duc and Zhang, Chaojie and Xiao, Zhujun and Chien, Andrew A.},
	month = dec,
	year = {2019},
	keywords = {Serverless, Bursty, Interface, Real-time},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\WENWQX8F\\Nguyen et al. - 2019 - Real-time Serverless Enabling Application Perform.pdf:application/pdf},
}

@article{aslanpour_autoscalesim_2021,
	title = {{AutoScaleSim}: {A} simulation toolkit for auto-scaling {Web} applications in clouds},
	volume = {108},
	issn = {1569-190X},
	shorttitle = {{AutoScaleSim}},
	url = {https://www.sciencedirect.com/science/article/pii/S1569190X20301738},
	doi = {10.1016/j.simpat.2020.102245},
	abstract = {Auto-scaling of Web applications is an extensively investigated issue in cloud computing. To evaluate auto-scaling mechanisms, the cloud community is facing considerable challenges on either real cloud platforms or custom test-beds. Challenges include – but not limited to – deployment impediments, the complexity of setting parameters, and most importantly, the cost of hosting and testing Web applications on a massive scale. Hence, simulation is presently one of the most popular evaluation solutions to overcome these obstacles. Existing simulators, however, fail to provide support for hosting, deploying and subsequently auto-scaling of Web applications. In this paper, we introduce AutoScaleSim, which extends the existing CloudSim simulator, to support auto-scaling of Web applications in cloud environments in a customizable, extendable and scalable manner. Using AutoScaleSim, the cloud community can freely implement/evaluate policies for all four phases of auto-scaling mechanisms, that is, Monitoring, Analysis, Planning and Execution. AutoScaleSim can also be used for evaluating load balancing algorithms similarly. We conducted a set of experiments to validate and carefully evaluate the performance of AutoScaleSim in a real cloud platform, with a wide range of performance metrics.},
	urldate = {2024-01-17},
	journal = {Simulation Modelling Practice and Theory},
	author = {Aslanpour, Mohammad S. and Toosi, Adel N. and Taheri, Javid and Gaire, Raj},
	month = apr,
	year = {2021},
	keywords = {Cloud computing, Simulation, Auto-scaling, Elasticity, Resource provisioning, Web application},
	pages = {102245},
	file = {ScienceDirect Snapshot:C\:\\Users\\brian\\Zotero\\storage\\GTCZELH2\\S1569190X20301738.html:text/html},
}

@article{hassan_survey_2021,
	title = {Survey on serverless computing},
	volume = {10},
	issn = {2192-113X},
	url = {https://doi.org/10.1186/s13677-021-00253-7},
	doi = {10.1186/s13677-021-00253-7},
	abstract = {Serverless computing has gained importance over the last decade as an exciting new field, owing to its large influence in reducing costs, decreasing latency, improving scalability, and eliminating server-side management, to name a few. However, to date there is a lack of in-depth survey that would help developers and researchers better understand the significance of serverless computing in different contexts. Thus, it is essential to present research evidence that has been published in this area. In this systematic survey, 275 research papers that examined serverless computing from well-known literature databases were extensively reviewed to extract useful data. Then, the obtained data were analyzed to answer several research questions regarding state-of-the-art contributions of serverless computing, its concepts, its platforms, its usage, etc. We moreover discuss the challenges that serverless computing faces nowadays and how future research could enable its implementation and usage.},
	number = {1},
	urldate = {2024-01-17},
	journal = {Journal of Cloud Computing},
	author = {Hassan, Hassan B. and Barakat, Saman A. and Sarhan, Qusay I.},
	month = jul,
	year = {2021},
	keywords = {Cloud computing, Serverless computing, Serverless benefits, Serverless challenges, Serverless platforms, Survey},
	pages = {39},
	file = {Full Text PDF:C\:\\Users\\brian\\Zotero\\storage\\IT7W88JT\\Hassan et al. - 2021 - Survey on serverless computing.pdf:application/pdf;Snapshot:C\:\\Users\\brian\\Zotero\\storage\\42WAP82D\\s13677-021-00253-7.html:text/html},
}

@article{wen_rise_2023,
	title = {Rise of the {Planet} of {Serverless} {Computing}: {A} {Systematic} {Review}},
	volume = {32},
	issn = {1049-331X},
	shorttitle = {Rise of the {Planet} of {Serverless} {Computing}},
	url = {https://doi.org/10.1145/3579643},
	doi = {10.1145/3579643},
	abstract = {Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.},
	number = {5},
	urldate = {2024-01-17},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Jin, Xin and Liu, Xuanzhe},
	month = jul,
	year = {2023},
	keywords = {Serverless computing, literature view},
	pages = {131:1--131:61},
	file = {Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\46LT4HMW\\Wen et al. - 2023 - Rise of the Planet of Serverless Computing A Syst.pdf:application/pdf},
}

@inproceedings{wen_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://doi.org/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, and cloud providers can leverage these findings and implications to better understand and further explore the serverless computing paradigm.},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	keywords = {Serverless Computing, Application Development, Empirical Study, Stack Overflow},
	pages = {416--428},
}

@inproceedings{bhattacharjee_barista_2019,
	title = {{BARISTA}: {Efficient} and {Scalable} {Serverless} {Serving} {System} for {Deep} {Learning} {Prediction} {Services}},
	shorttitle = {{BARISTA}},
	url = {https://ieeexplore.ieee.org/document/8790088/keywords#keywords},
	doi = {10.1109/IC2E.2019.00-10},
	abstract = {Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.},
	urldate = {2024-02-05},
	booktitle = {2019 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Bhattacharjee, Anirban and Chhokra, Ajay Dev and Kang, Zhuangwei and Sun, Hongyang and Gokhale, Aniruddha and Karsai, Gabor},
	month = jun,
	year = {2019},
	keywords = {Computational modeling, Containers, Deep learning, Analytical models, Forecasting, Load modeling, Predictive models, Resource Management, Machine Learning Models, Predictive Analytics, Serverless Computing, Containers},
	pages = {23--33},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\brian\\Zotero\\storage\\K5BGNSCE\\keywords.html:text/html;Submitted Version:C\:\\Users\\brian\\Zotero\\storage\\M9PJSBEP\\Bhattacharjee et al. - 2019 - BARISTA Efficient and Scalable Serverless Serving.pdf:application/pdf},
}
